{
    "author": "vasqu",
    "message": ":rotating_light: [`v5`] Remove headmasking (#41076)\n\n* first attempt at removing\n\n* copies\n\n* last bits in core\n\n* quick fixes\n\n* tests purge\n\n* docs and examples\n\n* some fixes\n\n* more\n\n* another round of cleanups\n\n* fix\n\n* fix a bunch of models\n\n* fix dummy bert\n\n* fix\n\n* fix new model\n\n* fix signature change\n\n* fix\n\n* fix style/copies\n\n* new models\n\n* fix copies didnt find that damn\n\n* test\n\n* this shouldnt have happened during model addition",
    "sha": "52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
    "files": [
        {
            "sha": "a3ace15a9a3c77e42fc57b458bf1105a48558ef5",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -121,7 +121,6 @@ print(output)\n \n - Pad inputs on the right because BioGPT uses absolute position embeddings.\n - BioGPT can reuse previously computed key-value attention pairs. Access this feature with the [past_key_values](https://huggingface.co/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel.forward.past_key_values) parameter in [`BioGPTModel.forward`].\n-- The `head_mask` argument is ignored when using an attention implementation other than \"eager\". If you want to use `head_mask`, make sure `attn_implementation=\"eager\"`).\n \n    ```py\n    from transformers import AutoModelForCausalLM"
        },
        {
            "sha": "a3845f3c0ff6f0a6bb0505cc8c11617b12e769a5",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -53,7 +53,6 @@ The original code for vision can be found [here](https://github.com/facebookrese\n - For Data2VecAudio, preprocessing is identical to [`Wav2Vec2Model`], including feature extraction\n - For Data2VecText, preprocessing is identical to [`RobertaModel`], including tokenization.\n - For Data2VecVision, preprocessing is identical to [`BeitModel`], including feature extraction.\n-- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n \n ### Using Scaled Dot Product Attention (SDPA)\n "
        },
        {
            "sha": "fec23ad0f14c5dab28219d790c8d1610db12940f",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -49,9 +49,6 @@ The main differences compared to GPT2.\n \n You can read more about the optimizations in the [original pull request](https://github.com/huggingface/transformers/pull/22575)\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n ## Combining Starcoder and Flash Attention 2\n \n First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature."
        },
        {
            "sha": "6d25b998d08c27a542d0bc96abd5d3df7f5559dc",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -114,11 +114,6 @@ print(transcription[0])\n ## Notes\n \n - HuBERT models expect raw audio input as a 1D float array sampled at 16kHz.\n-- If you want to use a `head_mask`, use the model with `attn_implementation=\"eager\"`.\n-\n-  ```python\n-  model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\", attn_implementation=\"eager\")\n-  ```\n \n ## HubertConfig\n "
        },
        {
            "sha": "842ba115bc857fd5f5241a1ecac1a363899ae1bf",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -51,9 +51,6 @@ multilingual it expects the sequences in a certain format: A special language id\n source and target text. The source text format is `[lang_code] X [eos]`, where `lang_code` is source language\n id for source text and target language id for target text, with `X` being the source or target text.\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n The [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the\n examples. To install `sentencepiece` run `pip install sentencepiece`.\n "
        },
        {
            "sha": "93b74d7b31b83d57eaa8094f7218eb1f212a33eb",
            "filename": "docs/source/en/model_doc/mbart.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -34,9 +34,6 @@ You can find all the original mBART checkpoints under the [AI at Meta](https://h\n > [!TIP]\n > Click on the mBART models in the right sidebar for more examples of applying mBART to different language tasks.\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n The example below demonstrates how to translate text with [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">"
        },
        {
            "sha": "c7c5efbc6e0cdca9c5189271609d0f2460c813ed",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -63,9 +63,6 @@ python src/transformers/models/musicgen/convert_musicgen_transformers.py \\\n     --checkpoint small --pytorch_dump_folder /output/path --safe_serialization \n ```\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n ## Generation\n \n MusicGen is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly"
        },
        {
            "sha": "ff670ef852972d432655daf630e5e4e0c072fd61",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -43,9 +43,6 @@ There are two key differences with MusicGen:\n 1. The audio prompt is used here as a conditional signal for the generated audio sample, whereas it's used for audio continuation in [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen).\n 2. Conditional text and audio signals are concatenated to the decoder's hidden states instead of being used as a cross-attention signal, as in MusicGen.\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n ## Generation\n \n MusicGen Melody is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly better results than greedy, thus we encourage sampling mode to be used where possible. Sampling is enabled by default, and can be explicitly specified by setting `do_sample=True` in the call to [`MusicgenMelodyForConditionalGeneration.generate`], or by overriding the model's generation config (see below)."
        },
        {
            "sha": "4b321f2d680b71b7672db631706f1dbadc636b6d",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -101,8 +101,6 @@ tokenizer.batch_decode(generated_ids)[0]\n \n - OPT adds an `EOS` token `</s>` to the beginning of every prompt.\n \n-- The `head_mask` argument is ignored if the attention implementation isn't `\"eager\"`. Set `attn_implementation=\"eager\"` to enable the `head_mask`.\n-\n ## Resources\n \n - Refer to this [notebook](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing) for an example of fine-tuning OPT with PEFT, bitsandbytes, and Transformers."
        },
        {
            "sha": "ae4cdd815e919f9c2b82c9388b2016506e67951f",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -40,9 +40,6 @@ The abstract from the paper is the following:\n \n `Qwen2-Audio-7B` and `Qwen2-Audio-7B-Instruct` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n ### Inference\n \n ```python"
        },
        {
            "sha": "1f6bbfd219e2033312c64673a2ae26185cfdb098",
            "filename": "docs/source/en/model_doc/sew.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -47,9 +47,6 @@ This model was contributed by [anton-l](https://huggingface.co/anton-l).\n - SEWForCTC is fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded using\n   [`Wav2Vec2CTCTokenizer`].\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n-\n ## Resources\n \n - [Audio classification task guide](../tasks/audio_classification)"
        },
        {
            "sha": "04c758e2e061930ffaee6ec9ad36aeec45296b59",
            "filename": "docs/source/en/model_doc/unispeech-sat.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -55,8 +55,6 @@ found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech-SAT).\n   decoded using [`Wav2Vec2CTCTokenizer`].\n - UniSpeechSat performs especially well on speaker verification, speaker identification, and speaker diarization tasks.\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n \n ## Resources\n "
        },
        {
            "sha": "ef4062c524646a9bf6dbbbc2eca4e13488178ad4",
            "filename": "docs/source/en/model_doc/unispeech.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -50,8 +50,6 @@ found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech).\n - UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be\n   decoded using [`Wav2Vec2CTCTokenizer`].\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n \n ## Resources\n "
        },
        {
            "sha": "4db7bacc8c6a5c997222af5d99c4dd364f5e302c",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -48,8 +48,6 @@ Note: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingf\n - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\n   using [`Wav2Vec2CTCTokenizer`].\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n \n ## Using Flash Attention 2\n "
        },
        {
            "sha": "2c4ef6257eeb9487be400da457319926a6bd789e",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -29,8 +29,6 @@ rendered properly in your Markdown viewer.\n \n You can find all the original Whisper checkpoints under the [Whisper](https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013) collection.\n \n-> [!NOTE]\n-> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n \n > [!TIP]\n > Click on the Whisper models in the right sidebar for more examples of how to apply Whisper to different audio tasks."
        },
        {
            "sha": "874cf2084e9577ac69603fc697077e702484ee02",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -175,7 +175,7 @@ There are three supported implementations available.\n - [xFormers](https://github.com/facebookresearch/xformers) or Memory-Efficient Attention is able to support models with the fp32 torch type.\n - C++ implementation of scaled dot product attention\n \n-SDPA is used by default for PyTorch v2.1.1. and greater when an implementation is available. You could explicitly enable SDPA by setting `attn_implementation=\"sdpa\"` in [`~PreTrainedModel.from_pretrained`] though. Certain attention parameters, such as `head_mask` and `output_attentions=True`, are unsupported and returns a warning that Transformers will fall back to the (slower) eager implementation.\n+SDPA is used by default for PyTorch v2.1.1. and greater when an implementation is available. You could explicitly enable SDPA by setting `attn_implementation=\"sdpa\"` in [`~PreTrainedModel.from_pretrained`] though. Certain attention parameters, such as `output_attentions=True`, are unsupported and returns a warning that Transformers will fall back to the (slower) eager implementation.\n \n Refer to the [AttentionInterface](./attention_interface) guide to learn how to change the attention implementation after loading a model.\n "
        },
        {
            "sha": "07beed4620323edc48e388f60389a9c5c809ffd3",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 382,
            "deletions": 396,
            "changes": 778,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -4,24 +4,29 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_dummy_bert.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...cache_utils import Cache, EncoderDecoderCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import check_model_inputs\n from .configuration_dummy_bert import DummyBertConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -58,7 +63,7 @@ def forward(\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n@@ -68,9 +73,10 @@ def forward(\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -87,18 +93,74 @@ def forward(\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class DummyBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -113,215 +175,152 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-\n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only dummy_bert can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n                 )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in DummyBertModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+class DummyBertCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+        self.config = config\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        return context_layer, attention_probs\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-class DummyBertSdpaSelfAttention(DummyBertSelfAttention):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-    # Adapted from DummyBertSelfAttention\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n-            logger.warning_once(\n-                \"DummyBertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n-                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n-                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n-                '`attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                past_key_values,\n-                output_attentions,\n-                cache_position,\n-            )\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        query_layer = (\n-            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n         else:\n-            key_layer = (\n-                self.key(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n-            value_layer = (\n-                self.value(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n-        # a causal mask in case tgt_len == 1.\n-        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout_prob if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n-\n-        return attn_output, None\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class DummyBertSelfOutput(nn.Module):\n@@ -338,19 +337,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-DUMMY_BERT_SELF_ATTENTION_CLASSES = {\n-    \"eager\": DummyBertSelfAttention,\n-    \"sdpa\": DummyBertSdpaSelfAttention,\n-}\n-\n-\n class DummyBertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = DUMMY_BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = DummyBertCrossAttention if is_cross_attention else DummyBertSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = DummyBertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -373,29 +368,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            attention_mask=attention_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n class DummyBertIntermediate(nn.Module):\n@@ -432,38 +425,40 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = DummyBertAttention(config, layer_idx=layer_idx)\n+        self.attention = DummyBertAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = DummyBertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = DummyBertAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = DummyBertIntermediate(config)\n         self.output = DummyBertOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -472,24 +467,20 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -498,92 +489,36 @@ def feed_forward_chunk(self, attention_output):\n \n \n class DummyBertEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([DummyBertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n@@ -644,10 +579,18 @@ def forward(self, hidden_states):\n \n @auto_docstring\n class DummyBertPreTrainedModel(PreTrainedModel):\n-    config: DummyBertConfig\n+    config_class = DummyBertConfig\n     base_model_prefix = \"dummy_bert\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": DummyBertLayer,\n+        \"attentions\": DummyBertSelfAttention,\n+        \"cross_attentions\": DummyBertCrossAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -688,13 +631,13 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = DummyBertEmbeddings(config)\n         self.encoder = DummyBertEncoder(config)\n \n         self.pooler = DummyBertPooler(config) if add_pooling_layer else None\n \n-        self.attn_implementation = config._attn_implementation\n         self.position_embedding_type = config.position_embedding_type\n \n         # Initialize weights and apply final processing\n@@ -714,14 +657,14 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -731,46 +674,37 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n+        else:\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n+\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -780,86 +714,138 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n-\n-        use_sdpa_attention_masks = (\n-            self.attn_implementation == \"sdpa\"\n-            and self.position_embedding_type == \"absolute\"\n-            and head_mask is None\n-            and not output_attentions\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n-        # Expand the attention mask\n-        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n-            # Expand the attention mask for SDPA.\n-            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-            if self.config.is_decoder:\n-                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                    attention_mask,\n-                    input_shape,\n-                    embedding_output,\n-                    past_key_values_length,\n-                )\n-            else:\n-                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-        else:\n-            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-            # ourselves in which case we just need to make it broadcastable to all heads.\n-            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-\n-            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n-                # Expand the attention mask for SDPA.\n-                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-            else:\n-                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n+            attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n+\n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask"
        },
        {
            "sha": "427e8f8d1572b1ca0b24dbd03b9e875659f7968e",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 383,
            "deletions": 400,
            "changes": 783,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -4,24 +4,29 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_roberta.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...cache_utils import Cache, EncoderDecoderCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import check_model_inputs\n from .configuration_roberta import RobertaConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -61,7 +66,7 @@ def forward(\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n@@ -71,9 +76,10 @@ def forward(\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -90,18 +96,74 @@ def forward(\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class RobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -116,215 +178,152 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-\n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only roberta can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n                 )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+class RobertaCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+        self.config = config\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        return context_layer, attention_probs\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-class RobertaSdpaSelfAttention(RobertaSelfAttention):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-    # Adapted from RobertaSelfAttention\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n-            logger.warning_once(\n-                \"RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n-                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n-                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n-                '`attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                past_key_values,\n-                output_attentions,\n-                cache_position,\n-            )\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        query_layer = (\n-            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n         else:\n-            key_layer = (\n-                self.key(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n-            value_layer = (\n-                self.value(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n-        # a causal mask in case tgt_len == 1.\n-        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout_prob if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n-\n-        return attn_output, None\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class RobertaSelfOutput(nn.Module):\n@@ -341,19 +340,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-ROBERTA_SELF_ATTENTION_CLASSES = {\n-    \"eager\": RobertaSelfAttention,\n-    \"sdpa\": RobertaSdpaSelfAttention,\n-}\n-\n-\n class RobertaAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = RobertaCrossAttention if is_cross_attention else RobertaSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = RobertaSelfOutput(config)\n         self.pruned_heads = set()\n@@ -376,29 +371,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            attention_mask=attention_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n class RobertaIntermediate(nn.Module):\n@@ -435,38 +428,40 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RobertaAttention(config, layer_idx=layer_idx)\n+        self.attention = RobertaAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = RobertaAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = RobertaIntermediate(config)\n         self.output = RobertaOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -475,24 +470,20 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -501,92 +492,36 @@ def feed_forward_chunk(self, attention_output):\n \n \n class RobertaEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([RobertaLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n@@ -647,10 +582,18 @@ def forward(self, hidden_states):\n \n @auto_docstring\n class RobertaPreTrainedModel(PreTrainedModel):\n-    config: RobertaConfig\n+    config_class = RobertaConfig\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": RobertaLayer,\n+        \"attentions\": RobertaSelfAttention,\n+        \"cross_attentions\": RobertaCrossAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -691,13 +634,13 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = RobertaEmbeddings(config)\n         self.encoder = RobertaEncoder(config)\n \n         self.pooler = RobertaPooler(config) if add_pooling_layer else None\n \n-        self.attn_implementation = config._attn_implementation\n         self.position_embedding_type = config.position_embedding_type\n \n         # Initialize weights and apply final processing\n@@ -717,63 +660,51 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n+        else:\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n+\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -783,86 +714,138 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n-\n-        use_sdpa_attention_masks = (\n-            self.attn_implementation == \"sdpa\"\n-            and self.position_embedding_type == \"absolute\"\n-            and head_mask is None\n-            and not output_attentions\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n-        # Expand the attention mask\n-        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n-            # Expand the attention mask for SDPA.\n-            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-            if self.config.is_decoder:\n-                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                    attention_mask,\n-                    input_shape,\n-                    embedding_output,\n-                    past_key_values_length,\n-                )\n-            else:\n-                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-        else:\n-            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-            # ourselves in which case we just need to make it broadcastable to all heads.\n-            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-\n-            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n-                # Expand the attention mask for SDPA.\n-                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-            else:\n-                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n+            attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n+\n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask"
        },
        {
            "sha": "592508752843585f06cf0cbdb45e5dee22196297",
            "filename": "examples/modular-transformers/modular_dummy_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -5,6 +5,8 @@\n from transformers.models.bert.modeling_bert import BertModel\n \n from ...modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs\n \n \n class DummyBertModel(BertModel):\n@@ -14,7 +16,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -24,5 +25,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        return super().forward(input_ids)\n+        return super().forward(input_ids, **kwargs)"
        },
        {
            "sha": "c5592114fc5c835b381cf29da232040ac6d12576",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -23,9 +23,9 @@ def flash_attention_forward(\n     softcap: Optional[float] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n-    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\") is not None:\n+    if kwargs.get(\"output_attentions\", False):\n         logger.warning_once(\n-            \"`flash_attention_2` does not support `output_attentions=True` or `head_mask`.\"\n+            \"`flash_attention_2` does not support `output_attentions=True`.\"\n             \" Please set your attention to `eager` if you want any of these features.\"\n         )\n "
        },
        {
            "sha": "2ccbad24261c726de789a41ee7f18925133a0d5e",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -240,15 +240,9 @@ def flex_attention_forward(\n     attention_mask: Union[torch.Tensor, \"BlockMask\"],\n     scaling: Optional[float] = None,\n     softcap: Optional[float] = None,\n-    head_mask: Optional[torch.Tensor] = None,\n     s_aux: Optional[torch.Tensor] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-    if head_mask is not None:\n-        logger.warning_once(\n-            \"`flex_attention` does not support `head_mask`. Please set your attention to `eager` if you want this feature.\"\n-        )\n-\n     if kwargs.get(\"dropout\", 0.0) > 0:\n         raise ValueError(\n             \"`flex_attention` does not support `dropout`. Please use it with inference\"\n@@ -270,8 +264,6 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n             score = softcap * torch.tanh(score / softcap)\n         if score_mask is not None:\n             score = score + score_mask[batch_idx][0][q_idx][kv_idx]\n-        if head_mask is not None:\n-            score = score + head_mask[batch_idx][head_idx][0][0]\n         # Note: attention sinks cannot be correctly implemented in score_mod\n         # because it requires operating on the full attention matrix before softmax.\n         # ==> this is done after flex attention"
        },
        {
            "sha": "301243b3fbfdae739f6a2ea74b50555fcb7eb7de",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -51,9 +51,9 @@ def sdpa_attention_forward(\n     is_causal: Optional[bool] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n-    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\") is not None:\n+    if kwargs.get(\"output_attentions\", False):\n         logger.warning_once(\n-            \"`sdpa` attention does not support `output_attentions=True` or `head_mask`.\"\n+            \"`sdpa` attention does not support `output_attentions=True`.\"\n             \" Please set your attention to `eager` if you want any of these features.\"\n         )\n     sdpa_kwargs = {}"
        },
        {
            "sha": "1c57072a0c72264e77d9fa786a653ce3524e9238",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -1587,44 +1587,6 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n         return extended_attention_mask\n \n-    def get_head_mask(\n-        self, head_mask: Optional[Tensor], num_hidden_layers: int, is_attention_chunked: bool = False\n-    ) -> Tensor:\n-        \"\"\"\n-        Prepare the head mask if needed.\n-\n-        Args:\n-            head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n-                The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n-            num_hidden_layers (`int`):\n-                The number of hidden layers in the model.\n-            is_attention_chunked (`bool`, *optional*, defaults to `False`):\n-                Whether or not the attentions scores are computed by chunks or not.\n-\n-        Returns:\n-            `torch.Tensor` with shape `[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or list with\n-            `[None]` for each layer.\n-        \"\"\"\n-        if head_mask is not None:\n-            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n-            if is_attention_chunked is True:\n-                head_mask = head_mask.unsqueeze(-1)\n-        else:\n-            head_mask = [None] * num_hidden_layers\n-\n-        return head_mask\n-\n-    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):\n-        \"\"\"-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\"\"\"\n-        if head_mask.dim() == 1:\n-            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n-            head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n-        elif head_mask.dim() == 2:\n-            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n-        assert head_mask.dim() == 5, f\"head_mask.dim != 5, instead {head_mask.dim()}\"\n-        head_mask = head_mask.to(dtype=self.dtype)  # switch to float if need + fp16 compatibility\n-        return head_mask\n-\n     def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n         \"\"\"\n         Get number of (optionally, trainable or non-embeddings) parameters in the module."
        },
        {
            "sha": "a1dfa5e2fc9c307b7c4e740c82b7764e8f69c8ff",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 35,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -125,7 +125,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -166,9 +165,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -231,7 +227,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -259,7 +254,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=False,\n             **kwargs,\n@@ -291,10 +285,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n-        attention_output, _ = self.attention(hidden_states, attention_mask, head_mask, **kwargs)\n+        attention_output, _ = self.attention(hidden_states, attention_mask, **kwargs)\n \n         ffn_output = apply_chunking_to_forward(\n             self.ff_chunk,\n@@ -322,11 +315,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n         for layer_index, albert_layer in enumerate(self.albert_layers):\n-            hidden_states = albert_layer(hidden_states, attention_mask, head_mask[layer_index], **kwargs)\n+            hidden_states = albert_layer(hidden_states, attention_mask, **kwargs)\n         return hidden_states\n \n \n@@ -342,24 +334,17 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[BaseModelOutput, tuple]:\n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n \n-        head_mask = [None] * self.config.num_hidden_layers if head_mask is None else head_mask\n-\n         for i in range(self.config.num_hidden_layers):\n-            # Number of layers in a hidden group\n-            layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n-\n             # Index of the hidden group\n             group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n \n             hidden_states = self.albert_layer_groups[group_idx](\n                 hidden_states,\n                 attention_mask,\n-                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n                 **kwargs,\n             )\n \n@@ -480,7 +465,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[BaseModelOutputWithPooling, tuple]:\n@@ -493,12 +477,9 @@ def forward(\n \n         attention_mask = self._update_full_mask(attention_mask, embedding_output)\n \n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask,\n-            head_mask=head_mask,\n             position_ids=position_ids,\n             **kwargs,\n         )\n@@ -522,8 +503,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -572,7 +551,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         sentence_order_label: Optional[torch.LongTensor] = None,\n@@ -609,7 +587,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -710,7 +687,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -755,7 +731,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -804,7 +779,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -820,7 +794,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -888,7 +861,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -902,7 +874,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -946,7 +917,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -957,7 +927,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1016,7 +985,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1067,7 +1035,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "f55c84b471760e80c02edb7237b08a3d3fa3e766",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -576,7 +576,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -587,9 +586,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n@@ -621,7 +617,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -644,7 +639,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -697,14 +691,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -757,14 +749,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -796,7 +786,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -809,12 +798,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 **kwargs,\n             )\n@@ -914,7 +900,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -970,13 +955,6 @@ def forward(\n         # ourselves in which case we just need to make it broadcastable to all heads.\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -986,7 +964,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n@@ -1130,7 +1107,6 @@ def get_text_features(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1156,7 +1132,6 @@ def get_text_features(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n         )\n         last_hidden_state = text_outputs[0][:, 0, :]\n@@ -1202,7 +1177,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1253,7 +1227,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "f40caa7af4cee189c73b98db701941742ddd9914",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -225,7 +225,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -267,10 +266,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -333,13 +328,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -392,14 +385,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -432,7 +423,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -445,12 +435,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 **kwargs,\n             )\n@@ -1028,7 +1015,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1068,9 +1054,6 @@ def forward(\n         # ourselves in which case we just need to make it broadcastable to all heads.\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -1080,7 +1063,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n@@ -1123,7 +1105,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -1154,7 +1135,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "bc9415b70e1f57d18fdcb609ece30fb4215a3c3d",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 24,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -149,9 +149,7 @@ def __init__(self, config: ASTConfig):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size = hidden_states.shape[0]\n         new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n \n@@ -168,7 +166,7 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n+            None,\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.dropout_prob,\n@@ -224,8 +222,8 @@ def prune_heads(self, heads: set[int]):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n-        self_attn_output, _ = self.attention(hidden_states, head_mask)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n         output = self.output(self_attn_output, hidden_states)\n         return output\n \n@@ -274,9 +272,9 @@ def __init__(self, config: ASTConfig):\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         hidden_states_norm = self.layernorm_before(hidden_states)\n-        attention_output = self.attention(hidden_states_norm, head_mask)\n+        attention_output = self.attention(hidden_states_norm)\n \n         # first residual connection\n         hidden_states = attention_output + hidden_states\n@@ -299,10 +297,9 @@ def __init__(self, config: ASTConfig):\n         self.layer = nn.ModuleList([ASTLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> BaseModelOutput:\n+    def forward(self, hidden_states: torch.Tensor) -> BaseModelOutput:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            hidden_states = layer_module(hidden_states, layer_head_mask)\n+            hidden_states = layer_module(hidden_states)\n \n         return BaseModelOutput(last_hidden_state=hidden_states)\n \n@@ -371,7 +368,6 @@ class PreTrainedModel\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -388,16 +384,9 @@ def forward(\n         if input_values is None:\n             raise ValueError(\"You have to specify input_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(input_values)\n \n-        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, head_mask=head_mask)\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output)\n         sequence_output = encoder_outputs.last_hidden_state\n         sequence_output = self.layernorm(sequence_output)\n \n@@ -442,7 +431,6 @@ def __init__(self, config: ASTConfig) -> None:\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> SequenceClassifierOutput:\n@@ -459,9 +447,7 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        outputs: BaseModelOutputWithPooling = self.audio_spectrogram_transformer(\n-            input_values, head_mask=head_mask, **kwargs\n-        )\n+        outputs: BaseModelOutputWithPooling = self.audio_spectrogram_transformer(input_values, **kwargs)\n \n         pooled_output = outputs.pooler_output\n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "9e583b0b818742816846a612675ff1428af37afc",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -453,7 +453,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -541,15 +540,6 @@ def forward(\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, channel)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, channel)\n-\n         if output_attentions:\n             # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n@@ -652,16 +642,13 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -670,7 +657,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -755,8 +741,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -771,10 +755,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -790,7 +770,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -809,7 +788,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -876,8 +854,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -922,7 +898,6 @@ def __init__(self, config: AutoformerConfig):\n     def forward(\n         self,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -937,12 +912,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -976,14 +945,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != (len(self.layers)):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n-\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -1000,7 +961,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -1056,8 +1016,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1088,19 +1046,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1176,15 +1121,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != (len(self.layers)):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n-\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1199,8 +1135,6 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1428,9 +1362,6 @@ def forward(\n         future_values: Optional[torch.Tensor] = None,\n         future_time_features: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1499,11 +1430,6 @@ def forward(\n             Transformer requires to provide additional features.\n \n             The Autoformer only learns additional embeddings for `static_categorical_features`.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n@@ -1563,7 +1489,6 @@ def forward(\n             )\n             encoder_outputs = self.encoder(\n                 inputs_embeds=enc_input,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -1612,8 +1537,6 @@ def forward(\n                 inputs_embeds=decoder_input,\n                 attention_mask=decoder_attention_mask,\n                 encoder_hidden_states=encoder_outputs[0],\n-                head_mask=decoder_head_mask,\n-                cross_attn_head_mask=cross_attn_head_mask,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1696,9 +1619,6 @@ def forward(\n         future_time_features: Optional[torch.Tensor] = None,\n         future_observed_mask: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1774,11 +1694,6 @@ def forward(\n             - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n \n             This mask is used to filter out missing values for the final loss calculation.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n@@ -1882,9 +1797,6 @@ def forward(\n             future_values=future_values,\n             future_time_features=future_time_features,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             past_key_values=past_key_values,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "e4e4cb5cdea22ddb38170d95498f66565191ef27",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 22,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -118,7 +118,7 @@ def _merge_heads(self, tensor, num_heads, attn_head_size):\n \n         return tensor\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _attn(self, query, key, value, attention_mask=None):\n         # unlike GPTNeo's SelfAttention, divide by the square root of the dimension of the query and the key\n         attn_weights = torch.matmul(query, key.transpose(-1, -2)) * (1.0 / math.sqrt(self.head_dim))\n \n@@ -139,10 +139,6 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         attn_weights = attn_weights.to(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         # (batch, num_heads, seq_len, seq_len) x (batch, num_heads, seq_len, attn_head_size)\n         # -> (batch, num_heads, seq_len, attn_head_size)\n         attn_output = torch.matmul(attn_weights, value)\n@@ -154,7 +150,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         past_key_values=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -169,7 +164,7 @@ def forward(\n         if past_key_values is not None:\n             key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n \n-        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+        attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n         attn_output = self.out_proj(attn_output)\n@@ -217,7 +212,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         past_key_values=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -298,7 +292,6 @@ def forward(\n         hidden_states,\n         past_key_values=None,\n         attention_mask=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -309,7 +302,6 @@ def forward(\n             intermediary_hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -438,7 +430,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         input_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -525,12 +516,6 @@ def forward(\n                 # from_seq_length is 1 to easily broadcast\n                 attention_mask = _prepare_4d_attention_mask(attention_mask, input_embeds.dtype, tgt_len=1)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x num_heads x N x N\n-        # head_mask has shape num_layers x batch x num_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n-\n         hidden_states = self.drop(input_embeds + position_embeds)\n         output_shape = input_shape + (hidden_states.size(-1),)\n \n@@ -545,7 +530,6 @@ def forward(\n                 hidden_states,\n                 past_key_values=past_key_values,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -1071,7 +1055,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         input_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1143,8 +1126,6 @@ def forward(\n                 # from_seq_length is 1 to easily broadcast\n                 attention_mask = _prepare_4d_attention_mask(attention_mask, input_embeds.dtype, tgt_len=1)\n \n-        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n-\n         hidden_states = self.drop(input_embeds + position_embeds)\n         output_shape = input_shape + (hidden_states.size(-1),)\n \n@@ -1158,7 +1139,6 @@ def forward(\n             outputs = block(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask[i],\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "720e562b38173137be37f1b85463fa2b1972673c",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 121,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -124,7 +124,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -136,9 +135,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -195,7 +191,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -264,7 +259,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -298,16 +292,13 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -316,7 +307,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -384,8 +374,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -400,10 +388,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -419,7 +403,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -436,7 +419,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -533,8 +515,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -690,8 +670,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -773,7 +751,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -796,12 +773,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -850,14 +821,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != (len(self.layers)):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n-\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -874,7 +837,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -935,8 +897,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -973,19 +933,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1101,15 +1048,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != (len(self.layers)):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n-\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1124,8 +1062,6 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1204,9 +1140,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1239,12 +1172,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n             information on the default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         # different to other models, Bart automatically creates decoder_input_ids from\n         # input_ids if no decoder_input_ids are provided\n@@ -1271,7 +1198,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1291,8 +1217,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1370,9 +1294,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1406,12 +1327,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n             information on the default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1475,9 +1390,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1546,9 +1458,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1581,12 +1490,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n             information on the default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n@@ -1605,9 +1508,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1691,9 +1591,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1727,12 +1624,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n             information on the default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if start_positions is not None and end_positions is not None:\n@@ -1743,9 +1634,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1853,8 +1741,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1865,11 +1751,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1904,8 +1785,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "0728600795d2d6305522616c9969d90a48dc6df0",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 32,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -257,7 +257,6 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n@@ -304,10 +303,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -323,22 +318,20 @@ class BeitSdpaSelfAttention(BeitSelfAttention):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n+        if output_attentions:\n             logger.warning_once(\n                 \"`BeitSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, \"\n+                \"support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                 \"but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n                 'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states=hidden_states,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 relative_position_bias=relative_position_bias,\n                 interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -445,14 +438,13 @@ def prune_heads(self, heads):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_outputs = self.attention(\n-            hidden_states, head_mask, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution\n+            hidden_states, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution\n         )\n \n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -514,15 +506,13 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None, drop\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int, int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in BEiT, layernorm is applied before self-attention\n-            head_mask,\n             output_attentions=output_attentions,\n             relative_position_bias=relative_position_bias,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -663,7 +653,6 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         interpolate_pos_encoding: bool = False,\n@@ -686,11 +675,8 @@ def forward(\n             else:\n                 relative_position_bias = None\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 relative_position_bias=relative_position_bias,\n                 interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -788,7 +774,6 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n@@ -804,19 +789,11 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n         resolution = pixel_values.shape[2:]\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             resolution=resolution,\n@@ -888,7 +865,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -932,7 +908,6 @@ def forward(\n         outputs = self.beit(\n             pixel_values,\n             bool_masked_pos=bool_masked_pos,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -983,7 +958,6 @@ def __init__(self, config: BeitConfig) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -999,7 +973,6 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         outputs = self.beit(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -1318,7 +1291,6 @@ def compute_loss(self, logits, auxiliary_logits, labels):\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1358,7 +1330,6 @@ def forward(\n \n         outputs = self.beit(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=True,  # we need the intermediate hidden states\n             interpolate_pos_encoding=interpolate_pos_encoding,"
        },
        {
            "sha": "1689da04aa52e9559c2cf4aa846b2a94e53e6178",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -126,7 +126,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -167,9 +166,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -211,7 +207,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -255,7 +250,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -299,7 +293,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -347,7 +340,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -405,7 +397,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -417,7 +408,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -480,7 +470,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -490,7 +479,6 @@ def forward(\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -507,7 +495,6 @@ def forward(\n             cross_attention_output, _ = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -536,7 +523,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -545,12 +531,9 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -764,7 +747,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -821,17 +803,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -912,8 +886,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -938,8 +910,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -995,7 +965,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         next_sentence_label: Optional[torch.Tensor] = None,\n@@ -1034,7 +1003,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1094,7 +1062,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1118,7 +1085,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1180,7 +1146,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1198,7 +1163,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1269,7 +1233,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1314,7 +1277,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1367,7 +1329,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1383,7 +1344,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1448,7 +1408,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1500,7 +1459,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1549,7 +1507,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1563,7 +1520,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1607,7 +1563,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1618,7 +1573,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "12aee8a014b3f17b439210e8c5254e5e7626396c",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -70,7 +70,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -111,9 +110,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -156,7 +152,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -200,7 +195,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -245,7 +239,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -293,7 +286,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -338,7 +330,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -350,7 +341,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -416,7 +406,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -426,7 +415,6 @@ def forward(\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -443,7 +431,6 @@ def forward(\n             cross_attention_output, _ = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -474,7 +461,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -483,12 +469,9 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -624,7 +607,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -680,17 +662,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -770,8 +744,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -796,8 +768,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -874,7 +844,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -915,7 +884,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,"
        },
        {
            "sha": "f774c61c596476e9969c76f4c255ba5064d8770e",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -158,7 +158,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -214,10 +213,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -1180,7 +1175,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -1204,7 +1198,6 @@ def forward(\n             self_outputs = self.self(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_values=past_key_values,\n@@ -1290,7 +1283,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         band_mask=None,\n@@ -1305,7 +1297,6 @@ def forward(\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -1330,7 +1321,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n@@ -1378,7 +1368,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -1417,12 +1406,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 band_mask,\n@@ -1680,7 +1666,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1804,13 +1789,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -1822,7 +1800,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             past_key_values=past_key_values,\n@@ -1965,7 +1942,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.FloatTensor] = None,\n         next_sentence_label: Optional[torch.LongTensor] = None,\n@@ -2008,7 +1984,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -2073,7 +2048,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -2136,7 +2110,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -2214,7 +2187,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -2240,7 +2212,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -2327,7 +2298,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2382,7 +2352,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -2446,7 +2415,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2501,7 +2469,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -2554,7 +2521,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2572,7 +2538,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -2646,7 +2611,6 @@ def forward(\n         question_lengths: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -2718,7 +2682,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "9ad5b5772b7d1a7deea0fe0d42f5ecf802b18f77",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -135,7 +135,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -191,10 +190,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -1144,22 +1139,17 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         band_mask=None,\n         from_mask=None,\n         to_mask=None,\n         from_blocked_mask=None,\n         to_blocked_mask=None,\n     ):\n-        # Expand dims to enable multiplication in the self-attention module\n-        head_mask = head_mask.reshape(1, -1, 1, 1) if head_mask is not None else None\n-\n         if self.attention_type == \"original_full\":\n             self_outputs = self.self(\n                 hidden_states,\n                 attention_mask,\n-                head_mask,\n                 output_attentions=output_attentions,\n             )\n         else:\n@@ -1181,7 +1171,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -1193,9 +1182,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -1253,7 +1239,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -1322,7 +1307,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -1350,7 +1334,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_head_mask: torch.Tensor,\n         band_mask=None,\n         from_mask=None,\n         to_mask=None,\n@@ -1373,7 +1356,6 @@ def forward(\n         self_attention_outputs = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             band_mask=band_mask,\n             from_mask=from_mask,\n@@ -1458,8 +1440,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -1474,10 +1454,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1494,7 +1470,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -1511,7 +1486,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n@@ -1741,8 +1715,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -1811,7 +1783,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1922,14 +1893,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n-\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -1946,7 +1909,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     band_mask=band_mask,\n                     from_mask=from_mask,\n                     to_mask=to_mask,\n@@ -2094,8 +2056,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -2132,19 +2092,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -2257,14 +2204,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != len(self.layers):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -2279,8 +2218,6 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -2357,9 +2294,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2381,11 +2315,6 @@ def forward(\n             If you want to change padding behavior, you should read\n             [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n             [the paper](https://huggingface.co/papers/1910.13461) for more information on the default strategy.\n-        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         # different to other models, BigBirdPegasus automatically creates decoder_input_ids from\n         # input_ids if no decoder_input_ids are provided\n@@ -2412,7 +2341,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -2432,8 +2360,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -2513,9 +2439,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2538,11 +2461,6 @@ def forward(\n             If you want to change padding behavior, you should read\n             [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n             [the paper](https://huggingface.co/papers/1910.13461) for more information on the default strategy.\n-        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -2589,9 +2507,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -2660,9 +2575,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2684,11 +2596,6 @@ def forward(\n             If you want to change padding behavior, you should read\n             [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n             [the paper](https://huggingface.co/papers/1910.13461) for more information on the default strategy.\n-        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n@@ -2707,9 +2614,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -2793,9 +2697,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -2818,11 +2719,6 @@ def forward(\n             If you want to change padding behavior, you should read\n             [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n             [the paper](https://huggingface.co/papers/1910.13461) for more information on the default strategy.\n-        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if start_positions is not None and end_positions is not None:\n@@ -2833,9 +2729,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -2939,8 +2832,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2951,11 +2842,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -2989,8 +2875,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "1ff6eddea256936fe23e9afa4778d65528e95976",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -101,7 +101,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -113,9 +112,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -172,7 +168,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -241,7 +236,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -280,7 +274,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -293,8 +286,6 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -315,7 +306,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             position_ids=position_ids,\n             cache_position=cache_position,\n@@ -515,7 +505,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n@@ -623,7 +612,6 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -686,7 +674,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -709,7 +696,6 @@ def forward(\n         outputs = self.biogpt(\n             input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n@@ -769,7 +755,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -792,7 +777,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             position_ids=position_ids,\n@@ -861,7 +845,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -884,7 +867,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             position_ids=position_ids,"
        },
        {
            "sha": "ad04a4ef5b82656934a3adfba5385e79e266b2fb",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -102,7 +102,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -115,8 +114,6 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -137,7 +134,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             position_ids=position_ids,\n             cache_position=cache_position,\n@@ -337,7 +333,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n@@ -445,7 +440,6 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -508,7 +502,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -531,7 +524,6 @@ def forward(\n         outputs = self.biogpt(\n             input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n@@ -591,7 +583,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -614,7 +605,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             position_ids=position_ids,\n@@ -683,7 +673,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -706,7 +695,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             position_ids=position_ids,"
        },
        {
            "sha": "86522323005f6780a87ebc2abb2ed91eae0cd39a",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -120,7 +120,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -132,9 +131,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -192,7 +188,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -261,7 +256,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -295,16 +289,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_head_mask: torch.Tensor,\n         output_attentions: bool = False,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -314,7 +305,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -375,8 +365,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -391,10 +379,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -411,7 +395,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -428,7 +411,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n@@ -498,8 +480,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -657,8 +637,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -724,7 +702,6 @@ def forward(\n         self,\n         input_ids=None,\n         attention_mask=None,\n-        head_mask=None,\n         inputs_embeds=None,\n         output_attentions=None,\n         output_hidden_states=None,\n@@ -747,12 +724,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -800,13 +771,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -823,7 +787,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -888,8 +851,6 @@ def forward(\n         attention_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        head_mask=None,\n-        cross_attn_head_mask=None,\n         past_key_values=None,\n         inputs_embeds=None,\n         use_cache=None,\n@@ -926,20 +887,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0,\n-                1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1054,14 +1001,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != len(self.layers):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1076,8 +1015,6 @@ def forward(\n                 causal_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1160,9 +1097,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1188,12 +1122,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n \n         Example:\n \n@@ -1222,7 +1150,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1242,8 +1169,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1329,9 +1254,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1358,12 +1280,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1418,9 +1334,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1503,8 +1416,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1515,11 +1426,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1554,8 +1460,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "0536146d8463097e30aeea3478b32765eb1934e5",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 95,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -104,7 +104,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -116,9 +115,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -176,7 +172,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -245,7 +240,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -280,16 +274,13 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -298,7 +289,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -367,8 +357,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -383,10 +371,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -402,7 +386,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -419,7 +402,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -491,8 +473,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -650,8 +630,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -715,7 +693,6 @@ def forward(\n         self,\n         input_ids=None,\n         attention_mask=None,\n-        head_mask=None,\n         inputs_embeds=None,\n         output_attentions=None,\n         output_hidden_states=None,\n@@ -738,12 +715,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -792,13 +763,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -815,7 +779,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -875,8 +838,6 @@ def forward(\n         attention_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        head_mask=None,\n-        cross_attn_head_mask=None,\n         past_key_values=None,\n         inputs_embeds=None,\n         use_cache=None,\n@@ -913,19 +874,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1042,14 +990,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != len(self.layers):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1064,8 +1004,6 @@ def forward(\n                 causal_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1132,9 +1070,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1160,12 +1095,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n \n         Example:\n \n@@ -1194,7 +1123,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1214,8 +1142,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1288,9 +1214,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1317,12 +1240,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1377,9 +1294,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1462,8 +1376,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1474,11 +1386,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1513,8 +1420,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "aa87b37b069d08919b9994e6bdb8a1f6dc3052ab",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -327,7 +327,6 @@ def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -353,10 +352,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n \n         new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim,)\n@@ -396,15 +391,13 @@ def __init__(self, config: BlipConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n-            head_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + residual\n@@ -475,14 +468,12 @@ def __init__(self, config: BlipConfig):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "18d4bdaab7212fc66c6b6a1a4b317c8fc2f10b59",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -137,7 +137,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -227,10 +226,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs_dropped = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs_dropped = attention_probs_dropped * head_mask\n-\n         context_layer = torch.matmul(attention_probs_dropped, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -286,7 +281,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n@@ -295,7 +289,6 @@ def forward(\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n@@ -357,7 +350,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -367,7 +359,6 @@ def forward(\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             past_key_values=past_key_values,\n             cache_position=cache_position,\n@@ -379,7 +370,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n@@ -410,7 +400,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -453,12 +442,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_values,\n@@ -689,7 +675,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n@@ -785,13 +770,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         if encoder_embeds is None:\n             embedding_output = self.embeddings(\n                 input_ids=input_ids,\n@@ -805,7 +783,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             past_key_values=past_key_values,\n@@ -860,7 +837,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -905,7 +881,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,"
        },
        {
            "sha": "cb4e36b3730800293b61fc8705630bcbec43fdb6",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -303,7 +303,6 @@ def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -369,15 +368,13 @@ def __init__(self, config: Blip2Config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n-            head_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + residual\n@@ -460,14 +457,12 @@ def __init__(self, config: Blip2Config):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n \n@@ -578,7 +573,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -636,10 +630,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs_dropped = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs_dropped = attention_probs_dropped * head_mask\n-\n         context_layer = torch.matmul(attention_probs_dropped, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -696,15 +686,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         attn_output, _ = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             **kwargs,\n@@ -770,7 +758,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         query_length=0,\n@@ -779,7 +766,6 @@ def forward(\n         attention_output = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -792,7 +778,6 @@ def forward(\n                 query_attention_output = self.crossattention(\n                     hidden_states=query_attention_output,\n                     attention_mask=attention_mask,\n-                    head_mask=head_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     **kwargs,\n@@ -847,20 +832,17 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         query_length=0,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n \n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 query_length=query_length,\n@@ -1014,7 +996,6 @@ def forward(\n         query_embeds: torch.FloatTensor,\n         query_length: Optional[int] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1070,17 +1051,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs: BaseModelOutput = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             query_length=query_length,"
        },
        {
            "sha": "8ba3d46496e48f024e140ce386f4ab134fe2bef1",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -263,7 +263,6 @@ def forward(\n         alibi: torch.Tensor,\n         attention_mask: torch.Tensor,\n         layer_past: Optional[Cache] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -302,9 +301,6 @@ def forward(\n         # [batch_size, num_heads, q_length, kv_length]\n         attention_probs = self.attention_dropout(attention_probs)\n \n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         # change view [batch_size x num_heads, q_length, kv_length]\n         attention_probs_reshaped = attention_probs.view(batch_size * self.num_heads, q_length, -1)\n \n@@ -382,7 +378,6 @@ def forward(\n         alibi: torch.Tensor,\n         attention_mask: torch.Tensor,\n         layer_past: Optional[Cache] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -405,7 +400,6 @@ def forward(\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n             alibi=alibi,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -491,7 +485,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -551,11 +544,6 @@ def forward(\n         if cache_position is None:\n             cache_position = torch.arange(past_length, past_length + seq_length, device=inputs_embeds.device)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape batch_size x num_heads x N x N\n-        # head_mask has shape n_layer x batch x num_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n         hidden_states = self.word_embeddings_layernorm(inputs_embeds)\n \n         all_self_attentions = () if output_attentions else None\n@@ -580,7 +568,6 @@ def forward(\n                 hidden_states,\n                 layer_past=past_key_values,\n                 attention_mask=causal_mask,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 alibi=alibi,\n@@ -829,7 +816,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -874,7 +860,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -941,7 +926,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -983,7 +967,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1077,7 +1060,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1119,7 +1101,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1169,7 +1150,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1196,7 +1176,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "896ee175c7b1b2c301972c0e149f7e4b92a804bd",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -415,7 +415,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -456,9 +455,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -501,7 +497,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -545,7 +540,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -590,7 +584,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -638,7 +631,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -683,7 +675,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -695,7 +686,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -727,15 +717,13 @@ def forward(\n         hidden_states,\n         encoder_hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_attention_mask=None,\n         past_key_value=None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         self_attention_output, self_attn_weights = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=None,\n             past_key_value=None,\n             **kwargs,\n         )\n@@ -744,7 +732,6 @@ def forward(\n         cross_attention_output, cross_attn_weights = self.crossattention(\n             attention_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n@@ -793,7 +780,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -804,7 +790,6 @@ def forward(\n         self_attention_output, self_attn_weights = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -821,7 +806,6 @@ def forward(\n             cross_attention_output, cross_attn_weights = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -857,7 +841,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -872,12 +855,9 @@ def forward(\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n \n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -1120,7 +1100,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1191,17 +1170,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -1288,8 +1259,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -1314,8 +1283,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -1414,7 +1381,6 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_embeds: Optional[torch.FloatTensor] = None,\n         image_token_type_idx: Optional[int] = None,\n@@ -1724,7 +1690,6 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1773,7 +1738,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             pixel_values=pixel_values,\n             pixel_mask=pixel_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             image_embeds=image_embeds,\n             output_attentions=output_attentions,\n@@ -1826,7 +1790,6 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1872,7 +1835,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             pixel_values=pixel_values,\n             pixel_mask=pixel_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             image_embeds=image_embeds,\n             output_attentions=output_attentions,\n@@ -1940,7 +1902,6 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1993,7 +1954,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             pixel_values=pixel_values,\n             pixel_mask=pixel_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             image_embeds=image_embeds,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "517ff8b9b87a8f9a48951f3d2fe6206dbdfae984",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -209,7 +209,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[torch.Tensor] = False,\n@@ -270,10 +269,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -335,7 +330,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -344,7 +338,6 @@ def forward(\n             hidden_states=hidden_states,\n             bbox_pos_emb=bbox_pos_emb,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             output_attentions=output_attentions,\n@@ -404,7 +397,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -413,7 +405,6 @@ def forward(\n             hidden_states,\n             bbox_pos_emb=bbox_pos_emb,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -433,7 +424,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -473,7 +463,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -488,13 +477,10 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 bbox_pos_emb=bbox_pos_emb,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -631,7 +617,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -709,13 +694,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -733,7 +711,6 @@ def forward(\n             embedding_output,\n             bbox_pos_emb=bbox_position_embeddings,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             output_attentions=output_attentions,\n@@ -779,7 +756,6 @@ def forward(\n         bbox_first_token_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -822,7 +798,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -900,7 +875,6 @@ def forward(\n         bbox_first_token_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         initial_token_labels: Optional[torch.Tensor] = None,\n         subsequent_token_labels: Optional[torch.Tensor] = None,\n@@ -948,7 +922,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1039,7 +1012,6 @@ def forward(\n         bbox_first_token_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1081,7 +1053,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "e5e361c9b7bb24c7e489ccc85586a1dd86fe7b39",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -65,7 +65,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -106,9 +105,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -150,7 +146,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -194,7 +189,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -238,7 +232,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -286,7 +279,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -344,7 +336,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -356,7 +347,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -419,7 +409,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -429,7 +418,6 @@ def forward(\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -446,7 +434,6 @@ def forward(\n             cross_attention_output, _ = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -645,7 +632,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -654,12 +640,9 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -744,7 +727,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -801,17 +783,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -891,8 +865,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -916,8 +888,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -973,7 +943,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1000,7 +969,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1073,7 +1041,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1098,7 +1065,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1160,7 +1126,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n@@ -1212,7 +1177,6 @@ def forward(\n             position_ids=flat_position_ids,\n             token_type_ids=flat_token_type_ids,\n             attention_mask=flat_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1262,7 +1226,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1285,7 +1248,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1331,7 +1293,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1353,7 +1314,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1426,7 +1386,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1475,7 +1434,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,"
        },
        {
            "sha": "2676a28fce8384220587cfbae7699c5f81c58c70",
            "filename": "src/transformers/models/camembert/modular_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -66,7 +66,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -93,7 +92,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -133,7 +131,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -158,7 +155,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -215,7 +211,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n@@ -267,7 +262,6 @@ def forward(\n             position_ids=flat_position_ids,\n             token_type_ids=flat_token_type_ids,\n             attention_mask=flat_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -308,7 +302,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -331,7 +324,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -372,7 +364,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -394,7 +385,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -448,7 +438,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -497,7 +486,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,"
        },
        {
            "sha": "e4ed912dd6b8a0a5342ed7fd66217ecdf6a980f2",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 31,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -310,7 +310,6 @@ def forward(\n         from_tensor: torch.Tensor,\n         to_tensor: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         batch_size, seq_length, _ = from_tensor.shape\n@@ -373,10 +372,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -476,11 +471,10 @@ def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         if not self.local:\n-            self_outputs = self.self(hidden_states, hidden_states, attention_mask, head_mask, output_attentions)\n+            self_outputs = self.self(hidden_states, hidden_states, attention_mask, output_attentions)\n             attention_output = self_outputs[0]\n         else:\n             from_seq_length = to_seq_length = hidden_states.shape[1]\n@@ -530,7 +524,7 @@ def forward(\n                     to_tensor_chunk = torch.cat([cls_position, to_tensor_chunk], dim=1)\n \n                 attention_outputs_chunk = self.self(\n-                    from_tensor_chunk, to_tensor_chunk, attention_mask_chunk, head_mask, output_attentions\n+                    from_tensor_chunk, to_tensor_chunk, attention_mask_chunk, output_attentions\n                 )\n                 attention_output_chunks.append(attention_outputs_chunk[0])\n                 if output_attentions:\n@@ -608,13 +602,11 @@ def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -669,7 +661,6 @@ def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -681,9 +672,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:\n@@ -907,7 +896,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -949,13 +937,6 @@ def forward(\n             molecule_attention_mask, (batch_size, molecule_attention_mask.shape[-1])\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         # `input_char_embeddings`: shape (batch_size, char_seq, char_dim)\n         input_char_embeddings = self.char_embeddings(\n             input_ids=input_ids,\n@@ -999,7 +980,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             init_molecule_encoding,\n             attention_mask=extended_molecule_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -1085,7 +1065,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1105,7 +1084,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1170,7 +1148,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1225,7 +1202,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1275,7 +1251,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1323,7 +1298,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1371,7 +1345,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1386,7 +1359,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "9872b397b318431c22a85acbf1506541d4b8e0c3",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -243,7 +243,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -254,9 +253,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n@@ -289,7 +285,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -312,7 +307,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -366,14 +360,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -498,14 +490,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -651,7 +641,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -664,12 +653,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 **kwargs,\n             )\n@@ -868,7 +854,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -921,7 +906,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,"
        },
        {
            "sha": "885286ea3f4930d56e7fda3e28aed05983985e95",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 4,
            "deletions": 42,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -386,7 +386,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         batch_size, dim, num_channels = hidden_states.shape\n@@ -425,10 +424,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n@@ -483,10 +478,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n+        self_outputs = self.self(hidden_states, attention_mask, output_attentions)\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n         return outputs\n@@ -583,7 +577,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -616,9 +609,7 @@ def forward(\n             height_pad, width_pad, dtype=hidden_states.dtype, device=hidden_states_windows.device\n         )\n \n-        attention_outputs = self.attention(\n-            hidden_states_windows, attn_mask, head_mask, output_attentions=output_attentions\n-        )\n+        attention_outputs = self.attention(hidden_states_windows, attn_mask, output_attentions=output_attentions)\n \n         attention_output = attention_outputs[0]\n \n@@ -679,17 +670,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         height, width = input_dimensions\n         for i, layer_module in enumerate(self.blocks):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n-                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-            )\n+            layer_outputs = layer_module(hidden_states, input_dimensions, output_attentions, always_partition)\n \n             hidden_states = layer_outputs[0]\n \n@@ -844,7 +830,6 @@ def forward(\n         self,\n         input_features,\n         is_longer: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         output_hidden_states_before_downsampling: Optional[bool] = False,\n@@ -881,13 +866,9 @@ def forward(\n             all_reshaped_hidden_states += (reshaped_hidden_state,)\n \n         for i, layer_module in enumerate(self.layers):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             input_dimensions = self.input_resolutions[i]\n \n-            layer_outputs = layer_module(\n-                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-            )\n+            layer_outputs = layer_module(hidden_states, input_dimensions, output_attentions, always_partition)\n \n             hidden_states = layer_outputs[0]\n \n@@ -1095,7 +1076,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -1106,9 +1086,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n@@ -1141,7 +1118,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -1164,7 +1140,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -1218,14 +1193,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -1279,14 +1252,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -1319,7 +1290,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -1332,12 +1302,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 **kwargs,\n             )\n@@ -1508,7 +1475,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1548,9 +1514,6 @@ def forward(\n         # ourselves in which case we just need to make it broadcastable to all heads.\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -1560,7 +1523,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,"
        },
        {
            "sha": "3598b29f42e835602d0366421f8874168a86cb40",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -307,7 +307,6 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[tuple[torch.FloatTensor]]]:\n@@ -366,10 +365,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n         attn_output = torch.matmul(attn_probs, value_states)\n \n@@ -615,7 +610,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -627,7 +621,6 @@ def forward(\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -1027,7 +1020,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1094,12 +1086,6 @@ def forward(\n             attention_mask, input_shape, inputs_embeds, past_key_values_length\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x num_attention_heads x N x N\n-        # head_mask has shape num_hidden_layers x batch x num_attention_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         hidden_states = inputs_embeds\n \n         if token_type_ids is not None:\n@@ -1124,7 +1110,6 @@ def forward(\n                     None,\n                     attention_mask,\n                     position_ids,\n-                    head_mask[i],\n                     cache_position,\n                 )\n             else:\n@@ -1133,7 +1118,6 @@ def forward(\n                     past_key_values=past_key_values,\n                     attention_mask=attention_mask,\n                     position_ids=position_ids,\n-                    head_mask=head_mask[i],\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n                     cache_position=cache_position,\n@@ -1193,7 +1177,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1215,7 +1198,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n@@ -1364,7 +1346,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1393,7 +1374,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "a9a82b5e7738700a0ad4b03d484b22f8c46d629a",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -120,7 +120,6 @@ def _attn(\n         key,\n         value,\n         attention_mask=None,\n-        head_mask=None,\n     ):\n         # Keep the attention weights computation in fp32 to avoid overflow issues\n         query = query.to(torch.float32)\n@@ -137,10 +136,6 @@ def _attn(\n         attn_weights = attn_weights.to(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n \n         return attn_output, attn_weights\n@@ -151,7 +146,6 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -211,7 +205,7 @@ def forward(\n             key, value = layer_past.update(key.to(hidden_states.dtype), value, self.layer_idx, cache_kwargs)\n \n         # compute self-attention: V x Softmax(QK^T)\n-        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+        attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n \n         attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)\n         attn_output = self.out_proj(attn_output)\n@@ -255,7 +249,6 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -267,7 +260,6 @@ def forward(\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -338,7 +330,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -388,11 +379,6 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x num_attention_heads x N x N\n-        # head_mask has shape n_layer x batch x num_attention_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n         hidden_states = inputs_embeds\n \n         if token_type_ids is not None:\n@@ -414,7 +400,6 @@ def forward(\n                 layer_past=past_key_values,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -593,7 +578,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -621,7 +605,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "2dc138f82d14e6ccd76c44b099b0c1346e80f115",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -197,7 +197,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -262,10 +261,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n \n@@ -325,14 +320,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, Optional[torch.FloatTensor]]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             encoder_hidden_states,\n             output_attentions,\n         )\n@@ -421,15 +414,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, Optional[torch.FloatTensor]]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -444,7 +435,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 encoder_attention_mask,\n-                head_mask,\n                 encoder_hidden_states,\n                 output_attentions,\n             )\n@@ -474,7 +464,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -488,12 +477,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 output_attentions,\n@@ -673,7 +659,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -709,7 +694,6 @@ def forward(\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n         extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n         hidden_states = self.embeddings(\n             input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n@@ -721,7 +705,6 @@ def forward(\n         hidden_states = self.encoder(\n             hidden_states,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -775,7 +758,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -795,7 +777,6 @@ def forward(\n             attention_mask,\n             token_type_ids,\n             position_ids,\n-            head_mask,\n             inputs_embeds,\n             output_attentions,\n             output_hidden_states,\n@@ -872,7 +853,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -892,7 +872,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -956,7 +935,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1012,7 +990,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1065,7 +1042,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1083,7 +1059,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1131,7 +1106,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1146,7 +1120,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "2415cfdf4b75032a4880668f1e530361abd7ce63",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -57,7 +57,7 @@ def positional_encoding(position, d_model_size, dtype):\n     return pos_encoding\n \n \n-def scaled_dot_product_attention(q, k, v, mask, attention_mask=None, head_mask=None):\n+def scaled_dot_product_attention(q, k, v, mask, attention_mask=None):\n     # calculate attention\n     matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2))\n \n@@ -74,10 +74,6 @@ def scaled_dot_product_attention(q, k, v, mask, attention_mask=None, head_mask=N\n \n     attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n \n-    # Mask heads if we want to\n-    if head_mask is not None:\n-        attention_weights = attention_weights * head_mask\n-\n     output = torch.matmul(attention_weights, v)\n \n     return output, attention_weights\n@@ -128,7 +124,6 @@ def forward(\n         mask,\n         layer_past=None,\n         attention_mask=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -146,7 +141,7 @@ def forward(\n         if layer_past is not None:\n             k, v = layer_past.update(k, v, self.layer_idx, {\"cache_position\": cache_position})\n \n-        output = scaled_dot_product_attention(q, k, v, mask, attention_mask, head_mask)\n+        output = scaled_dot_product_attention(q, k, v, mask, attention_mask)\n         scaled_attention = output[0].permute([0, 2, 1, 3])\n         attn = output[1]\n         original_size_attention = scaled_attention.reshape(batch_size, -1, self.d_model_size)\n@@ -177,7 +172,6 @@ def forward(\n         mask,\n         layer_past=None,\n         attention_mask=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -190,7 +184,6 @@ def forward(\n             mask,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -273,7 +266,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -371,9 +363,6 @@ def forward(\n             attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n             attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n \n-        # Prepare head mask if needed\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n-\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n             token_type_embeds = self.w(token_type_ids)\n@@ -407,7 +396,6 @@ def forward(\n                 mask,\n                 layer_past=past_key_values,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -458,7 +446,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -518,7 +505,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -610,7 +596,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -714,7 +699,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "3107b68847787e4f7c5428e97a30d106919c57b3",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -184,7 +184,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -196,9 +195,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -245,7 +241,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n@@ -284,7 +279,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -433,8 +427,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":"
        },
        {
            "sha": "6ea41b626fff5b79deb347fdcae9df355e1b8983",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -171,7 +171,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -212,9 +211,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -256,7 +252,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -300,7 +295,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -344,7 +338,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -392,7 +385,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -450,7 +442,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -462,7 +453,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -525,7 +515,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -535,7 +524,6 @@ def forward(\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -552,7 +540,6 @@ def forward(\n             cross_attention_output, _ = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -616,7 +603,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -625,12 +611,9 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -704,7 +687,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -761,17 +743,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -851,8 +825,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -876,8 +848,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -987,7 +957,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1027,7 +996,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1093,7 +1061,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1111,7 +1078,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1162,7 +1128,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1178,7 +1143,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1240,7 +1204,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n@@ -1291,7 +1254,6 @@ def forward(\n             position_ids=flat_position_ids,\n             token_type_ids=flat_token_type_ids,\n             attention_mask=flat_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1341,7 +1303,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1355,7 +1316,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1401,7 +1361,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1412,7 +1371,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "2152c7e92bae462fdcdbb22ff43054bc4415cb42",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 30,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -258,7 +258,6 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n@@ -305,10 +304,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -325,22 +320,20 @@ class Data2VecVisionSdpaSelfAttention(Data2VecVisionSelfAttention):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n+        if output_attentions:\n             logger.warning_once(\n                 \"`Data2VecVisionSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, \"\n+                \"support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                 \"but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n                 'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states=hidden_states,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 relative_position_bias=relative_position_bias,\n                 interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -451,14 +444,13 @@ def prune_heads(self, heads):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[\"Data2VecVisionRelativePositionBias\"] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_outputs = self.attention(\n-            hidden_states, head_mask, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution\n+            hidden_states, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution\n         )\n \n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -525,15 +517,13 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int, int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in Data2VecVision, layernorm is applied before self-attention\n-            head_mask,\n             output_attentions=output_attentions,\n             relative_position_bias=relative_position_bias,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -676,7 +666,6 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         interpolate_pos_encoding: bool = False,\n@@ -699,11 +688,8 @@ def forward(\n             else:\n                 relative_position_bias = None\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 relative_position_bias=relative_position_bias,\n                 interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -803,7 +789,6 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n@@ -819,19 +804,11 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n         resolution = pixel_values.shape[2:]\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             resolution=resolution,\n@@ -898,7 +875,6 @@ def __init__(self, config: Data2VecVisionConfig) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -914,7 +890,6 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         outputs = self.data2vec_vision(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -1245,7 +1220,6 @@ def compute_loss(self, logits, auxiliary_logits, labels):\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1285,7 +1259,6 @@ def forward(\n \n         outputs = self.data2vec_vision(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=True,  # we need the intermediate hidden states\n             interpolate_pos_encoding=interpolate_pos_encoding,"
        },
        {
            "sha": "de562f465aca3f9ea28add8b04355cd4bb8e0096",
            "filename": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -146,7 +146,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -186,7 +185,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -252,7 +250,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -270,7 +267,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -321,7 +317,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -337,7 +332,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -399,7 +393,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n@@ -450,7 +443,6 @@ def forward(\n             position_ids=flat_position_ids,\n             token_type_ids=flat_token_type_ids,\n             attention_mask=flat_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -500,7 +492,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -514,7 +505,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -560,7 +550,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -571,7 +560,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "f60c94b1bdbeed4b5081320bd74d09e05885374b",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 25,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -40,7 +40,7 @@\n \n \n # Copied from transformers.models.gpt2.modeling_gpt2.eager_attention_forward\n-def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n+def eager_attention_forward(module, query, key, value, attention_mask, **kwargs):\n     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n \n     if module.scale_attn_weights:\n@@ -73,10 +73,6 @@ def eager_attention_forward(module, query, key, value, attention_mask, head_mask\n     attn_weights = attn_weights.type(value.dtype)\n     attn_weights = module.attn_dropout(attn_weights)\n \n-    # Mask heads if we want to\n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2)\n \n@@ -144,7 +140,7 @@ def prune_heads(self, heads):\n         self.num_heads = self.num_heads - len(heads)\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()\n         _, _, k_seq_len, _ = key.size()\n@@ -188,10 +184,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n         attn_weights = attn_weights.type(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n         attn_output = attn_output.transpose(1, 2)\n \n@@ -204,7 +196,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -270,7 +261,7 @@ def forward(\n \n         if using_eager and self.reorder_and_upcast_attn:\n             attn_output, attn_weights = self._upcast_and_reordered_attn(\n-                query_states, key_states, value_states, attention_mask, head_mask\n+                query_states, key_states, value_states, attention_mask\n             )\n         else:\n             attn_output, attn_weights = attention_interface(\n@@ -279,7 +270,6 @@ def forward(\n                 key_states,\n                 value_states,\n                 attention_mask,\n-                head_mask=head_mask,\n                 dropout=self.attn_dropout.p if self.training else 0.0,\n                 is_causal=is_causal,\n                 **kwargs,\n@@ -337,7 +327,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -351,7 +340,6 @@ def forward(\n             past_key_values=past_key_values,\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             **kwargs,\n@@ -372,7 +360,6 @@ def forward(\n                 hidden_states,\n                 past_key_values=past_key_values,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -466,7 +453,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -547,12 +533,6 @@ def forward(\n         else:\n             encoder_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # head_mask has shape n_layer x batch x n_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n         position_embeds = self.wpe(position_ids)\n@@ -576,13 +556,12 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, block in enumerate(self.h):\n+        for block in self.h:\n             outputs = block(\n                 hidden_states,\n                 past_key_values if not (self.gradient_checkpointing and self.training) else None,\n                 cache_position,\n                 attention_mask,\n-                head_mask[i],\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,"
        },
        {
            "sha": "8c9b7e89ecd83a776f047a3e95d076ce1747c6d3",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 26,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -214,9 +214,7 @@ def __init__(self, config: DeiTConfig):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size = hidden_states.shape[0]\n         new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n \n@@ -233,7 +231,7 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n+            None,\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.dropout_prob,\n@@ -289,8 +287,8 @@ def prune_heads(self, heads: set[int]):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n-        self_attn_output, _ = self.attention(hidden_states, head_mask)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n         output = self.output(self_attn_output, hidden_states)\n         return output\n \n@@ -339,9 +337,9 @@ def __init__(self, config: DeiTConfig):\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         hidden_states_norm = self.layernorm_before(hidden_states)\n-        attention_output = self.attention(hidden_states_norm, head_mask)\n+        attention_output = self.attention(hidden_states_norm)\n \n         # first residual connection\n         hidden_states = attention_output + hidden_states\n@@ -364,10 +362,9 @@ def __init__(self, config: DeiTConfig):\n         self.layer = nn.ModuleList([DeiTLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> BaseModelOutput:\n+    def forward(self, hidden_states: torch.Tensor) -> BaseModelOutput:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            hidden_states = layer_module(hidden_states, layer_head_mask)\n+            hidden_states = layer_module(hidden_states)\n \n         return BaseModelOutput(last_hidden_state=hidden_states)\n \n@@ -447,7 +444,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n@@ -459,13 +455,6 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n         expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype\n         if pixel_values.dtype != expected_dtype:\n@@ -475,7 +464,7 @@ def forward(\n             pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n         )\n \n-        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, head_mask=head_mask)\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output)\n         sequence_output = encoder_outputs.last_hidden_state\n         sequence_output = self.layernorm(sequence_output)\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -538,7 +527,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MaskedImageModelingOutput:\n@@ -573,7 +561,6 @@ def forward(\n         outputs: BaseModelOutputWithPooling = self.deit(\n             pixel_values,\n             bool_masked_pos=bool_masked_pos,\n-            head_mask=head_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             **kwargs,\n         )\n@@ -634,7 +621,6 @@ def __init__(self, config: DeiTConfig) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -673,7 +659,6 @@ def forward(\n \n         outputs: BaseModelOutputWithPooling = self.deit(\n             pixel_values,\n-            head_mask=head_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             **kwargs,\n         )\n@@ -754,13 +739,11 @@ def __init__(self, config: DeiTConfig) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> DeiTForImageClassificationWithTeacherOutput:\n         outputs: BaseModelOutputWithPooling = self.deit(\n             pixel_values,\n-            head_mask=head_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             **kwargs,\n         )"
        },
        {
            "sha": "bcccd9b8c7b48f7b285b01579fd26c0c87220fe4",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -124,7 +124,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -205,10 +204,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -252,7 +247,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -261,7 +255,6 @@ def forward(\n         self_outputs = self.self_attn(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n             past_key_values,\n@@ -297,7 +290,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = True,\n     ):\n@@ -306,7 +298,6 @@ def forward(\n             hidden_states, attention_opt_weights = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n@@ -315,7 +306,6 @@ def forward(\n             hidden_states = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n@@ -346,7 +336,6 @@ def forward(\n         self,\n         input_embeds: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -359,12 +348,9 @@ def forward(\n         if output_hidden_states:\n             hidden_states = hidden_states + (output,)\n         for i, layer in enumerate(self.layers):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             output, opt_attn_weights = layer(\n                 hidden_states=output,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 past_key_values=past_key_values[i] if past_key_values is not None else None,\n             )\n \n@@ -458,12 +444,6 @@ def _init_weights(self, module):\n             config.max_position_embeddings - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n@@ -518,7 +498,6 @@ def forward(\n         input_ids: Optional[tensor] = None,\n         position_ids: Optional[tensor] = None,\n         attention_mask: Optional[tensor] = None,\n-        head_mask: Optional[tensor] = None,\n         inputs_embeds: Optional[tensor] = None,\n         past_key_values: Optional[tuple[tuple[tensor]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -536,8 +515,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.return_dict\n \n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         past_key_values_length = 0\n         if past_key_values is not None:\n             past_key_values_length = past_key_values.get_seq_length()\n@@ -567,7 +544,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -625,7 +601,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n@@ -646,7 +621,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             output_hidden_states=output_hidden_states,\n@@ -723,7 +697,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -752,7 +725,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -814,7 +786,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -832,7 +803,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n@@ -890,7 +860,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -914,7 +883,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -978,7 +946,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -999,7 +966,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "bb6a0d1f81d8e6709ed4ae92be781c5cf1db9634",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -386,7 +386,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -460,15 +459,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n         if output_attentions:\n             # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n@@ -522,7 +512,6 @@ def forward(\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n@@ -545,12 +534,6 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-            head_mask (`numpy.ndarray` of shape `({0})`, `optional):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n@@ -568,7 +551,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=self_attn_past_key_value,\n             attention_mask=(1 - attention_mask) * torch.finfo(hidden_states.dtype).min,\n-            layer_head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n         if output_attentions:\n@@ -604,7 +586,6 @@ def forward(\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         output_router_tuple: Optional[bool] = False,\n@@ -628,12 +609,6 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-            head_mask (`numpy.ndarray` of shape `({0})`, `optional):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n@@ -648,7 +623,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n         )\n@@ -808,8 +782,6 @@ def _shift_right(self, input_ids):\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n             `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n         use_cache (`bool`, *optional*):\n             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n             `past_key_values`).\n@@ -878,7 +850,6 @@ def forward(\n         token_type_ids: Optional[torch.FloatTensor] = None,\n         spout: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1000,12 +971,6 @@ def forward(\n         # Merge prefix_lm_mask and attention_mask\n         extended_attention_mask = prefix_lm_mask * attention_mask.unsqueeze(1).unsqueeze(2)\n \n-        # Prepare head mask if needed\n-        if head_mask is not None:\n-            head_mask = self.get_head_mask(\n-                head_mask, self.config.num_switch_layers + self.config.num_ext_layers\n-            )  # n_layer x batch x n_heads x N x N\n-\n         # outputs\n         present_key_value_states = () if self.config.use_cache or use_cache else None\n         all_hidden_states = () if self.config.output_hidden_states or output_hidden_states else None\n@@ -1030,7 +995,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 past_key_values=past,\n                 attention_mask=extended_attention_mask,\n-                head_mask=head_mask,\n                 use_cache=self.config.use_cache or use_cache,\n                 output_attentions=self.config.output_attentions or output_attentions,\n                 output_router_tuple=output_router_tuple,\n@@ -1104,7 +1068,6 @@ def forward(\n         token_type_ids: Optional[torch.FloatTensor] = None,\n         spout: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1194,7 +1157,6 @@ def forward(\n             token_type_ids,\n             spout,\n             past_key_values,\n-            head_mask,\n             use_cache,\n             inputs_embeds,\n             decoder_inputs_embeds,"
        },
        {
            "sha": "2f021dd7c69a9f951068df3ce9dfb86b857cef3b",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -228,7 +228,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         mixed_query_layer = self.query(hidden_states)\n@@ -260,10 +259,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).flatten(start_dim=-2)\n@@ -327,13 +322,11 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -387,12 +380,9 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n-        self_attention_outputs = self.attention(\n-            hidden_states, attention_mask, head_mask, output_attentions=output_attentions\n-        )\n+        self_attention_outputs = self.attention(hidden_states, attention_mask, output_attentions=output_attentions)\n         attention_output = self_attention_outputs[0]\n         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n@@ -504,11 +494,6 @@ def _get_feature_vector_attention_mask(self, feature_vector_length, attention_ma\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -535,7 +520,6 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        head_mask: torch.Tensor,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -564,14 +548,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, \"\n-                    f\"but it is for {head_mask.size()[0]}.\"\n-                )\n-\n         synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -633,7 +609,6 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -650,7 +625,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             input_features,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -703,7 +677,6 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -723,7 +696,6 @@ def forward(\n         outputs = self.mctct(\n             input_features,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,"
        },
        {
            "sha": "ed8e3847b578cdc8fe105616670575bbfee4ecbb",
            "filename": "src/transformers/models/deprecated/mmbt/modeling_mmbt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -142,12 +142,6 @@ def forward(self, input_modal, start_token=None, end_token=None, position_ids=No\n             Selected in the range `[0, config.max_position_embeddings - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, embedding_dim)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n@@ -197,7 +191,6 @@ def forward(\n         modal_token_type_ids=None,\n         position_ids=None,\n         modal_position_ids=None,\n-        head_mask=None,\n         inputs_embeds=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n@@ -269,12 +262,10 @@ def forward(\n \n         extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n         encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n         encoder_outputs = self.transformer.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             output_attentions=output_attentions,\n@@ -358,7 +349,6 @@ def forward(\n         modal_token_type_ids=None,\n         position_ids=None,\n         modal_position_ids=None,\n-        head_mask=None,\n         inputs_embeds=None,\n         labels=None,\n         return_dict=None,\n@@ -375,7 +365,6 @@ def forward(\n             modal_token_type_ids=modal_token_type_ids,\n             position_ids=position_ids,\n             modal_position_ids=modal_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=return_dict,\n         )"
        },
        {
            "sha": "a586c85832ce1d0b1d41f06b9d4c0f9861c78ffd",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -172,7 +172,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -245,10 +244,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         relations_values = self.relative_positions_encoding(to_seq_length)\n         attention_probs_t = attention_probs.permute(2, 0, 1, 3)\n@@ -317,7 +312,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -326,7 +320,6 @@ def forward(\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n             past_key_values,\n@@ -386,7 +379,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -397,7 +389,6 @@ def forward(\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n             past_key_values=self_attn_past_key_value,\n         )\n@@ -423,7 +414,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 cross_attn_past_key_value,\n@@ -464,7 +454,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -489,12 +478,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_values[i] if past_key_values is not None else None,\n@@ -719,12 +705,6 @@ class NezhaForPreTrainingOutput(ModelOutput):\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n@@ -794,7 +774,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -877,13 +856,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             token_type_ids=token_type_ids,\n@@ -892,7 +864,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             past_key_values=past_key_values,\n@@ -950,7 +921,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         next_sentence_label: Optional[torch.Tensor] = None,\n@@ -996,7 +966,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1063,7 +1032,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1085,7 +1053,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1151,7 +1118,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1202,7 +1168,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1264,7 +1229,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1283,7 +1247,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1361,7 +1324,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1389,7 +1351,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1454,7 +1415,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1471,7 +1431,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1529,7 +1488,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1553,7 +1511,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "7365e8382d85a14763101dfc6d8c5df213659bfd",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -172,7 +172,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -248,10 +247,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(\n             self.matmul_a_input_quantizer(attention_probs), self.matmul_v_input_quantizer(value_layer)\n         )\n@@ -321,7 +316,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -330,7 +324,6 @@ def forward(\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n             past_key_values,\n@@ -399,7 +392,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -410,7 +402,6 @@ def forward(\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n             past_key_values=self_attn_past_key_value,\n         )\n@@ -436,7 +427,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 cross_attn_past_key_value,\n@@ -476,7 +466,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         past_key_values=None,\n@@ -494,12 +483,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_values[i] if past_key_values is not None else None,\n@@ -699,12 +685,6 @@ def _init_weights(self, module):\n             config.max_position_embeddings - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n@@ -776,7 +756,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -860,13 +839,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -877,7 +849,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             past_key_values=past_key_values,\n@@ -935,7 +906,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -997,7 +967,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1106,7 +1075,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1129,7 +1097,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1198,7 +1165,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1249,7 +1215,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1308,7 +1273,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1328,7 +1292,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1404,7 +1367,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1435,7 +1397,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1496,7 +1457,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1514,7 +1474,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1573,7 +1532,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1598,7 +1556,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "382a4812fbd4894a1cdc69a6a95a6066ef24620b",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -144,7 +144,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -225,10 +224,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -293,7 +288,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -302,7 +296,6 @@ def forward(\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n             past_key_values,\n@@ -362,7 +355,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -373,7 +365,6 @@ def forward(\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n             past_key_values=self_attn_past_key_value,\n         )\n@@ -399,7 +390,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 cross_attn_past_key_value,\n@@ -440,7 +430,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -465,12 +454,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_values[i] if past_key_values is not None else None,\n@@ -807,12 +793,6 @@ def mask_to_score(mask, dtype=torch.float32):\n             config.max_position_embeddings - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n@@ -903,7 +883,6 @@ def forward(\n         attention_mask=None,\n         token_type_ids=None,\n         position_ids=None,\n-        head_mask=None,\n         inputs_embeds=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n@@ -966,13 +945,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -983,7 +955,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             past_key_values=past_key_values,\n@@ -1036,7 +1007,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1068,7 +1038,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1122,7 +1091,6 @@ def forward(\n         candidate_attention_mask: Optional[torch.FloatTensor] = None,\n         candidate_token_type_ids: Optional[torch.LongTensor] = None,\n         candidate_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1196,7 +1164,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1213,7 +1180,6 @@ def forward(\n             attention_mask=flattened_attention_mask,\n             token_type_ids=flattened_token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=candidate_inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1274,7 +1240,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         relevance_score: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1336,7 +1301,6 @@ def forward(\n             attention_mask=flattened_attention_mask,\n             token_type_ids=flattened_token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1411,7 +1375,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         relevance_score: Optional[torch.FloatTensor] = None,\n         block_mask: Optional[torch.BoolTensor] = None,\n@@ -1454,7 +1417,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "fa7695133fb896a78ec3b857f74744c0bebc04c1",
            "filename": "src/transformers/models/deprecated/retribert/modeling_retribert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -109,7 +109,6 @@ def embed_sentences_checkpointed(\n             device = input_ids.device\n             input_shape = input_ids.size()\n             token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-            head_mask = [None] * sent_encoder.config.num_hidden_layers\n             extended_attention_mask: torch.Tensor = sent_encoder.get_extended_attention_mask(\n                 attention_mask, input_shape\n             )\n@@ -119,7 +118,6 @@ def partial_encode(*inputs):\n                 encoder_outputs = sent_encoder.encoder(\n                     inputs[0],\n                     attention_mask=inputs[1],\n-                    head_mask=head_mask,\n                 )\n                 sequence_output = encoder_outputs[0]\n                 pooled_output = sent_encoder.pooler(sequence_output)"
        },
        {
            "sha": "86495448299affe52c080d1cf00015397331baa3",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -151,7 +151,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -225,15 +224,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n         if output_attentions:\n             # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n@@ -303,8 +293,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -318,10 +306,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size *(decoder_attention_heads,)*.\n             past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -337,7 +321,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=self_attn_past_key_value,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -356,7 +339,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=cross_attn_past_key_value,\n                 output_attentions=output_attentions,\n             )\n@@ -463,8 +445,6 @@ def forward(\n         attention_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        head_mask=None,\n-        cross_attn_head_mask=None,\n         past_key_values=None,\n         inputs_embeds=None,\n         use_cache=None,\n@@ -500,19 +480,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules in encoder to avoid performing cross-attention\n-                on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n@@ -591,14 +558,6 @@ def forward(\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n         next_decoder_cache = () if use_cache else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != (len(self.layers)):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -613,8 +572,6 @@ def forward(\n                 attention_mask=attention_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values[idx] if past_key_values is not None else None,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -706,8 +663,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -739,18 +694,6 @@ def forward(\n             encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n                 in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n@@ -836,8 +779,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "c92fcd6fbb37727947291f545b0614ee6116d971",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 36,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -153,7 +153,7 @@ def _rel_shift(self, x):\n \n         return x\n \n-    def forward(self, w, r, attn_mask=None, mems=None, head_mask=None, output_attentions=False):\n+    def forward(self, w, r, attn_mask=None, mems=None, output_attentions=False):\n         qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n \n         if mems is not None:\n@@ -211,10 +211,6 @@ def forward(self, w, r, attn_mask=None, mems=None, head_mask=None, output_attent\n         attn_prob = nn.functional.softmax(attn_score, dim=1)\n         attn_prob = self.dropatt(attn_prob)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_prob = attn_prob * head_mask\n-\n         # compute attention vector\n         attn_vec = torch.einsum(\"ijbn,jbnd->ibnd\", (attn_prob, w_head_v))\n \n@@ -249,13 +245,12 @@ def __init__(self, n_head, d_model, d_head, d_inner, dropout, layer_norm_epsilon\n             d_model, d_inner, dropout, pre_lnorm=kwargs.get(\"pre_lnorm\"), layer_norm_epsilon=layer_norm_epsilon\n         )\n \n-    def forward(self, dec_inp, r, dec_attn_mask=None, mems=None, head_mask=None, output_attentions=False):\n+    def forward(self, dec_inp, r, dec_attn_mask=None, mems=None, output_attentions=False):\n         attn_outputs = self.dec_attn(\n             dec_inp,\n             r,\n             attn_mask=dec_attn_mask,\n             mems=mems,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n         ff_output = self.pos_ff(attn_outputs[0])\n@@ -604,12 +599,6 @@ def logits(self):\n             Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model (see\n             `mems` output below). Can be used to speed up sequential decoding. The token ids which have their mems\n             given to this model should not be passed as `input_ids` as they have already been computed.\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n@@ -742,7 +731,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         mems: Optional[list[torch.FloatTensor]] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -770,23 +758,6 @@ def forward(\n         if mems is None:\n             mems = self.init_mems(bsz)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n-        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n-        if head_mask is not None:\n-            if head_mask.dim() == 1:\n-                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n-                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n-            elif head_mask.dim() == 2:\n-                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n-            head_mask = head_mask.to(\n-                dtype=next(self.parameters()).dtype\n-            )  # switch to float if need + fp16 compatibility\n-        else:\n-            head_mask = [None] * self.n_layer\n-\n         if inputs_embeds is not None:\n             word_emb = inputs_embeds\n         else:\n@@ -828,7 +799,6 @@ def forward(\n                     pos_emb,\n                     dec_attn_mask=dec_attn_mask,\n                     mems=mems_i,\n-                    head_mask=head_mask[i],\n                     output_attentions=output_attentions,\n                 )\n                 core_out = layer_outputs[0]\n@@ -937,7 +907,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         mems: Optional[list[torch.FloatTensor]] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -961,7 +930,6 @@ def forward(\n         transformer_outputs = self.transformer(\n             input_ids,\n             mems=mems,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1078,7 +1046,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         mems: Optional[list[torch.FloatTensor]] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1096,7 +1063,6 @@ def forward(\n         transformer_outputs = self.transformer(\n             input_ids,\n             mems=mems,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "7f51661b5270e8cdeb447fbdaa9113a97d37d86d",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -364,7 +364,7 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):\n+    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n@@ -385,10 +385,6 @@ def forward(self, hidden_states, attention_mask=None, head_mask=None, output_att\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -443,8 +439,8 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):\n-        self_outputs = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n+    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n+        self_outputs = self.attention(hidden_states, attention_mask, output_attentions)\n \n         attention_output = self.output(self_outputs[0], hidden_states)\n \n@@ -496,11 +492,10 @@ def __init__(self, config):\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):\n+    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in ViLT, layernorm is applied before self-attention\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -532,7 +527,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -544,9 +538,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "7269fbd000202fc31e8bbec7852a3367f892fed1",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 33,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -223,7 +223,7 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         return x.permute(0, 2, 1, 3)\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self, hidden_states: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         mixed_query_layer = self.query(hidden_states)\n \n@@ -243,10 +243,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -264,7 +260,7 @@ def __init__(self, config: ViTHybridConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self, hidden_states: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         mixed_query_layer = self.query(hidden_states)\n \n@@ -276,7 +272,6 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n             self.attention_probs_dropout_prob if self.training else 0.0,\n             is_causal=False,\n             scale=None,\n@@ -335,10 +330,9 @@ def prune_heads(self, heads: set[int]) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n+        self_outputs = self.attention(hidden_states, output_attentions)\n \n         attention_output = self.output(self_outputs[0], hidden_states)\n \n@@ -405,12 +399,10 @@ def __init__(self, config: ViTHybridConfig) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in ViTHybrid, layernorm is applied before self-attention\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -442,7 +434,6 @@ def __init__(self, config: ViTHybridConfig) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -454,9 +445,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -531,13 +520,6 @@ def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> No\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n             Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n             [`ViTHybridImageProcessor.__call__`] for details.\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -590,7 +572,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n@@ -609,13 +590,6 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n         expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype\n         if pixel_values.dtype != expected_dtype:\n@@ -627,7 +601,6 @@ def forward(\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -693,7 +666,6 @@ def __init__(self, config: ViTHybridConfig) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -710,7 +682,6 @@ def forward(\n \n         outputs = self.vit(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,"
        },
        {
            "sha": "95bd63226c3cc981c6c6951fce67437fa39ca6f1",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -97,24 +97,6 @@\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n@@ -156,12 +138,6 @@\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -657,7 +633,6 @@ def forward(\n         hidden_states,\n         key_value_states: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n-        layer_head_mask: Optional[Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n@@ -722,18 +697,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_attn_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_attn_heads,)}, but is\"\n-                f\" {layer_head_mask.size()}\"\n-            )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(\n-                batch_size, self.num_attn_heads, tgt_len, src_len\n-            )\n-\n-            # apply head_mask also on attn_weights_reshaped which is used for n-gram attention inside the model\n-            attn_weights_reshaped = layer_head_mask.view(1, -1, 1, 1) * attn_weights_reshaped\n-\n         attn_probs = nn.functional.dropout(\n             attn_weights,\n             p=self.attention_dropout,\n@@ -816,7 +779,6 @@ def forward(\n         hidden_states,\n         past_key_values: Optional[Cache] = None,\n         attention_mask=None,\n-        layer_head_mask=None,\n         extended_predict_attention_mask=None,\n         main_relative_position_buckets=None,\n         predict_relative_position_buckets=None,\n@@ -893,15 +855,6 @@ def forward(\n             onnx_trace=self.onnx_trace,\n         ).type_as(main_attn_weights)\n \n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_attn_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_attn_heads,)}, but is\"\n-                f\" {layer_head_mask.size()}\"\n-            )\n-            main_attn_probs = layer_head_mask.view(1, -1, 1, 1) * main_attn_probs.view(\n-                batch_size, self.num_attn_heads, -1, sequence_length\n-            )\n-\n         main_attn_probs = nn.functional.dropout(main_attn_probs, p=self.attention_dropout, training=self.training)\n         # project to attn_output\n         # [batch_size, number_heads, sequence_length, sequence_length]\n@@ -955,13 +908,6 @@ def forward(\n             onnx_trace=self.onnx_trace,\n         ).type_as(predict_attn_weights)\n \n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_attn_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_attn_heads,)}, but is\"\n-                f\" {layer_head_mask.size()}\"\n-            )\n-            predict_attn_probs = layer_head_mask.view(1, 1, -1, 1, 1) * predict_attn_probs\n-\n         predict_attn_probs = nn.functional.dropout(\n             predict_attn_probs, p=self.attention_dropout, training=self.training\n         )\n@@ -1113,14 +1059,12 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask,\n-        layer_head_mask,\n         output_attentions: bool = False,\n     ):\n         # 1st residual block\n         attention_output, attn_weights, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = self.self_attn_layer_norm(attention_output + hidden_states)\n@@ -1164,8 +1108,6 @@ def forward(\n         attention_mask=None,\n         encoder_hidden_states=None,\n         encoder_attn_mask=None,\n-        layer_head_mask=None,\n-        cross_attn_layer_head_mask=None,\n         extended_predict_attention_mask=None,\n         main_relative_position_buckets=None,\n         predict_relative_position_buckets=None,\n@@ -1181,7 +1123,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=self_attn_past_key_value,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             extended_predict_attention_mask=extended_predict_attention_mask,\n             main_relative_position_buckets=main_relative_position_buckets,\n             predict_relative_position_buckets=predict_relative_position_buckets,\n@@ -1198,7 +1139,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attn_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=cross_attn_past_key_value,\n                 output_attentions=output_attentions,\n             )\n@@ -1262,7 +1202,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1316,19 +1255,13 @@ def forward(\n         encoder_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            assert head_mask.size()[0] == (len(self.layers)), (\n-                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n-            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_hidden_states = encoder_hidden_states + (hidden_states,)\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n                 attention_mask=extended_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 output_attentions=output_attentions,\n             )\n \n@@ -1396,8 +1329,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1412,11 +1343,6 @@ def forward(\n         encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n \n         past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n             Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.\n@@ -1534,13 +1460,6 @@ def forward(\n \n         present_key_values = () if use_cache else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                assert attn_mask.size()[0] == (len(self.layers)), (\n-                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, decoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 # grad cannot be kept because tensor is sliced\n@@ -1553,8 +1472,6 @@ def forward(\n                 attention_mask=extended_attention_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attn_mask=extended_encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 extended_predict_attention_mask=extended_predict_attention_mask,\n                 main_relative_position_buckets=main_relative_position_buckets,\n                 predict_relative_position_buckets=predict_relative_position_buckets,\n@@ -1741,9 +1658,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1784,7 +1698,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1797,8 +1710,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             output_attentions=output_attentions,\n@@ -1857,9 +1768,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1906,9 +1814,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1988,9 +1893,6 @@ def prepare_inputs_for_generation(\n         decoder_input_ids,\n         past_key_values=None,\n         attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n         use_cache=None,\n         encoder_outputs=None,\n         **kwargs,\n@@ -2006,9 +1908,6 @@ def prepare_inputs_for_generation(\n             \"past_key_values\": past_key_values,\n             \"decoder_input_ids\": decoder_input_ids,\n             \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n             \"use_cache\": use_cache,\n         }\n \n@@ -2085,8 +1984,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -2102,11 +1999,6 @@ def forward(\n         encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n \n         past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n             Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.\n@@ -2173,8 +2065,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n@@ -2244,7 +2134,6 @@ def prepare_inputs_for_generation(\n         input_ids,\n         past_key_values=None,\n         attention_mask=None,\n-        head_mask=None,\n         use_cache=None,\n         **kwargs,\n     ):\n@@ -2258,7 +2147,6 @@ def prepare_inputs_for_generation(\n         return {\n             \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n             \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n             \"past_key_values\": past_key_values,\n             \"use_cache\": use_cache,\n         }"
        },
        {
            "sha": "153ddfc1f513b45e0526cf41e870c84a36db9916",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -239,7 +239,6 @@ def __init__(self, config: DepthProConfig):\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n     ) -> list[torch.Tensor]:\n         batch_size, num_channels, height, width = pixel_values.shape\n \n@@ -279,7 +278,6 @@ def forward(\n         encodings = self.model(\n             # each patch is processed as a separate batch\n             patches,\n-            head_mask=head_mask,\n             # required for intermediate features\n             output_hidden_states=self.n_intermediate_hooks > 0,\n         )\n@@ -344,7 +342,6 @@ def __init__(self, config: DepthProConfig):\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -361,7 +358,6 @@ def forward(\n         )\n         encodings = self.model(\n             pixel_values=pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n         )\n@@ -410,7 +406,6 @@ def __init__(self, config: DepthProConfig):\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -419,11 +414,9 @@ def forward(\n \n         patch_features = self.patch_encoder(\n             pixel_values,\n-            head_mask=head_mask,\n         )\n         image_encodings = self.image_encoder(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -646,7 +639,6 @@ def get_input_embeddings(self):\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -684,7 +676,6 @@ def forward(\n \n         encodings = self.encoder(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -852,7 +843,6 @@ def __init__(self, config: DepthProConfig):\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         batch_size, num_channels, height, width = pixel_values.shape\n \n@@ -866,7 +856,6 @@ def forward(\n         )\n         encodings = self.model(\n             pixel_values=pixel_values,\n-            head_mask=head_mask,\n         )\n         hidden_state = encodings[0]\n         hidden_state = self.neck(hidden_state)\n@@ -945,9 +934,8 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         global_features: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n-        fov_features = self.fov_encoder(pixel_values, head_mask)\n+        fov_features = self.fov_encoder(pixel_values)\n \n         global_features = self.conv(global_features)\n         global_features = self.activation(global_features)\n@@ -1032,7 +1020,6 @@ def __init__(self, config, use_fov_model=None):\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1093,7 +1080,6 @@ def forward(\n \n         depth_pro_outputs = self.depth_pro(\n             pixel_values=pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n@@ -1108,7 +1094,6 @@ def forward(\n             fov = self.fov_model(\n                 pixel_values=pixel_values,\n                 global_features=features_for_fov,\n-                head_mask=head_mask,\n             )\n         else:\n             fov = None"
        },
        {
            "sha": "3025c8de4faa82e3127e4a22290ff9ab299e23f0",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -493,8 +493,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -687,8 +685,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,"
        },
        {
            "sha": "432f0298430cfd0cbe44cffba0c7a0e227557ead",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -308,8 +308,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -502,8 +500,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,"
        },
        {
            "sha": "a080fcb5017ac5ba040fc0e9323f0846d126aa94",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 26,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -202,9 +202,7 @@ def __init__(self, config: Dinov2Config):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size = hidden_states.shape[0]\n         new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n \n@@ -221,7 +219,7 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n+            None,\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.dropout_prob,\n@@ -277,8 +275,8 @@ def prune_heads(self, heads: set[int]):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n-        self_attn_output, _ = self.attention(hidden_states, head_mask)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n         output = self.output(self_attn_output, hidden_states)\n         return output\n \n@@ -381,10 +379,9 @@ def __init__(self, config: Dinov2Config) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         hidden_states_norm = self.norm1(hidden_states)\n-        self_attention_output = self.attention(hidden_states_norm, head_mask)\n+        self_attention_output = self.attention(hidden_states_norm)\n         self_attention_output = self.layer_scale1(self_attention_output)\n \n         # first residual connection\n@@ -408,13 +405,10 @@ def __init__(self, config: Dinov2Config):\n         self.layer = nn.ModuleList([Dinov2Layer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None, output_hidden_states: bool = False\n-    ) -> BaseModelOutput:\n+    def forward(self, hidden_states: torch.Tensor, output_hidden_states: bool = False) -> BaseModelOutput:\n         all_hidden_states = [hidden_states] if output_hidden_states else None\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            hidden_states = layer_module(hidden_states, layer_head_mask)\n+            hidden_states = layer_module(hidden_states)\n             if all_hidden_states:\n                 all_hidden_states.append(hidden_states)\n \n@@ -502,7 +496,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> BaseModelOutputWithPooling:\n@@ -517,18 +510,9 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n \n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            embedding_output, head_mask=head_mask, output_hidden_states=output_hidden_states\n-        )\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, output_hidden_states=output_hidden_states)\n         sequence_output = encoder_outputs.last_hidden_state\n         sequence_output = self.layernorm(sequence_output)\n         pooled_output = sequence_output[:, 0, :]\n@@ -566,7 +550,6 @@ def __init__(self, config: Dinov2Config) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n@@ -576,7 +559,7 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        outputs: BaseModelOutputWithPooling = self.dinov2(pixel_values, head_mask=head_mask, **kwargs)\n+        outputs: BaseModelOutputWithPooling = self.dinov2(pixel_values, **kwargs)\n \n         sequence_output = outputs.last_hidden_state  # batch_size, sequence_length, hidden_size\n         cls_token = sequence_output[:, 0]"
        },
        {
            "sha": "89ce1d51a1be4dfb60f02dd59e50ed7b6830108d",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 9,
            "deletions": 26,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -221,9 +221,7 @@ def __init__(self, config: Dinov2WithRegistersConfig):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size = hidden_states.shape[0]\n         new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n \n@@ -240,7 +238,7 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n+            None,\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.dropout_prob,\n@@ -294,8 +292,8 @@ def prune_heads(self, heads: set[int]):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n-        self_attn_output, _ = self.attention(hidden_states, head_mask)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n         output = self.output(self_attn_output, hidden_states)\n         return output\n \n@@ -398,10 +396,9 @@ def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         hidden_states_norm = self.norm1(hidden_states)\n-        self_attention_output = self.attention(hidden_states_norm, head_mask)\n+        self_attention_output = self.attention(hidden_states_norm)\n         self_attention_output = self.layer_scale1(self_attention_output)\n \n         # first residual connection\n@@ -425,13 +422,10 @@ def __init__(self, config: Dinov2WithRegistersConfig):\n         self.layer = nn.ModuleList([Dinov2WithRegistersLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None, output_hidden_states: bool = False\n-    ) -> BaseModelOutput:\n+    def forward(self, hidden_states: torch.Tensor, output_hidden_states: bool = False) -> BaseModelOutput:\n         all_hidden_states = [hidden_states] if output_hidden_states else None\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            hidden_states = layer_module(hidden_states, layer_head_mask)\n+            hidden_states = layer_module(hidden_states)\n             if all_hidden_states:\n                 all_hidden_states.append(hidden_states)\n \n@@ -519,7 +513,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> BaseModelOutputWithPooling:\n@@ -534,18 +527,9 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n \n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            embedding_output, head_mask=head_mask, output_hidden_states=output_hidden_states\n-        )\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, output_hidden_states=output_hidden_states)\n         sequence_output = encoder_outputs.last_hidden_state\n         sequence_output = self.layernorm(sequence_output)\n         pooled_output = sequence_output[:, 0, :]\n@@ -583,7 +567,6 @@ def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n@@ -594,7 +577,7 @@ def forward(\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n \n-        outputs: BaseModelOutputWithPooling = self.dinov2_with_registers(pixel_values, head_mask=head_mask, **kwargs)\n+        outputs: BaseModelOutputWithPooling = self.dinov2_with_registers(pixel_values, **kwargs)\n         sequence_output = outputs.last_hidden_state  # batch_size, sequence_length, hidden_size\n \n         cls_token = sequence_output[:, 0]"
        },
        {
            "sha": "02c33e33d26090bbb493bb40429483e8ca73f336",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -317,7 +317,6 @@ class Dinov2WithRegistersForImageClassification(Dinov2ForImageClassification):\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n@@ -328,7 +327,7 @@ def forward(\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n \n-        outputs: BaseModelOutputWithPooling = self.dinov2_with_registers(pixel_values, head_mask=head_mask, **kwargs)\n+        outputs: BaseModelOutputWithPooling = self.dinov2_with_registers(pixel_values, **kwargs)\n         sequence_output = outputs.last_hidden_state  # batch_size, sequence_length, hidden_size\n \n         cls_token = sequence_output[:, 0]"
        },
        {
            "sha": "c7f56ce1fa4fa0d3517c00f1e5273bc6157fe983",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -500,7 +500,6 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -514,10 +513,8 @@ def forward(\n         position_embeddings = self.rope_embeddings(pixel_values)\n \n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n             hidden_states = layer_module(\n                 hidden_states,\n-                attention_mask=layer_head_mask,\n                 position_embeddings=position_embeddings,\n             )\n "
        },
        {
            "sha": "43c8672b8249167a4a06675711d5b72a2def28a1",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -395,7 +395,6 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -409,10 +408,8 @@ def forward(\n         position_embeddings = self.rope_embeddings(pixel_values)\n \n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n             hidden_states = layer_module(\n                 hidden_states,\n-                attention_mask=layer_head_mask,\n                 position_embeddings=position_embeddings,\n             )\n "
        },
        {
            "sha": "4cbbf2c4f0df8c78978c6ed3b1da358541e07959",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 31,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -173,7 +173,6 @@ def forward(\n         key: torch.Tensor,\n         value: torch.Tensor,\n         mask: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, ...]:\n         \"\"\"\n@@ -218,10 +217,6 @@ def unshape(x: torch.Tensor) -> torch.Tensor:\n         weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\n         weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            weights = weights * head_mask\n-\n         context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n         context = unshape(context)  # (bs, q_length, dim)\n         context = self.out_lin(context)  # (bs, q_length, dim)\n@@ -253,7 +248,6 @@ def forward(\n         key: torch.Tensor,\n         value: torch.Tensor,\n         mask: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, ...]:\n         \"\"\"\n@@ -344,7 +338,6 @@ def forward(\n         key: torch.Tensor,\n         value: torch.Tensor,\n         mask: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, ...]:\n         \"\"\"\n@@ -358,10 +351,10 @@ def forward(\n             weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n             seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n         \"\"\"\n-        if output_attentions or head_mask is not None:\n+        if output_attentions:\n             logger.warning_once(\n                 \"DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support\"\n-                \" `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying\"\n+                \" `output_attentions=True`. Falling back to the manual attention implementation, but specifying\"\n                 \" the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be\"\n                 ' removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n@@ -370,7 +363,6 @@ def forward(\n                 key,\n                 value,\n                 mask,\n-                head_mask,\n                 output_attentions,\n             )\n \n@@ -450,7 +442,6 @@ def forward(\n         self,\n         x: torch.Tensor,\n         attn_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, ...]:\n         \"\"\"\n@@ -468,7 +459,6 @@ def forward(\n             key=x,\n             value=x,\n             mask=attn_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n         if output_attentions:\n@@ -501,7 +491,6 @@ def forward(\n         self,\n         x: torch.Tensor,\n         attn_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: Optional[bool] = None,\n@@ -531,7 +520,6 @@ def forward(\n             layer_outputs = layer_module(\n                 hidden_state,\n                 attn_mask,\n-                head_mask[i],\n                 output_attentions,\n             )\n \n@@ -664,7 +652,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -701,10 +688,6 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        head_mask_is_none = head_mask is None\n-        # Prepare head mask if needed\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n \n         if self.config._attn_implementation == \"flash_attention_2\":\n@@ -713,15 +696,14 @@ def forward(\n             if attention_mask is None:\n                 attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n \n-            if self.config._attn_implementation == \"sdpa\" and head_mask_is_none and not output_attentions:\n+            if self.config._attn_implementation == \"sdpa\" and not output_attentions:\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     attention_mask, embeddings.dtype, tgt_len=input_shape[1]\n                 )\n \n         return self.transformer(\n             x=embeddings,\n             attn_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -782,7 +764,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -811,7 +792,6 @@ def forward(\n         dlbrt_output = self.distilbert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -884,7 +864,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -902,7 +881,6 @@ def forward(\n         distilbert_output = self.distilbert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -990,7 +968,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1016,7 +993,6 @@ def forward(\n         distilbert_output = self.distilbert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1098,7 +1074,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1114,7 +1089,6 @@ def forward(\n         outputs = self.distilbert(\n             input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1181,7 +1155,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1240,7 +1213,6 @@ def forward(\n         outputs = self.distilbert(\n             input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "f92371fdaba6950aad5126c02936e26cd93e39d9",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -186,7 +186,6 @@ def flex_attention_forward(\n     attention_mask: Union[torch.Tensor, \"BlockMask\"],\n     scaling: Optional[float] = None,\n     softcap: Optional[float] = None,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, torch.Tensor]:\n     block_mask = None\n@@ -204,8 +203,6 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n             score = softcap * torch.tanh(score / softcap)\n         if causal_mask is not None:\n             score = score + causal_mask[batch_idx][head_idx][q_idx][kv_idx]\n-        if head_mask is not None:\n-            score = score + head_mask[batch_idx][head_idx][0][0]\n         return score\n \n     attn_output, attention_weights = compile_friendly_flex_attention("
        },
        {
            "sha": "0d1a1e06afb4cce4a6cd64397a6d0ebf058324a3",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -282,7 +282,6 @@ def flex_attention_forward(\n     attention_mask: Union[torch.Tensor, \"BlockMask\"],\n     scaling: Optional[float] = None,\n     softcap: Optional[float] = None,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, torch.Tensor]:\n     block_mask = None\n@@ -300,8 +299,6 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n             score = softcap * torch.tanh(score / softcap)\n         if causal_mask is not None:\n             score = score + causal_mask[batch_idx][head_idx][q_idx][kv_idx]\n-        if head_mask is not None:\n-            score = score + head_mask[batch_idx][head_idx][0][0]\n         return score\n \n     attn_output, attention_weights = compile_friendly_flex_attention("
        },
        {
            "sha": "d388e386ae49c24ca8e47346cff0ad6b8327172f",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 4,
            "deletions": 34,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -403,7 +403,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         batch_size, dim, num_channels = hidden_states.shape\n@@ -442,10 +441,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n@@ -500,10 +495,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n+        self_outputs = self.self(hidden_states, attention_mask, output_attentions)\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n         return outputs\n@@ -600,7 +594,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -633,9 +626,7 @@ def forward(\n             height_pad, width_pad, dtype=hidden_states.dtype, device=hidden_states_windows.device\n         )\n \n-        attention_outputs = self.attention(\n-            hidden_states_windows, attn_mask, head_mask, output_attentions=output_attentions\n-        )\n+        attention_outputs = self.attention(hidden_states_windows, attn_mask, output_attentions=output_attentions)\n \n         attention_output = attention_outputs[0]\n \n@@ -696,17 +687,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         height, width = input_dimensions\n         for i, layer_module in enumerate(self.blocks):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n-                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-            )\n+            layer_outputs = layer_module(hidden_states, input_dimensions, output_attentions, always_partition)\n \n             hidden_states = layer_outputs[0]\n \n@@ -753,7 +739,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         output_hidden_states_before_downsampling: Optional[bool] = False,\n@@ -773,11 +758,7 @@ def forward(\n             all_reshaped_hidden_states += (reshaped_hidden_state,)\n \n         for i, layer_module in enumerate(self.layers):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n-                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-            )\n+            layer_outputs = layer_module(hidden_states, input_dimensions, output_attentions, always_partition)\n \n             hidden_states = layer_outputs[0]\n             hidden_states_before_downsampling = layer_outputs[1]\n@@ -882,7 +863,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n@@ -901,21 +881,13 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, len(self.config.depths))\n-\n         embedding_output, input_dimensions = self.embeddings(\n             pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n         )\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n             input_dimensions,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -976,7 +948,6 @@ def __init__(self, config):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -993,7 +964,6 @@ def forward(\n \n         outputs = self.donut(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,"
        },
        {
            "sha": "d43279307aeb104ac7c809704aa07df743375f9d",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 26,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -320,9 +320,7 @@ def __init__(self, config: DPTConfig):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size = hidden_states.shape[0]\n         new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n \n@@ -339,7 +337,7 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n+            None,\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.dropout_prob,\n@@ -395,8 +393,8 @@ def prune_heads(self, heads: set[int]):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n-        self_attn_output, _ = self.attention(hidden_states, head_mask)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n         output = self.output(self_attn_output, hidden_states)\n         return output\n \n@@ -445,9 +443,9 @@ def __init__(self, config: DPTConfig):\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         hidden_states_norm = self.layernorm_before(hidden_states)\n-        attention_output = self.attention(hidden_states_norm, head_mask)\n+        attention_output = self.attention(hidden_states_norm)\n \n         # first residual connection\n         hidden_states = attention_output + hidden_states\n@@ -470,13 +468,10 @@ def __init__(self, config: DPTConfig):\n         self.layer = nn.ModuleList([DPTViTLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None, output_hidden_states: bool = False\n-    ) -> BaseModelOutput:\n+    def forward(self, hidden_states: torch.Tensor, output_hidden_states: bool = False) -> BaseModelOutput:\n         all_hidden_states = [hidden_states] if output_hidden_states else None\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            hidden_states = layer_module(hidden_states, layer_head_mask)\n+            hidden_states = layer_module(hidden_states)\n             if all_hidden_states:\n                 all_hidden_states.append(hidden_states)\n \n@@ -811,25 +806,17 @@ class PreTrainedModel\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> BaseModelOutputWithPoolingAndIntermediateActivations:\n         if output_hidden_states is None:\n             output_hidden_states = self.config.output_hidden_states\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output: BaseModelOutputWithIntermediateActivations = self.embeddings(pixel_values)\n         embedding_last_hidden_states = embedding_output.last_hidden_states\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n-            embedding_last_hidden_states, head_mask=head_mask, output_hidden_states=output_hidden_states\n+            embedding_last_hidden_states, output_hidden_states=output_hidden_states\n         )\n         sequence_output = encoder_outputs.last_hidden_state\n \n@@ -987,7 +974,6 @@ def __init__(self, config):\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n@@ -1040,7 +1026,7 @@ def forward(\n             outputs = self.backbone.forward_with_filtered_kwargs(pixel_values, output_hidden_states=True, **kwargs)\n             hidden_states = outputs.feature_maps\n         else:\n-            outputs = self.dpt(pixel_values, head_mask=head_mask, output_hidden_states=True, **kwargs)\n+            outputs = self.dpt(pixel_values, output_hidden_states=True, **kwargs)\n             hidden_states = outputs.hidden_states\n             # only keep certain features based on config.backbone_out_indices\n             # note that the hidden_states also include the initial embeddings\n@@ -1137,7 +1123,6 @@ def __init__(self, config: DPTConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n@@ -1171,7 +1156,7 @@ def forward(\n             raise ValueError(\"The number of labels should be greater than one\")\n \n         outputs: BaseModelOutputWithPoolingAndIntermediateActivations = self.dpt(\n-            pixel_values, head_mask=head_mask, output_hidden_states=True, **kwargs\n+            pixel_values, output_hidden_states=True, **kwargs\n         )\n         hidden_states = outputs.hidden_states\n "
        },
        {
            "sha": "921e545afc3517abc60f4b9f83984c835692ec0d",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -132,7 +132,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -173,9 +172,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -218,7 +214,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -262,7 +257,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -307,7 +301,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -355,7 +348,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -415,7 +407,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -427,7 +418,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -493,7 +483,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -503,7 +492,6 @@ def forward(\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -520,7 +508,6 @@ def forward(\n             cross_attention_output, _ = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -550,7 +537,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -559,12 +545,9 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -702,7 +685,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -761,17 +743,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -849,8 +823,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -875,8 +847,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -1047,7 +1017,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1063,7 +1032,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1127,7 +1095,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1168,7 +1135,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1231,7 +1197,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1247,7 +1212,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1300,7 +1264,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1314,7 +1277,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1360,7 +1322,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1371,7 +1332,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1430,7 +1390,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1482,7 +1441,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1541,7 +1499,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1581,7 +1538,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,"
        },
        {
            "sha": "29aa667d9b8a3d44d541e8f5cd127159b7052641",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -891,10 +891,10 @@ def __init__(self, config: EomtConfig) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         hidden_states_norm = self.norm1(hidden_states)\n-        self_attention_output, _ = self.attention(hidden_states_norm, head_mask)\n+        self_attention_output, _ = self.attention(hidden_states_norm, attention_mask)\n         self_attention_output = self.layer_scale1(self_attention_output)\n \n         # first residual connection"
        },
        {
            "sha": "807a130c764ade9be4a7319876869e47a73bd8b2",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -297,10 +297,10 @@ class EomtLayer(Dinov2Layer):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         hidden_states_norm = self.norm1(hidden_states)\n-        self_attention_output, _ = self.attention(hidden_states_norm, head_mask)\n+        self_attention_output, _ = self.attention(hidden_states_norm, attention_mask)\n         self_attention_output = self.layer_scale1(self_attention_output)\n \n         # first residual connection"
        },
        {
            "sha": "01a5d9dddd2a0f070adcefa66cc9e14e27f9553c",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -143,7 +143,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -184,9 +183,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -228,7 +224,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -272,7 +267,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -316,7 +310,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -364,7 +357,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             # only for relevant for non-absolute positional embeddings\n             use_cache=past_key_value is not None,\n             **kwargs,\n@@ -422,7 +414,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -434,7 +425,6 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -497,7 +487,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -507,7 +496,6 @@ def forward(\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_value=past_key_value,\n             cache_position=cache_position,\n             **kwargs,\n@@ -524,7 +512,6 @@ def forward(\n             cross_attention_output, _ = self.crossattention(\n                 self_attention_output,\n                 None,  # attention_mask\n-                head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_value=past_key_value,\n@@ -608,7 +595,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -617,12 +603,9 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_value=past_key_values,\n@@ -727,7 +710,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -830,17 +812,9 @@ def forward(\n                     )\n                 encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -920,8 +894,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -945,8 +917,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -1040,7 +1010,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         next_sentence_label: Optional[torch.Tensor] = None,\n@@ -1085,7 +1054,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1156,7 +1124,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1186,7 +1153,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1254,7 +1220,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1278,7 +1243,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1360,7 +1324,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1411,7 +1374,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1465,7 +1427,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1487,7 +1448,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1553,7 +1513,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1611,7 +1570,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1661,7 +1619,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1681,7 +1638,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1726,7 +1682,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1745,7 +1700,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "eba860e93185ac348ae2c7e6d80ef3f8b88c022a",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -212,7 +212,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -315,17 +314,9 @@ def forward(\n                     )\n                 encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n@@ -356,8 +347,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -382,8 +371,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -422,7 +409,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         next_sentence_label: Optional[torch.Tensor] = None,\n@@ -467,7 +453,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -502,7 +487,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -532,7 +516,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -577,7 +560,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -601,7 +583,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -635,7 +616,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -686,7 +666,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -719,7 +698,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -741,7 +719,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -793,7 +770,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -851,7 +827,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -886,7 +861,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -906,7 +880,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -940,7 +913,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         task_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -959,7 +931,6 @@ def forward(\n             token_type_ids=token_type_ids,\n             task_type_ids=task_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "76a0e178b0e25fbbd1ed8470596f90270786f69d",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -259,7 +259,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     # ESM applies relative position embeddings and we don't copy from Llama\n@@ -292,9 +291,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -340,7 +336,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -382,7 +377,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -433,7 +427,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -442,7 +435,6 @@ def forward(\n         attn_output, _ = self.self(\n             hidden_states_ln,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             **kwargs,\n@@ -495,15 +487,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -517,7 +507,6 @@ def forward(\n             attention_output = self.crossattention(\n                 attention_output,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 **kwargs,\n@@ -546,17 +535,14 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 **kwargs,\n@@ -683,7 +669,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -736,17 +721,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             inputs_embeds,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             **kwargs,\n@@ -804,7 +781,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -822,7 +798,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -896,7 +871,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -912,7 +886,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             **kwargs,\n         )\n@@ -973,7 +946,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -987,7 +959,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             **kwargs,\n         )"
        },
        {
            "sha": "75db8a22a022430b8a226dd9ceaa4efb7b086cd5",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -227,7 +227,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     # EVOLLA_SA_PROT applies relative position embeddings and we don't copy from Llama\n@@ -260,9 +259,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -308,7 +304,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -350,7 +345,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -401,7 +395,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -410,7 +403,6 @@ def forward(\n         attn_output, _ = self.self(\n             hidden_states_ln,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             **kwargs,\n@@ -470,15 +462,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -492,7 +482,6 @@ def forward(\n             attention_output = self.crossattention(\n                 attention_output,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 **kwargs,\n@@ -521,17 +510,14 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 **kwargs,"
        },
        {
            "sha": "b32981c51353a37f656e8c2fdf0d35e8bcc4ddaf",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 28,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -290,7 +290,6 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_ids: Optional[torch.LongTensor] = None,\n         layer_past: Optional[Cache] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -353,7 +352,7 @@ def forward(\n                 attention_scores /= math.sqrt(self.head_dim)\n \n                 attention_scores = F.softmax(attention_scores + attention_mask, dim=-1, dtype=hidden_states.dtype)\n-                # It is unclear why neither dropout nor head_mask is applied here (while it is with alibi).\n+                # It is unclear why dropout is not applied here (while it is with alibi).\n                 attn_output = attention_scores @ value_layer\n \n             attn_output = attn_output.view(batch_size, self.num_heads, query_length, self.head_dim)\n@@ -365,7 +364,7 @@ def forward(\n             return attn_output, attention_scores\n \n         else:\n-            if self.config._attn_implementation == \"sdpa\" and not output_attentions and head_mask is None:\n+            if self.config._attn_implementation == \"sdpa\" and not output_attentions:\n                 # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n                 # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n                 is_causal = self.is_causal and attention_mask is None and query_length > 1\n@@ -400,9 +399,6 @@ def forward(\n                 # [batch_size, num_heads, q_length, kv_length]\n                 attention_probs = self.attention_dropout(attention_probs)\n \n-                if head_mask is not None:\n-                    attention_probs = attention_probs * head_mask\n-\n                 # change view [batch_size, num_heads, q_length, kv_length]\n                 attention_probs_reshaped = attention_probs.view(batch_size, self.num_heads, query_length, kv_length)\n \n@@ -439,7 +435,6 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_ids: Optional[torch.LongTensor] = None,\n         layer_past: Optional[Cache] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -582,7 +577,6 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_ids: Optional[torch.LongTensor] = None,\n         layer_past: Optional[Union[Cache, tuple[torch.Tensor, torch.Tensor]]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -604,7 +598,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             alibi=alibi,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -716,7 +709,6 @@ def forward(\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -783,14 +775,9 @@ def forward(\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions, head_mask, alibi\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions, alibi\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape batch_size x num_heads x N x N\n-        # head_mask has shape n_layer x batch x num_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -808,7 +795,6 @@ def forward(\n                 layer_past=past_key_values,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 alibi=alibi,\n@@ -845,7 +831,6 @@ def _update_causal_mask(\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n         output_attentions: bool,\n-        head_mask: torch.Tensor,\n         alibi: torch.Tensor,\n     ):\n         # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n@@ -869,7 +854,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and not using_static_cache\n             and not output_attentions\n-            and head_mask is None\n             and alibi is None\n         ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n@@ -904,7 +888,7 @@ def _update_causal_mask(\n         )\n \n         # We take care to integrate alibi bias in the causal_mask here\n-        if head_mask is None and alibi is not None:\n+        if alibi is not None:\n             alibi = alibi.reshape(batch_size, -1, *alibi.shape[1:])\n             causal_mask = torch.masked_fill(\n                 alibi / math.sqrt(self.config.hidden_size // self.num_heads),\n@@ -1008,7 +992,6 @@ def forward(\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1044,7 +1027,6 @@ def forward(\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1109,7 +1091,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1141,7 +1122,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1235,7 +1215,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1267,7 +1246,6 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1314,7 +1292,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1340,7 +1317,6 @@ def forward(\n         outputs = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "5812aa457cbcb50bc416bbfed6471d208091d39d",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -116,7 +116,6 @@ def forward(\n         mask,\n         kv=None,\n         cache=None,\n-        head_mask=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -168,10 +167,6 @@ def forward(\n         weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)  # (bs, n_heads, qlen, klen)\n         weights = nn.functional.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            weights = weights * head_mask\n-\n         context = torch.matmul(weights, v)  # (bs, n_heads, qlen, head_dim)\n         context = context.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.head_dim)\n \n@@ -814,7 +809,6 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         lengths: Optional[torch.LongTensor] = None,\n         cache: Optional[dict[str, torch.FloatTensor]] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -900,9 +894,6 @@ def forward(\n             assert langs.size() == (bs, slen)  # (slen, bs)\n             # langs = langs.transpose(0, 1)\n \n-        # Prepare head mask if needed\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layers)\n-\n         # do not recompute cached elements\n         if cache is not None and input_ids is not None:\n             _slen = slen - cache.get_seq_length()\n@@ -945,7 +936,6 @@ def forward(\n                     tensor,\n                     attn_mask,\n                     cache=cache,\n-                    head_mask=head_mask[i],\n                     output_attentions=output_attentions,\n                     cache_position=cache_position,\n                 )\n@@ -957,7 +947,7 @@ def forward(\n                 tensor = self.layer_norm1[i](tensor)\n             else:\n                 tensor_normalized = self.layer_norm1[i](tensor)\n-                attn_outputs = self.attentions[i](tensor_normalized, attn_mask, cache=cache, head_mask=head_mask[i])\n+                attn_outputs = self.attentions[i](tensor_normalized, attn_mask, cache=cache[i])\n                 attn = attn_outputs[0]\n                 if output_attentions:\n                     attentions = attentions + (attn_outputs[1],)\n@@ -1032,7 +1022,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         lengths: Optional[torch.Tensor] = None,\n         cache: Optional[dict[str, torch.Tensor]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1072,7 +1061,6 @@ def forward(\n             position_ids=position_ids,\n             lengths=lengths,\n             cache=cache,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1122,7 +1110,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         lengths: Optional[torch.Tensor] = None,\n         cache: Optional[dict[str, torch.Tensor]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1160,7 +1147,6 @@ def forward(\n             position_ids=position_ids,\n             lengths=lengths,\n             cache=cache,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1229,7 +1215,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         lengths: Optional[torch.Tensor] = None,\n         cache: Optional[dict[str, torch.Tensor]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1265,7 +1250,6 @@ def forward(\n             position_ids=position_ids,\n             lengths=lengths,\n             cache=cache,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1321,7 +1305,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         lengths: Optional[torch.Tensor] = None,\n         cache: Optional[dict[str, torch.Tensor]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1356,7 +1339,6 @@ def forward(\n             position_ids=position_ids,\n             lengths=lengths,\n             cache=cache,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1457,7 +1439,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         lengths: Optional[torch.Tensor] = None,\n         cache: Optional[dict[str, torch.Tensor]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1521,7 +1502,6 @@ def forward(\n             position_ids=position_ids,\n             lengths=lengths,\n             cache=cache,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1578,7 +1558,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         lengths: Optional[torch.Tensor] = None,\n         cache: Optional[dict[str, torch.Tensor]] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1659,7 +1638,6 @@ def forward(\n             position_ids=position_ids,\n             lengths=lengths,\n             cache=cache,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "5d63b5e132adb397d4d3bc7a78aa5babf442ce5a",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 40,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -444,7 +444,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         batch_size, seq_length, _ = hidden_states.shape\n@@ -479,10 +478,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -541,11 +536,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         self_outputs = self.attention(\n-            hidden_states, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions\n+            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n         )\n \n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -606,13 +600,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -644,7 +636,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -656,9 +647,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -768,7 +757,6 @@ def forward(\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -786,21 +774,13 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n         )\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -863,7 +843,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -894,12 +873,6 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones(input_shape, device=input_ids.device)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n             attention_mask, input_shape, input_ids.device\n         )\n@@ -913,7 +886,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -971,7 +943,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -996,20 +967,13 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones((batch_size, seq_length), device=hidden_states.device)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n             attention_mask, (batch_size, seq_length), hidden_states.device\n         )\n \n         encoder_outputs = self.encoder(\n             hidden_states,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -1136,7 +1100,6 @@ def get_image_features(\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`):\n@@ -1169,7 +1132,6 @@ def get_image_features(\n             pixel_values=pixel_values,\n             bool_masked_pos=bool_masked_pos,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n         pooled_output = image_outputs.last_hidden_state"
        },
        {
            "sha": "9d1cb837e8ce867a707ccbaf4d639fcc46869a92",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -201,7 +201,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -213,9 +212,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -711,11 +707,8 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -749,7 +742,6 @@ def forward(\n \n             encoder_outputs = self.language_model.encoder(\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -766,8 +758,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -868,9 +858,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -933,9 +920,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,"
        },
        {
            "sha": "949e7f23f559ed7b9c4feed728ebb5b58620ebe4",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -1545,11 +1545,8 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1583,7 +1580,6 @@ def forward(\n \n             encoder_outputs = self.language_model.encoder(\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1600,8 +1596,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1652,9 +1646,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1717,9 +1708,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,"
        },
        {
            "sha": "7ac29b403905a5cfbea6c35ebe7e39d35cef8221",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 74,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -297,16 +297,14 @@ def __init__(self, config: FSMTConfig):\n         self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = LayerNorm(self.embed_dim)\n \n-    def forward(self, x, encoder_padding_mask, layer_head_mask, output_attentions=False):\n+    def forward(self, x, encoder_padding_mask, output_attentions=False):\n         \"\"\"\n         Args:\n             x (`torch.Tensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\n             encoder_padding_mask (`torch.ByteTensor`): binary ByteTensor of shape\n                 *(batch, src_len)* where padding elements are indicated by `1`.\n             for t_tgt, t_src is excluded (or masked out), =0 means it is\n             included in attention\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                *(config.encoder_attention_heads,)*.\n \n         Returns:\n             encoded output of shape *(seq_len, batch, embed_dim)*\n@@ -316,7 +314,6 @@ def forward(self, x, encoder_padding_mask, layer_head_mask, output_attentions=Fa\n             query=x,\n             key=x,\n             key_padding_mask=encoder_padding_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n@@ -359,7 +356,6 @@ def forward(\n         input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -371,11 +367,6 @@ def forward(\n             attention_mask (`torch.LongTensor`): indicating which indices are padding tokens\n             inputs_embeds (`torch.FloatTensor`):\n                 embedding vectors of shape *(batch, src_len, embed_dim)*\n-            head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n \n         Returns:\n             BaseModelOutput or Tuple comprised of:\n@@ -416,11 +407,6 @@ def forward(\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            assert head_mask.size()[0] == (len(self.layers)), (\n-                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n-            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 x = x.transpose(0, 1)  # T x B x C -> B x T x C\n@@ -434,7 +420,6 @@ def forward(\n                 x, attn = encoder_layer(\n                     x,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -487,8 +472,6 @@ def forward(\n         encoder_attn_mask=None,\n         layer_state=None,\n         causal_mask=None,\n-        layer_head_mask=None,\n-        cross_attn_layer_head_mask=None,\n         decoder_padding_mask=None,\n         output_attentions=False,\n         cache_position=None,\n@@ -502,7 +485,6 @@ def forward(\n             layer_state=layer_state,  # adds keys to layer state\n             key_padding_mask=decoder_padding_mask,\n             attn_mask=causal_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -518,7 +500,6 @@ def forward(\n             key=encoder_hidden_states,\n             key_padding_mask=encoder_attn_mask,\n             layer_state=layer_state,  # mutates layer state\n-            layer_head_mask=cross_attn_layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -583,9 +564,7 @@ def forward(\n         encoder_padding_mask: torch.Tensor,\n         decoder_padding_mask: torch.Tensor,\n         decoder_causal_mask: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n@@ -604,17 +583,6 @@ def forward(\n                 encoder-side attention\n             encoder_padding_mask: for ignoring pad tokens\n             past_key_values (dict or None): dictionary used for storing state during generation\n-            head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n \n         Returns:\n             BaseModelOutputWithPast or tuple:\n@@ -671,13 +639,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attns = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                assert attn_mask.size()[0] == (len(self.layers)), (\n-                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -696,8 +657,6 @@ def forward(\n                 decoder_padding_mask=decoder_padding_mask,\n                 layer_state=past_key_values,\n                 causal_mask=decoder_causal_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -773,7 +732,6 @@ def forward(\n         key_padding_mask: Optional[Tensor] = None,\n         layer_state: Optional[Cache] = None,\n         attn_mask: Optional[Tensor] = None,\n-        layer_head_mask: Optional[Tensor] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n@@ -847,13 +805,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n-            )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n         if output_attentions:\n             # make sure that attn_weights are included in graph\n             attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n@@ -918,9 +869,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n@@ -945,12 +893,6 @@ def forward(\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         if decoder_input_ids is None:\n             use_cache = False\n@@ -982,7 +924,6 @@ def forward(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 inputs_embeds=inputs_embeds,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -1003,8 +944,6 @@ def forward(\n             decoder_padding_mask,\n             decoder_causal_mask=causal_mask,\n             inputs_embeds=decoder_inputs_embeds,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1064,9 +1003,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1092,12 +1028,6 @@ def forward(\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1132,9 +1062,6 @@ def forward(\n             decoder_inputs_embeds=decoder_inputs_embeds,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "1b477dbb551abd54d384817bb204edf05a8491d8",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -760,7 +760,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -789,7 +788,6 @@ def forward(\n         if token_type_ids is None:\n             token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        # TODO: deal with head_mask\n         inputs_embeds = self.embeddings(input_ids, inputs_embeds=inputs_embeds)\n \n         encoder_outputs = self.encoder(\n@@ -856,7 +854,6 @@ def forward(\n         if token_type_ids is None:\n             token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        # TODO: deal with head_mask\n         inputs_embeds = self.embeddings(input_ids, inputs_embeds=inputs_embeds)\n \n         encoder_outputs = self.encoder("
        },
        {
            "sha": "c1e8237671350572e8dbd7ff1b8049859aad4bab",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -154,7 +154,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n@@ -222,10 +221,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -288,15 +283,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         attn_output, self_attn_weights = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             past_key_values,\n             output_attentions,\n             pixel_values_present,\n@@ -350,7 +343,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n@@ -359,7 +351,6 @@ def forward(\n         attention_output, self_attention_weights = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n             past_key_values=past_key_values,\n             pixel_values_present=pixel_values_present,\n@@ -387,7 +378,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n@@ -411,12 +401,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 past_key_values,\n                 output_attentions,\n                 pixel_values_present,\n@@ -1035,7 +1022,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -1093,13 +1079,6 @@ def forward(\n                 else past_key_values.get_seq_length()\n             )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         projected_visual_features = None\n         if pixel_values is not None:\n             if pixel_values.ndim == 4:\n@@ -1174,7 +1153,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             hidden_states,\n             attention_mask=combined_attention_mask,\n-            head_mask=head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1225,7 +1203,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.Tensor]]] = None,\n@@ -1376,7 +1353,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             pixel_values=pixel_values,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,"
        },
        {
            "sha": "b3e4bb51f408ba50c42b325ba6ff505b68c71d11",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 35,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -50,7 +50,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n+def eager_attention_forward(module, query, key, value, attention_mask, **kwargs):\n     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n \n     if module.scale_attn_weights:\n@@ -83,10 +83,6 @@ def eager_attention_forward(module, query, key, value, attention_mask, head_mask\n     attn_weights = attn_weights.type(value.dtype)\n     attn_weights = module.attn_dropout(attn_weights)\n \n-    # Mask heads if we want to\n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2)\n \n@@ -153,7 +149,7 @@ def prune_heads(self, heads):\n         self.num_heads = self.num_heads - len(heads)\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()\n         _, _, k_seq_len, _ = key.size()\n@@ -197,10 +193,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n         attn_weights = attn_weights.type(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n         attn_output = attn_output.transpose(1, 2)\n \n@@ -213,7 +205,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -279,7 +270,7 @@ def forward(\n \n         if using_eager and self.reorder_and_upcast_attn:\n             attn_output, attn_weights = self._upcast_and_reordered_attn(\n-                query_states, key_states, value_states, attention_mask, head_mask\n+                query_states, key_states, value_states, attention_mask\n             )\n         else:\n             attn_output, attn_weights = attention_interface(\n@@ -288,7 +279,6 @@ def forward(\n                 key_states,\n                 value_states,\n                 attention_mask,\n-                head_mask=head_mask,\n                 dropout=self.attn_dropout.p if self.training else 0.0,\n                 is_causal=is_causal,\n                 **kwargs,\n@@ -341,7 +331,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -355,7 +344,6 @@ def forward(\n             past_key_values=past_key_values,\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             **kwargs,\n@@ -376,7 +364,6 @@ def forward(\n                 hidden_states,\n                 past_key_values=past_key_values,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -617,7 +604,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -718,7 +704,7 @@ def forward(\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None\n+        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False\n         if self.config.add_cross_attention and encoder_hidden_states is not None:\n             encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n@@ -733,12 +719,6 @@ def forward(\n         else:\n             encoder_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # head_mask has shape n_layer x batch x n_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n-\n         if token_type_ids is not None:\n             token_type_embeds = self.wte(token_type_ids)\n             hidden_states = hidden_states + token_type_embeds\n@@ -759,7 +739,6 @@ def forward(\n                 past_key_values if not (self.gradient_checkpointing and self.training) else None,\n                 cache_position,\n                 causal_mask,\n-                head_mask[i],\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n@@ -824,7 +803,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -863,7 +841,6 @@ def forward(\n             cache_position=cache_position,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -931,7 +908,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         mc_token_ids: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1000,7 +976,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1074,7 +1049,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1108,7 +1082,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1203,7 +1176,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1237,7 +1209,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1285,7 +1256,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1314,7 +1284,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "543a6dca195c160b79afb7f6cefee5dfcaee5354",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 24,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -49,7 +49,7 @@\n \n # Fused kernels\n # Use separate functions for each case because conditionals prevent kernel fusion.\n-# TODO: Could have better fused kernels depending on scaling, dropout and head mask.\n+# TODO: Could have better fused kernels depending on scaling and dropout.\n #  Is it doable without writing 32 functions?\n @torch.jit.script\n def upcast_masked_softmax(\n@@ -97,7 +97,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n@@ -111,9 +110,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_output = torch.matmul(attn_weights, value_states)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n@@ -172,7 +168,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -243,7 +238,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attn_dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -298,7 +292,6 @@ def forward(\n         hidden_states: Optional[tuple[torch.Tensor]],\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -314,7 +307,6 @@ def forward(\n             hidden_states,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -337,7 +329,6 @@ def forward(\n             cross_attn_outputs = self.crossattention(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -434,7 +425,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -533,12 +523,6 @@ def forward(\n             else:\n                 encoder_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # head_mask has shape n_layer x batch x n_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n-\n         position_embeds = self.wpe(position_ids)\n         hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n \n@@ -561,7 +545,6 @@ def forward(\n                 hidden_states,\n                 past_key_values,\n                 causal_mask,\n-                head_mask[i],\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n@@ -617,7 +600,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -655,7 +637,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -724,7 +705,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -759,7 +739,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -857,7 +836,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -891,7 +869,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "67234abbda3673c8792da4d2ac3b86b0f3d83e95",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 27,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -116,7 +116,7 @@ def _merge_heads(self, tensor, num_heads, attn_head_size):\n         new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n         return tensor.view(new_shape)\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _attn(self, query, key, value, attention_mask=None):\n         # Keep the attention weights computation in fp32 to avoid overflow issues\n         query = query.to(torch.float32)\n         key = key.to(torch.float32)\n@@ -140,10 +140,6 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         attn_weights = attn_weights.to(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n \n         return attn_output, attn_weights\n@@ -153,7 +149,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         layer_past=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -170,7 +165,7 @@ def forward(\n             cache_kwargs = {\"cache_position\": cache_position}\n             key, value = layer_past.update(key, value, self.layer_id, cache_kwargs)\n \n-        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+        attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n         attn_output = self.out_proj(attn_output)\n@@ -199,7 +194,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         layer_past=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -309,7 +303,6 @@ def forward(\n         hidden_states,\n         layer_past=None,\n         attention_mask=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -318,7 +311,6 @@ def forward(\n             hidden_states,\n             attention_mask=attention_mask,\n             layer_past=layer_past,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -357,7 +349,6 @@ def forward(\n         hidden_states,\n         layer_past=None,\n         attention_mask=None,\n-        head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -368,7 +359,6 @@ def forward(\n             hidden_states,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -444,7 +434,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -501,11 +490,6 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x num_heads x N x N\n-        # head_mask has shape n_layer x batch x num_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         position_embeds = self.wpe(position_ids)\n         hidden_states = inputs_embeds + position_embeds\n \n@@ -527,7 +511,6 @@ def forward(\n                 hidden_states,\n                 layer_past=past_key_values,\n                 attention_mask=causal_mask,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -707,7 +690,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -743,7 +725,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -817,7 +798,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -851,7 +831,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -940,7 +919,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -974,7 +952,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1022,7 +999,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1051,7 +1027,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "07072c077089ef12f92d32cc5cdfe0847917dfb5",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -102,7 +102,6 @@ def eager_attention_forward(\n     attention_mask: torch.Tensor,\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -113,10 +112,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n \n-    # Mask heads if we want to\n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n \n@@ -144,7 +139,6 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -183,7 +177,6 @@ def forward(\n             attention_mask,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.attention_dropout,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -210,7 +203,6 @@ def forward(\n         hidden_states: Optional[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n@@ -223,7 +215,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             layer_past=layer_past,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -400,7 +391,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n@@ -449,18 +439,6 @@ def forward(\n             position_ids=position_ids,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        converted_head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-        # Flex Attention converts it to a separate mask\n-        if head_mask is not None:\n-            converted_head_mask = ~converted_head_mask.bool() * torch.finfo(inputs_embeds.dtype).min\n-            converted_head_mask = converted_head_mask.to(dtype=self.dtype, device=self.device)\n-        head_mask = converted_head_mask\n-\n         hidden_states = self.emb_dropout(inputs_embeds)\n \n         # create position embeddings to be shared across the decoder layers\n@@ -476,7 +454,6 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                head_mask=head_mask[i],\n                 layer_past=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -541,7 +518,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -578,7 +554,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n@@ -638,7 +613,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -656,7 +630,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n@@ -719,7 +692,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -738,7 +710,6 @@ def forward(\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -780,7 +751,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -791,7 +761,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "82ab8170d2b4ed7c72e6bf3f62d7c7ece863ddcc",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -85,7 +85,6 @@ def eager_attention_forward(\n     attention_mask: torch.Tensor,\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -96,10 +95,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n \n-    # Mask heads if we want to\n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n \n@@ -127,7 +122,6 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -166,7 +160,6 @@ def forward(\n             attention_mask,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.attention_dropout,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -193,7 +186,6 @@ def forward(\n         hidden_states: Optional[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n@@ -206,7 +198,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             layer_past=layer_past,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -277,7 +268,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n@@ -326,18 +316,6 @@ def forward(\n             position_ids=position_ids,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        converted_head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-        # Flex Attention converts it to a separate mask\n-        if head_mask is not None:\n-            converted_head_mask = ~converted_head_mask.bool() * torch.finfo(inputs_embeds.dtype).min\n-            converted_head_mask = converted_head_mask.to(dtype=self.dtype, device=self.device)\n-        head_mask = converted_head_mask\n-\n         hidden_states = self.emb_dropout(inputs_embeds)\n \n         # create position embeddings to be shared across the decoder layers\n@@ -353,7 +331,6 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                head_mask=head_mask[i],\n                 layer_past=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -412,7 +389,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -449,7 +425,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n@@ -509,7 +484,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -527,7 +501,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n@@ -590,7 +563,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -609,7 +581,6 @@ def forward(\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -651,7 +622,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -662,7 +632,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "a930070bfba7529be0e746986cdf434d90d5a709",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 19,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -98,7 +98,6 @@ def forward(\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n         position_ids: torch.LongTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         layer_past: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n@@ -142,7 +141,7 @@ def forward(\n             key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)\n \n         # Compute attention\n-        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+        attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n \n         # Reshape outputs\n         attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n@@ -175,7 +174,7 @@ def _merge_heads(cls, tensor, num_attention_heads, attn_head_size):\n         # -> [bs, seq_len, hidden_size]\n         return tensor\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _attn(self, query, key, value, attention_mask=None):\n         # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n         # compute causal mask from causal mask buffer\n         batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n@@ -209,10 +208,6 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         attn_weights = self.attention_dropout(attn_weights)\n         attn_weights = attn_weights.to(value.dtype)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n         return attn_output, attn_weights\n \n@@ -344,7 +339,6 @@ def forward(\n         hidden_states: Optional[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n@@ -357,7 +351,6 @@ def forward(\n             ln_out,\n             attention_mask=attention_mask,\n             layer_past=layer_past,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             position_ids=position_ids,\n@@ -411,7 +404,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -464,12 +456,6 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -485,7 +471,6 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                head_mask=head_mask[i],\n                 layer_past=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -670,7 +655,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -709,7 +693,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,"
        },
        {
            "sha": "8e09ce9ead440410a46c4941d40ceeda0417e0ba",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -150,7 +150,6 @@ def _attn(\n         key,\n         value,\n         attention_mask=None,\n-        head_mask=None,\n     ):\n         # Keep the attention weights computation in fp32 to avoid overflow issues\n         query = query.to(torch.float32)\n@@ -167,10 +166,6 @@ def _attn(\n         attn_weights = attn_weights.to(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n \n         return attn_output, attn_weights\n@@ -188,7 +183,6 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -244,7 +238,7 @@ def forward(\n             key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)\n \n         # compute self-attention: V x Softmax(QK^T)\n-        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+        attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n \n         attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)\n         attn_output = self.out_proj(attn_output)\n@@ -274,7 +268,6 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -436,7 +429,6 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -448,7 +440,6 @@ def forward(\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -519,7 +510,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -570,11 +560,6 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x num_attention_heads x N x N\n-        # head_mask has shape n_layer x batch x num_attention_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n         hidden_states = inputs_embeds\n \n         if token_type_ids is not None:\n@@ -596,7 +581,6 @@ def forward(\n                 layer_past=past_key_values,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -773,7 +757,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -801,7 +784,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -875,7 +857,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -901,7 +882,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -988,7 +968,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1009,7 +988,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "499c0b454600d857775ea255407b9fef96ec4bee",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 5,
            "deletions": 37,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -364,7 +364,6 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input should be of shape [batch, tokens, channels].\"\"\"\n@@ -388,10 +387,6 @@ def forward(\n         attn_weights = (query * self.scale) @ key.transpose(-1, -2)\n         attn_weights = attn_weights.softmax(dim=-1)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = attn_weights @ value\n         attn_output = attn_output.transpose(1, 3).reshape(batch_size, -1, self.hidden_size_output)\n         attn_output = self.proj(attn_output)\n@@ -482,7 +477,6 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         batch_size, seq_len, _ = hidden_states.shape\n@@ -495,9 +489,7 @@ def forward(\n                 hidden_states.view(batch_size, self.query_stride, -1, self.hidden_size_output).max(dim=1).values\n             )\n \n-        (hidden_states_norm, attn_weights) = self.attn(\n-            hidden_states_norm, head_mask, output_attentions=output_attentions\n-        )\n+        (hidden_states_norm, attn_weights) = self.attn(hidden_states_norm, output_attentions=output_attentions)\n         hidden_states = hidden_states + self.drop_path(hidden_states_norm)\n \n         residual = hidden_states\n@@ -547,13 +539,10 @@ def __init__(\n         )\n \n     def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.FloatTensor], output_attentions: bool = False\n+        self, hidden_states: torch.Tensor, output_attentions: bool = False\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         for i, layer_module in enumerate(self.layers):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            (hidden_states, attn_weights) = layer_module(\n-                hidden_states, layer_head_mask, output_attentions=output_attentions\n-            )\n+            (hidden_states, attn_weights) = layer_module(hidden_states, output_attentions=output_attentions)\n \n         return hidden_states, attn_weights\n \n@@ -685,7 +674,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -700,9 +688,7 @@ def forward(\n             all_reshaped_hidden_states = all_reshaped_hidden_states + (reshaped_hidden_states,)\n \n         for i, stage_module in enumerate(self.stages):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = stage_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = stage_module(hidden_states, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -863,7 +849,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         noise: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n@@ -882,13 +867,6 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, len(self.config.depths))\n-\n         embedding_output, bool_masked_pos, ids_restore = self.embeddings(\n             pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, noise=noise\n         )\n@@ -912,7 +890,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             hidden_states,\n             bool_masked_pos=bool_masked_pos,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -984,7 +961,6 @@ def forward(\n         self,\n         encoder_hidden_states: torch.Tensor,\n         bool_masked_pos: torch.BoolTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> tuple[torch.Tensor, torch.BoolTensor]:\n         # Embed tokens\n@@ -1034,9 +1010,7 @@ def forward(\n         hidden_states = hidden_states + self.decoder_position_embeddings\n \n         # Apply decoder blocks\n-        hidden_states, attn_weights = self.decoder_block(\n-            hidden_states, head_mask=head_mask, output_attentions=output_attentions\n-        )\n+        hidden_states, attn_weights = self.decoder_block(hidden_states, output_attentions=output_attentions)\n         hidden_states = self.decoder_norm(hidden_states)\n \n         # Predictor projection\n@@ -1160,7 +1134,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         noise: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n@@ -1200,7 +1173,6 @@ def forward(\n         outputs = self.hiera(\n             pixel_values,\n             noise=noise,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=True,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -1219,7 +1191,6 @@ def forward(\n         logits, bool_masked_pos = self.decoder(\n             fused_hidden_states,\n             bool_masked_pos=bool_masked_pos,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n         )\n \n@@ -1279,7 +1250,6 @@ def __init__(self, config: HieraConfig) -> None:\n     def forward(\n         self,\n         pixel_values,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1300,7 +1270,6 @@ def forward(\n \n         outputs = self.hiera(\n             pixel_values,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -1398,7 +1367,6 @@ def forward(\n \n         outputs = self.encoder(\n             embedding_output,\n-            head_mask=None,\n             output_attentions=output_attentions,\n             output_hidden_states=True,\n             return_dict=return_dict,"
        },
        {
            "sha": "c792d0431444154e060a2ec630434279013b1236",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -244,7 +244,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -256,9 +255,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -305,7 +301,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n@@ -344,7 +339,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -493,8 +487,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -661,8 +653,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":"
        },
        {
            "sha": "761fd515acc6b26de9988fce3ec30c148f8d32db",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -228,7 +228,6 @@ def forward(\n         hidden_states,\n         hidden_states_scaling_factor,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         # Projection\n@@ -277,10 +276,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         if attention_probs_scaling_factor is not None:\n             context_layer_scaling_factor = attention_probs_scaling_factor * value_layer_scaling_factor\n@@ -384,14 +379,12 @@ def forward(\n         hidden_states,\n         hidden_states_scaling_factor,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         self_outputs, self_outputs_scaling_factor = self.self(\n             hidden_states,\n             hidden_states_scaling_factor,\n             attention_mask,\n-            head_mask,\n             output_attentions,\n         )\n         attention_output, attention_output_scaling_factor = self.output(\n@@ -502,14 +495,12 @@ def forward(\n         hidden_states,\n         hidden_states_scaling_factor,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         self_attention_outputs, self_attention_outputs_scaling_factor = self.attention(\n             hidden_states,\n             hidden_states_scaling_factor,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -553,7 +544,6 @@ def forward(\n         hidden_states,\n         hidden_states_scaling_factor,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -566,13 +556,10 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 hidden_states_scaling_factor,\n                 attention_mask,\n-                layer_head_mask,\n                 output_attentions,\n             )\n \n@@ -692,7 +679,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -726,13 +712,6 @@ def forward(\n         # ourselves in which case we just need to make it broadcastable to all heads.\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output, embedding_output_scaling_factor = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -743,7 +722,6 @@ def forward(\n             embedding_output,\n             embedding_output_scaling_factor,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -790,7 +768,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -810,7 +787,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -891,7 +867,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -911,7 +886,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -974,7 +948,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1028,7 +1001,6 @@ def forward(\n             position_ids=flat_position_ids,\n             token_type_ids=flat_token_type_ids,\n             attention_mask=flat_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1077,7 +1049,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1095,7 +1066,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1162,7 +1132,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1177,7 +1146,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "2a15c40da4d301f16e23454f3d8ac3988fe0178c",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 9,
            "deletions": 22,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -192,9 +192,7 @@ def __init__(self, config: IJepaConfig):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size = hidden_states.shape[0]\n         new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n \n@@ -211,7 +209,7 @@ def forward(\n             query_layer,\n             key_layer,\n             value_layer,\n-            head_mask,\n+            None,\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.dropout_prob,\n@@ -265,8 +263,8 @@ def prune_heads(self, heads: set[int]):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n-        self_attn_output, _ = self.attention(hidden_states, head_mask)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n         output = self.output(self_attn_output, hidden_states)\n         return output\n \n@@ -312,9 +310,9 @@ def __init__(self, config: IJepaConfig):\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         hidden_states_norm = self.layernorm_before(hidden_states)\n-        attention_output = self.attention(hidden_states_norm, head_mask)\n+        attention_output = self.attention(hidden_states_norm)\n \n         # first residual connection\n         hidden_states = attention_output + hidden_states\n@@ -375,10 +373,9 @@ def __init__(self, config: IJepaConfig):\n         self.layer = nn.ModuleList([IJepaLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None) -> BaseModelOutput:\n+    def forward(self, hidden_states: torch.Tensor) -> BaseModelOutput:\n         for i, layer_module in enumerate(self.layer):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-            hidden_states = layer_module(hidden_states, layer_head_mask)\n+            hidden_states = layer_module(hidden_states)\n \n         return BaseModelOutput(last_hidden_state=hidden_states)\n \n@@ -435,7 +432,6 @@ def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n@@ -447,13 +443,6 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n         expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype\n         if pixel_values.dtype != expected_dtype:\n@@ -463,7 +452,7 @@ def forward(\n             pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n         )\n \n-        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, head_mask=head_mask)\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output)\n \n         sequence_output = encoder_outputs.last_hidden_state\n         sequence_output = self.layernorm(sequence_output)\n@@ -504,7 +493,6 @@ def __init__(self, config: IJepaConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -518,7 +506,6 @@ def forward(\n \n         outputs: BaseModelOutputWithPooling = self.ijepa(\n             pixel_values,\n-            head_mask=head_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             **kwargs,\n         )"
        },
        {
            "sha": "b37bc41d13bf95f9e35da94b65287469cdee2ccd",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -146,7 +146,6 @@ def __init__(self, config: IJepaConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -160,7 +159,6 @@ def forward(\n \n         outputs: BaseModelOutputWithPooling = self.ijepa(\n             pixel_values,\n-            head_mask=head_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             **kwargs,\n         )"
        },
        {
            "sha": "7536b2812b288186f189e254b630121fb8537b81",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 28,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -115,7 +115,7 @@ def prune_heads(self, heads):\n         self.num_heads = self.num_heads - len(heads)\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _attn(self, query, key, value, attention_mask=None):\n         attn_weights = torch.matmul(query, key.transpose(-1, -2))\n \n         if self.scale_attn_weights:\n@@ -145,15 +145,11 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         attn_weights = attn_weights.type(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n \n         return attn_output, attn_weights\n \n-    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n+    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()\n         _, _, k_seq_len, _ = key.size()\n@@ -197,10 +193,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n         attn_weights = attn_weights.type(value.dtype)\n         attn_weights = self.attn_dropout(attn_weights)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n         attn_output = torch.matmul(attn_weights, value)\n \n         return attn_output, attn_weights\n@@ -226,7 +218,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -281,9 +272,9 @@ def forward(\n         query = query.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n \n         if self.reorder_and_upcast_attn:\n-            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n+            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask)\n         else:\n-            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+            attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n         attn_output = self.c_proj(attn_output)\n@@ -330,7 +321,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -343,7 +333,6 @@ def forward(\n             hidden_states,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -366,7 +355,6 @@ def forward(\n                 hidden_states,\n                 layer_past=layer_past,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -461,7 +449,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -580,12 +567,6 @@ def forward(\n         else:\n             encoder_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # head_mask has shape n_layer x batch x n_heads x N x N\n-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n         position_embeds = self.wpe(position_ids)\n@@ -609,7 +590,6 @@ def forward(\n                 hidden_states,\n                 past_key_values,\n                 attention_mask,\n-                head_mask[i],\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n@@ -671,7 +651,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -742,7 +721,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -803,7 +781,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -853,7 +830,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "1906870e0c69bab1599a69966b5a0e0988a90a51",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 99,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -272,8 +272,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -297,8 +295,6 @@ def _update_causal_mask(\n             # 2d mask is passed through the layers\n             attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n         elif self.config._attn_implementation == \"sdpa\":\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n             attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n                 attention_mask,\n                 input_shape,\n@@ -338,9 +334,6 @@ def _update_cross_attn_mask(\n             if self.config._attn_implementation == \"flash_attention_2\":\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n                     inputs_embeds.dtype,\n@@ -370,7 +363,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -382,9 +374,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -441,7 +430,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -510,7 +498,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -566,7 +553,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -682,15 +668,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, u, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n-\n         if output_attentions:\n             # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n@@ -796,16 +773,13 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -814,7 +788,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -891,8 +864,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -907,10 +878,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -926,7 +893,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -943,7 +909,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -1007,7 +972,6 @@ def __init__(self, config: InformerConfig):\n     def forward(\n         self,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1022,12 +986,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -1061,14 +1019,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != (len(self.layers)):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n-\n         for idx, (encoder_layer, conv_layer) in enumerate(zip(self.layers, self.conv_layers)):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -1085,7 +1035,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n                 if conv_layer is not None:\n@@ -1139,8 +1088,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1169,19 +1116,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1262,15 +1196,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != (len(self.layers)):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n-\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1285,8 +1210,6 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1472,9 +1395,6 @@ def forward(\n         future_values: Optional[torch.Tensor] = None,\n         future_time_features: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1567,11 +1487,6 @@ def forward(\n             must but known at prediction time.\n \n             The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n@@ -1626,7 +1541,6 @@ def forward(\n             enc_input = transformer_inputs[:, : self.config.context_length, ...]\n             encoder_outputs = self.encoder(\n                 inputs_embeds=enc_input,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -1653,8 +1567,6 @@ def forward(\n             inputs_embeds=dec_input,\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1766,9 +1678,6 @@ def forward(\n         future_time_features: Optional[torch.Tensor] = None,\n         future_observed_mask: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1869,11 +1778,6 @@ def forward(\n             - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n \n             This mask is used to filter out missing values for the final loss calculation.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n@@ -1938,9 +1842,6 @@ def forward(\n             future_values=future_values,\n             future_time_features=future_time_features,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             past_key_values=past_key_values,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "955b463cd15e656af028ca76155fedffc173be36",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -113,8 +113,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -138,8 +136,6 @@ def _update_causal_mask(\n             # 2d mask is passed through the layers\n             attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n         elif self.config._attn_implementation == \"sdpa\":\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n             attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n                 attention_mask,\n                 input_shape,\n@@ -179,9 +175,6 @@ def _update_cross_attn_mask(\n             if self.config._attn_implementation == \"flash_attention_2\":\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n                     inputs_embeds.dtype,\n@@ -253,7 +246,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -369,15 +361,6 @@ def forward(\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, u, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n-\n         if output_attentions:\n             # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n@@ -531,7 +514,6 @@ def __init__(self, config: InformerConfig):\n     def forward(\n         self,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -546,12 +528,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -585,14 +561,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != (len(self.layers)):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n-\n         for idx, (encoder_layer, conv_layer) in enumerate(zip(self.layers, self.conv_layers)):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -609,7 +577,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n                 if conv_layer is not None:\n@@ -760,11 +727,6 @@ def forward(self, **super_kwargs):\n             must but known at prediction time.\n \n             The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n@@ -920,11 +882,6 @@ def forward(self, **super_kwargs):\n             - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n \n             This mask is used to filter out missing values for the final loss calculation.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of"
        },
        {
            "sha": "5cddbcdfd3bf340c9f0a78d74c3abf9d209cc84a",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -219,7 +219,6 @@ def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -285,15 +284,13 @@ def __init__(self, config: InstructBlipConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n-            head_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + residual\n@@ -366,14 +363,12 @@ def __init__(self, config: InstructBlipConfig):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n \n@@ -483,7 +478,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -542,10 +536,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs_dropped = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs_dropped = attention_probs_dropped * head_mask\n-\n         context_layer = torch.matmul(attention_probs_dropped, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -600,15 +590,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         attn_output, _ = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             **kwargs,\n@@ -673,7 +661,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         query_length=0,\n@@ -682,7 +669,6 @@ def forward(\n         attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -695,7 +681,6 @@ def forward(\n                 query_attention_output = self.crossattention(\n                     query_attention_output,\n                     attention_mask=attention_mask,\n-                    head_mask=head_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     **kwargs,\n@@ -751,20 +736,17 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         query_length=0,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n \n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 query_length=query_length,\n@@ -923,7 +905,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         query_embeds: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -974,17 +955,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs: BaseModelOutput = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             query_length=query_length,"
        },
        {
            "sha": "abcaa17f70f7e4d3ec463dcb08fcd0db38ab5ff7",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -229,7 +229,6 @@ def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -293,15 +292,13 @@ def __init__(self, config: InstructBlipVideoConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n-            head_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + residual\n@@ -334,14 +331,12 @@ def __init__(self, config: InstructBlipVideoConfig):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n \n@@ -450,7 +445,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -509,10 +503,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs_dropped = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs_dropped = attention_probs_dropped * head_mask\n-\n         context_layer = torch.matmul(attention_probs_dropped, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -565,15 +555,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         attn_output, _ = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n             **kwargs,\n@@ -636,7 +624,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         query_length=0,\n@@ -645,7 +632,6 @@ def forward(\n         attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -658,7 +644,6 @@ def forward(\n                 query_attention_output = self.crossattention(\n                     query_attention_output,\n                     attention_mask=attention_mask,\n-                    head_mask=head_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     **kwargs,\n@@ -713,20 +698,17 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         query_length=0,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n \n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 query_length=query_length,\n@@ -885,7 +867,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         query_embeds: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -936,17 +917,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs: BaseModelOutput = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             query_length=query_length,"
        },
        {
            "sha": "00e0dbbc6c81dce7de819240a8dcf4942e6f421d",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -459,7 +459,6 @@ def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        head_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -523,15 +522,13 @@ def __init__(self, config: JanusConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n-            head_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + residual"
        },
        {
            "sha": "de6bc098f58c05b73974377d114d063c63b1c23b",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -707,7 +707,6 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n@@ -842,8 +841,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -857,7 +854,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n             **kwargs,\n@@ -881,7 +877,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -1000,8 +995,6 @@ def forward(\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -1081,15 +1074,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != (len(self.layers)):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n-\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1104,8 +1088,6 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1253,8 +1235,6 @@ def forward(\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -1274,11 +1254,6 @@ def forward(\n \n             - 1 for places where to put the image features,\n             - 0 for places that are not for image features (i.e. for text tokens).\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         return self.model(\n             input_ids=input_ids,\n@@ -1287,8 +1262,6 @@ def forward(\n             image_embeds_position_mask=image_embeds_position_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             position_ids=position_ids,\n@@ -1336,8 +1309,6 @@ def forward(\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -1358,11 +1329,6 @@ def forward(\n \n             - 1 for places where to put the image features,\n             - 0 for places that are not for image features (i.e. for text tokens).\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n@@ -1382,8 +1348,6 @@ def forward(\n             image_embeds_position_mask=image_embeds_position_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             position_ids=position_ids,\n@@ -1556,7 +1520,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1628,7 +1591,6 @@ def forward(\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n             image_embeds_position_mask=image_embeds_position_mask,\n-            head_mask=head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             position_ids=position_ids,\n@@ -1692,7 +1654,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1782,7 +1743,6 @@ def forward(\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n             image_embeds_position_mask=image_embeds_position_mask,\n-            head_mask=head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             position_ids=position_ids,"
        },
        {
            "sha": "61444e9b3c4cbb58e8e4757cb6dc4484c6096dae",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -127,7 +127,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -138,9 +137,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n@@ -173,7 +169,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -196,7 +191,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -250,14 +244,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -311,14 +303,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -351,7 +341,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -364,12 +353,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 **kwargs,\n             )\n@@ -516,7 +502,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -591,16 +576,6 @@ def forward(\n         extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)\n         extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n \n-        if head_mask is not None:\n-            if head_mask.dim() == 1:\n-                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n-                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n-            elif head_mask.dim() == 2:\n-                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n-            head_mask = head_mask.to(dtype=next(self.parameters()).dtype)\n-        else:\n-            head_mask = [None] * self.config.num_hidden_layers\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             bbox=bbox,\n@@ -611,7 +586,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n@@ -659,7 +633,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -722,7 +695,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -777,7 +749,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -840,7 +811,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -913,7 +883,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -974,7 +943,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1027,7 +995,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1092,7 +1059,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "4df6a46cf88c2e573ccad2bd14b1f15d90d5ea32",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -143,7 +143,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         rel_pos=None,\n         rel_2d_pos=None,\n@@ -171,10 +170,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n@@ -194,15 +189,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         rel_pos=None,\n         rel_2d_pos=None,\n     ):\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions,\n             rel_pos=rel_pos,\n             rel_2d_pos=rel_2d_pos,\n@@ -270,15 +263,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         rel_pos=None,\n         rel_2d_pos=None,\n     ):\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n             rel_pos=rel_pos,\n             rel_2d_pos=rel_2d_pos,\n@@ -413,7 +404,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -430,12 +420,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 output_attentions,\n                 rel_pos=rel_pos,\n                 rel_2d_pos=rel_2d_pos,\n@@ -717,7 +704,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -820,22 +806,11 @@ def forward(\n         extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)\n         extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n \n-        if head_mask is not None:\n-            if head_mask.dim() == 1:\n-                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n-                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n-            elif head_mask.dim() == 2:\n-                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n-            head_mask = head_mask.to(dtype=next(self.parameters()).dtype)\n-        else:\n-            head_mask = [None] * self.config.num_hidden_layers\n-\n         encoder_outputs = self.encoder(\n             final_emb,\n             extended_attention_mask,\n             bbox=final_bbox,\n             position_ids=final_position_ids,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -885,7 +860,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -999,7 +973,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1090,7 +1063,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1180,7 +1152,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1242,7 +1213,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1331,7 +1301,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "f4c58096735a14bf66beac553a699e981096b696",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -258,7 +258,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         rel_pos=None,\n         rel_2d_pos=None,\n@@ -302,10 +301,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -343,15 +338,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         rel_pos=None,\n         rel_2d_pos=None,\n     ):\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions,\n             rel_pos=rel_pos,\n             rel_2d_pos=rel_2d_pos,\n@@ -375,15 +368,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         rel_pos=None,\n         rel_2d_pos=None,\n     ):\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n             rel_pos=rel_pos,\n             rel_2d_pos=rel_2d_pos,\n@@ -498,7 +489,6 @@ def forward(\n         hidden_states,\n         bbox=None,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -516,12 +506,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 output_attentions,\n                 rel_pos=rel_pos,\n                 rel_2d_pos=rel_2d_pos,\n@@ -680,7 +667,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -839,19 +825,11 @@ def forward(\n             attention_mask, None, device, dtype=embedding_output.dtype\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             bbox=final_bbox,\n             position_ids=final_position_ids,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -928,7 +906,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -975,7 +952,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1028,7 +1004,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1079,7 +1054,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1149,7 +1123,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1195,7 +1168,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "898edad049c4d4e8ee0e14c2b0c89169c10b1498",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 130,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -130,7 +130,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        layer_head_mask=None,\n         is_index_masked=None,\n         is_index_global_attn=None,\n         is_global_attn=None,\n@@ -223,12 +222,6 @@ def forward(\n             attn_scores, dim=-1, dtype=torch.float32\n         )  # use fp32 for numerical stability\n \n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n-            )\n-            attn_probs = layer_head_mask.view(1, 1, -1, 1) * attn_probs\n-\n         # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n         attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n         attn_probs = attn_probs.type_as(attn_scores)\n@@ -266,7 +259,6 @@ def forward(\n             global_attn_output, global_attn_probs = self._compute_global_attn_output_from_hidden(\n                 hidden_states=hidden_states,\n                 max_num_global_attn_indices=max_num_global_attn_indices,\n-                layer_head_mask=layer_head_mask,\n                 is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero,\n                 is_index_global_attn_nonzero=is_index_global_attn_nonzero,\n                 is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero,\n@@ -620,7 +612,6 @@ def _compute_global_attn_output_from_hidden(\n         self,\n         hidden_states,\n         max_num_global_attn_indices,\n-        layer_head_mask,\n         is_local_index_global_attn_nonzero,\n         is_index_global_attn_nonzero,\n         is_local_index_no_global_attn_nonzero,\n@@ -689,18 +680,6 @@ def _compute_global_attn_output_from_hidden(\n             global_attn_scores, dim=-1, dtype=torch.float32\n         )  # use fp32 for numerical stability\n \n-        # apply layer head masking\n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n-            )\n-            global_attn_probs_float = layer_head_mask.view(1, -1, 1, 1) * global_attn_probs_float.view(\n-                batch_size, self.num_heads, max_num_global_attn_indices, seq_len\n-            )\n-            global_attn_probs_float = global_attn_probs_float.view(\n-                batch_size * self.num_heads, max_num_global_attn_indices, seq_len\n-            )\n-\n         global_attn_probs = nn.functional.dropout(\n             global_attn_probs_float.type_as(global_attn_scores), p=self.dropout, training=self.training\n         )\n@@ -735,7 +714,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         is_index_masked: Optional[torch.Tensor] = None,\n         is_index_global_attn: Optional[torch.Tensor] = None,\n         is_global_attn: Optional[bool] = None,\n@@ -746,7 +724,6 @@ def forward(\n         self_outputs = self.longformer_self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             is_index_masked=is_index_masked,\n             is_index_global_attn=is_index_global_attn,\n             is_global_attn=is_global_attn,\n@@ -797,7 +774,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n@@ -868,14 +844,6 @@ def forward(\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n         if output_attentions:\n             # this operation is a bit awkward, but it's required to\n@@ -925,7 +893,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_head_mask: torch.Tensor,\n         is_index_masked=None,\n         is_index_global_attn=None,\n         is_global_attn=None,\n@@ -936,14 +903,11 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape *(batch, seq_len, embed_dim)*\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                *(encoder_attention_heads,)*.\n         \"\"\"\n         residual = hidden_states\n         attn_outputs = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             is_index_masked=is_index_masked,\n             is_index_global_attn=is_index_global_attn,\n             is_global_attn=is_global_attn,\n@@ -1006,8 +970,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -1022,10 +984,6 @@ def forward(\n                 cross attention input to the layer of shape *(batch, seq_len, embed_dim)*\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                *(decoder_attention_heads,)*.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for encoder attention heads in a given layer of\n-                size *(decoder_attention_heads,)*.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`): Whether the base model outputs attentions.\n                 This requires the attentions tensor to be reshaped in this function.\n@@ -1037,7 +995,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -1055,7 +1012,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -1436,7 +1392,6 @@ def forward(\n         input_ids=None,\n         attention_mask=None,\n         global_attention_mask=None,\n-        head_mask=None,\n         inputs_embeds=None,\n         output_attentions=None,\n         output_hidden_states=None,\n@@ -1469,11 +1424,6 @@ def forward(\n \n                 - 0 for local attention (a sliding window attention),\n                 - 1 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -1545,13 +1495,6 @@ def forward(\n         all_attentions = () if output_attentions else None\n         all_global_attentions = () if (output_attentions and is_global_attn) else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -1564,7 +1507,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask=attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     is_index_masked=is_index_masked,\n                     is_index_global_attn=is_index_global_attn,\n                     is_global_attn=is_global_attn,\n@@ -1644,8 +1586,6 @@ def forward(\n         global_attention_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        head_mask=None,\n-        cross_attn_head_mask=None,\n         past_key_values=None,\n         inputs_embeds=None,\n         use_cache=None,\n@@ -1692,18 +1632,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1799,14 +1727,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != len(self.layers):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1821,8 +1741,6 @@ def forward(\n                 combined_attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1887,9 +1805,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         global_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1919,12 +1834,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_led._prepare_decoder_inputs`] and modify\n             to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more information on the\n             default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to decide the attention given on each token, local attention or global attention for the encoder.\n             Tokens with global attention attends to all other tokens, and all other tokens attend to them. This is\n@@ -1956,7 +1865,6 @@ def forward(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 global_attention_mask=global_attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1977,8 +1885,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -2052,9 +1958,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         global_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -2085,12 +1988,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_led._prepare_decoder_inputs`] and modify\n             to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more information on the\n             default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to decide the attention given on each token, local attention or global attention for the encoder.\n             Tokens with global attention attends to all other tokens, and all other tokens attend to them. This is\n@@ -2175,9 +2072,6 @@ def forward(\n             decoder_attention_mask=decoder_attention_mask,\n             encoder_outputs=encoder_outputs,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -2250,9 +2144,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         global_attention_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2281,12 +2172,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_led._prepare_decoder_inputs`] and modify\n             to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more information on the\n             default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to decide the attention given on each token, local attention or global attention for the encoder.\n             Tokens with global attention attends to all other tokens, and all other tokens attend to them. This is\n@@ -2316,9 +2201,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -2401,9 +2283,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         global_attention_mask: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n@@ -2433,12 +2312,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_led._prepare_decoder_inputs`] and modify\n             to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more information on the\n             default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to decide the attention given on each token, local attention or global attention for the encoder.\n             Tokens with global attention attends to all other tokens, and all other tokens attend to them. This is\n@@ -2460,9 +2333,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,"
        },
        {
            "sha": "191a58836b064b9b20c7be0afe9fcef33e5c70cb",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -230,7 +230,6 @@ def forward(\n         hidden_states,\n         layout_inputs,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         layout_value_layer = self.transpose_for_scores(self.layout_value(layout_inputs), r=self.channel_shrink_ratio)\n@@ -280,10 +279,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         layout_attention_probs = self.dropout(layout_attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            layout_attention_probs = layout_attention_probs * head_mask\n-\n         layout_context_layer = torch.matmul(layout_attention_probs, layout_value_layer)\n \n         layout_context_layer = layout_context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -301,10 +296,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -371,14 +362,12 @@ def forward(\n         hidden_states: torch.Tensor,\n         layout_inputs: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             layout_inputs,\n             attention_mask,\n-            head_mask,\n             output_attentions,\n         )\n         attention_output = self.output(self_outputs[0][0], hidden_states)\n@@ -441,14 +430,12 @@ def forward(\n         hidden_states: torch.Tensor,\n         layout_inputs: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             layout_inputs,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         attention_output = self_attention_outputs[0][0]\n@@ -489,7 +476,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         layout_inputs: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -501,13 +487,10 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 layout_inputs,\n                 attention_mask,\n-                layer_head_mask,\n                 output_attentions,\n             )\n \n@@ -616,7 +599,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -685,13 +667,6 @@ def forward(\n         # ourselves in which case we just need to make it broadcastable to all heads.\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output, position_ids = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -705,7 +680,6 @@ def forward(\n             embedding_output,\n             layout_embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -751,7 +725,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -797,7 +770,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -868,7 +840,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -911,7 +882,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -986,7 +956,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1033,7 +1002,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "19e3019bbe6416c4cf8f18356d319ff750e47810",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -484,7 +484,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        layer_head_mask=None,\n         is_index_masked=None,\n         is_index_global_attn=None,\n         is_global_attn=None,\n@@ -577,12 +576,6 @@ def forward(\n             attn_scores, dim=-1, dtype=torch.float32\n         )  # use fp32 for numerical stability\n \n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n-            )\n-            attn_probs = layer_head_mask.view(1, 1, -1, 1) * attn_probs\n-\n         # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n         attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n         attn_probs = attn_probs.type_as(attn_scores)\n@@ -620,7 +613,6 @@ def forward(\n             global_attn_output, global_attn_probs = self._compute_global_attn_output_from_hidden(\n                 hidden_states=hidden_states,\n                 max_num_global_attn_indices=max_num_global_attn_indices,\n-                layer_head_mask=layer_head_mask,\n                 is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero,\n                 is_index_global_attn_nonzero=is_index_global_attn_nonzero,\n                 is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero,\n@@ -974,7 +966,6 @@ def _compute_global_attn_output_from_hidden(\n         self,\n         hidden_states,\n         max_num_global_attn_indices,\n-        layer_head_mask,\n         is_local_index_global_attn_nonzero,\n         is_index_global_attn_nonzero,\n         is_local_index_no_global_attn_nonzero,\n@@ -1043,18 +1034,6 @@ def _compute_global_attn_output_from_hidden(\n             global_attn_scores, dim=-1, dtype=torch.float32\n         )  # use fp32 for numerical stability\n \n-        # apply layer head masking\n-        if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (self.num_heads,), (\n-                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n-            )\n-            global_attn_probs_float = layer_head_mask.view(1, -1, 1, 1) * global_attn_probs_float.view(\n-                batch_size, self.num_heads, max_num_global_attn_indices, seq_len\n-            )\n-            global_attn_probs_float = global_attn_probs_float.view(\n-                batch_size * self.num_heads, max_num_global_attn_indices, seq_len\n-            )\n-\n         global_attn_probs = nn.functional.dropout(\n             global_attn_probs_float.type_as(global_attn_scores), p=self.dropout, training=self.training\n         )\n@@ -1123,7 +1102,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        layer_head_mask=None,\n         is_index_masked=None,\n         is_index_global_attn=None,\n         is_global_attn=None,\n@@ -1132,7 +1110,6 @@ def forward(\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             is_index_masked=is_index_masked,\n             is_index_global_attn=is_index_global_attn,\n             is_global_attn=is_global_attn,\n@@ -1187,7 +1164,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        layer_head_mask=None,\n         is_index_masked=None,\n         is_index_global_attn=None,\n         is_global_attn=None,\n@@ -1196,7 +1172,6 @@ def forward(\n         self_attn_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             is_index_masked=is_index_masked,\n             is_index_global_attn=is_index_global_attn,\n             is_global_attn=is_global_attn,\n@@ -1228,7 +1203,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         padding_len=0,\n         output_attentions=False,\n         output_hidden_states=False,\n@@ -1244,19 +1218,13 @@ def forward(\n         all_attentions = () if output_attentions else None  # All local attentions.\n         all_global_attentions = () if (output_attentions and is_global_attn) else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            assert head_mask.size()[0] == (len(self.layer)), (\n-                f\"The head_mask should be specified for {len(self.layer)} layers, but it is for {head_mask.size()[0]}.\"\n-            )\n         for idx, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                layer_head_mask=head_mask[idx] if head_mask is not None else None,\n                 is_index_masked=is_index_masked,\n                 is_index_global_attn=is_index_global_attn,\n                 is_global_attn=is_global_attn,\n@@ -1490,7 +1458,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         global_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1595,7 +1562,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             padding_len=padding_len,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1641,7 +1607,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         global_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1699,7 +1664,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             inputs_embeds=inputs_embeds,\n@@ -1754,7 +1718,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         global_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1791,7 +1754,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             inputs_embeds=inputs_embeds,\n@@ -1877,7 +1839,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         global_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1942,7 +1903,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             inputs_embeds=inputs_embeds,\n@@ -2008,7 +1968,6 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         global_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -2037,7 +1996,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             global_attention_mask=global_attention_mask,\n-            head_mask=head_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             inputs_embeds=inputs_embeds,\n@@ -2090,7 +2048,6 @@ def forward(\n         token_type_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         global_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -2174,7 +2131,6 @@ def forward(\n             token_type_ids=flat_token_type_ids,\n             attention_mask=flat_attention_mask,\n             global_attention_mask=flat_global_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "3361e7aafe80b20f9f46b228bfdf21d6ac079dad",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 95,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -16,7 +16,6 @@\n \n import copy\n import math\n-import warnings\n from typing import Any, Optional, Union\n \n import torch\n@@ -449,7 +448,6 @@ def forward(\n         key_value_states=None,\n         position_bias=None,\n         past_key_values=None,\n-        layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n@@ -537,10 +535,6 @@ def forward(\n         attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n         attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n-        # Mask heads if we want to\n-        if layer_head_mask is not None:\n-            attn_weights = attn_weights * layer_head_mask\n-\n         attn_output = torch.matmul(attn_weights, value_states)\n \n         attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -674,7 +668,6 @@ def forward(\n         hidden_states,\n         mask=None,\n         position_bias=None,\n-        layer_head_mask=None,\n         output_attentions=False,\n     ):\n         batch_size, seq_length = hidden_states.shape[:2]\n@@ -729,9 +722,6 @@ def unshape(states):\n         # (batch_size, num_blocks, n_heads, block_len, 3 * block_len)\n         attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n-        # Mask heads if we want to\n-        if layer_head_mask is not None:\n-            attn_weights = attn_weights * layer_head_mask\n         attn_weights = attn_weights.type(value_states.dtype)\n         attn_output = unshape(torch.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states))\n         attn_output = attn_output[:, :seq_length, :]\n@@ -893,7 +883,6 @@ def forward(\n         hidden_states,\n         mask=None,\n         position_bias=None,\n-        layer_head_mask=None,\n         output_attentions=False,\n     ):\n         batch_size, seq_length = hidden_states.shape[:2]\n@@ -993,9 +982,6 @@ def unshape(states):\n         attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n         attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n-        # Mask heads if we want to\n-        if layer_head_mask is not None:\n-            attn_weights = attn_weights * layer_head_mask\n         attn_weights = attn_weights.type(value_states.dtype)\n         attn_output = unshape(torch.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states))\n         attn_output = attn_output[:, :seq_length, :]\n@@ -1024,7 +1010,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n-        layer_head_mask=None,\n         past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n@@ -1035,7 +1020,6 @@ def forward(\n             normed_hidden_states,\n             mask=attention_mask,\n             position_bias=position_bias,\n-            layer_head_mask=layer_head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1060,7 +1044,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n-        layer_head_mask=None,\n         output_attentions=False,\n         **kwargs: Any,  # to accept past_key_values and use_cache kwargs\n     ):\n@@ -1069,7 +1052,6 @@ def forward(\n             normed_hidden_states,\n             mask=attention_mask,\n             position_bias=position_bias,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n@@ -1093,7 +1075,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n-        layer_head_mask=None,\n         output_attentions=False,\n         **kwargs: Any,  # to accept past_key_values and use_cache kwargs\n     ):\n@@ -1102,7 +1083,6 @@ def forward(\n             normed_hidden_states,\n             mask=attention_mask,\n             position_bias=position_bias,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n@@ -1125,7 +1105,6 @@ def forward(\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n-        layer_head_mask=None,\n         past_key_values=None,\n         use_cache=False,\n         query_length=None,\n@@ -1138,7 +1117,6 @@ def forward(\n             mask=attention_mask,\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n-            layer_head_mask=layer_head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n@@ -1183,8 +1161,6 @@ def forward(\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         encoder_decoder_position_bias=None,\n-        layer_head_mask=None,\n-        cross_attn_layer_head_mask=None,\n         past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n@@ -1195,7 +1171,6 @@ def forward(\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n-            layer_head_mask=layer_head_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n@@ -1216,7 +1191,6 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n@@ -1404,8 +1378,6 @@ def forward(\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n         inputs_embeds=None,\n-        head_mask=None,\n-        cross_attn_head_mask=None,\n         past_key_values=None,\n         use_cache=None,\n         output_attentions=None,\n@@ -1498,9 +1470,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n-        cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n@@ -1510,9 +1479,6 @@ def forward(\n         hidden_states = self.dropout(inputs_embeds)\n \n         for i, layer_module in enumerate(self.block):\n-            layer_head_mask = head_mask[i]\n-            cross_attn_layer_head_mask = cross_attn_head_mask[i]\n-\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -1523,8 +1489,6 @@ def forward(\n                 encoder_hidden_states,\n                 encoder_extended_attention_mask,\n                 encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n-                layer_head_mask=layer_head_mask,\n-                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1702,15 +1666,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n-__HEAD_MASK_WARNING_MSG = \"\"\"\n-The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n-`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\n-If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\n-num_heads)`.\n-\"\"\"\n-\n-\n @auto_docstring\n class LongT5Model(LongT5PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [\n@@ -1768,9 +1723,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        decoder_head_mask: Optional[torch.FloatTensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1810,18 +1762,6 @@ def forward(\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        decoder_head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in\n-            `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n \n         Example:\n \n@@ -1845,19 +1785,12 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n-        if head_mask is not None and decoder_head_mask is None:\n-            if self.config.num_layers == self.config.num_decoder_layers:\n-                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n-                decoder_head_mask = head_mask\n-\n         # Encode if needed (training, first prediction pass)\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 inputs_embeds=inputs_embeds,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -1879,8 +1812,6 @@ def forward(\n             past_key_values=past_key_values,\n             encoder_hidden_states=hidden_states,\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1960,9 +1891,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        decoder_head_mask: Optional[torch.FloatTensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2003,18 +1931,6 @@ def forward(\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        decoder_head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in\n-            `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n@@ -2041,20 +1957,13 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n-        if head_mask is not None and decoder_head_mask is None:\n-            if self.config.num_layers == self.config.num_decoder_layers:\n-                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n-                decoder_head_mask = head_mask\n-\n         # Encode if needed (training, first prediction pass)\n         if encoder_outputs is None:\n             # Convert encoder inputs in embeddings if needed\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 inputs_embeds=inputs_embeds,\n-                head_mask=head_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -2080,8 +1989,6 @@ def forward(\n             past_key_values=past_key_values,\n             encoder_hidden_states=hidden_states,\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -2167,7 +2074,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -2203,7 +2109,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             inputs_embeds=inputs_embeds,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,"
        },
        {
            "sha": "3d6b484a7dfcaa6e837af3f1ab3e0c3801fd41bb",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -436,7 +436,6 @@ def forward(\n         word_hidden_states,\n         entity_hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         word_size = word_hidden_states.size(1)\n@@ -490,10 +489,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -544,15 +539,13 @@ def forward(\n         word_hidden_states,\n         entity_hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         word_size = word_hidden_states.size(1)\n         self_outputs = self.self(\n             word_hidden_states,\n             entity_hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions,\n         )\n         if entity_hidden_states is None:\n@@ -621,7 +614,6 @@ def forward(\n         word_hidden_states,\n         entity_hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n     ):\n         word_size = word_hidden_states.size(1)\n@@ -630,7 +622,6 @@ def forward(\n             word_hidden_states,\n             entity_hidden_states,\n             attention_mask,\n-            head_mask,\n             output_attentions=output_attentions,\n         )\n         if entity_hidden_states is None:\n@@ -671,7 +662,6 @@ def forward(\n         word_hidden_states,\n         entity_hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -685,12 +675,10 @@ def forward(\n                 all_word_hidden_states = all_word_hidden_states + (word_hidden_states,)\n                 all_entity_hidden_states = all_entity_hidden_states + (entity_hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n             layer_outputs = layer_module(\n                 word_hidden_states,\n                 entity_hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 output_attentions,\n             )\n \n@@ -849,7 +837,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -941,13 +928,6 @@ def forward(\n             if entity_token_type_ids is None:\n                 entity_token_type_ids = torch.zeros((batch_size, entity_seq_length), dtype=torch.long, device=device)\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         # First, compute word embeddings\n         word_embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -970,7 +950,6 @@ def forward(\n             word_embedding_output,\n             entity_embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -1118,7 +1097,6 @@ def forward(\n         entity_position_ids: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         entity_labels: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1165,7 +1143,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1252,7 +1229,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1314,7 +1290,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1382,7 +1357,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1447,7 +1421,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1519,7 +1492,6 @@ def forward(\n         entity_position_ids: Optional[torch.LongTensor] = None,\n         entity_start_positions: Optional[torch.LongTensor] = None,\n         entity_end_positions: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1596,7 +1568,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1676,7 +1647,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1719,7 +1689,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1804,7 +1773,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1847,7 +1815,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1906,7 +1873,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1946,7 +1912,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -2026,7 +1991,6 @@ def forward(\n         entity_attention_mask: Optional[torch.FloatTensor] = None,\n         entity_token_type_ids: Optional[torch.LongTensor] = None,\n         entity_position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2121,7 +2085,6 @@ def forward(\n             entity_attention_mask=entity_attention_mask,\n             entity_token_type_ids=entity_token_type_ids,\n             entity_position_ids=entity_position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "1b47bdff99a336b4498866c764fe5ce47549787e",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -196,7 +196,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -208,9 +207,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -268,7 +264,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -337,7 +332,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -371,16 +365,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_head_mask: torch.Tensor,\n         output_attentions: bool = False,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -390,7 +381,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -451,8 +441,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -467,10 +455,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -487,7 +471,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -504,7 +487,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n@@ -565,8 +547,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -724,8 +704,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -792,7 +770,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -815,12 +792,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -869,13 +840,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for idx, encoder_layer in enumerate(self.layers):\n@@ -892,7 +856,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -958,8 +921,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -996,19 +957,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1115,14 +1063,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != len(self.layers):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {head_mask.size()[0]}.\"\n-                    )\n         synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for idx, decoder_layer in enumerate(self.layers):\n@@ -1141,10 +1081,6 @@ def forward(\n                     attention_mask,\n                     encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                     encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n                     past_key_values=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n@@ -1221,9 +1157,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1249,12 +1182,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1267,7 +1194,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1287,8 +1213,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1343,9 +1267,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1372,12 +1293,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1413,9 +1328,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,"
        },
        {
            "sha": "18d2bf99d609ae70f89bb6df47728a666af09d3a",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -121,7 +121,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -133,9 +132,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -193,7 +189,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -262,7 +257,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -297,16 +291,13 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -315,7 +306,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -384,8 +374,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -400,10 +388,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -419,7 +403,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -436,7 +419,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -510,8 +492,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -669,8 +649,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -732,7 +710,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -755,12 +732,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -808,11 +779,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            assert head_mask.size()[0] == (len(self.layers)), (\n-                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n-            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -829,7 +795,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -885,8 +850,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -923,19 +886,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1050,13 +1000,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                assert attn_mask.size()[0] == (len(self.layers)), (\n-                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1071,8 +1014,6 @@ def forward(\n                 causal_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1194,9 +1135,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple[torch.Tensor], BaseModelOutput]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1222,12 +1160,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n \n         Example:\n \n@@ -1260,7 +1192,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1280,8 +1211,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1448,9 +1377,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple[torch.Tensor], BaseModelOutput]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1477,12 +1403,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1525,9 +1445,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1613,8 +1530,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1625,11 +1540,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1664,8 +1574,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "353c8faafa64a40f0eb414c54a74c333460a3d6e",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -329,7 +329,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n@@ -340,9 +339,6 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n@@ -375,7 +371,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -398,7 +393,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n \n@@ -437,14 +431,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -467,14 +459,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n@@ -507,7 +497,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -520,12 +509,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 **kwargs,\n             )\n@@ -614,7 +600,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -671,16 +656,6 @@ def forward(\n         extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n \n-        if head_mask is not None:\n-            if head_mask.dim() == 1:\n-                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n-                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n-            elif head_mask.dim() == 2:\n-                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n-            head_mask = head_mask.to(dtype=next(self.parameters()).dtype)\n-        else:\n-            head_mask = [None] * self.config.num_hidden_layers\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             xpath_tags_seq=xpath_tags_seq,\n@@ -692,7 +667,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n@@ -731,7 +705,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -778,7 +751,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -849,7 +821,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -894,7 +865,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -953,7 +923,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -997,7 +966,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "9d847e32624f0c0844a5d58c21a44e6f3f981626",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 5,
            "deletions": 30,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -353,7 +353,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         batch_size, dim, num_channels = hidden_states.shape\n@@ -392,10 +391,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n@@ -450,10 +445,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n+        self_outputs = self.self(hidden_states, attention_mask, output_attentions)\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n         return outputs\n@@ -538,7 +532,7 @@ def maybe_pad(self, hidden_states, height, width):\n         hidden_states = nn.functional.pad(hidden_states, pad_values)\n         return hidden_states, pad_values\n \n-    def forward(self, hidden_states, input_dimensions, head_mask=None, output_attentions=False):\n+    def forward(self, hidden_states, input_dimensions, output_attentions=False):\n         height, width = input_dimensions\n         batch_size, dim, channels = hidden_states.size()\n         shortcut = hidden_states\n@@ -562,9 +556,7 @@ def forward(self, hidden_states, input_dimensions, head_mask=None, output_attent\n         if attn_mask is not None:\n             attn_mask = attn_mask.to(hidden_states_windows.device)\n \n-        self_attention_outputs = self.attention(\n-            hidden_states_windows, attn_mask, head_mask, output_attentions=output_attentions\n-        )\n+        self_attention_outputs = self.attention(hidden_states_windows, attn_mask, output_attentions=output_attentions)\n \n         attention_output = self_attention_outputs[0]\n \n@@ -626,19 +618,15 @@ def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, d\n \n         self.pointing = False\n \n-    def forward(\n-        self, hidden_states, input_dimensions, head_mask=None, output_attentions=False, output_hidden_states=False\n-    ):\n+    def forward(self, hidden_states, input_dimensions, output_attentions=False, output_hidden_states=False):\n         all_hidden_states = () if output_hidden_states else None\n \n         height, width = input_dimensions\n         for i, block_module in enumerate(self.blocks):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            block_hidden_states = block_module(hidden_states, input_dimensions, layer_head_mask, output_attentions)\n+            block_hidden_states = block_module(hidden_states, input_dimensions, output_attentions)\n \n             hidden_states = block_hidden_states[0]\n \n@@ -683,7 +671,6 @@ def forward(\n         self,\n         hidden_states,\n         input_dimensions,\n-        head_mask=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -696,12 +683,9 @@ def forward(\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n         for i, layer_module in enumerate(self.layers):\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_hidden_states, output_dimensions, layer_all_hidden_states = layer_module(\n                 hidden_states,\n                 input_dimensions,\n-                layer_head_mask,\n                 output_attentions,\n                 output_hidden_states,\n             )\n@@ -778,7 +762,6 @@ class PreTrainedModel\n     def forward(\n         self,\n         pixel_values=None,\n-        head_mask=None,\n         output_attentions=None,\n         output_hidden_states=None,\n         interpolate_pos_encoding=False,\n@@ -793,21 +776,13 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, len(self.config.depths))\n-\n         embedding_output, input_dimensions = self.embeddings(\n             pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n         )\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n             input_dimensions,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,"
        },
        {
            "sha": "9e19183e38fcfdf6197229d7d39b361539d2abc1",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -132,7 +132,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -144,9 +143,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -204,7 +200,6 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n@@ -273,7 +268,6 @@ def forward(\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n             output_attentions=output_attentions,\n-            head_mask=layer_head_mask,\n             **kwargs,\n         )\n \n@@ -306,16 +300,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_head_mask: torch.Tensor,\n         output_attentions: bool = False,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -325,7 +316,6 @@ def forward(\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -385,8 +375,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -401,10 +389,6 @@ def forward(\n                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n             past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -421,7 +405,6 @@ def forward(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -438,7 +421,6 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n@@ -533,8 +515,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -692,8 +672,6 @@ def _update_cross_attn_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n@@ -766,7 +744,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -789,12 +766,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -843,13 +814,6 @@ def forward(\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n-        # check if head_mask has a correct number of layers specified if desired\n-        if head_mask is not None:\n-            if head_mask.size()[0] != len(self.layers):\n-                raise ValueError(\n-                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n-                    f\" {head_mask.size()[0]}.\"\n-                )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -866,7 +830,6 @@ def forward(\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     output_attentions=output_attentions,\n                 )\n \n@@ -931,8 +894,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -969,19 +930,6 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n-                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n@@ -1096,14 +1044,6 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n-        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n-        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n-            if attn_mask is not None:\n-                if attn_mask.size()[0] != len(self.layers):\n-                    raise ValueError(\n-                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n-                        f\" {attn_mask.size()[0]}.\"\n-                    )\n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             if output_hidden_states:\n@@ -1118,8 +1058,6 @@ def forward(\n                 causal_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1193,9 +1131,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1226,12 +1161,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1249,7 +1178,6 @@ def forward(\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                head_mask=head_mask,\n                 inputs_embeds=inputs_embeds,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n@@ -1269,8 +1197,6 @@ def forward(\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             encoder_attention_mask=attention_mask,\n-            head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n@@ -1343,9 +1269,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1377,12 +1300,6 @@ def forward(\n         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1442,9 +1359,6 @@ def forward(\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1511,9 +1425,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1546,12 +1457,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n             information on the default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n@@ -1570,9 +1475,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1657,9 +1559,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1693,12 +1592,6 @@ def forward(\n             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n             information on the default strategy.\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if start_positions is not None and end_positions is not None:\n@@ -1709,9 +1602,6 @@ def forward(\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n-            head_mask=head_mask,\n-            decoder_head_mask=decoder_head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             encoder_outputs=encoder_outputs,\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n@@ -1816,8 +1706,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1828,11 +1716,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1867,8 +1750,6 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            head_mask=head_mask,\n-            cross_attn_head_mask=cross_attn_head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        },
        {
            "sha": "07c01135902362fd46d228ffbd46b5153f197a75",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -138,7 +138,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n@@ -225,10 +224,6 @@ def forward(\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_probs = self.dropout(attention_probs)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         context_layer = torch.matmul(attention_probs, value_layer)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n@@ -283,7 +278,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n@@ -293,7 +287,6 @@ def forward(\n         self_outputs = self.self(\n             ln_outputs,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n@@ -355,7 +348,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -366,7 +358,6 @@ def forward(\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             past_key_values=past_key_values,\n             cache_position=cache_position,\n@@ -384,7 +375,6 @@ def forward(\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n@@ -420,7 +410,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -454,12 +443,9 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n                 past_key_values,\n@@ -692,7 +678,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -755,13 +740,6 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n@@ -772,7 +750,6 @@ def forward(\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n             past_key_values=past_key_values,\n@@ -834,7 +811,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         next_sentence_label: Optional[torch.LongTensor] = None,\n@@ -876,7 +852,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -940,7 +915,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -982,7 +956,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1053,7 +1026,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n@@ -1076,7 +1048,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n@@ -1142,7 +1113,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1191,7 +1161,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1244,7 +1213,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1264,7 +1232,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1329,7 +1296,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1384,7 +1350,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1434,7 +1399,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1452,7 +1416,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -1500,7 +1463,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1515,7 +1477,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "7729bab4802e043ae1dd86933cd99eb61c0947f2",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -154,7 +154,6 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     if scaling is None:\n@@ -166,9 +165,6 @@ def eager_attention_forward(\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n-    if head_mask is not None:\n-        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -200,7 +196,6 @@ def forward(\n         key_tensor: torch.Tensor,\n         value_tensor: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = query_tensor.shape[:-1]\n@@ -223,7 +218,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -279,15 +273,13 @@ def forward(\n         value_tensor: torch.Tensor,\n         layer_input: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_output, attn_weights = self.self(\n             query_tensor,\n             key_tensor,\n             value_tensor,\n             attention_mask,\n-            head_mask,\n             **kwargs,\n         )\n         # Run a linear projection of `hidden_size` then add a residual\n@@ -439,7 +431,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         if self.use_bottleneck:\n@@ -453,7 +444,6 @@ def forward(\n             value_tensor,\n             layer_input,\n             attention_mask,\n-            head_mask,\n             **kwargs,\n         )\n         attention_output = self_attention_output\n@@ -476,14 +466,12 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         for i, layer_module in enumerate(self.layer):\n             hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                head_mask[i],\n                 **kwargs,\n             )\n         return BaseModelOutput(last_hidden_state=hidden_states)\n@@ -670,7 +658,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n@@ -689,17 +676,9 @@ def forward(\n             embedding_output,\n         )\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=attention_mask,\n-            head_mask=head_mask,\n             **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n@@ -720,8 +699,6 @@ def _update_full_mask(\n             if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             elif self.config._attn_implementation == \"flex_attention\":\n@@ -774,7 +751,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         next_sentence_label: Optional[torch.LongTensor] = None,\n@@ -813,7 +789,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -872,7 +847,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -888,7 +862,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -943,7 +916,6 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -987,7 +959,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1040,7 +1011,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1056,7 +1026,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1119,7 +1088,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n@@ -1130,7 +1098,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1193,7 +1160,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1245,7 +1211,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,\n@@ -1295,7 +1260,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1309,7 +1273,6 @@ def forward(\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             return_dict=True,\n             **kwargs,"
        },
        {
            "sha": "3c4c5ca7d74d56a66a6cda6f0fb0be752afb149e",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb",
            "patch": "@@ -146,7 +146,6 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         position_bias=None,\n         output_attentions=False,\n         **kwargs,\n@@ -184,9 +183,6 @@ def forward(\n \n         attention_probs = self.dropout(attention_probs)\n \n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n         c = torch.matmul(attention_probs, v)\n \n         c = c.permute(0, 2, 1, 3).contiguous()\n@@ -228,15 +224,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         position_bias=None,\n         output_attentions=False,\n         **kwargs,\n     ):\n         self_outputs = self.attn(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             position_bias,\n             output_attentions=output_attentions,\n         )\n@@ -287,15 +281,13 @@ def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n-        head_mask=None,\n         position_bias=None,\n         output_attentions=False,\n         **kwargs,\n     ):\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n-            head_mask,\n             position_bias=position_bias,\n             output_attentions=output_attentions,\n         )\n@@ -320,7 +312,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = False,\n@@ -336,7 +327,6 @@ def forward(\n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n-                head_mask[i],\n                 position_bias,\n                 output_attentions=output_attentions,\n                 **kwargs,\n@@ -450,7 +440,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -479,12 +468,10 @@ def forward(\n             attention_mask = torch.ones(input_shape, device=device)\n         extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -528,7 +515,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -547,7 +533,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -625,7 +610,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -645,7 +629,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -706,7 +689,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -752,7 +734,6 @@ def forward(\n             flat_input_ids,\n             position_ids=flat_position_ids,\n             attention_mask=flat_attention_mask,\n-            head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -800,7 +781,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -818,7 +798,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n@@ -884,7 +863,6 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -898,7 +876,6 @@ def forward(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "1616bcfdf979e21082207c7ef7f577be1efc02b9",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "2288187abc6b48f4ffc7b13eb638d185efb5392f",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7326ede89e71a414ea168243fe5f20a118c6ab57",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "cea583599ee2291a597eb72ed6e71bce6be13cfa",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "554e6bf54edce2e6b1b1e103da937390a42088ea",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 121,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "584385fac00b064a5866d76e9a5e4423ec7200e3",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 93,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "ffd46ed0c27832ce3b27907a47751a6b8ca4ae33",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "34b4ca5adf0d0e08977040b8445341c89982bad0",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 5,
            "deletions": 20,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5ba51c46030c5145404fa759109bde0d3d1e01c6",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "dd59ee37b203ebf13b4a3b6c8438ed1fdcbc5a71",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "055ea0cc22036c13cc4228d148783e5119409c4d",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "2dcbfab69444cb7691e1b6ff78bfd38ff66faabe",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 95,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "778ca656d39bb30426f584c15d2118e0d0350102",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "499d01774d0607cb7aff320d2bb372ff5dcc68c8",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "dfb95b5ccd5abfb61cd98c97acede630197f235a",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 66,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5da8cb2134882d921b9baf7726ae2965e2c2f417",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "e7d9642b06f87393085ed02943a54c52c7a356b7",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "94c2a7515a449551619be873826cca917e09f4a5",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 40,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "fa28f07f49cf0d64871f4984d8b6507054e5bb25",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 93,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "f2ce95e726afdbea2dfed8f418ad79e1360ca2c5",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "226025c259bb93b0da65e43e0315ba8229d5711c",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "21b8afaac49d056e7e168a7a41fccfdceee68813",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 22,
            "deletions": 26,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "a02142482aab13cb4a56d21249568911efc7d46b",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 22,
            "deletions": 24,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "e2cb1c4657a8d28b034b859fd48a9021cf026a42",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "08aa6ca3d53b6e28317ba330473b672b981c23ac",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7462a68fa97fde4fde9995f6195b87298ce2b968",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b7b65f0044996cc43c03e833b1561b1b307399dd",
            "filename": "src/transformers/models/roberta/modular_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "0a5652f117b7ffdfe6bc4aaa7d4fc58984c99947",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b97787f557fd2a03c9d038515670ff76a409ae82",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5a9e41129b4abcc6cbe3494f8d5f0c68491b5346",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "4a4cd9587979d021b3de490cca9b93885d441ea5",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "43b6511b231450c483bc41d5e4f105b52c0f9009",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 85,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b6f74b527ffb9dc5c2ffcc867042b4e0848fb535",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "490ae8ae479108fa9dd9b85d72ab3dba1a77afef",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "3e22cf3fc5c78b47671a9944f3105949bf6c1acb",
            "filename": "src/transformers/models/squeezebert/modeling_squeezebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "0f2f86799b7fa297fbf388043762df07c977db3e",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c9fdc0d7d044a0d084bf370c21be6e3834189ccc",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 4,
            "deletions": 37,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "19bd7b208a190259d337a67f98c5d73a9da9d788",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 28,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "33be714f96b3c1305bc3718d4cb7ab1cffab88b6",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 31,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "653a25c0cf3d1c80234607e616b377711db83ca2",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 83,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "704e1ca428a452ab52b4b2f1dbffaa24fb5b6846",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "a9f3c833eab65a79ae6541e9b509dcc2bf4478f6",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5a7c6fac3e102aaa194e5c05590da3fc05e7f198",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "0aba44015fa63098cd3b4521df7e3bf74bcc48f1",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "eb6e3da17b382d74ece67af56b28a257a5b60f6e",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "58de9c10b117ab954ba747eec0aeeec9fa801c42",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "8d9d74fc24e6906963c2c4724ff4f318b3f9e5c5",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 99,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "2359ad1c9512d19371aee659e177e3a25728ff9e",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "f880c960556bccc19063c83c4f009c9fc47dd201",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "693bc82a047392d38ad57f42f75eca042b840fae",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 12,
            "deletions": 25,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "3868839699167c188422365a2eb35fe792396b16",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 32,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "4ee8d2701738524bd8f5d0e69f1f760882546cb4",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "849085bc08b1c6818b090f82cd885e5861eaa0bc",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 24,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "2db4df13bc95bbaa5633d7b1ae47cde8966dc1a9",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1fd3f0ed473f7848b7be88f9dc0ca273d3fe652c",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 10,
            "deletions": 24,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "d5b38e0c48ae28f00d55b8eb9b4f5ebc5a27fcca",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "712293fcd0d15ffbadfca8cae212d325c7746da3",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 7,
            "deletions": 20,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "bae8d44e0d136a7dc89a28e7804b486d8118f62a",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7170d3ff7de334008bc1f0274295ab092528c08e",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 17,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "0b309610aec76351ee317f4682119c59677a8183",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 61,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "da2e79ad62f6a442c7fdf34bfe38a8cbcb19b780",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7e7da130daac7a38eb04b9816a1b05882ff41be0",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c517c26288c1d7de905f6885de6113697e5eb7c9",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "acbe3fa77b177688dec0f24253f6d5b87bf66105",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "d9102a0276a47403fc9a172b9d17278ac461cc82",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "6fd21d0490deaa57e0401c2b5f1f77c19f7cb330",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "00bbab96668dd668e078149c3fcc8f65eef054e1",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "0df2c5e6a5ac8c116a3bdf153a80bed9d02cceaf",
            "filename": "src/transformers/models/xlm_roberta/modular_xlm_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7cb1af8a2e688636d5ded3e23578ce8c9ff03cee",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "a8fdf8433e29b8d04092f50b42c8ae2400e3d9fe",
            "filename": "src/transformers/models/xlm_roberta_xl/modular_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "48fb1b41a61f13e77b3504d652313d86a7a4926b",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5536eea30452e0daef08623b6cb7f2d5e0d9c8de",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "4512a3f698efa4c7922e68d3b05c8646f3b32f32",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 8,
            "deletions": 22,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "f830936cc7b74b409644ff088a76c2ee022a7fe0",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "fe344df532793c31e47863201532307e9364cb49",
            "filename": "src/transformers/pipelines/image_to_image.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "9bf44c8bb42639664ed4b7aa91b6db88ad0dc6e4",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "a6f98c2aca3f173020284e7c301ea79f49ff148f",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "dc940740ca360296264c72624bcddcd508d9d56a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "4e1bbf75c5073f6ded683fc3288aba19b6160ef7",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1e41e5fc9c1d19be33131d07c26f13153f59c0da",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "9ed9a83316174fe5811defc64a9fbe99099d55b4",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "6ac9b216c24c262ec2c90bb75681c5fa9f9667cf",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "2345cd54c4baa9bff5f866389d8d4961e11a9ae3",
            "filename": "tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "6f25b1865351fbf449a1717d233636f0f718e321",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "499c4a268e563e99886a1c2e8b3639fc269b7508",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "a1c832948209c752c0b10562afd58ef9b5c1d239",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "2e3f1948cd37307e5ffdeffba6ddd4b2eb6196b5",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "f78ed65c3d935d322921cd540977724b1a02066d",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "65892b48fbaa072f140b9276335208ee649c3a1c",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7b48c614e892345ed52328ae62287c6c79de492a",
            "filename": "tests/models/bert_generation/test_modeling_bert_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "d6a54407015b8fe0f7fbee690e75899c260710da",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "e2d3af51c3565725f4c314db0f0ddba71ee0d23f",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "06477f3204770fe75974565a7960c633b35c7e6f",
            "filename": "tests/models/bit/test_modeling_bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "58e3723e8317421acf582d035800b26d04a15908",
            "filename": "tests/models/bitnet/test_modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "13b12f58515ccb85b224d242a2a34b4b6fa5f183",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "d4a7121b52802404abbe436fdd7897121dc2dfab",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "27b1b1202c112bc028a3c72c8383a35811a96f63",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1a41c82119c18d54e1bff40ce0c1b0c9a02b1b6b",
            "filename": "tests/models/blip/test_modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "6523fddb47ccbb4a81b2c970db19c7f5693e7387",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "0817b80bc7b6817e6a65de55b7c8d2f0df463899",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "f1a3d94c20e4f5c15b7c47b4966711241c8d0ad6",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "3e05e02e9e10f1e4405568d77b94a6e8c08a50e8",
            "filename": "tests/models/canine/test_modeling_canine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "3472d253af10ceb7bc298fd2203e3da1824d0464",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "337e31c67dbb74fad76d617f2321f44a2f214bd4",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b3914ee07fd53a1b3f5d301dd4e1a8ae9845c082",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "60104222c09654e72b8b2b169b99eab954873730",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "753815c5989a36868e423d9b0920107ec0ef7ddf",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b8d559943c4e132f633467a7b4d3fedb5624d931",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "02926da154cb239c454a3ecb3e7dca08f8fb60a7",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 8,
            "deletions": 17,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "25d7107a6652938a26be67aae47d82d9ac684088",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "93169a34ca5e335a2c5546ef26086baa3fe63930",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c8571b1a5d74211dfb23c129f22f28225f8bc27f",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "bb18ee7681ef8622a9ce65f917481c4baf4637cb",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c4d83e1b7bdaa4d4ebfdc63421a3d80c3cb1215a",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1b5fa754a402d1b8e5159b410f1e6aa5aadfe6d9",
            "filename": "tests/models/convbert/test_modeling_convbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "4a83421e8d2b57b74882204d643a7897457db0b7",
            "filename": "tests/models/convnext/test_modeling_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "e9091a0ff506a2e38f59694302c9feb49cf61e7b",
            "filename": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c53a8862dd6a20729ae260dd3252bc3891024b84",
            "filename": "tests/models/cpmant/test_modeling_cpmant.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "34140779556724953633bf523665b5bbff424c77",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b679ea32ea0e50f09445257363f4d4b662e09ed3",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1300ece178ec5d869f9af615d3475d1afb50b50a",
            "filename": "tests/models/cvt/test_modeling_cvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1348226857e819d83edcffa7dd7cd2a23e20aaef",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "93ee29b0d1268816f071a069717d18c883f16d0e",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "eb5ad7225a8a6627e3ea4bc07e57100f3bbef8ba",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "be5f9d6a0c57145278b48d14c73ea69e9eb78304",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "f8810685f0fce388f15db65bb0b44524f2b6b660",
            "filename": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c39baea90bc4933c200b6ea2c4184adc374adf46",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b9f6acc1af3ec6d7c00d672a8ae931b2e5fb5e2d",
            "filename": "tests/models/deberta/test_modeling_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "9d67b3ccb16bafec88257155cd7dc35c3abed1d7",
            "filename": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "3e8548aa361c352908b867616117a7c777aabdf7",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "6277b07093db627a61b45bc4cd0dcf6ce31c90e3",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "7941695c6583952c12f7c776e37b4e09034e6ab2",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5ce1679f5997b292c935771b3b2262d6f7ee4727",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "dcd95a009ead20178f021c95246ce2d351eeca05",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "5796b5ad105daaf875a1dfa90a920e30ddb513ff",
            "filename": "tests/models/deit/test_modeling_deit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "4164e506a58dd5793bb3fa0ec19edfb87b6bd5b5",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "6574923592a904d4c97a7a8dfc57fab4a77187ab",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b17d94bb031a01e77de0b4463437e37f1b8b5e50",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "83eaf0e336f5cac41a2e65f96ccc34745dbba493",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b28f7c167b69f46f1cddbd6095f55970639d89b9",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "721af9e8cab70bd941c8f562386e146ab60ad20e",
            "filename": "tests/models/dinat/test_modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "87a799df3709d8b16a5d450c0d98bf5da19ec2fd",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c9696dedb2aef8ea6d9f5b66813ee05b8a11f2f9",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "bbb635c309716673a68bac4bca36077268e49c11",
            "filename": "tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "93af786e4c3bc3454cd36af8e75561fa28e45f55",
            "filename": "tests/models/dinov3_vit/test_modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "bf39f12f7afe19213e89e246b3d59ca2a62b3118",
            "filename": "tests/models/doge/test_modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "1f26e3514843610a7b0bd4005df468cd0576cb3f",
            "filename": "tests/models/donut/test_modeling_donut_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c500c8e5dc9449bbbc7ff394f9eeb6f296131562",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "ae8d83343e936b7e11f73f7e280d6fc68eada7c1",
            "filename": "tests/models/dpr/test_modeling_dpr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "10453e59c906e096b3d49695e63df3eae8226b29",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "c8fd288bfbf5d83d4ae2af0d2b3fa632c6e9c6f9",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "3e10d0014ada922c4d39f796ed51527b010a0e64",
            "filename": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "687a895cc9394ad1e3fe4c4f0a5b9ab0845d300b",
            "filename": "tests/models/edgetam/test_modeling_edgetam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "24716e924b7aaf1c48634d189bd096fb0a3f537b",
            "filename": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "ac0e82d687194d530c4a00d6514d6b7d07d98763",
            "filename": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "8019b01a767d268c5eefc5d6cf31129c4f2e3345",
            "filename": "tests/models/electra/test_modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "946226eaf7f68c45d748c2f341b876c8374aece0",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "21091c164c9deb4595bd179ab7f5ab8d1b3c463c",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "042bf5d79d43c88c44777deb9cb74031eb050389",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "367b29651f0daa9ea0aca1dc61d235126eeb7ca6",
            "filename": "tests/models/eomt/test_modeling_eomt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "b38d7f8633eb38add84bfdd9619da11e3441004e",
            "filename": "tests/models/ernie/test_modeling_ernie.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "27f27105cc3769d4311d27d1fef6ac36e4d54742",
            "filename": "tests/models/esm/test_modeling_esmfold.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "78716cf36a33d8d3bf999f2b5d801b3bd771ba0a",
            "filename": "tests/models/evolla/test_modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        },
        {
            "sha": "84afa75ccc01990803ed832f37268660e2549cf8",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52f5eca7c96dd21618c2481e23f5f4f3a6a756eb/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=52f5eca7c96dd21618c2481e23f5f4f3a6a756eb"
        }
    ],
    "stats": {
        "total": 9842,
        "additions": 1154,
        "deletions": 8688
    }
}