{
    "author": "zucchini-nlp",
    "message": "Fix processing tests (#40379)\n\n* fix tests\n\n* skip failing test in generation as well\n\n* grounding dino was overwritten\n\n* one more overwritten code\n\n* clear comment",
    "sha": "3b5b9f65185bc4879da5960bdad8a1b53c050c74",
    "files": [
        {
            "sha": "9cf30a476bce2494b96e58157a3f55d51123fe3b",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -104,7 +104,7 @@ class DeepseekVLHybridImageProcessor(BaseImageProcessor):\n             Whether to convert the image to RGB.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\"]\n+    model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d720c48e412407e56218332577a31b1719c36140",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -91,6 +91,7 @@ class DeepseekVLHybridImageProcessorFast(BaseImageProcessorFast):\n     high_res_image_std = OPENAI_CLIP_STD\n     high_res_size = {\"height\": 1024, \"width\": 1024}\n     high_res_resample = PILImageResampling.BICUBIC\n+    model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n \n     def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:"
        },
        {
            "sha": "19865daed94fb477d345acb69f72d79be3373351",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -490,6 +490,8 @@ class DeepseekVLHybridImageProcessor(DeepseekVLImageProcessor):\n             Whether to convert the image to RGB.\n     \"\"\"\n \n+    model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n+\n     def __init__(\n         self,\n         do_resize: bool = True,\n@@ -747,6 +749,7 @@ class DeepseekVLHybridImageProcessorFast(DeepseekVLImageProcessorFast):\n     high_res_image_std = OPENAI_CLIP_STD\n     high_res_size = {\"height\": 1024, \"width\": 1024}\n     high_res_resample = PILImageResampling.BICUBIC\n+    model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n \n     def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:"
        },
        {
            "sha": "c33b55eea3b2d37481f1ddca2fb1314b3dfdee89",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -293,5 +293,12 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n+    @property\n+    def model_input_names(self):\n+        # Overwritten because InternVL renames video inputs to `pixel_values` before returning\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return tokenizer_input_names + image_processor_input_names\n+\n \n __all__ = [\"InternVLProcessor\"]"
        },
        {
            "sha": "9e7e4ae977bd70708154248ea3f6c7222d212bd9",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -207,7 +207,7 @@ class Pix2StructImageProcessor(BaseImageProcessor):\n             rendered onto the input images.\n     \"\"\"\n \n-    model_input_names = [\"flattened_patches\"]\n+    model_input_names = [\"flattened_patches\", \"attention_mask\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "f21dd5d7a0025e16b6ed15edcc14b29abe404ad2",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -135,10 +135,9 @@ def __call__(\n \n     @property\n     def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         decoder_ids = [\"decoder_attention_mask\", \"decoder_input_ids\"]\n-        return tokenizer_input_names + image_processor_input_names + decoder_ids\n+        return image_processor_input_names + decoder_ids\n \n \n __all__ = [\"Pix2StructProcessor\"]"
        },
        {
            "sha": "dcd15dea58beafe11c03d6f1d42b03826f9310f6",
            "filename": "tests/models/aria/test_processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processing_aria.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -290,7 +290,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n \n         _ = processor("
        },
        {
            "sha": "399483eb3e56eec6a62fd39953df2fa2df965692",
            "filename": "tests/models/chameleon/test_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -50,7 +50,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n \n         _ = processor("
        },
        {
            "sha": "1490f78436e5a7ed1e7f94f4703d17640f4b5f5e",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -123,7 +123,7 @@ def test_pan_and_scan(self):\n         processor_kwargs = self.prepare_processor_dict()\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n \n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -143,7 +143,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         _ = processor(\n             text=input_str,"
        },
        {
            "sha": "b5d6a2a9d7e893430290f48e2ebcc59620b32d6f",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -255,3 +255,14 @@ def test_apply_chat_template_video_frame_sampling(self):\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 4)\n+\n+    def test_model_input_names(self):\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(modalities=[\"image\", \"video\"])\n+        image_input = self.prepare_image_inputs()\n+        video_inputs = self.prepare_video_inputs()\n+        inputs_dict = {\"text\": text, \"images\": image_input, \"videos\": video_inputs}\n+        inputs = processor(**inputs_dict, return_tensors=\"pt\", do_sample_frames=False)\n+\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))"
        },
        {
            "sha": "a2d1e5427b523b539a5f524d4e5ac81a274ca27d",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -79,7 +79,7 @@ def setUpClass(cls):\n         cls.embed_dim = 5\n         cls.seq_length = 5\n \n-    def prepare_text_inputs(self, batch_size: Optional[int] = None, modality: Optional[str] = None):\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None, **kwargs):\n         labels = [\"a cat\", \"remote control\"]\n         labels_longer = [\"a person\", \"a car\", \"a dog\", \"a cat\"]\n "
        },
        {
            "sha": "b9e64cc56a5dde370b12970220edebaf4b6b8e52",
            "filename": "tests/models/llava/test_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fllava%2Ftest_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fllava%2Ftest_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processing_llava.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -95,7 +95,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n \n         _ = processor("
        },
        {
            "sha": "a02662759293ebe3af6a7c2ffa1700f6995dabfe",
            "filename": "tests/models/mistral3/test_processing_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -291,7 +291,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n \n         _ = processor("
        },
        {
            "sha": "e5e2ce680cbdbcedd22fe3c87bb2e7b2197659a3",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -342,7 +342,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         image_input = [[image_input[0]], [image_input[1]]]\n         inputs = processor(\n@@ -366,7 +366,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         image_input = [[image_input[0]], [image_input[1]]]\n         _ = processor("
        },
        {
            "sha": "0582bc857a5a64c11d6ea3aa82b89dd879457dab",
            "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -309,3 +309,12 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertEqual(inputs[\"flattened_patches\"].shape[1], 1024)\n \n         self.assertEqual(len(inputs[\"decoder_input_ids\"][0]), 76)\n+\n+    def test_model_input_names(self):\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(modalities=\"image\")\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=text, images=image_input, return_tensors=\"pt\")\n+\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))"
        },
        {
            "sha": "b346a1b802beb67bbe0c69a3f5673f6aa68a5d0b",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -354,7 +354,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n \n         _ = processor("
        },
        {
            "sha": "53b2ae9aaa4ec3ce2364838602cc35fc706373f1",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -450,7 +450,7 @@ def test_unstructured_kwargs_batched(self):\n         )\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         image_input = [[image_input[0]], [image_input[1]]]\n         inputs = processor(\n@@ -477,7 +477,7 @@ def test_unstructured_kwargs_batched_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"video\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"video\")\n         video_input = self.prepare_video_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n@@ -572,7 +572,7 @@ def test_special_mm_token_truncation(self):\n \n         processor = self.get_processor()\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         image_input = [[image_input[0]], [image_input[1]]]\n         _ = processor("
        },
        {
            "sha": "d4ec8d1836746dde8884a25b3a500e59251b2940",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 35,
            "deletions": 32,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b5b9f65185bc4879da5960bdad8a1b53c050c74/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=3b5b9f65185bc4879da5960bdad8a1b53c050c74",
            "patch": "@@ -18,7 +18,7 @@\n import random\n import tempfile\n from pathlib import Path\n-from typing import Optional\n+from typing import Optional, Union\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n@@ -136,11 +136,14 @@ def get_processor(self):\n         processor = self.processor_class(**components, **self.prepare_processor_dict())\n         return processor\n \n-    def prepare_text_inputs(self, batch_size: Optional[int] = None, modality: Optional[str] = None):\n-        if modality is not None:\n-            special_token_to_add = getattr(self, f\"{modality}_token\", \"\")\n-        else:\n-            special_token_to_add = \"\"\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None, modalities: Optional[Union[str, list]] = None):\n+        if isinstance(modalities, str):\n+            modalities = [modalities]\n+\n+        special_token_to_add = \"\"\n+        if modalities is not None:\n+            for modality in modalities:\n+                special_token_to_add += getattr(self, f\"{modality}_token\", \"\")\n \n         if batch_size is None:\n             return f\"lower newer {special_token_to_add}\"\n@@ -217,7 +220,7 @@ def test_processor_from_and_save_pretrained(self):\n     def test_model_input_names(self):\n         processor = self.get_processor()\n \n-        text = self.prepare_text_inputs(modality=\"image\")\n+        text = self.prepare_text_inputs(modalities=[\"image\", \"video\", \"audio\"])\n         image_input = self.prepare_image_inputs()\n         video_inputs = self.prepare_video_inputs()\n         audio_inputs = self.prepare_audio_inputs()\n@@ -256,7 +259,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n@@ -279,7 +282,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n@@ -294,7 +297,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n@@ -314,7 +317,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n@@ -328,7 +331,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -351,7 +354,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n@@ -377,7 +380,7 @@ def test_doubly_passed_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [self.prepare_text_inputs(modality=\"image\")]\n+        input_str = [self.prepare_text_inputs(modalities=\"image\")]\n         image_input = self.prepare_image_inputs()\n         with self.assertRaises(ValueError):\n             _ = processor(\n@@ -408,7 +411,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -431,7 +434,7 @@ def test_structured_kwargs_nested_from_dict(self):\n         processor_kwargs = self.prepare_processor_dict()\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -458,7 +461,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n+        input_str = self.prepare_text_inputs(batch_size=3, modalities=\"audio\")\n         raw_speech = self.prepare_audio_inputs(batch_size=3)\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\")\n         self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n@@ -475,7 +478,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n+        input_str = self.prepare_text_inputs(batch_size=3, modalities=\"audio\")\n         raw_speech = self.prepare_audio_inputs(batch_size=3)\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n \n@@ -493,7 +496,7 @@ def test_unstructured_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n+        input_str = self.prepare_text_inputs(batch_size=3, modalities=\"audio\")\n         raw_speech = self.prepare_audio_inputs(batch_size=3)\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n \n@@ -511,7 +514,7 @@ def test_doubly_passed_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n+        input_str = self.prepare_text_inputs(batch_size=3, modalities=\"audio\")\n         raw_speech = self.prepare_audio_inputs(batch_size=3)\n         with self.assertRaises(ValueError):\n             _ = processor(\n@@ -534,7 +537,7 @@ def test_structured_kwargs_audio_nested(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n+        input_str = self.prepare_text_inputs(batch_size=3, modalities=\"audio\")\n         raw_speech = self.prepare_audio_inputs(batch_size=3)\n \n         # Define the kwargs for each modality\n@@ -556,7 +559,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(text=input_str, videos=video_input, do_sample_frames=False, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 167)\n@@ -579,7 +582,7 @@ def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         inputs = processor(text=input_str, videos=video_input, do_sample_frames=False, return_tensors=\"pt\")\n@@ -594,7 +597,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -619,7 +622,7 @@ def test_kwargs_overrides_default_video_processor_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         inputs = processor(\n@@ -640,7 +643,7 @@ def test_unstructured_kwargs_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -664,7 +667,7 @@ def test_unstructured_kwargs_batched_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2, modality=\"video\")\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"video\")\n         video_input = self.prepare_video_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n@@ -691,7 +694,7 @@ def test_doubly_passed_kwargs_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [self.prepare_text_inputs(modality=\"video\")]\n+        input_str = [self.prepare_text_inputs(modalities=\"video\")]\n         video_input = self.prepare_video_inputs()\n         with self.assertRaises(ValueError):\n             _ = processor(\n@@ -711,7 +714,7 @@ def test_structured_kwargs_nested_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         # Define the kwargs for each modality\n@@ -734,7 +737,7 @@ def test_structured_kwargs_nested_from_dict_video(self):\n         processor_kwargs = self.prepare_processor_dict()\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs(modality=\"video\")\n+        input_str = self.prepare_text_inputs(modalities=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         # Define the kwargs for each modality\n@@ -758,7 +761,7 @@ def test_overlapping_text_image_kwargs_handling(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(modality=\"image\")\n+        input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         with self.assertRaises(ValueError):\n@@ -783,7 +786,7 @@ def test_overlapping_text_audio_kwargs_handling(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n+        input_str = self.prepare_text_inputs(batch_size=3, modalities=\"audio\")\n         audio_lengths = [4000, 8000, 16000, 32000]\n         raw_speech = [np.asarray(audio)[:length] for audio, length in zip(floats_list((3, 32_000)), audio_lengths)]\n "
        }
    ],
    "stats": {
        "total": 131,
        "additions": 82,
        "deletions": 49
    }
}