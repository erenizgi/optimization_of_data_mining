{
    "author": "ydshieh",
    "message": "Update expected values (after switching to A10) - part 5 (#39205)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "1168f57abffd077d7d2687087aa10ba644a76a0d",
    "files": [
        {
            "sha": "978febaa4977dfe0b35401223cb55fef0147fda7",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -404,10 +404,15 @@ def test_model_generation_batched(self):\n                     \"USER: 64*64Describe what do you see here? ASSISTANT: The image depicts a black panther in a crouched position. The panther's body is elongated and its head is lowered, suggesting a state of alertness or readiness. The animal's\",\n                     \"USER: 64*64What can you say about the image? ASSISTANT: The image depicts a serene natural landscape. The foreground consists of a grassy area with some patches of bare earth. The middle ground shows a gently sloping hill with a reddish-brown hue,\",\n                 ],\n-                (\"cuda\", 7): [\n+                (None, None): [\n                     \"USER: 64*64Describe what do you see here? ASSISTANT: The image depicts a black panther in a crouched position. The panther's body is elongated and curved, with its head lowered and ears pointed forward, suggesting alertness or focus.\",\n                     \"USER: 64*64What can you say about the image? ASSISTANT: The image depicts a serene natural landscape. The foreground consists of a grassy area with some patches of bare earth. The middle ground shows a steep, reddish-brown cliff, which could be a\",\n                 ],\n+                # We switch to A10 on 2025/06/29, and A10 gives strange values\n+                (\"cuda\", 8): [\n+                    'USER: 64*64Describe what do you see here? ASSISTANT: 1.Filed with 1.Computing theComputing.Computing.',\n+                    'USER: 64*64What can you say about the image? ASSISTANT: 1.Filed with theComputing theComputing.Computing.',\n+                ],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n@@ -433,7 +438,9 @@ def test_model_generation_multi_image(self):\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n                 {\n                     (\"xpu\", 3): ['USER: 64*6464*64What do these two images have in common? ASSISTANT: The two images both depict a rhinoceros, yet they are significantly different in terms of focus and clarity. The rhinoceros in the upper image is in sharp focus, showing detailed textures'],\n-                    (\"cuda\", 7): [\"USER: 64*6464*64What do these two images have in common? ASSISTANT: Both images feature a black animal, but they are not the same animal. The top image shows a close-up of a black cow's head, while the bottom image depicts a black cow in a natural\"],\n+                    (None, None): [\"USER: 64*6464*64What do these two images have in common? ASSISTANT: Both images feature a black animal, but they are not the same animal. The top image shows a close-up of a black cow's head, while the bottom image depicts a black cow in a natural\"],\n+                    # We switch to A10 on 2025/06/29, and A10 gives strange values\n+                    (\"cuda\", 8): ['USER: 64*6464*64What do these two images have in common? ASSISTANT:Computing.Filed.Filed.11.Computing theComputing.Computing.'],\n                 }\n             )  # fmt: skip\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()"
        },
        {
            "sha": "2f3485cbb0320a1d3bd079203938403930f3caa6",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 31,
            "deletions": 8,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -24,6 +24,7 @@\n \n from transformers import AutoProcessor, EncodecConfig\n from transformers.testing_utils import (\n+    Expectations,\n     is_torch_available,\n     require_torch,\n     slow,\n@@ -459,10 +460,21 @@ def test_integration_24kHz(self):\n             \"1.5\": 0.0025,\n             \"24.0\": 0.0015,\n         }\n-        expected_codesums = {\n-            \"1.5\": [371955],\n-            \"24.0\": [6659962],\n-        }\n+\n+        expectations = Expectations(\n+            {\n+                (None, None): {\n+                    \"1.5\": [371955],\n+                    \"24.0\": [6659962],\n+                },\n+                (\"cuda\", 8): {\n+                    \"1.5\": [371955],\n+                    \"24.0\": [6655079],\n+                },\n+            }\n+        )\n+        expected_codesums = expectations.get_expectation()\n+\n         librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         model_id = \"facebook/encodec_24khz\"\n \n@@ -513,10 +525,21 @@ def test_integration_48kHz(self):\n             \"3.0\": 0.001,\n             \"24.0\": 0.0005,\n         }\n-        expected_codesums = {\n-            \"3.0\": [144259, 146765, 156435, 176871, 161971],\n-            \"24.0\": [1568553, 1294948, 1306190, 1464747, 1663150],\n-        }\n+\n+        expectations = Expectations(\n+            {\n+                (None, None): {\n+                    \"3.0\": [144259, 146765, 156435, 176871, 161971],\n+                    \"24.0\": [1568553, 1294948, 1306190, 1464747, 1663150],\n+                },\n+                (\"cuda\", 8): {\n+                    \"3.0\": [144259, 146765, 156205, 176871, 161971],\n+                    \"24.0\": [1566878, 1300459, 1310165, 1464747, 1663150],\n+                },\n+            }\n+        )\n+        expected_codesums = expectations.get_expectation()\n+\n         librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         model_id = \"facebook/encodec_48khz\"\n "
        },
        {
            "sha": "31c9b72e28b529d91f17b43f77efca5de989afb1",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -21,6 +21,8 @@\n \n from transformers import FalconH1Config, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n+    get_device_properties,\n     require_torch,\n     require_torch_gpu,\n     slow,\n@@ -484,7 +486,7 @@ def test_falcon_h1_hard(self):\n         \"\"\"\n         An integration test for Falcon-H1.\n         \"\"\"\n-        EXPECTED_TEXT = \"\"\"\n+        EXPECTED_TEXT_DEFAULT = \"\"\"\n             user\n             Tell me about the french revolution.\n             assistant\n@@ -503,9 +505,43 @@ def test_falcon_h1_hard(self):\n             4. **Rise of the Jacobins and Reign of Terror (1793–1794)**: Radical leaders like Maximilien Robespierre sought to purge France of counter-revolutionaries, leading to mass executions and widespread fear.\n             5. **Thermidorian Reaction\n         \"\"\"\n+\n+        EXPECTED_TEXT_A10 = \"\"\"\n+            user\n+            Tell me about the french revolution.\n+            assistant\n+            The French Revolution (1789–1799) was a period of profound social upheaval and radical political change in France that fundamentally transformed the nation and had far-reaching effects on the rest of Europe and the world. Here are the key aspects of the revolution:\n+\n+            ### **Causes**\n+            1. **Economic Crisis**: France was in severe financial trouble due to costly wars (particularly the American Revolution), extravagant spending by the monarchy, and an inefficient tax system.\n+            2. **Social Inequality**: The privileged classes (the nobility and clergy) enjoyed immense wealth and power, while the majority of the population (the Third Estate, comprising commoners) faced poverty and lack of representation.\n+            3. **Enlightenment Ideas**: Philosophers like Voltaire, Rousseau, and Montesquieu inspired ideas of liberty, equality, and popular sovereignty, which fueled revolutionary fervor.\n+            4. **Political Instability**: The absolute monarchy under King Louis XVI proved unable to address the nation's problems, leading to growing discontent.\n+\n+            ### **Key Events**\n+            1. **Estates-General (1789)**: The Third Estate broke away and formed the National Assembly, forcing King Louis XVI to convene the Estates-General, an old legislative body, to address the financial crisis.\n+            2. **Storming of the Bastille (July 14, 1789)**: A symbol of royal tyranny, the Bastille fortress was stormed by revolutionaries, sparking widespread rebellion.\n+            3. **Declaration of the Rights of Man and of the Citizen (August 1789)**: This foundational document proclaimed liberty, equality, and fraternity as fundamental rights.\n+            4. **Abolition of Feudalism (November 1789)**: The National Assembly abolished feudal privileges, redistributing church lands to the people.\n+            5. **Tennis Court Oath (May 5, 1789)**: The National Assembly members, meeting on a tennis court, pledged to continue their work until a new constitution was established.\n+            6.\n+        \"\"\"\n+\n+        expected_texts = Expectations(\n+            {\n+                (None, None): EXPECTED_TEXT_DEFAULT,\n+                (\"cuda\", 8): EXPECTED_TEXT_A10,\n+            }\n+        )\n+        EXPECTED_TEXT = expected_texts.get_expectation()\n         # Remove the first char (`\\n`) and the consecutive whitespaces caused by the formatting.\n         EXPECTED_TEXT = EXPECTED_TEXT.strip().replace(\" \" * 12, \"\")\n \n+        device_properties = get_device_properties()\n+        # For A10, there is an ending \" \"\n+        if device_properties[0] == \"cuda\" and device_properties[1] == 8:\n+            EXPECTED_TEXT = EXPECTED_TEXT + \" \"\n+\n         model_id = \"tiiuae/Falcon-H1-1.5B-Deep-Instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         model = FalconH1ForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
        },
        {
            "sha": "8b3e12b01eb501db64da9e61f3f25a970445bf26",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -403,8 +403,7 @@ def test_model_4b_bf16(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n-                (\"cuda\", 7): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n-                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and'],\n+                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant coastline in the background. It looks'],\n                 (\"rocm\", (9, 5)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant coastline in the background. It looks'],\n             }\n         )  # fmt: skip\n@@ -452,11 +451,10 @@ def test_model_4b_batch(self):\n                         'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and',\n                         'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes:\\n\\n*   **Image 1** shows a cow standing on a beach.',\n                     ],\n-                (\"cuda\", 7): [],\n                 (\"cuda\", 8):\n                     [\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and',\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes:\\n\\n*   **Image 1** shows a cow standing on a beach.',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant island in the background. It looks',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes. \\n\\n*   **Image 1** shows a cow standing on a beach'\n                     ],\n                 (\"rocm\", (9, 5)):\n                     ["
        },
        {
            "sha": "ed1a0d459098c5c59efa431ab08bdf27f0833093",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -120,10 +120,14 @@ def test_model_9b_bf16(self):\n \n     def test_model_9b_eager(self):\n         expected_texts = Expectations({\n-            (\"cuda\", None): [\n+            (None, None): [\n                 \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n                 \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n             ],\n+            (\"cuda\", 8): [\n+                'Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the',\n+                'Hi today I am going to show you how to make a simple and easy to make a DIY paper lantern.',\n+            ],\n             (\"rocm\", (9, 5)) : [\n                 \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n                 \"Hi today I am going to show you how to make a simple and easy to make a paper airplane. First\","
        },
        {
            "sha": "b25bb7a79e68ad99e208aeba2f9338282fe86e5d",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -87,7 +87,8 @@ def test_model_2b(self):\n         expected_texts = Expectations(\n             {\n                 (\"rocm\", (9, 5)): [\"Hello, today is a great day to start a new project. I have been working on a new project for a while now, and I\"],\n-                (\"cuda\", None): [\"Hello, today is a great day to start a new project. I have been working on a new project for a while now and I have\"],\n+                (None, None): [\"Hello, today is a great day to start a new project. I have been working on a new project for a while now and I have\"],\n+                (\"cuda\", 8): ['Hello, today is a great day to start a new project. I have been working on a new project for a while now, and I'],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXTS = expected_texts.get_expectation()"
        },
        {
            "sha": "7bac1dca608c53b671d34f536afdf299f88d7d8c",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -545,7 +545,8 @@ def test_integration_detection_bug(self):\n         expected_decoded_texts = Expectations(\n             {\n                 (\"rocm\", (9, 5)): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0644> shoe\",\n-                (\"cuda\", None): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0646> shoe\",\n+                (None, None): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0646> shoe\",\n+                (\"cuda\", 8): \"detect shoe\\n<loc0045><loc0309><loc0708><loc0646> shoe\",\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = expected_decoded_texts.get_expectation()"
        },
        {
            "sha": "fb15c213455ec22214fb4f4d5bae3037d652a797",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -256,6 +256,9 @@ def test_export_static_cache(self):\n             (\"cuda\", None): [\n                 \"My favourite condiment is 100% natural, organic, gluten free, vegan, and free from preservatives. I\"\n             ],\n+            (\"cuda\", 8): [\n+                \"My favourite condiment is 100% natural, organic, gluten free, vegan, and vegetarian. I love to use\"\n+            ],\n             (\"rocm\", (9, 5)): [\n                 \"My favourite condiment is 100% natural, organic, gluten free, vegan, and vegetarian. I love to use\"\n             ]"
        },
        {
            "sha": "2d5ccfa9cf725e78d9282693a9b13ebe3dde8784",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -466,6 +466,10 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n                     'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n                     'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets',\n                 ],\n+                (\"cuda\", 8): [\n+                    'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+                    'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices'\n+                ],\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()"
        },
        {
            "sha": "274b2e49896740cdea31f0dcb7cb689c3330c8ed",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1168f57abffd077d7d2687087aa10ba644a76a0d/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=1168f57abffd077d7d2687087aa10ba644a76a0d",
            "patch": "@@ -19,6 +19,7 @@\n \n from transformers import Starcoder2Config, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -148,10 +149,20 @@ def test_starcoder2_batched_generation_fa2(self):\n \n     @require_bitsandbytes\n     def test_starcoder2_batched_generation_4bit(self):\n-        EXPECTED_TEXT = [\n-            'Hello my name is Younes and I am a student at the University of Maryland. I am currently working on a project that is related to the topic of \"How to make a game\". I am currently working on a project',\n-            'def hello_world():\\n\\treturn \"Hello World\"\\n\\n@app.route(\\'/hello/<name>\\')\\ndef hello_name(name):\\n\\treturn \"Hello \" + name\\n\\n@app.route',\n-        ]\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    'Hello my name is Younes and I am a student at the University of Maryland. I am currently working on a project that is related to the topic of \"How to make a game\". I am currently working on a project',\n+                    'def hello_world():\\n\\treturn \"Hello World\"\\n\\n@app.route(\\'/hello/<name>\\')\\ndef hello_name(name):\\n\\treturn \"Hello \" + name\\n\\n@app.route',\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello my name is Younes and I am a student at the University of Maryland. I am currently working on a project that is aimed at creating a new way of learning. I am hoping to create a new way of\",\n+                    'def hello_world():\\n\\treturn \"Hello World\"\\n\\n@app.route(\\'/hello/<name>\\')\\ndef hello_name(name):\\n\\treturn \"Hello \" + name\\n\\n@app.route',\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = expectations.get_expectation()\n+\n         model_id = \"bigcode/starcoder2-7b\"\n \n         model = Starcoder2ForCausalLM.from_pretrained(model_id, load_in_4bit=True)"
        }
    ],
    "stats": {
        "total": 134,
        "additions": 111,
        "deletions": 23
    }
}