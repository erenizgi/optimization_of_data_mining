{
    "author": "MekkCyber",
    "message": "Attention Quantization with FBGemm & TP (#37384)\n\n* fix\n\n* keep fused\n\n* contiguous\n\n* rm print\n\n* update\n\n* update\n\n* rm print",
    "sha": "f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7",
    "files": [
        {
            "sha": "1cc5a8b23aa53127e2be855fef6e753ed5c5415f",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7",
            "patch": "@@ -50,7 +50,7 @@ def forward(self, x):\n         # x_quantized and x_scale are not necessarily on the same device as x, this is an issue.\n         # https://github.com/pytorch/FBGEMM/blob/e08af8539c391437f447173863df0f3f6f6f1855/fbgemm_gpu/experimental/gen_ai/src/quantize/quantize.cu#L1237C3-L1237C45\n         x_quantized, x_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n-            x.view(-1, x.shape[-1]), scale_ub=self.input_scale_ub\n+            x.view(-1, x.shape[-1]).contiguous(), scale_ub=self.input_scale_ub\n         )\n         # moving x_quantized, x_scale here creates glibberish output ... However, if we move the output, it works\n         # x_quantized, x_scale = x_quantized.to(x.device), x_scale.to(x.device)\n@@ -207,9 +207,6 @@ def _replace_with_fbgemm_fp8_linear(\n                 (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n             ):\n                 with init_empty_weights(include_buffers=True):\n-                    tp_plan[re.sub(r\"\\d+\", \"*\", current_key_name_str + \".gate_up_proj_scale\")] = tp_plan[\n-                        re.sub(r\"\\d+\", \"*\", current_key_name_str + \".gate_up_proj\")\n-                    ]\n                     tp_plan[re.sub(r\"\\d+\", \"*\", current_key_name_str + \".down_proj_scale\")] = None\n                     model._modules[name] = FbgemmFp8Llama4TextExperts(\n                         config.text_config,"
        },
        {
            "sha": "76215e4666ec67f43e9230eeb965223547bec859",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7",
            "patch": "@@ -219,7 +219,7 @@ def __init__(\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n-        if isinstance(inputs[0], DTensor):\n+        if inputs and isinstance(inputs[0], DTensor):\n             inputs = inputs[0].to_local()\n         return inputs\n "
        },
        {
            "sha": "e075af96189a57eb434aec0e104866afdbbd897f",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7",
            "patch": "@@ -220,7 +220,6 @@ def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n         \"\"\"\n         model.is_quantized = True\n         model.quantization_method = self.quantization_config.quant_method\n-        print(\"self.pre_quantized\", self.pre_quantized)\n         if self.pre_quantized:\n             self._convert_model_for_quantization(model)\n         return self._process_model_before_weight_loading(model, **kwargs)\n@@ -345,6 +344,9 @@ def forward(\n MODULES_TO_PATCH_FOR_QUANTIZATION = {\n     \"Llama4TextExperts\": {\n         \"module_name\": SequentialLlama4TextExperts,\n-        \"quantization_methods\": [QuantizationMethod.COMPRESSED_TENSORS, QuantizationMethod.BITS_AND_BYTES],\n+        \"quantization_methods\": [\n+            QuantizationMethod.COMPRESSED_TENSORS,\n+            QuantizationMethod.BITS_AND_BYTES,\n+        ],\n     }\n }"
        },
        {
            "sha": "c0392fc5f53314b91efb061f83c3cb254eea5b89",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=f834ca2c19215f1e4fb0959cc3faafeaf56cd4f7",
            "patch": "@@ -241,6 +241,42 @@ def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> Li\n                         not_missing_keys.append(missing)\n         return [k for k in missing_keys if k not in not_missing_keys]\n \n+    def update_tp_plan(self, config):\n+        text_plan = {\n+            \"layers.*.self_attn.q_proj.weight\": \"local_colwise\",\n+            \"layers.*.self_attn.q_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.self_attn.k_proj.weight\": \"local_colwise\",\n+            \"layers.*.self_attn.k_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.self_attn.v_proj.weight\": \"local_colwise\",\n+            \"layers.*.self_attn.v_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.self_attn.o_proj.weight\": \"local_rowwise\",\n+            \"layers.*.self_attn\": \"gather\",\n+            \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n+            \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n+            \"norm.weight\": \"sequence_parallel\",\n+            \"layers.*.feed_forward.shared_expert.gate_proj.weight\": \"local_colwise\",\n+            \"layers.*.feed_forward.shared_expert.gate_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.feed_forward.shared_expert.up_proj.weight\": \"local_colwise\",\n+            \"layers.*.feed_forward.shared_expert.up_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.feed_forward.shared_expert.down_proj.weight\": \"local_rowwise\",\n+            \"layers.*.feed_forward.experts\": \"local\",\n+            \"layers.*.feed_forward\": \"gather\",\n+            \"layers.*.feed_forward.experts.*.gate_proj.weight\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.gate_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.up_proj.weight\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.up_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.down_proj.weight\": \"local_rowwise\",\n+            # For Fused implementation\n+            \"layers.*.feed_forward.experts.gate_up_proj\": \"local_packed_rowwise\",\n+            \"layers.*.feed_forward.experts.gate_up_proj_scale\": \"local_packed_rowwise\",\n+            \"layers.*.feed_forward.experts.down_proj\": \"local_colwise\",\n+        }\n+        if config.get_text_config() is not None:\n+            config.get_text_config().base_model_tp_plan = text_plan\n+        else:\n+            config.base_model_tp_plan = text_plan\n+        return config\n+\n     def is_serializable(self, safe_serialization=None):\n         return True\n "
        }
    ],
    "stats": {
        "total": 49,
        "additions": 42,
        "deletions": 7
    }
}