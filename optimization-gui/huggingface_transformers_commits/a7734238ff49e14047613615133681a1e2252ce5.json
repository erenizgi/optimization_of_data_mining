{
    "author": "gante",
    "message": "Generation tests: update imagegpt input name, remove unused functions (#33663)",
    "sha": "a7734238ff49e14047613615133681a1e2252ce5",
    "files": [
        {
            "sha": "4dfbae1238c48e4ee9ceb9397fb4027ea195b051",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -690,8 +690,7 @@ def forward(\n \n         if \"pixel_values\" in kwargs:\n             warnings.warn(\n-                \"The `pixel_values` argument is deprecated and will be removed in a future version, use `input_ids`\"\n-                \" instead.\",\n+                \"The `pixel_values` argument is deprecated and will be removed in v4.47, use `input_ids` instead.\",\n                 FutureWarning,\n             )\n \n@@ -1004,8 +1003,7 @@ def forward(\n \n         if \"pixel_values\" in kwargs:\n             warnings.warn(\n-                \"The `pixel_values` argument is deprecated and will be removed in a future version, use `input_ids`\"\n-                \" instead.\",\n+                \"The `pixel_values` argument is deprecated and will be removed in v4.47, use `input_ids` instead.\",\n                 FutureWarning,\n             )\n \n@@ -1137,8 +1135,7 @@ def forward(\n \n         if \"pixel_values\" in kwargs:\n             warnings.warn(\n-                \"The `pixel_values` argument is deprecated and will be removed in a future version, use `input_ids`\"\n-                \" instead.\",\n+                \"The `pixel_values` argument is deprecated and will be removed in v4.47, use `input_ids` instead.\",\n                 FutureWarning,\n             )\n "
        },
        {
            "sha": "9754a4b7dcc6968c915bed46fe259da3e86f50a1",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -190,26 +190,6 @@ def _get_constrained_beam_kwargs(self, num_return_sequences=1):\n         }\n         return beam_kwargs\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.repeat_interleave(\n-            num_interleave, dim=0\n-        )\n-        generation_config = copy.deepcopy(model.generation_config)\n-        model._prepare_special_tokens(generation_config)\n-        input_ids = torch.zeros_like(input_ids[:, :1]) + generation_config.decoder_start_token_id\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def _greedy_generate(\n         self,\n         model,"
        },
        {
            "sha": "1ee4c7f57dbc9d7a6447dc4d3517807f7d4e629a",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -150,35 +150,6 @@ def get_config(self):\n             rotary_dim=self.rotary_dim,\n         )\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def create_and_check_codegen_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         model = CodeGenModel(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "893132f4337dd4026acf2112d78c7ce733f9fcf4",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -150,25 +150,6 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        return (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n     def create_and_check_falcon_mamba_model(self, config, input_ids, *args):\n         config.output_hidden_states = True\n         model = FalconMambaModel(config=config)"
        },
        {
            "sha": "9d7750f5cf20cce699af07b6fb1592becae968ee",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -22,7 +22,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -178,35 +178,6 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def create_and_check_gpt_bigcode_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         model = GPTBigCodeModel(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "245fee4b71f1c42ceeb6ee72e43b26ae76d6c4f5",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -22,7 +22,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -157,35 +157,6 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def create_and_check_gpt_neo_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         model = GPTNeoModel(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "71c121dbaa5a9eb1e8a3cf2c9ba000d14fda136c",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -32,7 +32,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -173,35 +173,6 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def create_and_check_gptj_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         model = GPTJModel(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "d8ceed6885f0c805599debe3c434f8a5e2da5d07",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 274,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -14,10 +14,7 @@\n # limitations under the License.\n \n \n-import copy\n import inspect\n-import os\n-import tempfile\n import unittest\n \n from transformers import ImageGPTConfig\n@@ -26,13 +23,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import (\n-    ModelTesterMixin,\n-    _config_zero_init,\n-    floats_tensor,\n-    ids_tensor,\n-    random_attention_mask,\n-)\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -108,7 +99,7 @@ def get_large_model_config(self):\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):\n-        pixel_values = ids_tensor([self.batch_size, self.seq_length], self.vocab_size - 1)\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size - 1)\n \n         input_mask = None\n         if self.use_input_mask:\n@@ -140,7 +131,7 @@ def prepare_config_and_inputs(\n \n         return (\n             config,\n-            pixel_values,\n+            input_ids,\n             input_mask,\n             head_mask,\n             token_type_ids,\n@@ -177,74 +168,45 @@ def get_pipeline_config(self):\n         config.max_position_embeddings = 1024\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            pixel_values,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            pixel_values,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-    def create_and_check_imagegpt_model(self, config, pixel_values, input_mask, head_mask, token_type_ids, *args):\n+    def create_and_check_imagegpt_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         model = ImageGPTModel(config=config)\n         model.to(torch_device)\n         model.eval()\n \n-        result = model(pixel_values, token_type_ids=token_type_ids, head_mask=head_mask)\n-        result = model(pixel_values, token_type_ids=token_type_ids)\n-        result = model(pixel_values)\n+        result = model(input_ids, token_type_ids=token_type_ids, head_mask=head_mask)\n+        result = model(input_ids, token_type_ids=token_type_ids)\n+        result = model(input_ids)\n \n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n         self.parent.assertEqual(len(result.past_key_values), config.n_layer)\n \n-    def create_and_check_lm_head_model(self, config, pixel_values, input_mask, head_mask, token_type_ids, *args):\n+    def create_and_check_lm_head_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         model = ImageGPTForCausalImageModeling(config)\n         model.to(torch_device)\n         model.eval()\n \n         labels = ids_tensor([self.batch_size, self.seq_length], self.vocab_size - 1)\n-        result = model(pixel_values, token_type_ids=token_type_ids, labels=labels)\n+        result = model(input_ids, token_type_ids=token_type_ids, labels=labels)\n         self.parent.assertEqual(result.loss.shape, ())\n         # ImageGPTForCausalImageModeling doens't have tied input- and output embeddings\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size - 1))\n \n     def create_and_check_imagegpt_for_image_classification(\n-        self, config, pixel_values, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n+        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n     ):\n         config.num_labels = self.num_labels\n         model = ImageGPTForImageClassification(config)\n         model.to(torch_device)\n         model.eval()\n-        result = model(pixel_values, attention_mask=input_mask, token_type_ids=token_type_ids, labels=sequence_labels)\n+        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=sequence_labels)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n \n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n \n         (\n             config,\n-            pixel_values,\n+            input_ids,\n             input_mask,\n             head_mask,\n             token_type_ids,\n@@ -255,7 +217,7 @@ def prepare_config_and_inputs_for_common(self):\n         ) = config_and_inputs\n \n         inputs_dict = {\n-            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n             \"token_type_ids\": token_type_ids,\n             \"head_mask\": head_mask,\n         }\n@@ -275,7 +237,6 @@ class ImageGPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         else {}\n     )\n     test_missing_keys = False\n-    input_name = \"pixel_values\"\n \n     # as ImageGPTForImageClassification isn't included in any auto mapping, we add labels here\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n@@ -351,228 +312,6 @@ def test_forward_signature(self):\n             expected_arg_names = [\"input_ids\"]\n             self.assertListEqual(arg_names[:1], expected_arg_names)\n \n-    def test_resize_tokens_embeddings(self):\n-        (\n-            original_config,\n-            inputs_dict,\n-        ) = self.model_tester.prepare_config_and_inputs_for_common()\n-        if not self.test_resize_embeddings:\n-            self.skipTest(reason=\"test_resize_embeddings is set to False\")\n-\n-        for model_class in self.all_model_classes:\n-            config = copy.deepcopy(original_config)\n-            model = model_class(config)\n-            model.to(torch_device)\n-\n-            if self.model_tester.is_training is False:\n-                model.eval()\n-\n-            model_vocab_size = config.vocab_size\n-            # Retrieve the embeddings and clone theme\n-            model_embed = model.resize_token_embeddings(model_vocab_size)\n-            cloned_embeddings = model_embed.weight.clone()\n-\n-            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n-            model_embed = model.resize_token_embeddings(model_vocab_size + 10)\n-            self.assertEqual(model.config.vocab_size, model_vocab_size + 10)\n-            # Check that it actually resizes the embeddings matrix\n-            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)\n-            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n-\n-            # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n-            model_embed = model.resize_token_embeddings(model_vocab_size - 15)\n-            self.assertEqual(model.config.vocab_size, model_vocab_size - 15)\n-            # Check that it actually resizes the embeddings matrix\n-            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] - 15)\n-\n-            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            # Input ids should be clamped to the maximum size of the vocabulary\n-            inputs_dict[\"pixel_values\"].clamp_(max=model_vocab_size - 15 - 1)\n-\n-            # Check that adding and removing tokens has not modified the first part of the embedding matrix.\n-            models_equal = True\n-            for p1, p2 in zip(cloned_embeddings, model_embed.weight):\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n-    def test_resize_embeddings_untied(self):\n-        (\n-            original_config,\n-            inputs_dict,\n-        ) = self.model_tester.prepare_config_and_inputs_for_common()\n-        if not self.test_resize_embeddings:\n-            self.skipTest(reason=\"test_resize_embeddings is set to False\")\n-\n-        original_config.tie_word_embeddings = False\n-\n-        # if model cannot untied embeddings -> leave test\n-        if original_config.tie_word_embeddings:\n-            self.skipTest(reason=\"tie_word_embeddings is set to False\")\n-\n-        for model_class in self.all_model_classes:\n-            config = copy.deepcopy(original_config)\n-            model = model_class(config).to(torch_device)\n-\n-            # if no output embeddings -> leave test\n-            if model.get_output_embeddings() is None:\n-                continue\n-\n-            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n-            model_vocab_size = config.vocab_size\n-            model.resize_token_embeddings(model_vocab_size + 10)\n-            self.assertEqual(model.config.vocab_size, model_vocab_size + 10)\n-            output_embeds = model.get_output_embeddings()\n-            self.assertEqual(output_embeds.weight.shape[0], model_vocab_size + 10)\n-            # Check bias if present\n-            if output_embeds.bias is not None:\n-                self.assertEqual(output_embeds.bias.shape[0], model_vocab_size + 10)\n-            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n-\n-            # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n-            model.resize_token_embeddings(model_vocab_size - 15)\n-            self.assertEqual(model.config.vocab_size, model_vocab_size - 15)\n-            # Check that it actually resizes the embeddings matrix\n-            output_embeds = model.get_output_embeddings()\n-            self.assertEqual(output_embeds.weight.shape[0], model_vocab_size - 15)\n-            # Check bias if present\n-            if output_embeds.bias is not None:\n-                self.assertEqual(output_embeds.bias.shape[0], model_vocab_size - 15)\n-            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            # Input ids should be clamped to the maximum size of the vocabulary\n-            inputs_dict[\"pixel_values\"].clamp_(max=model_vocab_size - 15 - 1)\n-            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n-\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-\n-            pixel_values = inputs[\"pixel_values\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(pixel_values)\n-\n-            with torch.no_grad():\n-                model(**inputs)[0]\n-\n-    # override because ImageGPT main input name is `pixel_values`\n-    # NOTE: in latest transformers this is deprecated, `input_ids` should be used. TODO\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            with torch.no_grad():\n-                out_ids = model(**inputs)[0]\n-\n-            pixel_values = inputs[\"pixel_values\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(pixel_values)\n-\n-            with torch.no_grad():\n-                out_embeds = model(**inputs)[0]\n-\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n-\n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            try:\n-                pixel_values = inputs[\"pixel_values\"]\n-                traced_model = torch.jit.trace(model, pixel_values)\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                if layer_name in loaded_model_state_dict:\n-                    p2 = loaded_model_state_dict[layer_name]\n-                    if p1.data.ne(p2.data).sum() > 0:\n-                        models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     @unittest.skip(reason=\"The model doesn't support left padding\")  # and it's not used enough to be worth fixing :)\n     def test_left_padding_compatibility(self):\n         pass"
        },
        {
            "sha": "3b4a18bb48ebf40a67adb004e3ed8548d3129fe6",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -143,25 +143,6 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        return (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n     def create_and_check_mamba_model(self, config, input_ids, *args):\n         config.output_hidden_states = True\n         model = MambaModel(config=config)"
        },
        {
            "sha": "5e82956e3efa6c449f4ca39767ec6eaee9c98850",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -22,7 +22,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -161,35 +161,6 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def create_and_check_rwkv_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n         config.output_hidden_states = True\n         model = RwkvModel(config=config)"
        },
        {
            "sha": "79f705785541b679881f6b01fee37ec2d1b0a662",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -399,29 +399,6 @@ def _get_input_ids_and_config(self, batch_size=2):\n \n         return config, input_ids.float(), attention_mask, max_length\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.repeat_interleave(\n-            num_interleave, dim=0\n-        )\n-        generation_config = copy.deepcopy(model.generation_config)\n-        model._prepare_special_tokens(generation_config)\n-        input_ids = (\n-            torch.zeros(input_ids.shape[:2], dtype=torch.int64, layout=input_ids.layout, device=input_ids.device)\n-            + generation_config.decoder_start_token_id\n-        )\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "1d11cbb247caca1480bf1a57b31091e9cd435704",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -415,29 +415,6 @@ def _get_input_ids_and_config(self, batch_size=2):\n \n         return config, input_ids.float(), attention_mask, max_length\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.repeat_interleave(\n-            num_interleave, dim=0\n-        )\n-        generation_config = copy.deepcopy(model.generation_config)\n-        model._prepare_special_tokens(generation_config)\n-        input_ids = (\n-            torch.zeros(input_ids.shape[:2], dtype=torch.int64, layout=input_ids.layout, device=input_ids.device)\n-            + generation_config.decoder_start_token_id\n-        )\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "cef2a6781775a917b1fa1bdab17d81663f79a2ca",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -632,27 +632,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.repeat_interleave(\n-            num_interleave, dim=0\n-        )\n-        input_ids = input_ids[:, :, 0]\n-        generation_config = copy.deepcopy(model.generation_config)\n-        model._prepare_special_tokens(generation_config)\n-        input_ids = torch.zeros_like(input_ids[:, :1]) + generation_config.decoder_start_token_id\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, seq_length = input_ids.shape[:2]\n         subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)"
        },
        {
            "sha": "26fee7d93c3e395eef47bb67c665d1b7773cfebc",
            "filename": "tests/models/speech_to_text/test_modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -416,24 +416,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = tf.repeat(encoder_outputs.last_hidden_state, num_interleave, axis=0)\n-\n-        input_ids = input_ids[:, :, 0]\n-        input_ids = tf.zeros_like(input_ids[:, :1], dtype=tf.int64) + model._get_decoder_start_token_id()\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, seq_length = input_ids.shape[:2]\n         subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)"
        },
        {
            "sha": "73303e374c8484bd4cb9e5e6077f2dc25d9e98b5",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -519,26 +519,6 @@ def test_attention_outputs(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.repeat_interleave(\n-            num_interleave, dim=0\n-        )\n-        input_ids = input_ids[:, :, 0]\n-        input_ids = tf.zeros_like(input_ids[:, :1], dtype=tf.int64) + tf.convert_to_tensor(\n-            [model._get_decoder_start_token_id()]\n-        )\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, mel, seq_length = input_ids.shape\n         subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)"
        },
        {
            "sha": "b4e71ca72e56ed456b13c52ad259c1c1f4fff671",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -868,26 +868,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @staticmethod\n-    def _get_encoder_outputs(\n-        model, input_ids, attention_mask, output_attentions=None, output_hidden_states=None, num_interleave=1\n-    ):\n-        encoder = model.get_encoder()\n-        encoder_outputs = encoder(\n-            input_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.repeat_interleave(\n-            num_interleave, dim=0\n-        )\n-        generation_config = copy.deepcopy(model.generation_config)\n-        model._prepare_special_tokens(generation_config)\n-        input_ids = input_ids[:, :, 0]\n-        input_ids = torch.zeros_like(input_ids[:, :1], dtype=torch.long) + generation_config.decoder_start_token_id\n-        attention_mask = None\n-        return encoder_outputs, input_ids, attention_mask\n-\n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, mel, seq_length = input_ids.shape\n         subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n@@ -3894,13 +3874,6 @@ def prepare_config_and_inputs_for_common(self):\n \n         return config, inputs_dict\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        config, input_features = self.prepare_config_and_inputs()\n-        input_ids = input_features[\"input_ids\"]\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.decoder_seq_length, self.hidden_size])\n-\n-        return (config, input_ids, encoder_hidden_states)\n-\n     def create_and_check_decoder_model_past(self, config, input_ids):\n         config.use_cache = True\n         model = WhisperDecoder(config=config).to(torch_device).eval()"
        },
        {
            "sha": "b9a43ff88b7d2b53a2f4a732e915cebbf57c3a60",
            "filename": "tests/models/xglm/test_modeling_flax_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -22,7 +22,7 @@\n from transformers.testing_utils import is_pt_flax_cross_test, require_flax, require_sentencepiece, slow\n \n from ...generation.test_flax_utils import FlaxGenerationTesterMixin\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n \n \n if is_flax_available():\n@@ -116,20 +116,6 @@ def prepare_config_and_inputs_for_common(self):\n         inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n         return config, inputs_dict\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        config, input_ids, attention_mask = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def check_use_cache_forward(self, model_class_name, config, input_ids, attention_mask):\n         max_decoder_length = 20\n         model = model_class_name(config)"
        },
        {
            "sha": "07a1e579c60b1f52958c5f4e3cb2f98abe775a19",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7734238ff49e14047613615133681a1e2252ce5/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=a7734238ff49e14047613615133681a1e2252ce5",
            "patch": "@@ -29,7 +29,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -125,26 +125,6 @@ def get_config(\n             gradient_checkpointing=gradient_checkpointing,\n         )\n \n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def create_and_check_xglm_model(self, config, input_ids, input_mask, head_mask, *args):\n         model = XGLMModel(config=config)\n         model.to(torch_device)"
        }
    ],
    "stats": {
        "total": 679,
        "additions": 23,
        "deletions": 656
    }
}