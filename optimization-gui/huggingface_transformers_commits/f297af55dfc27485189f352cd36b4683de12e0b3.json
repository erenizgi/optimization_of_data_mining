{
    "author": "tibor-reiss",
    "message": "Fix: take into account meta device (#34134)\n\n* Do not load for meta device\r\n\r\n* Make some minor improvements\r\n\r\n* Add test\r\n\r\n* Update tests/utils/test_modeling_utils.py\r\n\r\nUpdate test parameters\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Make the test simpler\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "f297af55dfc27485189f352cd36b4683de12e0b3",
    "files": [
        {
            "sha": "6f2c6c194f26f99462a5cb8d0f2d2444606a5a5f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f297af55dfc27485189f352cd36b4683de12e0b3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f297af55dfc27485189f352cd36b4683de12e0b3",
            "patch": "@@ -361,6 +361,9 @@ def check_support_param_buffer_assignment(model_to_load, state_dict, start_prefi\n \n     Note: We fully disable this if we are using `deepspeed`\n     \"\"\"\n+    if model_to_load.device.type == \"meta\":\n+        return False\n+\n     if len([key for key in state_dict if key.startswith(start_prefix)]) == 0:\n         return False\n \n@@ -375,7 +378,7 @@ def check_support_param_buffer_assignment(model_to_load, state_dict, start_prefi\n         return False\n \n     # If the model does, the incoming `state_dict` and the `model_to_load` must be the same dtype\n-    first_key = list(model_to_load.state_dict().keys())[0]\n+    first_key = next(iter(model_to_load.state_dict().keys()))\n     if start_prefix + first_key in state_dict:\n         return state_dict[start_prefix + first_key].dtype == model_to_load.state_dict()[first_key].dtype\n "
        },
        {
            "sha": "85e7c20dd5272e2bf0797ccb0c164a5cb1466b23",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f297af55dfc27485189f352cd36b4683de12e0b3/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=f297af55dfc27485189f352cd36b4683de12e0b3",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n import copy\n import glob\n+import itertools\n import json\n import os\n import os.path\n@@ -459,6 +460,19 @@ def test_model_from_config_torch_dtype_str(self):\n         with self.assertRaises(ValueError):\n             model = AutoModel.from_pretrained(TINY_T5, torch_dtype=\"int64\")\n \n+    @require_torch\n+    def test_model_from_pretrained_meta_device(self):\n+        def is_on_meta(model_id, dtype):\n+            with torch.device(\"meta\"):\n+                model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)\n+                return all(value.device.type == \"meta\" for value in model.state_dict().values())\n+\n+        model_ids = (\"fxmarty/tiny-llama-fast-tokenizer\", \"fxmarty/small-llama-testing\")\n+        dtypes = (None, \"auto\", torch.float16)\n+\n+        for model_id, dtype in itertools.product(model_ids, dtypes):\n+            self.assertTrue(is_on_meta(model_id, dtype))\n+\n     def test_model_from_pretrained_torch_dtype(self):\n         # test that the model can be instantiated with dtype of either\n         # 1. explicit from_pretrained's torch_dtype argument"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 18,
        "deletions": 1
    }
}