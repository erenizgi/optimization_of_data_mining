{
    "author": "SunMarc",
    "message": "Clean bnb integration using weight converter (#42426)\n\n* clean bnb\n\n* style\n\n* let's go\n\n* style\n\n* torch\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d",
    "files": [
        {
            "sha": "30ce9d6b673279c904bdd047e076c668dcbffb00",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d",
            "patch": "@@ -674,9 +674,6 @@ def convert_and_load_state_dict_in_model(\n \n     renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]\n     converters = [entry for entry in weight_mapping if isinstance(entry, WeightConverter)]\n-    if hf_quantizer:\n-        # We will add the quantizer's deserialization WeightConverter here.\n-        pass\n \n     param_name_to_load: dict[str, Union[WeightRenaming | WeightConverter]] = {}\n \n@@ -724,11 +721,19 @@ def convert_and_load_state_dict_in_model(\n                 source_pattern = renamed_key\n \n             # 5. Handle dtype casting\n-            if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, renamed_key):\n+            if (\n+                hf_quantizer\n+                and not hf_quantizer.pre_quantized\n+                and hf_quantizer.param_needs_quantization(model, renamed_key)\n+            ):\n                 mapping.quantization_operation = hf_quantizer.get_quantize_ops()\n \n             _dtype = dtype\n-            if dtype_plan != {} and dtype_policy_alt.search(renamed_key):\n+            if hf_quantizer and hf_quantizer.pre_quantized and original_key != renamed_key:\n+                # if the key was renamed as it is not available in the state dict otherwise, it means that we are deserializing it,\n+                # so we need to make sure to load the tensor with the same dtype from the checkpoint\n+                _dtype = None\n+            elif dtype_plan != {} and dtype_policy_alt.search(renamed_key):\n                 matched_dtype_pattern = dtype_policy_alt.search(renamed_key)\n                 if matched_dtype_pattern is not None:\n                     _dtype = dtype_plan[matched_dtype_pattern.group()]"
        },
        {
            "sha": "a682b21ab96db77d90837a84d8e8f71a4974c468",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 95,
            "deletions": 99,
            "changes": 194,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d",
            "patch": "@@ -1,5 +1,4 @@\n import inspect\n-from collections import defaultdict\n from inspect import signature\n \n from ..core_model_loading import ConversionOps\n@@ -37,54 +36,64 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, list[torch.Tensor]],\n+        full_layer_name: str | None = None,\n         model: torch.nn.Module | None = None,\n-        missing_keys=None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         \"\"\"\n         we need to store some parameters to create the quantized weight. For example, bnb requires 6 values that are stored in the checkpoint to recover the quantized weight. So we store them in a dict that it stored in hf_quantizer for now as we can't save it in the op since we create an op per tensor.\n         \"\"\"\n-        target_key, value = tuple(input_dict.items())[0]\n+        value = list(input_dict.values())[0]\n         value = value[0] if isinstance(value, list) else value\n \n-        full_name = target_key\n         # update param name to get the weights instead of the quantized stats\n-        target_key = self.hf_quantizer.get_param_name(target_key)\n-        module, _ = get_module_from_name(model, target_key)\n-\n-        if not self.hf_quantizer.pre_quantized:\n-            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n-            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n-            if issubclass(module.source_cls, Conv1D):\n-                value = value.T\n-            old_value = model.get_parameter_or_buffer(target_key)\n-            new_value = bnb.nn.Params4bit(value, requires_grad=False, **old_value.__dict__).to(value.device)\n-            # remove missing keys that were create when initializing Params4bit\n-            for key in new_value.quant_state.as_dict(packed=True).keys():\n-                missing_keys.discard(f\"{full_name}.{key}\")\n-            module._is_hf_initialized = True\n-            return {target_key: new_value}\n-        else:\n-            module_name = target_key.rsplit(\".\", 1)[0]\n-            # Save the states for later quantization when they are all gathered\n-            if not hasattr(self.hf_quantizer, \"param_quant_stats\"):\n-                self.hf_quantizer.param_quant_stats = defaultdict(dict)\n-            self.hf_quantizer.param_quant_stats[module_name].update({full_name: value})\n-            missing_keys.discard(full_name)\n-            # We are ready for quantization in this case (note, the +1 is for the weight itself)\n-            if len(self.hf_quantizer.param_quant_stats[module_name]) == len(self.hf_quantizer.bnb_keys) + 1:\n-                weight = self.hf_quantizer.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n-                new_value = bnb.nn.Params4bit.from_prequantized(\n-                    data=weight,\n-                    quantized_stats=self.hf_quantizer.param_quant_stats[module_name],\n-                    requires_grad=False,\n-                    device=value.device,\n-                    module=module,\n-                )\n-                module._is_hf_initialized = True\n-                del self.hf_quantizer.param_quant_stats[module_name]\n-                return {target_key: new_value}\n-            return {}\n+        module, _ = get_module_from_name(model, full_layer_name)\n+\n+        # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n+        # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n+        if issubclass(module.source_cls, Conv1D):\n+            value = value.T\n+\n+        old_value = model.get_parameter_or_buffer(full_layer_name)\n+        new_value = bnb.nn.Params4bit(value, requires_grad=False, **old_value.__dict__).to(value.device)\n+        module._is_hf_initialized = True\n+        return {full_layer_name: new_value}\n+\n+\n+class Bnb4bitDeserialize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        model: torch.nn.Module | None = None,\n+        full_layer_name: str | None = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"\n+        Deserialization of bnb keys. We need 6 keys to recreate the quantized weights\n+        \"\"\"\n+        if len(input_dict) == 1:\n+            # special case when we only fetched the weight\n+            # since we collected keys, we need to return it like that\n+            return {full_layer_name: input_dict[\"weight\"]}\n+\n+        for key, value in input_dict.items():\n+            if isinstance(value, list):\n+                input_dict[key] = value[0]\n+\n+        weight = input_dict.pop(\"weight\")\n+        module, _ = get_module_from_name(model, full_layer_name)\n+        new_value = bnb.nn.Params4bit.from_prequantized(\n+            data=weight,\n+            quantized_stats=input_dict,\n+            requires_grad=False,\n+            device=weight.device,\n+            module=module,\n+        )\n+        module._is_hf_initialized = True\n+        return {full_layer_name: new_value}\n \n \n class Bnb8bitQuantize(ConversionOps):\n@@ -95,48 +104,56 @@ def convert(\n         self,\n         input_dict: dict[str, list[torch.Tensor]],\n         model: torch.nn.Module | None = None,\n-        missing_keys=None,\n+        full_layer_name: str | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n-        target_key, value = tuple(input_dict.items())[0]\n+        value = list(input_dict.values())[0]\n         value = value[0] if isinstance(value, list) else value\n \n-        module, tensor_name = get_module_from_name(model, target_key)\n-        module_name = target_key.rsplit(\".\", 1)[0]\n-\n-        if not self.hf_quantizer.pre_quantized:\n-            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n-            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n-            if issubclass(module.source_cls, Conv1D):\n-                value = value.T\n-            value_device = value.device\n-            kwargs = getattr(module, tensor_name).__dict__\n-            kwargs.pop(\"SCB\", None)\n-            new_value = bnb.nn.Int8Params(value.to(\"cpu\"), requires_grad=False, **kwargs).to(value_device)\n-            missing_keys.discard(f\"{module_name}.weight_format\")\n-            missing_keys.discard(f\"{module_name}.SCB\")\n-            return {target_key: new_value}\n-        else:\n-            missing_keys.discard(target_key)\n-            # useless key that gets saved for no reason\n-            if tensor_name.endswith(\"weight_format\"):\n-                return {}\n-            # Save the states for later quantization when they are all gathered\n-            if not hasattr(self.hf_quantizer, \"param_quant_stats\"):\n-                self.hf_quantizer.param_quant_stats = defaultdict(dict)\n-            self.hf_quantizer.param_quant_stats[module_name].update({target_key: value})\n-            # We are ready for quantization in this case (SCB and the weight)\n-            if len(self.hf_quantizer.param_quant_stats[module_name]) == 2:\n-                weight = self.hf_quantizer.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n-                kwargs = getattr(module, \"weight\").__dict__\n-                weight_device = weight.device\n-                new_value = bnb.nn.Int8Params(weight.to(\"cpu\"), requires_grad=False, **kwargs).to(weight_device)\n-                setattr(new_value, \"SCB\", self.hf_quantizer.param_quant_stats[module_name][f\"{module_name}.SCB\"])\n-                del self.hf_quantizer.param_quant_stats[module_name]\n-                # sometimes, weight_format is not saved so we need to remove it from missing keys ...\n-                missing_keys.discard(f\"{module_name}.weight_format\")\n-                return {f\"{module_name}.weight\": new_value}\n-            return {}\n+        module, _ = get_module_from_name(model, full_layer_name)\n+\n+        # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n+        # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n+        if issubclass(module.source_cls, Conv1D):\n+            value = value.T\n+        value_device = value.device\n+        kwargs = model.get_parameter_or_buffer(full_layer_name).__dict__\n+        kwargs.pop(\"SCB\", None)\n+        new_value = bnb.nn.Int8Params(value.to(\"cpu\"), requires_grad=False, **kwargs).to(value_device)\n+        return {full_layer_name: new_value}\n+\n+\n+class Bnb8bitDeserialize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        model: torch.nn.Module | None = None,\n+        full_layer_name: str | None = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"\n+        Deserialization of bnb keys.\n+        \"\"\"\n+        if len(input_dict) == 1:\n+            # special case when we only fetched the weight\n+            # since we collected keys, we need to return it like that\n+            return {full_layer_name: input_dict[\"weight\"]}\n+\n+        for key, value in input_dict.items():\n+            if isinstance(value, list):\n+                input_dict[key] = value[0]\n+\n+        module, _ = get_module_from_name(model, full_layer_name)\n+\n+        weight = input_dict[\"weight\"]\n+        kwargs = model.get_parameter_or_buffer(full_layer_name).__dict__\n+        kwargs[\"SCB\"] = input_dict[\"SCB\"]\n+        new_value = bnb.nn.Int8Params(weight, requires_grad=False, **kwargs).to(weight.device)\n+        module._is_hf_initialized = True\n+        return {full_layer_name: new_value}\n \n \n def _replace_with_bnb_linear(\n@@ -178,8 +195,6 @@ def _replace_with_bnb_linear(\n                             has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,\n                             threshold=quantization_config.llm_int8_threshold,\n                         )\n-                        # hack to create the correct keys in the state dict with the right dtype\n-                        new_module.weight.SCB = torch.empty(1, dtype=torch.float32)\n                         if pre_quantized:\n                             new_module.weight.data = new_module.weight.data.to(dtype=torch.int8)\n                         model._modules[name] = new_module\n@@ -205,25 +220,6 @@ def _replace_with_bnb_linear(\n                                 quant_type=quantization_config.bnb_4bit_quant_type,\n                                 **extra_kwargs,\n                             )\n-                            from bitsandbytes.functional import QuantState\n-\n-                            # hack to create the correct keys in the state dict with the right dtype\n-                            absmax_dtype = (\n-                                torch.uint8 if quantization_config.bnb_4bit_use_double_quant else torch.float32\n-                            )\n-                            new_module.weight.quant_state = QuantState(\n-                                absmax=torch.empty(1, dtype=absmax_dtype),\n-                                code=torch.empty(1, dtype=torch.float32),\n-                                shape=(1,),\n-                                offset=torch.empty(1),\n-                                quant_type=quantization_config.bnb_4bit_quant_type,\n-                                state2=QuantState(\n-                                    absmax=torch.empty(1, dtype=torch.float32),\n-                                    code=torch.empty(1, dtype=torch.float32),\n-                                )\n-                                if quantization_config.bnb_4bit_use_double_quant\n-                                else None,\n-                            )\n                             if pre_quantized:\n                                 # this is kind of an edge case when supporting both loading and quantization ...\n                                 # we need to set the right dtype as we cast the checkpoint with the dtype of the meta model"
        },
        {
            "sha": "63980620464c55cec5a57fa8c9094d391665e7d5",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d",
            "patch": "@@ -37,6 +37,7 @@\n if is_torch_available():\n     import torch\n \n+    from ..core_model_loading import WeightConverter\n     from ..pytorch_utils import Conv1D\n \n logger = logging.get_logger(__name__)\n@@ -298,3 +299,24 @@ def get_quantize_ops(self):\n         from ..integrations.bitsandbytes import Bnb4bitQuantize\n \n         return Bnb4bitQuantize(self)\n+\n+    def get_weight_conversions(self):\n+        from ..integrations.bitsandbytes import Bnb4bitDeserialize\n+\n+        if self.pre_quantized:\n+            return [\n+                WeightConverter(\n+                    source_keys=[\n+                        \"weight.nested_absmax\",\n+                        \"weight.nested_quant_map\",\n+                        \"weight.quant_map\",\n+                        \"weight.absmax\",\n+                        \"weight.quant_state.bitsandbytes__nf4\",\n+                        \"weight.quant_state.bitsandbytes__fp4\",\n+                        \"weight\",\n+                    ],\n+                    target_keys=\"weight\",\n+                    operations=[Bnb4bitDeserialize(self)],\n+                )\n+            ]\n+        return []"
        },
        {
            "sha": "c71d2a9a0be854bc4ea6df46937439c3cdb2c05d",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=1ae4d917ed3badbdb1ffc167e0529f5a6d3c080d",
            "patch": "@@ -34,6 +34,7 @@\n if is_torch_available():\n     import torch\n \n+    from ..core_model_loading import WeightConverter\n     from ..pytorch_utils import Conv1D\n \n logger = logging.get_logger(__name__)\n@@ -246,3 +247,16 @@ def get_quantize_ops(self):\n         from ..integrations.bitsandbytes import Bnb8bitQuantize\n \n         return Bnb8bitQuantize(self)\n+\n+    def get_weight_conversions(self):\n+        from ..integrations.bitsandbytes import Bnb8bitDeserialize\n+\n+        if self.pre_quantized:\n+            return [\n+                WeightConverter(\n+                    source_keys=[\"SCB\", \"weight_format\", \"weight\"],\n+                    target_keys=\"weight\",\n+                    operations=[Bnb8bitDeserialize(self)],\n+                )\n+            ]\n+        return []"
        }
    ],
    "stats": {
        "total": 245,
        "additions": 141,
        "deletions": 104
    }
}