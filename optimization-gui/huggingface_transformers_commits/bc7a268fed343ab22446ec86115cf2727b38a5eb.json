{
    "author": "SunMarc",
    "message": "Fix fp8 + some enhancement (#42455)\n\n* Fix fp8 + some enhancement\n\n* style\n\n* Add coauthor\n\nCo-authored-by: Yang Kai <kai.yang@intel.com>\n\n* fix\n\n* style\n\n* fix tests\n\n* style\n\n* assertin\n\n* style\n\n* fix\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Yang Kai <kai.yang@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "bc7a268fed343ab22446ec86115cf2727b38a5eb",
    "files": [
        {
            "sha": "673b0caf2dd0e09e44bb2e245c3780c5be8aff45",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -826,6 +826,7 @@ def convert_and_load_state_dict_in_model(\n             if hf_quantizer and hf_quantizer.pre_quantized and original_key != renamed_key:\n                 # if the key was renamed as it is not available in the state dict otherwise, it means that we are deserializing it,\n                 # so we need to make sure to load the tensor with the same dtype from the checkpoint\n+                # TODO: make the condition more srict for native fp8 model such as qwen2moe fp8\n                 _dtype = None\n             elif dtype_plan != {} and dtype_policy_alt.search(renamed_key):\n                 matched_dtype_pattern = dtype_policy_alt.search(renamed_key)"
        },
        {
            "sha": "2daf67052ccb16e26593a477e79f8d0b29d1db0b",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 61,
            "deletions": 134,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -13,10 +13,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import re\n-from typing import Optional\n-\n from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import should_convert_module\n from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n \n \n@@ -307,44 +305,38 @@ def w8a8_block_fp8_matmul_compile(\n \n \n class FP8Linear(nn.Linear):\n-    dtype = torch.float8_e4m3fn\n-\n     def __init__(\n         self,\n         in_features: int,\n         out_features: int,\n         bias: bool = False,\n-        dtype=None,\n+        dtype=torch.float8_e4m3fn,\n         block_size: tuple[int, int] | None = None,\n-        device=None,\n         activation_scheme=\"dynamic\",\n     ):\n         super().__init__(in_features, out_features)\n-        self.in_features = in_features\n-        self.out_features = out_features\n \n+        # If block size, is not passed, it means that we are doing per-tensor quantization\n         if block_size is not None:\n             self.block_size = block_size\n         else:\n             self.block_size = (out_features, in_features)\n \n-        self.weight = torch.nn.Parameter(torch.empty(out_features, in_features, dtype=FP8Linear.dtype, device=device))\n+        self.activation_scheme = activation_scheme\n \n-        if self.weight.element_size() == 1:\n-            scale_out_features = (out_features + self.block_size[0] - 1) // self.block_size[0]\n-            scale_in_features = (in_features + self.block_size[1] - 1) // self.block_size[1]\n-            if scale_out_features * scale_in_features == 1:\n-                self.weight_scale_inv = nn.Parameter(torch.tensor(1.0, dtype=torch.float32, device=device))\n-            else:\n-                self.weight_scale_inv = nn.Parameter(\n-                    torch.empty(scale_out_features, scale_in_features, dtype=torch.float32, device=device)\n-                )\n+        self.weight = torch.nn.Parameter(torch.empty(out_features, in_features, dtype=dtype))\n+        scale_out_features = (out_features + block_size[0] - 1) // block_size[0]\n+        scale_in_features = (in_features + block_size[1] - 1) // block_size[1]\n+\n+        if scale_out_features * scale_in_features == 1:\n+            self.weight_scale_inv = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))\n         else:\n-            self.register_parameter(\"weight_scale_inv\", None)\n-        self.activation_scheme = activation_scheme\n+            self.weight_scale_inv = nn.Parameter(\n+                torch.empty(scale_out_features, scale_in_features, dtype=torch.float32)\n+            )\n \n         if self.activation_scheme == \"static\":\n-            self.activation_scale = nn.Parameter(torch.tensor(1.0, dtype=torch.float32, device=device))\n+            self.activation_scale = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))\n \n         if bias:\n             self.bias = nn.Parameter(torch.empty(self.out_features))\n@@ -400,9 +392,7 @@ def _ceil_div(a, b):\n \n \n class FP8Expert(nn.Module):\n-    dtype = torch.float8_e4m3fn\n-\n-    def __init__(self, config, block_size, device):\n+    def __init__(self, config, block_size, dtype=torch.float8_e4m3fn):\n         super().__init__()\n \n         from ..activations import ACT2FN\n@@ -415,34 +405,24 @@ def __init__(self, config, block_size, device):\n         Wg_out, Wg_in = 2 * self.intermediate_dim, self.hidden_dim\n         Wd_out, Wd_in = self.hidden_dim, self.intermediate_dim\n \n-        self.gate_up_proj = nn.Parameter(\n-            torch.zeros(self.num_experts, Wg_out, Wg_in, dtype=FP8Expert.dtype, device=device)\n-        )\n-        self.down_proj = nn.Parameter(\n-            torch.zeros(self.num_experts, Wd_out, Wd_in, dtype=FP8Expert.dtype, device=device)\n-        )\n+        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, Wg_out, Wg_in, dtype=dtype))\n+        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, Wd_out, Wd_in, dtype=dtype))\n \n-        # Create inverse scale tiles only when using 1-byte types (fp8)\n-        if self.gate_up_proj.element_size() == 1:\n-            bo, bi = self.block_size\n+        bo, bi = self.block_size\n \n-            # gate_up tiles: ceil(Wg_out/bo) x ceil(Wg_in/bi)\n-            gu_scale_o = _ceil_div(Wg_out, bo)\n-            gu_scale_i = _ceil_div(Wg_in, bi)\n-            self.gate_up_proj_scale_inv = nn.Parameter(\n-                torch.zeros(self.num_experts, gu_scale_o, gu_scale_i, dtype=torch.float32, device=device)\n-            )\n+        # gate_up tiles: ceil(Wg_out/bo) x ceil(Wg_in/bi)\n+        gu_scale_o = _ceil_div(Wg_out, bo)\n+        gu_scale_i = _ceil_div(Wg_in, bi)\n+        self.gate_up_proj_scale_inv = nn.Parameter(\n+            torch.zeros(self.num_experts, gu_scale_o, gu_scale_i, dtype=torch.float32)\n+        )\n \n-            # down tiles: ceil(Wd_out/bo) x ceil(Wd_in/bi)\n-            dp_scale_o = _ceil_div(Wd_out, bo)\n-            dp_scale_i = _ceil_div(Wd_in, bi)\n-            self.down_proj_scale_inv = nn.Parameter(\n-                torch.zeros(self.num_experts, dp_scale_o, dp_scale_i, dtype=torch.float32, device=device)\n-            )\n-        else:\n-            # Match FP8Linear behavior when not using 1-byte weights\n-            self.register_parameter(\"gate_up_proj_scale_inv\", None)\n-            self.register_parameter(\"down_proj_scale_inv\", None)\n+        # down tiles: ceil(Wd_out/bo) x ceil(Wd_in/bi)\n+        dp_scale_o = _ceil_div(Wd_out, bo)\n+        dp_scale_i = _ceil_div(Wd_in, bi)\n+        self.down_proj_scale_inv = nn.Parameter(\n+            torch.zeros(self.num_experts, dp_scale_o, dp_scale_i, dtype=torch.float32)\n+        )\n \n         # (Optional) bias per projection — many MoEs omit bias; keep None to match your FP8Linear default\n         self.register_parameter(\"gate_up_bias\", None)\n@@ -508,90 +488,46 @@ def linear(self, input: torch.Tensor, weight: torch.Tensor, weight_scale_inv: to\n             return output.to(dtype=input.dtype)\n \n \n-# TODO: we do need this.... but not recursive...\n-def _replace_with_fp8_linear(\n-    model,\n-    tp_plan=None,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    has_been_replaced=False,\n-):\n-    iterator = list(model.named_parameters()).copy()\n-    for name, empty_tensor in iterator:\n-        current_key_name = name\n-        name = name.rsplit(\".\", 1)[0] if \".\" in name else name\n-        module = model.get_submodule(name)\n-\n-        current_key_name_str = re.sub(r\"\\d+\", \"*\", current_key_name)\n-        if not any(key in current_key_name_str for key in (modules_to_not_convert or [])):\n-            with init_empty_weights():\n-                if (\n-                    \"gate_up_proj\" in current_key_name\n-                    or \"down_proj\" in current_key_name\n-                    and \"experts\" in current_key_name\n-                ):  # Experts!\n-                    in_features = empty_tensor.size(-2)\n-                    out_features = empty_tensor.size(-1)\n-                    model.set_submodule(\n-                        name,\n-                        FP8Expert(\n-                            config=model.config,\n-                            block_size=quantization_config.weight_block_size,\n-                            device=empty_tensor.device,\n-                        ),\n-                    )\n-\n-                elif isinstance(module, nn.Linear):\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n-                    model.set_submodule(\n-                        name,\n-                        FP8Linear(\n-                            in_features=in_features,\n-                            out_features=out_features,\n-                            bias=module.bias is not None,\n-                            device=module.weight.device,\n-                            dtype=module.weight.dtype,\n-                            activation_scheme=quantization_config.activation_scheme,\n-                            block_size=quantization_config.weight_block_size,\n-                        ),\n-                    )\n-                has_been_replaced = True\n-        # when changing a layer the TP PLAN for that layer should be updated. TODO\n-\n-    return model, has_been_replaced\n-\n-\n def replace_with_fp8_linear(\n     model,\n     modules_to_not_convert=None,\n     quantization_config=None,\n+    pre_quantized=False,\n ):\n     \"\"\"Helper function to replace model layers with FP8 versions.\"\"\"\n     if quantization_config.dequantize:\n         return model\n \n-    if modules_to_not_convert is None:\n-        modules_to_not_convert = []\n-    modules_to_not_convert += [\"lm_head\"]\n-\n-    if quantization_config.modules_to_not_convert is not None:\n-        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n-    modules_to_not_convert = list(set(modules_to_not_convert))\n-    model, has_been_replaced = _replace_with_fp8_linear(\n-        model,\n-        tp_plan=model._tp_plan,\n-        modules_to_not_convert=modules_to_not_convert,\n-        quantization_config=quantization_config,\n-    )\n+    has_been_replaced = False\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        # we need this to correctly materialize the weights during quantization\n+        module_kwargs = {} if pre_quantized else {\"dtype\": None}\n+        new_module = None\n+        with init_empty_weights():\n+            if \"gate_up_proj\" in module_name or \"down_proj\" in module_name and \"experts\" in module_name:\n+                new_module = FP8Expert(\n+                    config=model.config, block_size=quantization_config.weight_block_size, **module_kwargs\n+                )\n+            elif isinstance(module, nn.Linear):\n+                new_module = FP8Linear(\n+                    in_features=module.in_features,\n+                    out_features=module.out_features,\n+                    bias=module.bias is not None,\n+                    activation_scheme=quantization_config.activation_scheme,\n+                    block_size=quantization_config.weight_block_size,\n+                    **module_kwargs,\n+                )\n+            if new_module is not None:\n+                model.set_submodule(module_name, new_module)\n+                has_been_replaced = True\n \n     if not has_been_replaced:\n         logger.warning(\n             \"You are loading your model using fp8 but no linear modules were found in your model.\"\n             \" Please double check your model architecture.\"\n         )\n-\n     return model\n \n \n@@ -606,7 +542,7 @@ def __init__(self, hf_quantizer):\n     def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]:\n         # Unpack single key/value (value may be wrapped in a list)\n         target_keys, value = tuple(input_dict.items())[0]\n-        value = value[0] if isinstance(value, list) else value\n+        value = value[0]\n \n         # Resolve block size (support dict-like or attr-like quant_config)\n         block_size = None\n@@ -681,24 +617,15 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n-        model: Optional[torch.nn.Module] = None,\n         full_layer_name: str | None = None,\n-        missing_keys=None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         if len(input_dict) < 2:\n-            # in case of no scales, the weights are not quantized, so we return the weights as is\n-            return {\n-                full_layer_name: input_dict[\"weight$\"][0]\n-                if isinstance(input_dict[\"weight$\"], list)\n-                else input_dict[\"weight$\"]\n-            }\n-        quantized = input_dict[\"weight$\"][0] if isinstance(input_dict[\"weight$\"], list) else input_dict[\"weight$\"]\n-        scales = (\n-            input_dict[\"weight_scale_inv\"][0]\n-            if isinstance(input_dict[\"weight_scale_inv\"], list)\n-            else input_dict[\"weight_scale_inv\"]\n-        )\n+            # case where we only got weights, need to check for \"weight$\"\n+            return {full_layer_name: input_dict[\"weight$\"]}\n+\n+        quantized = input_dict[\"weight$\"][0]\n+        scales = input_dict[\"weight_scale_inv\"][0]\n \n         rows, cols = quantized.shape[-2:]\n         block_size = self.hf_quantizer.quantization_config.weight_block_size"
        },
        {
            "sha": "4f4fee40bf9692e9c9f19a01eb182faa2b1e4258",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 65,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -26,10 +26,9 @@\n if is_accelerate_available():\n     from accelerate import init_empty_weights\n \n-import re\n from contextlib import contextmanager\n \n-from ..quantizers.quantizers_utils import get_module_from_name\n+from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n \n \n logger = logging.get_logger(__name__)\n@@ -436,15 +435,6 @@ def mlp_forward(self, hidden_states):\n     return routed_out, router_logits\n \n \n-def should_convert_module(current_key_name, patterns):\n-    current_key_name_str = \".\".join(current_key_name)\n-    if not any(\n-        re.match(f\"{key}\\\\.\", current_key_name_str) or re.match(f\"{key}\", current_key_name_str) for key in patterns\n-    ):\n-        return True\n-    return False\n-\n-\n def dequantize(module, param_name, param_value, target_device, dq_param_name, **kwargs):\n     from ..integrations.tensor_parallel import shard_and_distribute_module\n \n@@ -604,70 +594,28 @@ def swizzle_mxfp4_convertops(blocks, scales, module, proj, target_device, triton\n     )\n \n \n-def _replace_with_mxfp4_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    has_been_replaced=False,\n-    config=None,\n-):\n-    if current_key_name is None:\n-        current_key_name = []\n+def replace_with_mxfp4_linear(model, modules_to_not_convert=None, quantization_config=None):\n+    if quantization_config.dequantize:\n+        return model\n+\n+    from kernels import get_kernel\n+\n+    global triton_kernels_hub\n+    triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n \n-    for name, module in model.named_children():\n-        current_key_name.append(name)\n-        if not should_convert_module(current_key_name, modules_to_not_convert):\n-            current_key_name.pop(-1)\n+    has_been_replaced = False\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n         if module.__class__.__name__ == \"GptOssExperts\" and not quantization_config.dequantize:\n             with init_empty_weights():\n-                model._modules[name] = Mxfp4GptOssExperts(config)\n+                model.set_submodule(module_name, Mxfp4GptOssExperts(model.config))\n                 has_been_replaced = True\n         if module.__class__.__name__ == \"GptOssMLP\" and not quantization_config.dequantize:\n             from types import MethodType\n \n             module.forward = MethodType(mlp_forward, module)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _replace_with_mxfp4_linear(\n-                module,\n-                modules_to_not_convert,\n-                current_key_name,\n-                quantization_config,\n-                has_been_replaced=has_been_replaced,\n-                config=config,\n-            )\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n-\n \n-def replace_with_mxfp4_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    config=None,\n-):\n-    if quantization_config.dequantize:\n-        return model\n-    else:\n-        from kernels import get_kernel\n-\n-        global triton_kernels_hub\n-        triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n-\n-    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n-\n-    if quantization_config.modules_to_not_convert is not None:\n-        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n-    modules_to_not_convert = list(set(modules_to_not_convert))\n-    model, has_been_replaced = _replace_with_mxfp4_linear(\n-        model,\n-        modules_to_not_convert,\n-        current_key_name,\n-        quantization_config,\n-        config=config,\n-    )\n     if not has_been_replaced:\n         logger.warning(\n             \"You are loading your model using mixed-precision FP4 quantization but no linear modules were found in your model.\""
        },
        {
            "sha": "d8f5609a36f4783ee471d11eaf482b16129c04c5",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 24,
            "deletions": 46,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -12,17 +12,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from abc import ABC, abstractmethod\n-from copy import deepcopy\n from typing import TYPE_CHECKING, Any\n \n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_torch_available, logging\n from ..utils.quantization_config import QuantizationConfigMixin, QuantizationMethod\n from .quantizers_utils import get_module_from_name\n \n \n-if is_accelerate_available():\n-    from accelerate.utils import find_tied_parameters\n-\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n@@ -47,48 +43,28 @@ def _assign_original_dtype(module, original_dtype):\n \n def get_keys_to_not_convert(model):\n     r\"\"\"\n-    An utility function to get the key of the module to keep in full precision if any For example for CausalLM modules\n-    we may want to keep the lm_head in full precision for numerical stability reasons. For other architectures, we want\n-    to keep the tied weights of the model. The function will return a list of the keys of the modules to not convert in\n-    int8.\n-\n-    Parameters:\n-    model (`torch.nn.Module`):\n-        Input model\n+    Function to automatically detect keys to not convert for usage like quantization. For example for CausalLM modules\n+    we may want to keep the lm_head in full precision for numerical stability reasons.\n     \"\"\"\n-    # Create a copy of the model and tie the weights, then\n-    # check if it contains tied weights\n-    tied_model = deepcopy(model)  # this has 0 cost since it is done inside `init_empty_weights` context manager`\n-    tied_model.tie_weights()\n-\n-    tied_params = find_tied_parameters(tied_model)\n-    tied_keys = sum(tied_params, [])\n-    has_tied_params = len(tied_keys) > 0\n-\n-    # If there is not tied weights, we want to keep the lm_head（output_embedding) in full precision\n-    if not has_tied_params:\n-        output_emb = model.get_output_embeddings()\n-        if output_emb is not None:\n-            list_last_module = [name for name, module in model.named_modules() if id(module) == id(output_emb)]\n-            return list_last_module\n-\n-    # otherwise, no tied weights, no output embedding defined, simply keep the last module in full precision\n-    list_modules = list(model.named_parameters())\n-    list_last_module = [list_modules[-1][0]]\n-    # add last module together with tied weights\n-    intersection = set(list_last_module) - set(tied_keys)\n-    list_untouched = list(set(tied_keys)) + list(intersection)\n-\n-    # remove \".weight\" from the keys\n-    names_to_remove = [\".weight\", \".bias\"]\n-    filtered_module_names = []\n-    for name in list_untouched:\n-        for name_to_remove in names_to_remove:\n-            if name_to_remove in name:\n-                name = name.replace(name_to_remove, \"\")\n-        filtered_module_names.append(name)\n-\n-    return filtered_module_names\n+    # remove tied weights\n+    tied_keys = set()\n+    if len(model.all_tied_weights_keys) > 0:\n+        tied_keys = set(model.all_tied_weights_keys.values()) | set(model.all_tied_weights_keys.keys())\n+\n+    # remove last module\n+    last_module_key = {list(model.named_parameters())[-1][0]}\n+\n+    # remove output emb\n+    output_emb_module = model.get_output_embeddings()\n+    output_emb_keys = {\n+        name\n+        for name, module in model.named_modules()\n+        if output_emb_module is not None and id(module) == id(output_emb_module)\n+    }\n+    candidates = tied_keys | last_module_key | output_emb_keys\n+\n+    modules_to_not_convert = {name.replace(suffix, \"\") for name in candidates for suffix in [\".weight\", \".bias\"]}\n+    return modules_to_not_convert\n \n \n class HfQuantizer(ABC):\n@@ -360,6 +336,8 @@ def get_modules_to_not_convert(\n         if keep_in_fp32_modules is not None:\n             modules_to_not_convert.extend(keep_in_fp32_modules)\n \n+        modules_to_not_convert = list(set(modules_to_not_convert))\n+\n         return modules_to_not_convert\n \n     @property"
        },
        {
            "sha": "ddc6c037ff14f73844f368e69d7fc98f53730e68",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -165,16 +165,15 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations.finegrained_fp8 import replace_with_fp8_linear\n \n-        # takes 2 fucking seconds\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n \n-        # while this one is 81ms :)\n         model = replace_with_fp8_linear(\n             model,\n             modules_to_not_convert=self.modules_to_not_convert,\n             quantization_config=self.quantization_config,\n+            pre_quantized=self.pre_quantized,\n         )\n \n         model.config.quantization_config = self.quantization_config"
        },
        {
            "sha": "e5e70af1ab6b7fe9cd32bd7e8524b14dcdc48609",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -301,6 +301,7 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         keep_in_fp32_modules: list[str] | None = None,\n+        use_kernels: bool = False,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_mxfp4_linear\n@@ -309,7 +310,6 @@ def _process_model_before_weight_loading(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n \n-        use_kernels = kwargs.get(\"use_kernels\", False)\n         # if we are using kernels, we can't use the quantized model, since the forward pass is different and needs special handling\n         if use_kernels:\n             logger.warning_once(\n@@ -318,12 +318,8 @@ def _process_model_before_weight_loading(\n             )\n             self.quantization_config.dequantize = True\n \n-        config = model.config\n         model = replace_with_mxfp4_linear(\n-            model,\n-            modules_to_not_convert=self.modules_to_not_convert,\n-            quantization_config=self.quantization_config,\n-            config=config,\n+            model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n \n         model.config.quantization_config = self.quantization_config"
        },
        {
            "sha": "5e3429b602f8778e35511e31ecec5196321ce97c",
            "filename": "src/transformers/quantizers/quantizers_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fquantizers_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/src%2Ftransformers%2Fquantizers%2Fquantizers_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizers_utils.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -11,11 +11,31 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Any\n+import re\n+from typing import Any, Optional\n \n \n def get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n     if \".\" in tensor_name:\n         module_name, tensor_name = tensor_name.rsplit(\".\", 1)\n         module = module.get_submodule(module_name)\n     return module, tensor_name\n+\n+\n+def should_convert_module(full_name, patterns: Optional[list] = None):\n+    if patterns is None:\n+        return True\n+\n+    # We should avoid converting in the following situations:\n+    # 1. The pattern appears as a prefix followed by a dot in `full_name`\n+    #    (e.g., \"model.decoder.layer.11.\" matches \"model.decoder.layer.11.attn.weight\").\n+    # 2. The pattern matches `full_name` exactly or via regex\n+    #    (e.g., \"lm_head\" matches \"lm_head\"; \"model.decoder.layer.*\" matches \"model.decoder.layer.11.attn.weight\").\n+    # 3. `full_name` ends with the pattern\n+    #    (e.g., \"fc1\" matches \"model.decoder.layers.23.fc1\").\n+\n+    should_not_convert = any(\n+        re.match(f\"{key}\\\\.\", full_name) or re.match(f\"{key}\", full_name) or full_name.endswith(key)\n+        for key in patterns\n+    )\n+    return not should_not_convert"
        },
        {
            "sha": "ea569bfc33470961c5c64dc64ff5d2cc65a173e8",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc7a268fed343ab22446ec86115cf2727b38a5eb/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc7a268fed343ab22446ec86115cf2727b38a5eb/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=bc7a268fed343ab22446ec86115cf2727b38a5eb",
            "patch": "@@ -86,7 +86,10 @@ class FP8QuantizerTest(unittest.TestCase):\n     quantized_model_name = \"hf-internal-testing/Llama-3.2-1B-Instruct-fp8\"\n     input_text = \"Once upon a time\"\n     max_new_tokens = 10\n-    EXPECTED_OUTPUT = \"Once upon a time, there was a man who was very rich.\"\n+    EXPECTED_OUTPUTS = {\n+        \"Once upon a time, there was a little girl who loved to play\",\n+        \"Once upon a time, there was a man who was very rich.\",\n+    }\n     EXPECTED_DEQUANTIZED_OUTPUT = \"Once upon a time, in a small village nestled in the rolling hills\"\n     device_map = torch_device\n     offload_device_map = {\n@@ -146,25 +149,21 @@ def test_quantized_model_conversion(self):\n         for module in model.modules():\n             if isinstance(module, torch.nn.Linear):\n                 nb_linears += 1\n-\n         model = replace_with_fp8_linear(model, quantization_config=quantization_config)\n         nb_fp8_linear = 0\n         for module in model.modules():\n             if isinstance(module, FP8Linear):\n                 nb_fp8_linear += 1\n-        print(model)\n-        self.assertEqual(nb_linears - 1, nb_fp8_linear)\n-\n+        self.assertEqual(nb_linears, nb_fp8_linear)\n         with init_empty_weights():\n             model = OPTForCausalLM(config)\n-        quantization_config = FineGrainedFP8Config(modules_to_not_convert=[\"fc1\"])\n-        model = replace_with_fp8_linear(model, quantization_config=quantization_config)\n+        quantization_config = FineGrainedFP8Config()\n+        model = replace_with_fp8_linear(model, modules_to_not_convert=[\"fc1\"], quantization_config=quantization_config)\n         nb_fp8_linear = 0\n         for module in model.modules():\n             if isinstance(module, FP8Linear):\n                 nb_fp8_linear += 1\n-\n-        self.assertEqual(nb_linears - 25, nb_fp8_linear)\n+        self.assertEqual(nb_linears - 24, nb_fp8_linear)\n \n     def test_quantizer_validation_no_accelerator(self):\n         \"\"\"Test quantizer validation when CUDA/XPU is not available\"\"\"\n@@ -193,7 +192,7 @@ def test_quantized_model(self):\n \n         output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n         output_tokens = self.tokenizer.decode(output[0], skip_special_tokens=True)\n-        self.assertEqual(output_tokens, self.EXPECTED_OUTPUT)\n+        self.assertIn(output_tokens, self.EXPECTED_OUTPUTS)\n \n     def test_dequantized_model(self):\n         \"\"\"\n@@ -233,7 +232,7 @@ def test_save_pretrained(self):\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n \n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n     def test_weight_and_weight_scale_inv(self):\n         \"\"\"\n@@ -268,11 +267,10 @@ def test_quantized_model_multi_accelerator(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name, device_map=\"auto\", quantization_config=quantization_config\n         )\n-        print(\"hf_device_map\", quantized_model.hf_device_map)\n         self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n     @require_torch_multi_accelerator\n     def test_save_pretrained_multi_accelerators(self):\n@@ -288,7 +286,7 @@ def test_save_pretrained_multi_accelerators(self):\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n \n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n     def test_quantized_model_offload(self):\n         \"\"\"\n@@ -312,7 +310,7 @@ def test_save_pretrained_offload(self):\n \n             quantized_model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.offload_device_map)\n             output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n \n @require_torch_accelerator\n@@ -330,7 +328,7 @@ def test_linear_preserves_shape(self):\n         \"\"\"\n         from transformers.integrations import FP8Linear\n \n-        linear = FP8Linear(256, 256, block_size=(128, 128), device=self.device)\n+        linear = FP8Linear(256, 256, block_size=(128, 128)).to(self.device)\n         x = torch.rand((1, 5, 256)).to(self.device)\n \n         x_ = linear(x)\n@@ -342,7 +340,7 @@ def test_linear_with_diff_feature_size_preserves_shape(self):\n         \"\"\"\n         from transformers.integrations import FP8Linear\n \n-        linear = FP8Linear(128, 256, block_size=(128, 128), device=self.device)\n+        linear = FP8Linear(128, 256, block_size=(128, 128)).to(self.device)\n         x = torch.rand((1, 5, 128)).to(self.device)\n \n         x_ = linear(x)"
        }
    ],
    "stats": {
        "total": 409,
        "additions": 138,
        "deletions": 271
    }
}