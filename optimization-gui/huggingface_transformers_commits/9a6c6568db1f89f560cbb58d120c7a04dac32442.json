{
    "author": "McPatate",
    "message": "feat: support request cancellation (#40599)\n\n* feat: support request cancellation\n\n* test: add cancellation test\n\n* refactor: use exisitng fn to check req cancellation\n\n* feat(cb): make cancellation thread safe\n\n* refactor(serve): update test to use `requests` instead of `httpx`",
    "sha": "9a6c6568db1f89f560cbb58d120c7a04dac32442",
    "files": [
        {
            "sha": "434ba0bdbcb6da216aced063b38fb67000bcfcbb",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 23,
            "deletions": 10,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6c6568db1f89f560cbb58d120c7a04dac32442/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6c6568db1f89f560cbb58d120c7a04dac32442/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=9a6c6568db1f89f560cbb58d120c7a04dac32442",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import asyncio\n import base64\n import copy\n import datetime\n@@ -24,7 +25,7 @@\n import threading\n import time\n from argparse import ArgumentParser, Namespace\n-from collections.abc import Generator, Iterable\n+from collections.abc import AsyncGenerator, Generator, Iterable\n from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n from io import BytesIO\n@@ -127,10 +128,11 @@ class TransformersResponseCreateParamsStreaming(ResponseCreateParamsStreaming, t\n \n     class TransformersCompletionCreateParamsStreaming(CompletionCreateParamsStreaming, total=False):\n         \"\"\"\n-        OpenAI's CompletionCreateParamsStreaming with an additional field for the generation config (as a json string).\n+        OpenAI's CompletionCreateParamsStreaming with additional fields for the generation config (as a json string) and passing the request_id\n         \"\"\"\n \n         generation_config: str\n+        request_id: str\n \n     class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total=False):\n         \"\"\"\n@@ -784,7 +786,7 @@ def get_gen_models(self) -> list[dict[str, any]]:\n                 for model in model_infos\n             ]\n \n-    def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+    def continuous_batching_chat_completion(self, req: dict) -> AsyncGenerator[str, None]:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n \n@@ -832,13 +834,8 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n             model.device\n         )\n \n-        def stream_chat_completion(_inputs):\n+        def stream_chat_completion(request_id, decode_stream):\n             try:\n-                decode_stream = DecodeStream(_inputs.tolist(), False)\n-                request_id = self.running_continuous_batching_manager.add_request(\n-                    _inputs, request_id=req.get(\"request_id\"), max_new_tokens=generation_config.max_new_tokens\n-                )\n-\n                 # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n                 # they come from the assistant.\n                 yield self.build_chat_completion_chunk(request_id, role=\"assistant\", model=model_id_and_revision)\n@@ -862,9 +859,25 @@ def stream_chat_completion(_inputs):\n \n             except Exception as e:\n                 logger.error(str(e))\n+                self.running_continuous_batching_manager.cancel_request(request_id)\n                 yield f'data: {{\"error\": \"{str(e)}\"}}'\n \n-        return stream_chat_completion(inputs[0])\n+        async def cancellation_wrapper(_inputs):\n+            request_id = None\n+            try:\n+                decode_stream = DecodeStream(_inputs.tolist(), False)\n+                request_id = self.running_continuous_batching_manager.add_request(\n+                    _inputs, request_id=req.get(\"request_id\"), max_new_tokens=generation_config.max_new_tokens\n+                )\n+                for chunk in stream_chat_completion(request_id, decode_stream):\n+                    yield chunk\n+                    await asyncio.sleep(0)  # Yield control to the event loop to check for cancellations\n+            except asyncio.CancelledError:\n+                if request_id is not None:\n+                    self.running_continuous_batching_manager.cancel_request(request_id)\n+                    logger.warning(f\"Request {request_id} was cancelled.\")\n+\n+        return cancellation_wrapper(inputs[0])\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:"
        },
        {
            "sha": "0ddd5bde8968e225078251542e3c946016054e37",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6c6568db1f89f560cbb58d120c7a04dac32442/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6c6568db1f89f560cbb58d120c7a04dac32442/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=9a6c6568db1f89f560cbb58d120c7a04dac32442",
            "patch": "@@ -226,6 +226,7 @@ def prepare_next_batch(self) -> bool:\n         \"\"\"Prepare tensors and metadata for the next model forward pass.\"\"\"\n         # Get new requests from the queue\n         self._get_new_requests()\n+        self.scheduler.clear_cancelled_requests()\n         if not self.scheduler.has_pending_requests():\n             return False\n \n@@ -547,6 +548,15 @@ def add_requests(self, inputs: list[list[int]], **kwargs):\n         for input_ids in inputs:\n             self.add_request(input_ids, **kwargs)\n \n+    def cancel_request(self, request_id: str):\n+        \"\"\"Cancel a request by its ID.\n+\n+        Args:\n+            request_id: The ID of the request to cancel\n+        \"\"\"\n+        if self.batch_processor is not None:\n+            self.batch_processor.scheduler.set_request_cancellation(request_id)\n+\n     def get_result(self, request_id=None, timeout=None) -> Optional[GenerationOutput]:\n         \"\"\"Retrieve one result from the output queue.\n \n@@ -577,10 +587,13 @@ def __iter__(self):\n \n     def request_id_iter(self, request_id):\n         \"\"\"Iterate over results matching a specific request id as they become available.\"\"\"\n-        while self._generation_thread is not None and self._generation_thread.is_alive():\n+        request_cancelled = False\n+        while self._generation_thread is not None and self._generation_thread.is_alive() and not request_cancelled:\n             result = self.get_result(request_id=request_id, timeout=0.1)\n             if result is not None:\n                 yield result\n+            if self.batch_processor is not None:\n+                request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n     @traced\n     def warmup(self, batch_processor):"
        },
        {
            "sha": "63ac244723fbc2d5d0dc4c7a123861410187a577",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6c6568db1f89f560cbb58d120c7a04dac32442/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6c6568db1f89f560cbb58d120c7a04dac32442/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=9a6c6568db1f89f560cbb58d120c7a04dac32442",
            "patch": "@@ -12,6 +12,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import threading\n from abc import ABC, abstractmethod\n from collections import deque\n \n@@ -32,6 +33,8 @@ def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = Fa\n         self.waiting_requests_order: deque[str] = deque()\n         self.cache = cache\n         self.retain_cache_on_finish = retain_cache_on_finish\n+        self._cancellation_lock = threading.Lock()\n+        self._requests_to_cancel: set[str] = set()\n \n     @abstractmethod\n     def add_waiting_request(self, state: RequestState):\n@@ -58,6 +61,30 @@ def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n             return self.active_requests[request_id].static_outputs\n         return []\n \n+    @traced\n+    def set_request_cancellation(self, request_id: str):\n+        with self._cancellation_lock:\n+            self._requests_to_cancel.add(request_id)\n+\n+    @traced\n+    def clear_cancelled_requests(self):\n+        with self._cancellation_lock:\n+            for request_id in self._requests_to_cancel:\n+                if request_id in self.active_requests:\n+                    del self.active_requests[request_id]\n+                if request_id in self.waiting_requests:\n+                    del self.waiting_requests[request_id]\n+                if request_id in self.waiting_requests_order:\n+                    self.waiting_requests_order.remove(request_id)\n+                self.cache.free_blocks(request_id)\n+            self._requests_to_cancel = set()\n+\n+    @traced\n+    def request_is_cancelled(self, request_id: str) -> bool:\n+        return request_id in self._requests_to_cancel or (\n+            request_id not in self.active_requests and request_id not in self.waiting_requests\n+        )\n+\n \n @attach_tracer()\n class FIFOScheduler(Scheduler):"
        },
        {
            "sha": "b6180d1308cf13018623db8b57e2208fb27473ce",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6c6568db1f89f560cbb58d120c7a04dac32442/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6c6568db1f89f560cbb58d120c7a04dac32442/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=9a6c6568db1f89f560cbb58d120c7a04dac32442",
            "patch": "@@ -19,6 +19,7 @@\n from unittest.mock import patch\n \n import aiohttp.client_exceptions\n+import requests\n from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n from parameterized import parameterized\n \n@@ -492,6 +493,37 @@ def test_tool_call(self):\n         self.assertTrue(all(reason is None for reason in finish_reasons[:-1]))\n \n \n+def _get_scheduler(serve_command):\n+    # Defensive navigation in case any layer is renamed in the future\n+    cbm = getattr(serve_command, \"running_continuous_batching_manager\", None)\n+    assert cbm is not None, \"ServeCommand has no running_continuous_batching_manager\"\n+    bp = getattr(cbm, \"batch_processor\", None)\n+    assert bp is not None, \"CBM has no batch_processor\"\n+    sched = getattr(bp, \"scheduler\", None)\n+    assert sched is not None, \"batch_processor has no scheduler\"\n+    return sched\n+\n+\n+def _open_stream_and_cancel(base_url: str, request_id: str):\n+    with requests.Session() as s:\n+        with s.post(\n+            f\"{base_url}/v1/chat/completions\",\n+            json={\n+                \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+                \"stream\": True,\n+                \"messages\": [{\"role\": \"user\", \"content\": \"Count slowly so I can cancel you.\"}],\n+                \"request_id\": request_id,\n+            },\n+            stream=True,\n+            timeout=30,\n+        ) as resp:\n+            assert resp.status_code == 200\n+\n+            for _ in resp.iter_content(chunk_size=None):\n+                resp.close()\n+                break\n+\n+\n @slow  # server startup time is slow on our push CI\n @require_openai\n class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, unittest.TestCase):\n@@ -560,6 +592,33 @@ def test_max_tokens_not_set_in_req(self):\n             )\n         )\n \n+    def test_request_cancellation(self):\n+        \"\"\"Tests that a request can be cancelled.\"\"\"\n+\n+        base_url = f\"http://127.0.0.1:{self.port}\"\n+        request_id = \"test-cancel\"\n+\n+        _open_stream_and_cancel(base_url, request_id)\n+\n+        scheduler = _get_scheduler(self.serve_command)\n+\n+        # Because cancellation is non-blocking, poll for a short, bounded time.\n+        deadline = time.time() + 8.0  # generous but still CI-friendly\n+        last_seen = None\n+        while time.time() < deadline:\n+            is_cancelled = scheduler.request_is_cancelled(request_id)\n+            if is_cancelled:\n+                break\n+            last_seen = time.time()\n+            time.sleep(0.1)  # don't spin the CPU\n+\n+        is_cancelled = scheduler.request_is_cancelled(request_id)\n+        self.assertTrue(\n+            is_cancelled,\n+            f\"Request {request_id} still present in scheduler after cancellation \"\n+            f\"(last seen at {last_seen}). Check cancellation propagation.\",\n+        )\n+\n \n @require_openai\n class ServeResponsesMixin:"
        }
    ],
    "stats": {
        "total": 134,
        "additions": 123,
        "deletions": 11
    }
}