{
    "author": "zucchini-nlp",
    "message": "[chameleon] fix num image token check (#36918)\n\n* [chameleon] fix num image token check\n\n* embed after merging image token\n\n* skip this also\n\n* mistral require_read_token",
    "sha": "57f551c78d681a5441d0f1f04e9a69d4984e8990",
    "files": [
        {
            "sha": "33193365359f7c2740ccbb98b8c01de91832dd39",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/57f551c78d681a5441d0f1f04e9a69d4984e8990/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57f551c78d681a5441d0f1f04e9a69d4984e8990/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=57f551c78d681a5441d0f1f04e9a69d4984e8990",
            "patch": "@@ -1289,13 +1289,10 @@ def forward(\n                 \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n             )\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n-\n         if pixel_values is not None:\n             image_tokens = self.get_image_tokens(pixel_values)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_tokens.numel():\n+            if not is_torchdynamo_compiling() and input_ids[special_image_mask].numel() != image_tokens.numel():\n                 n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum()\n                 n_image_features = image_tokens.shape[0] * image_tokens.shape[1]\n                 raise ValueError(\n@@ -1304,6 +1301,9 @@ def forward(\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n \n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n             past_key_values = DynamicCache()"
        },
        {
            "sha": "f096d667d6da5b7b698513f4c15690c9c7499784",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/57f551c78d681a5441d0f1f04e9a69d4984e8990/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57f551c78d681a5441d0f1f04e9a69d4984e8990/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=57f551c78d681a5441d0f1f04e9a69d4984e8990",
            "patch": "@@ -126,6 +126,7 @@\n     \"ayavision\",\n     \"gemma3\",\n     \"mistral3\",\n+    \"chameleon\",\n ]\n \n "
        },
        {
            "sha": "ae06de34ab25d627da76158f4269dbde0e4e13bd",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 153,
            "deletions": 8,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/57f551c78d681a5441d0f1f04e9a69d4984e8990/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57f551c78d681a5441d0f1f04e9a69d4984e8990/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=57f551c78d681a5441d0f1f04e9a69d4984e8990",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch chameleon model.\"\"\"\n \n+import copy\n import unittest\n \n import requests\n@@ -30,7 +31,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -52,12 +53,12 @@ def __init__(\n         self,\n         parent,\n         batch_size=13,\n-        seq_length=7,\n+        seq_length=35,\n         is_training=False,\n         use_input_mask=True,\n         use_labels=True,\n         vocab_size=99,\n-        image_token_id=98,\n+        image_token_id=4,\n         hidden_size=32,\n         num_hidden_layers=2,\n         num_attention_heads=2,\n@@ -73,9 +74,9 @@ def __init__(\n         num_labels=3,\n         num_choices=4,\n         pad_token_id=0,\n-        vq_num_embeds=12,\n-        vq_embed_dim=12,\n-        vq_channel_multiplier=[1, 2],\n+        vq_num_embeds=5,\n+        vq_embed_dim=5,\n+        vq_channel_multiplier=[1, 4],\n         vq_img_token_start_id=10,  # has to be less than vocab size when added with vq_num_embeds\n         scope=None,\n     ):\n@@ -138,7 +139,9 @@ def get_config(self):\n         start = self.vq_img_token_start_id\n         end = self.vq_img_token_start_id + self.vq_num_embeds\n         for i in range(start, end):\n-            vocab_map[i] = f\"IMGIMGBS{i}\"  # dummy str for each token, anything starting with IMGIMG\n+            image_token_infix = \"\".join(chr(ord(\"A\") + int(c)) for c in str(i))\n+            # dummy str for each image token, anything starting with IMGIMG\n+            vocab_map[i] = f\"IMGIMG{image_token_infix}Z\"\n \n         return ChameleonConfig(\n             vocab_size=self.vocab_size,\n@@ -275,7 +278,6 @@ class ChameleonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         {\n             \"feature-extraction\": ChameleonModel,\n             \"text-generation\": ChameleonForConditionalGeneration,\n-            \"image-text-to-text\": ChameleonForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}\n@@ -330,6 +332,149 @@ def test_model_rope_scaling(self, scaling_type):\n     def test_batching_equivalence(self):\n         pass\n \n+    @unittest.skip(\"Chameleon VQ model cannot be squishes more due to hardcoded layer params in model code\")\n+    def test_model_is_small(self):\n+        pass\n+\n+\n+class ChameleonVision2SeqModelTester(ChameleonModelTester):\n+    def __init__(self, parent, image_size=10, **kwargs):\n+        super().__init__(parent, **kwargs)\n+        self.image_size = image_size\n+        self.image_seq_length = 25\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[:, : self.image_seq_length] = self.image_token_id\n+        attention_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n+        pixel_values = floats_tensor([self.batch_size, 3, self.image_size, self.image_size])\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class ChameleonVision2SeqModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (ChameleonModel, ChameleonForConditionalGeneration) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-text-to-text\": ChameleonForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+\n+    def setUp(self):\n+        self.model_tester = ChameleonVision2SeqModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=ChameleonConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(\"Chameleon forces some token ids to be -inf!\")\n+    def test_batching_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\"Chameleon cannot do offload because it uses `self.linear.weight` in forward\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(\"Chameleon cannot do offload because it uses `self.linear.weight` in forward\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(\"Chameleon cannot do offload because it uses `self.linear.weight` in forward\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @unittest.skip(\"Chameleon VQ model cannot be squishes more due to hardcoded layer params in model code\")\n+    def test_model_is_small(self):\n+        pass\n+\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            curr_input_dict = copy.deepcopy(input_dict)  # the below tests modify dict in-place\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**curr_input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n \n @require_torch\n class ChameleonIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "4bebc8a3ad567a96fee623430efd846c42e8cc29",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/57f551c78d681a5441d0f1f04e9a69d4984e8990/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57f551c78d681a5441d0f1f04e9a69d4984e8990/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=57f551c78d681a5441d0f1f04e9a69d4984e8990",
            "patch": "@@ -20,7 +20,7 @@\n import requests\n \n from transformers import PixtralProcessor\n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import require_read_token, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -35,6 +35,7 @@\n \n \n @require_vision\n+@require_read_token\n class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     \"\"\"This tests Pixtral processor with the new `spatial_merge_size` argument in Mistral3.\"\"\"\n "
        }
    ],
    "stats": {
        "total": 173,
        "additions": 160,
        "deletions": 13
    }
}