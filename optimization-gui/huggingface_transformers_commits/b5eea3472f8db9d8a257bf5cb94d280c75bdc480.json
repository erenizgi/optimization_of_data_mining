{
    "author": "MekkCyber",
    "message": "[Quantization] CI green by end of year (#42951)\n\n* initial\n\n* initial commit\n\n* fix\n\n* fix\n\n* first fix\n\n* second fix\n\n* second fix\n\n* revert\n\n* fix",
    "sha": "b5eea3472f8db9d8a257bf5cb94d280c75bdc480",
    "files": [
        {
            "sha": "1d051b72286cf0377b4ef442d80194304efff6be",
            "filename": "src/transformers/integrations/compressed_tensors.py",
            "status": "added",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py?ref=b5eea3472f8db9d8a257bf5cb94d280c75bdc480",
            "patch": "@@ -0,0 +1,48 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+import torch\n+\n+from transformers.utils import logging\n+from transformers.utils.import_utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import get_module_from_name\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class CompressedTensorsMarkInitialize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        full_layer_name: str | None = None,\n+        missing_keys=None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        _, value = tuple(input_dict.items())[0]\n+        module, tensor_name = get_module_from_name(model, full_layer_name)\n+        module._is_hf_initialized = True\n+\n+        return {full_layer_name: value}"
        },
        {
            "sha": "6e9e743d565a97c353a91c80eb7aba86662d3cf7",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=b5eea3472f8db9d8a257bf5cb94d280c75bdc480",
            "patch": "@@ -109,3 +109,17 @@ def is_qat_trainable(self) -> bool:\n     def is_serializable(self) -> bool:\n         \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\"\n         return True\n+\n+    def get_weight_conversions(self):\n+        from ..core_model_loading import WeightConverter\n+        from ..integrations.compressed_tensors import CompressedTensorsMarkInitialize\n+\n+        return [\n+            WeightConverter(\n+                source_patterns=[\n+                    \"compressed\",\n+                ],\n+                target_patterns=\"compressed\",\n+                operations=[CompressedTensorsMarkInitialize(self)],\n+            ),\n+        ]"
        },
        {
            "sha": "3ab6b430cb7cad12bf0ab6004f0dfe8fcc79d9ec",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=b5eea3472f8db9d8a257bf5cb94d280c75bdc480",
            "patch": "@@ -177,7 +177,7 @@ def test_compute_module_sizes(self):\n \n             # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n             hf_quantizer = AutoHfQuantizer.from_config(BitsAndBytesConfig(load_in_4bit=True), pre_quantized=False)\n-            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            hf_quantizer.preprocess_model(model=model, config=model.config, device_map=expanded_device_map)\n             quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n \n             expected_keys = [name for name, _ in model.named_parameters()] + ["
        },
        {
            "sha": "685cd1f67354f8b0991150fda602d311a47157f2",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5eea3472f8db9d8a257bf5cb94d280c75bdc480/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=b5eea3472f8db9d8a257bf5cb94d280c75bdc480",
            "patch": "@@ -446,8 +446,10 @@ def test_compute_module_sizes(self):\n \n             # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n             hf_quantizer = AutoHfQuantizer.from_config(BitsAndBytesConfig(load_in_8bit=True), pre_quantized=False)\n-            hf_quantizer.preprocess_model(model=model, config=model.config)\n-            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+            hf_quantizer.preprocess_model(model=model, config=model.config, device_map=expanded_device_map)\n+            quantized_model_size, _ = compute_module_sizes(\n+                model, hf_quantizer, only_modules=False, device_map=expanded_device_map\n+            )\n \n             expected_keys = [name for name, _ in model.named_parameters()] + [\n                 name for name, _ in model.named_buffers()"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 67,
        "deletions": 3
    }
}