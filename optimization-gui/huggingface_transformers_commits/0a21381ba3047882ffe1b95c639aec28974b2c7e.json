{
    "author": "leloykun",
    "message": "Uniformize kwargs for chameleon processor (#32181)\n\n* uniformize kwargs of Chameleon\r\n\r\n* fix linter nit\r\n\r\n* rm stride default\r\n\r\n* add tests for chameleon processor\r\n\r\n* fix tests\r\n\r\n* add comment on get_component\r\n\r\n* rm Chameleon's slow tokenizer\r\n\r\n* add check order images text + nit\r\n\r\n* update docs and tests\r\n\r\n* Fix LlamaTokenizer tests\r\n\r\n* fix gated repo access\r\n\r\n* fix wrong import\r\n\r\n---------\r\n\r\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "0a21381ba3047882ffe1b95c639aec28974b2c7e",
    "files": [
        {
            "sha": "6cbbdf3982746e9ca0816af07edee7ecd959e95d",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a21381ba3047882ffe1b95c639aec28974b2c7e/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a21381ba3047882ffe1b95c639aec28974b2c7e/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=0a21381ba3047882ffe1b95c639aec28974b2c7e",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models\n-](https://arxiv.org/abs/2405.09818v1) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet. \n+](https://arxiv.org/abs/2405.09818v1) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n \n \n The abstract from the paper is the following:\n@@ -61,7 +61,7 @@ The original code can be found [here](https://github.com/facebookresearch/chamel\n \n ### Single image inference\n \n-Chameleon is a gated model so make sure to have access and login to Hugging Face Hub using a token. \n+Chameleon is a gated model so make sure to have access and login to Hugging Face Hub using a token.\n Here's how to load the model and perform inference in half-precision (`torch.bfloat16`):\n \n ```python\n@@ -78,7 +78,7 @@ url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n image = Image.open(requests.get(url, stream=True).raw)\n prompt = \"What do you see in this image?<image>\"\n \n-inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n \n # autoregressively complete prompt\n output = model.generate(**inputs, max_new_tokens=50)\n@@ -117,7 +117,7 @@ prompts = [\n \n # We can simply feed images in the order they have to be used in the text prompt\n # Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\n-inputs = processor(text=prompts, images=[image_stop, image_cats, image_snowman], padding=True, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n+inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n \n # Generate\n generate_ids = model.generate(**inputs, max_new_tokens=50)\n@@ -162,8 +162,8 @@ from transformers import ChameleonForConditionalGeneration\n \n model_id = \"facebook/chameleon-7b\"\n model = ChameleonForConditionalGeneration.from_pretrained(\n-    model_id, \n-    torch_dtype=torch.bfloat16, \n+    model_id,\n+    torch_dtype=torch.bfloat16,\n     low_cpu_mem_usage=True,\n     attn_implementation=\"flash_attention_2\"\n ).to(0)"
        },
        {
            "sha": "ff45c9b597e0b4280dca33e8cba632ca056d10bd",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a21381ba3047882ffe1b95c639aec28974b2c7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a21381ba3047882ffe1b95c639aec28974b2c7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=0a21381ba3047882ffe1b95c639aec28974b2c7e",
            "patch": "@@ -24,7 +24,7 @@\n \n from transformers import (\n     ChameleonConfig,\n-    ChameleonForCausalLM,\n+    ChameleonForConditionalGeneration,\n     ChameleonImageProcessor,\n     ChameleonProcessor,\n )\n@@ -49,10 +49,10 @@\n Thereafter, models can be loaded via:\n \n ```py\n-from transformers import ChameleonForCausalLM, LlamaTokenizer\n+from transformers import ChameleonForConditionalGeneration, LlamaTokenizerFast\n \n-model = ChameleonForCausalLM.from_pretrained(\"/output/path\")\n-tokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"/output/path\")\n+tokenizer = LlamaTokenizerFast.from_pretrained(\"/output/path\")\n ```\n \n Important note: you need to be able to host the whole model in RAM to execute this script (even if the biggest versions\n@@ -372,7 +372,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         vocabulary_map=vocabulary_map,\n     )\n     with init_empty_weights():\n-        model = ChameleonForCausalLM(config)\n+        model = ChameleonForConditionalGeneration(config)\n \n     model.load_state_dict(state_dict, assign=True, strict=False)\n     model.save_pretrained(model_path, safe_serialization=True)\n@@ -397,7 +397,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n     # taken from https://github.com/facebookresearch/chameleon/blob/7a72f40aa5f462965c8374f25257f55b65b25ff4/data/prompts_for_human_evaluations.jsonl\n     print(\"Loading the checkpoint in a Chameleon model...\")\n     print(\"*\" * 100)\n-    model = ChameleonForCausalLM.from_pretrained(\n+    model = ChameleonForConditionalGeneration.from_pretrained(\n         model_path, attn_implementation=\"eager\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n     )\n     processor = ChameleonProcessor.from_pretrained(model_path)"
        },
        {
            "sha": "c4eb1eade6e2f751287349773d8927d1add9bfb3",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a21381ba3047882ffe1b95c639aec28974b2c7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a21381ba3047882ffe1b95c639aec28974b2c7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=0a21381ba3047882ffe1b95c639aec28974b2c7e",
            "patch": "@@ -1568,7 +1568,7 @@ def forward(\n         >>> image = Image.open(requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw)\n         >>> image_2 = Image.open(requests.get(\"https://www.kxan.com/wp-content/uploads/sites/40/2020/10/ORION.jpg\", stream=True).raw)\n \n-        >>> inputs = processor(prompt, images=[image, image_2], return_tensors=\"pt\").to(model.device, torch.bfloat16)\n+        >>> inputs = processor(images=[image, image_2], text=prompt, return_tensors=\"pt\").to(model.device, torch.bfloat16)\n \n         >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n         >>> processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
        },
        {
            "sha": "2d699c8f663a611ea7a05ef048ec08ab822ea3da",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 41,
            "deletions": 36,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a21381ba3047882ffe1b95c639aec28974b2c7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a21381ba3047882ffe1b95c639aec28974b2c7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=0a21381ba3047882ffe1b95c639aec28974b2c7e",
            "patch": "@@ -20,9 +20,25 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class ChameleonTextKwargs(TextKwargs, total=False):\n+    return_for_text_completion: bool\n+\n+\n+class ChameleonProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: ChameleonTextKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"return_for_text_completion\": False,\n+        },\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+        },\n+    }\n \n \n class ChameleonProcessor(ProcessorMixin):\n@@ -57,13 +73,11 @@ def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, ima\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: int = None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n-        return_for_text_completion: bool = False,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[ChameleonProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -73,26 +87,13 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n             text (`str`, `List[str]`, `List[List[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n \n@@ -110,10 +111,21 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n+        if text is None and images is None:\n+            raise ValueError(\"You must provide either text or images\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            ChameleonProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        return_for_text_completion = output_kwargs[\"text_kwargs\"].pop(\"return_for_text_completion\", False)\n \n         # Replace the image token with the expanded image token sequence\n         prompt_strings = []\n@@ -124,19 +136,12 @@ def __call__(\n                 sample += self.tokenizer.sep_token  # special Chameleon treatment to add sep for chat mode\n             prompt_strings.append(sample)\n \n-        data = self.tokenizer(\n-            prompt_strings,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-        )\n+        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n \n         if images is not None:\n-            pixel_values = self.image_processor(images, return_tensors=return_tensors)[\"pixel_values\"]\n-            data[\"pixel_values\"] = pixel_values\n+            data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"][\"return_tensors\"])\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "00e3ad40a57652275b35b22c8b37eae3c0f53830",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a21381ba3047882ffe1b95c639aec28974b2c7e/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a21381ba3047882ffe1b95c639aec28974b2c7e/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=0a21381ba3047882ffe1b95c639aec28974b2c7e",
            "patch": "@@ -350,7 +350,7 @@ def test_flash_attn_2_generate_padding_right(self):\n \n         processor.tokenizer.padding_side = \"right\"\n \n-        inputs = processor(texts, return_tensors=\"pt\", padding=True).to(0)\n+        inputs = processor(text=texts, return_tensors=\"pt\", padding=True).to(0)\n \n         output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_native = processor.tokenizer.batch_decode(output_native)\n@@ -392,7 +392,7 @@ def test_model_7b(self):\n         )\n         prompt = \"<image>Describe what do you see here and tell me about the history behind it?\"\n \n-        inputs = processor(prompt, images=image, return_tensors=\"pt\").to(model.device, torch.float16)\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n \n         # greedy generation outputs\n         EXPECTED_TEXT_COMPLETION = ['Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue line extending across the center of the image. The line is labeled \"390 light years\" and is accompanied by a small black and']  # fmt: skip\n@@ -420,7 +420,7 @@ def test_model_7b_batched(self):\n             \"What constellation is this image showing?<image>\",\n         ]\n \n-        inputs = processor(prompts, images=[image, image_2], padding=True, return_tensors=\"pt\").to(\n+        inputs = processor(images=[image, image_2], text=prompts, padding=True, return_tensors=\"pt\").to(\n             model.device, torch.float16\n         )\n \n@@ -450,7 +450,7 @@ def test_model_7b_multi_image(self):\n         )\n         prompt = \"What do these two images have in common?<image><image>\"\n \n-        inputs = processor(prompt, images=[image, image_2], return_tensors=\"pt\").to(model.device, torch.float16)\n+        inputs = processor(images=[image, image_2], text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n \n         # greedy generation outputs\n         EXPECTED_TEXT_COMPLETION = ['What do these two images have in common?The two images show a connection between two things that are not necessarily related. The first image shows a group of stars, while the second image shows a network of lines connecting two points. The connection between']  # fmt: skip"
        },
        {
            "sha": "0bf2c2ddf2b4b6a15ba1203eee7d4d77a3219937",
            "filename": "tests/models/chameleon/test_processor_chameleon.py",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a21381ba3047882ffe1b95c639aec28974b2c7e/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a21381ba3047882ffe1b95c639aec28974b2c7e/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py?ref=0a21381ba3047882ffe1b95c639aec28974b2c7e",
            "patch": "@@ -0,0 +1,44 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch chameleon model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+from transformers import ChameleonProcessor, LlamaTokenizer\n+from transformers.testing_utils import get_tests_dir\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import ChameleonImageProcessor\n+\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n+\n+\n+class ChameleonProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = ChameleonProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = ChameleonImageProcessor()\n+        tokenizer = LlamaTokenizer(vocab_file=SAMPLE_VOCAB)\n+        tokenizer.pad_token_id = 0\n+        tokenizer.sep_token_id = 1\n+        processor = self.processor_class(image_processor=image_processor, tokenizer=tokenizer)\n+        processor.save_pretrained(self.tmpdirname)"
        }
    ],
    "stats": {
        "total": 155,
        "additions": 102,
        "deletions": 53
    }
}