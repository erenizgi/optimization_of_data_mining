{
    "author": "ydshieh",
    "message": "skip some `padding_matches_padding_free_with_position_ids` for FA2 (#40521)\n\nskip 1\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "d61fab15498630321df36e62481ecd09be9336a6",
    "files": [
        {
            "sha": "c2e7c435dbfae593aec6d20442c350a2069b97fd",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -537,6 +537,9 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n+    @unittest.skip(\n+        \"NotImplementedError: seq_idx support requires fast path support. Please install mamba_ssm and causal_conv1d\"\n+    )\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_idx_and_fa_kwargs(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "fa9e45506929f7f732eaada4a5c7196f9ee10c9b",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -325,6 +325,10 @@ def test_disk_offload_safetensors(self):\n     def test_model_is_small(self):\n         pass\n \n+    @unittest.skip(\"Chameleon applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     @unittest.skip(\"Chameleon applies key/query norm which doesn't work with packing\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "ee3de73bf2cbfde09cb8dbb70538c98d26fddadf",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -244,7 +244,15 @@ def test_model_parallelism(self):\n         super().test_model_parallelism()\n \n     @unittest.skip(reason=\"Fuyu `prepare_inputs_for_generation` function doesn't have cache position.\")\n-    def test_generate_continue_from_inputs_embeds():\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n         pass\n \n     @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")"
        },
        {
            "sha": "4ed4fb11af3b45f02631fa993eb26a29b7968f71",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -81,6 +81,14 @@ def setUp(self):\n         self.model_tester = Gemma3ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Gemma3Config, hidden_size=37)\n \n+    @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        pass\n+\n     @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "6ce8f8d02f36251faa4f333d072b67bd163e7154",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -463,6 +463,14 @@ def test_eager_matches_sdpa_inference(\n     ):\n         pass\n \n+    @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        pass\n+\n     @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "71086466b073f7d6860fc2ef02f0bee634edb0e0",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -242,6 +242,14 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n     def test_attention_outputs(self):\n         pass\n \n+    @unittest.skip(\"MiniMax is special\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"MiniMax is special\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        pass\n+\n     @unittest.skip(\"MiniMax is special\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "cd701f12244e8da6fa47b4b508b86557dcc7d5e3",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -394,6 +394,14 @@ def test_disk_offload_bin(self):\n     def test_disk_offload_safetensors(self):\n         pass\n \n+    @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        pass\n+\n     @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "dd537de47b6bc47e60979170b80f9b608e176ec9",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d61fab15498630321df36e62481ecd09be9336a6/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=d61fab15498630321df36e62481ecd09be9336a6",
            "patch": "@@ -76,6 +76,10 @@ class PersimmonModelTest(CausalLMModelTest, unittest.TestCase):\n     test_headmasking = False\n     test_pruning = False\n \n+    @unittest.skip(\"Persimmon applies key/query norm which doesn't work with packing\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     @unittest.skip(\"Persimmon applies key/query norm which doesn't work with packing\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 52,
        "deletions": 1
    }
}