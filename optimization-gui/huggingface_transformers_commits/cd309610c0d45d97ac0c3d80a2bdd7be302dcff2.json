{
    "author": "sahil-kabir",
    "message": "Integrate colqwen2.5 using colqwen2 modelling code (#40600)\n\n* adding option for 2.5\n\n* minor - arg in conversion script\n\n* getting started on modelling.py\n\n* minor - shouldve been using modular\n\n* adressing comments + fixing datatype/device _get method\n\n* minor\n\n* commiting suggestion\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* docs + first test\n\n* ruff fix\n\n* minor fix\n\n* ruff fix\n\n* model fix\n\n* model fix\n\n* fine-grained check, with a hardcoded score from the original Hf implementation.\n\n* minor ruff\n\n* update tests values with CI hardware\n\n* adding 2.5 to conversion script\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: Sahil Kabir <sahilkabir@Sahils-MacBook-Pro.local>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "cd309610c0d45d97ac0c3d80a2bdd7be302dcff2",
    "files": [
        {
            "sha": "7c9a9627e2c7007aeb6bf0ece34b708643786d9e",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=cd309610c0d45d97ac0c3d80a2bdd7be302dcff2",
            "patch": "@@ -158,6 +158,24 @@ print(\"Retrieval scores (query x image):\")\n print(scores)\n ```\n \n+You can also use checkpoints for `ColQwen2.5` that are **compatible with the ColQwen2 architecture**. This version of the model uses [Qwen2_5_VL](./qwen2_5_vl) as the backbone.\n+\n+```python\n+import torch\n+from transformers import ColQwen2ForRetrieval, ColQwen2Processor\n+from transformers.utils.import_utils import is_flash_attn_2_available\n+\n+model_name = \"Sahil-Kabir/colqwen2.5-v0.2-hf\" # An existing compatible checkpoint\n+\n+model = ColQwen2ForRetrieval.from_pretrained(\n+    model_name,\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\"\n+)\n+processor = ColQwen2Processor.from_pretrained(model_name)\n+```\n+\n ## Notes\n \n - [`~ColQwen2Processor.score_retrieval`] returns a 2D tensor where the first dimension is the number of queries and the second dimension is the number of images. A higher score indicates more similarity between the query and image."
        },
        {
            "sha": "e8fbc502466c62edf49ee4b3c2181ec7b695baf6",
            "filename": "src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py?ref=cd309610c0d45d97ac0c3d80a2bdd7be302dcff2",
            "patch": "@@ -39,9 +39,10 @@\n \n import torch\n from huggingface_hub import snapshot_download\n+from peft import PeftModel\n from safetensors import safe_open\n \n-from transformers import AutoConfig\n+from transformers import AutoConfig, AutoModel\n from transformers.models.colqwen2 import ColQwen2ForRetrieval\n from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config\n from transformers.utils import logging\n@@ -69,7 +70,7 @@ def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> d\n                     original_state_dict[key] = f.get_tensor(key)\n \n     # Some weights are tied, so `lm.head`` is not saved. Let's clone to load state dict.\n-    if \"lm_head.weight\" not in original_state_dict:\n+    if \"lm_head.weight\" not in original_state_dict and \"model.embed_tokens.weight\" in original_state_dict:\n         original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n \n     return original_state_dict\n@@ -124,7 +125,21 @@ def convert_colqwen2_weights_to_hf(\n     config.is_composition = False\n \n     # Load the untrained model\n-    model = ColQwen2ForRetrieval(config=config).to(\"cpu\").eval()\n+    vlm_name_or_path = getattr(config.vlm_config, \"_name_or_path\", None)\n+    if vlm_name_or_path and \"2.5\" in str(vlm_name_or_path):\n+        print(\n+            \"Detected colqwen2.5 adapters in vlm_config; loading base model %s and merging PEFT weights.\"\n+            % vlm_name_or_path\n+        )\n+        base_model = AutoModel.from_pretrained(\n+            vlm_name_or_path,\n+            device_map=\"cpu\",\n+            trust_remote_code=True,\n+        )\n+        peft_model = PeftModel.from_pretrained(base_model, model_id)\n+        model = peft_model.merge_and_unload()\n+    else:\n+        model = ColQwen2ForRetrieval(config=config).to(\"cpu\").eval()\n     print(\"Created model with new config and randomly initialized weights\")\n \n     # NOTE: The new model was initialized with float32 weights. We need to convert it to the desired precision.\n@@ -201,6 +216,7 @@ def convert_colqwen2_weights_to_hf(\n         help=\"Name or path of the original VLM backbone model\",\n         default=None,\n     )\n+\n     args = parser.parse_args()\n \n     convert_colqwen2_weights_to_hf("
        },
        {
            "sha": "c3a6c04ee4db55f7e77f1d2804a2c1c3696c04b9",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=cd309610c0d45d97ac0c3d80a2bdd7be302dcff2",
            "patch": "@@ -172,7 +172,6 @@ def forward(\n             inputs_embeds = self.vlm.language_model.embed_tokens(input_ids)\n \n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.vlm.visual.get_dtype())\n                 image_embeds = self.vlm.visual(pixel_values, grid_thw=image_grid_thw)\n                 image_mask = (\n                     (input_ids == self.config.vlm_config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)"
        },
        {
            "sha": "072591abbab8fad41f1c4e30436fc9a21df30990",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=cd309610c0d45d97ac0c3d80a2bdd7be302dcff2",
            "patch": "@@ -359,7 +359,6 @@ def forward(\n             inputs_embeds = self.vlm.language_model.embed_tokens(input_ids)\n \n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.vlm.visual.get_dtype())\n                 image_embeds = self.vlm.visual(pixel_values, grid_thw=image_grid_thw)\n                 image_mask = (\n                     (input_ids == self.config.vlm_config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)"
        },
        {
            "sha": "4d9d2470368262a9ccb7af2837503446ee5640b9",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 52,
            "deletions": 3,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd309610c0d45d97ac0c3d80a2bdd7be302dcff2/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=cd309610c0d45d97ac0c3d80a2bdd7be302dcff2",
            "patch": "@@ -335,12 +335,61 @@ def test_model_integration_test(self):\n                     [15.6562, 12.2656, 20.2969],\n                 ],\n                 (\"cuda\", 8): [\n-                    [15.0703, 8.7422, 15.0312],\n-                    [9.5078, 16.8906, 10.6250],\n-                    [15.6484, 12.3984, 20.4688],\n+                    [16.2812, 8.3672, 14.5703],\n+                    [9.4922, 17.1875, 10.3281],\n+                    [15.0312, 11.3984, 20.1719],\n                 ],\n             }\n         )\n         expected_scores = torch.tensor(expectations.get_expectation(), dtype=scores.dtype)\n \n         assert torch.allclose(scores, expected_scores, atol=1e-3), f\"Expected scores {expected_scores}, got {scores}\"\n+\n+    @slow\n+    def test_model_integration_test_2(self):\n+        \"\"\"\n+        Test if the model is able to retrieve the correct pages for a small and easy dataset.\n+        This test uses a ColQwen2.5 checkpoint that is compatible with the ColQwen2 architecture.\n+        \"\"\"\n+        model = ColQwen2ForRetrieval.from_pretrained(\n+            \"Sahil-Kabir/colqwen2.5-v0.2-hf\",\n+            device_map=torch_device,\n+            dtype=torch.bfloat16,\n+        ).eval()\n+        processor = ColQwen2Processor.from_pretrained(\"Sahil-Kabir/colqwen2.5-v0.2-hf\", trust_remote_code=True)\n+\n+        # Load the test dataset\n+        ds = load_dataset(\"hf-internal-testing/document-visual-retrieval-test\", split=\"test\")\n+\n+        # Preprocess the examples\n+        batch_images = processor(images=list(ds[\"image\"])).to(torch_device)\n+        batch_queries = processor(text=list(ds[\"query\"])).to(torch_device)\n+\n+        with torch.inference_mode():\n+            image_embeddings = model(**batch_images).embeddings\n+            query_embeddings = model(**batch_queries).embeddings\n+\n+        # Compute retrieval scores\n+        scores = processor.score_retrieval(\n+            query_embeddings=query_embeddings,\n+            passage_embeddings=image_embeddings,\n+        )\n+\n+        assert scores.ndim == 2, f\"Expected 2D tensor, got {scores.ndim}\"\n+        assert scores.shape == (len(ds), len(ds)), f\"Expected shape {(len(ds), len(ds))}, got {scores.shape}\"\n+\n+        # Check if the maximum scores per row are in the diagonal of the matrix score\n+        self.assertTrue((scores.argmax(axis=1) == torch.arange(len(ds), device=scores.device)).all())\n+        # Further validation: fine-grained check, with a hardcoded score from the original Hf implementation.\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [\n+                    [16.3750, 10.9375, 14.7500],\n+                    [11.3750, 16.8750, 12.0625],\n+                    [15.3125, 13.1250, 21.5000],\n+                ]\n+            }\n+        )\n+        expected_scores = torch.tensor(expectations.get_expectation(), dtype=scores.dtype)\n+\n+        assert torch.allclose(scores, expected_scores, atol=0.15), f\"Expected scores {expected_scores}, got {scores}\""
        }
    ],
    "stats": {
        "total": 97,
        "additions": 89,
        "deletions": 8
    }
}