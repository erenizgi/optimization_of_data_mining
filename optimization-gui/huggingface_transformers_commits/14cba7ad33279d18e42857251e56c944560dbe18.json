{
    "author": "jiqing-feng",
    "message": "enable xpu on kv-cache and hqq doc (#39246)\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "14cba7ad33279d18e42857251e56c944560dbe18",
    "files": [
        {
            "sha": "c6c5f655582c0fdc8b010dd9ba66bb875096168b",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/14cba7ad33279d18e42857251e56c944560dbe18/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/14cba7ad33279d18e42857251e56c944560dbe18/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=14cba7ad33279d18e42857251e56c944560dbe18",
            "patch": "@@ -44,7 +44,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)\n@@ -59,7 +59,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n past_key_values = DynamicCache()\n@@ -142,13 +142,14 @@ Enable [`QuantizedCache`] by configuring `cache_implementation=\"quantized\"` in [\n For [`HQQQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `1`.\n \n ```py\n+import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache, QuantizedCacheConfig\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"axis-key\": 1, \"axis-value\": 1, \"backend\": \"hqq\"})\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"backend\": \"HQQ\"})\n print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n I like rock music because it's loud and energetic. It's a great way to express myself and rel\n ```\n@@ -159,13 +160,14 @@ I like rock music because it's loud and energetic. It's a great way to express m\n For [`QuantoQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `0`.\n \n ```py\n+import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache, QuantizedCacheConfig\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"axis-key\": 0, \"axis-value\": 0, \"backend\": \"quanto\"})\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"backend\": \"quanto\"})\n print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n I like rock music because it's loud and energetic. It's a great way to express myself and rel\n ```\n@@ -207,14 +209,14 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map={\"\": 0})\n inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n \"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n ```\n-Cache offloading requires a CUDA GPU.\n+Cache offloading requires a CUDA GPU or Intel XPU.\n \n ### Sliding window cache\n \n@@ -227,7 +229,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n@@ -306,23 +308,23 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map={\"\": 0})\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n # Init StaticCache with big enough max-length (1024 tokens for the below example)\n # You can also init a DynamicCache, if that suits you better\n-prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n+prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=model.device.type, dtype=torch.bfloat16)\n \n INITIAL_PROMPT = \"You are a helpful assistant. \"\n-inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n+inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(model.device.type)\n # This is the common prompt cached, we need to run forward without grad to be able to copy\n with torch.no_grad():\n      prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n \n prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n responses = []\n for prompt in prompts:\n-    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n+    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(model.device.type)\n     past_key_values = copy.deepcopy(prompt_cache)\n     outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n     response = tokenizer.batch_decode(outputs)[0]"
        },
        {
            "sha": "4c0ea92f430c9eb493362832d4f73c80f05bec71",
            "filename": "docs/source/en/quantization/hqq.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/14cba7ad33279d18e42857251e56c944560dbe18/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/14cba7ad33279d18e42857251e56c944560dbe18/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md?ref=14cba7ad33279d18e42857251e56c944560dbe18",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n HQQ further supports fine-tuning with [PEFT](https://huggingface.co/docs/peft) and is fully compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for even faster inference and training.\n \n-Install HQQ with the following command to get the latest version and to build its corresponding CUDA kernels.\n+Install HQQ with the following command to get the latest version and to build its corresponding CUDA kernels if you are using a cuda device. It also support Intel XPU with pure pytorch implementation.\n \n ```bash\n pip install hqq\n@@ -34,13 +34,14 @@ You can choose to either replace all the linear layers in a model with the same\n Quantize a model by creating a [`HqqConfig`] and specifying the `nbits` and `group_size` to replace for all the linear layers ([torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) of the model.\n \n ``` py\n+import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n \n quant_config = HqqConfig(nbits=8, group_size=64)\n model = transformers.AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B\", \n     torch_dtype=torch.float16, \n-    device_map=\"cuda\", \n+    device_map=\"auto\", \n     quantization_config=quant_config\n )\n ```\n@@ -67,7 +68,7 @@ quant_config  = HqqConfig(dynamic_config={\n model = transformers.AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B\", \n     torch_dtype=torch.float16, \n-    device_map=\"cuda\", \n+    device_map=\"auto\", \n     quantization_config=quant_config\n )\n ```"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 19,
        "deletions": 16
    }
}