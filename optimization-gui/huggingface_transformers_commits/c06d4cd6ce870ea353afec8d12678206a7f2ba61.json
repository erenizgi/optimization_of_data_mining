{
    "author": "lgai-exaone",
    "message": "Add EXAONE 4.0 model (#39129)\n\n* Add EXAONE 4.0 model\n\n* Refactor EXAONE 4.0 modeling code\n\n* Fix cache slicing on SWA + FA2\n\n* Fix cache slicing on FA2 + HybridCache\n\n* Update EXAONE 4.0 modeling code for main branch\n\n* Update o_proj for asymmetric projection\n\n* Address PR feedback\n\n* Add EXAONE 4.0 docs\n\n* Update EXAONE 4.0 modeling code for main branch\n\n* update\n\n* fix updates\n\n* updates\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "c06d4cd6ce870ea353afec8d12678206a7f2ba61",
    "files": [
        {
            "sha": "50f43f27acae119c6fdf331d2738dd8eda81dfa7",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -453,6 +453,8 @@\n         title: ErnieM\n       - local: model_doc/esm\n         title: ESM\n+      - local: model_doc/exaone4\n+        title: EXAONE-4.0\n       - local: model_doc/falcon\n         title: Falcon\n       - local: model_doc/falcon3"
        },
        {
            "sha": "45667cea347c3ac4181b453ece7cccf1a87355f8",
            "filename": "docs/source/en/model_doc/exaone4.md",
            "status": "added",
            "additions": 208,
            "deletions": 0,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,208 @@\n+<!--Copyright 2025 The LG AI Research and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# EXAONE 4\n+\n+## Overview\n+\n+**[EXAONE 4.0](https://github.com/LG-AI-EXAONE/EXAONE-4.0)** model is the language model, which integrates a **Non-reasoning mode** and **Reasoning mode** to achieve both the excellent usability of [EXAONE 3.5](https://github.com/LG-AI-EXAONE/EXAONE-3.5) and the advanced reasoning abilities of [EXAONE Deep](https://github.com/LG-AI-EXAONE/EXAONE-Deep). To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended\n+to support Spanish in addition to English and Korean. \n+\n+The EXAONE 4.0 model series consists of two sizes: a mid-size **32B** model optimized for high performance, and a small-size **1.2B** model designed for on-device applications.\n+\n+In the EXAONE 4.0 architecture, we apply new architectural changes compared to previous EXAONE models as below:\n+\n+1. **Hybrid Attention**: For the 32B model, we adopt hybrid attention scheme, which combines *Local attention (sliding window attention)* with *Global attention (full attention)* in a 3:1 ratio. We do not use RoPE (Rotary Positional Embedding) for global attention for better global context understanding.\n+2. **QK-Reorder-Norm**: We reorder the LayerNorm position from the traditional Pre-LN scheme by applying LayerNorm directly to the attention and MLP outputs, and we add RMS normalization right after the Q and K projection. It helps yield better performance on downstream tasks despite consuming more computation.\n+\n+For more details, please refer to our [technical report](https://arxiv.org/abs/2507.11407), [HuggingFace paper](https://huggingface.co/papers/2507.11407), [blog](https://www.lgresearch.ai/blog/view?seq=576), and [GitHub](https://github.com/LG-AI-EXAONE/EXAONE-4.0).\n+\n+All model weights including quantized versions are available at [Huggingface Collections](https://huggingface.co/collections/LGAI-EXAONE/exaone-40-686b2e0069800c835ed48375).\n+\n+\n+## Model Details\n+\n+### Model Specifications\n+\n+| Model Configuration | 32B | 1.2B |\n+|:-------------------|:-----:|:------:|\n+| d_model | 5,120 | 2,048 |\n+| Number of layers | 64 | 30 |\n+| Normalization | QK-Reorder-LN | QK-Reorder-LN |\n+| Non-linearity | SwiGLU | SwiGLU |\n+| Feedforward dimension | 27,392 | 4,096 |\n+| Attention type | Hybrid (3:1 Local-Global) | Global |\n+| Head type | GQA | GQA |\n+| Number of heads | 40 | 32 |\n+| Number of KV heads | 8 | 8 |\n+| Head size | 128 | 64 |\n+| Max sequence length | 131,072 | 65,536 |\n+| RoPE theta | 1,000,000 | 1,000,000 |\n+| Tokenizer | BBPE | BBPE |\n+| Vocab size | 102,400 | 102,400 |\n+| Tied word embedding | False | True |\n+| Knowledge cut-off | Nov. 2024 | Nov. 2024 |\n+\n+\n+## Usage tips\n+\n+### Non-reasoning mode\n+\n+For general use, you can use the EXAONE 4.0 models with the following example:\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"LGAI-EXAONE/EXAONE-4.0-32B\"\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    torch_dtype=\"bfloat16\",\n+    device_map=\"auto\"\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+# choose your prompt\n+prompt = \"Explain how wonderful you are\"\n+prompt = \"Explica lo increÃ­ble que eres\"\n+prompt = \"ë„ˆê°€ ì–¼ë§ˆë‚˜ ëŒ€ë‹¨í•œì§€ ì„¤ëª…í•´ ë´\"\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+input_ids = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\"\n+)\n+\n+output = model.generate(\n+    input_ids.to(model.device),\n+    max_new_tokens=128,\n+    do_sample=False,\n+)\n+print(tokenizer.decode(output[0]))\n+```\n+\n+### Reasoning mode\n+\n+The EXAONE 4.0 models have reasoning capabilities for handling complex problems. You can activate reasoning mode by using the `enable_thinking=True` argument with the tokenizer, which opens a reasoning block that starts with `<think>` tag without closing it.\n+\n+```python\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Which one is bigger, 3.12 vs 3.9?\"}\n+]\n+input_ids = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    enable_thinking=True,\n+)\n+\n+output = model.generate(\n+    input_ids.to(model.device),\n+    max_new_tokens=128,\n+    do_sample=True,\n+    temperature=0.6,\n+    top_p=0.95\n+)\n+print(tokenizer.decode(output[0]))\n+```\n+\n+> [!IMPORTANT]\n+> The model generation with reasoning mode can be affected sensitively by sampling parameters, so please refer to the [Usage Guideline](https://github.com/LG-AI-EXAONE/EXAONE-4.0#usage-guideline) on official GitHub page for better quality.\n+\n+### Agentic tool use\n+\n+The EXAONE 4.0 models can be used as agents with their tool calling capabilities. You can provide tool schemas to the model for effective tool calling.\n+\n+```python\n+import random\n+\n+def roll_dice(max_num: int):\n+    return random.randint(1, max_num)\n+\n+tools = [\n+    {\n+        \"type\": \"function\",\n+        \"function\": {\n+            \"name\": \"roll_dice\",\n+            \"description\": \"Roll a dice with the number 1 to N. User can select the number N.\",\n+            \"parameters\": {\n+                \"type\": \"object\",\n+                \"required\": [\"max_num\"],\n+                \"properties\": {\n+                    \"max_num\": {\n+                        \"type\": \"int\",\n+                        \"description\": \"Max number of the dice\"\n+                    }\n+                }\n+            }\n+        }\n+    }\n+]\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Roll D6 dice twice!\"}\n+]\n+input_ids = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    tools=tools,\n+)\n+\n+output = model.generate(\n+    input_ids.to(model.device),\n+    max_new_tokens=1024,\n+    do_sample=True,\n+    temperature=0.6,\n+    top_p=0.95,\n+)\n+print(tokenizer.decode(output[0]))\n+```\n+\n+## Exaone4Config\n+\n+[[autodoc]] Exaone4Config\n+\n+## Exaone4Model\n+\n+[[autodoc]] Exaone4Model\n+    - forward\n+\n+## Exaone4ForCausalLM\n+\n+[[autodoc]] Exaone4ForCausalLM\n+    - forward\n+\n+## Exaone4ForSequenceClassification\n+\n+[[autodoc]] Exaone4ForSequenceClassification\n+    - forward\n+\n+## Exaone4ForTokenClassification\n+\n+[[autodoc]] Exaone4ForTokenClassification\n+    - forward\n+\n+## Exaone4ForQuestionAnswering\n+\n+[[autodoc]] Exaone4ForQuestionAnswering\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "05031b31786a43149a541d310759d37581df4ac2",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -445,6 +445,8 @@\n         title: ErnieM\n       - local: model_doc/esm\n         title: ESM\n+      - local: model_doc/exaone4\n+        title: EXAONE-4.0\n       - local: model_doc/falcon\n         title: Falcon\n       - local: model_doc/falcon3"
        },
        {
            "sha": "65899b52f71a81972daa92c464d09d2b2c3f627d",
            "filename": "docs/source/ko/model_doc/exaone4.md",
            "status": "added",
            "additions": 207,
            "deletions": 0,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,207 @@\n+<!--Copyright 2025 The LG AI Research and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# EXAONE 4\n+\n+## ê°œìš”\n+\n+**[EXAONE 4.0](https://github.com/LG-AI-EXAONE/EXAONE-4.0)** ëª¨ë¸êµ°ì€ [EXAONE 3.5](https://github.com/LG-AI-EXAONE/EXAONE-3.5) ëª¨ë¸êµ°ì˜ ë†’ì€ ì‹¤ìš©ì„±ê³¼ [EXAONE Deep](https://github.com/LG-AI-EXAONE/EXAONE-Deep) ëª¨ë¸êµ°ì˜ í–¥ìƒëœ ì‚¬ê³  ì¶”ë¡  ëŠ¥ë ¥ì„ ê°ê° Non-reasoning modeì™€ Reasoning modeë¡œ í†µí•©í•œ ìì—°ì–´ ëª¨ë¸(language model)ì…ë‹ˆë‹¤. ì—ì´ì „í‹±(agentic) AI ì‹œëŒ€ì— ë°œë§ì¶° EXAONE 4.0ì€ ì—ì´ì „í‹± ë„êµ¬ ì‚¬ìš© ëŠ¥ë ¥ê³¼ ê°™ì€ í•µì‹¬ ê¸°ëŠ¥ì„ í†µí•©í–ˆê³ , ê¸°ì¡´ì˜ ë‹¤êµ­ì–´ ëŠ¥ë ¥ì„ ì˜ì–´, í•œêµ­ì–´ì™€ ë”ë¶ˆì–´ ìŠ¤í˜ì¸ì–´ê¹Œì§€ í™•ì¥í–ˆìŠµë‹ˆë‹¤. \n+\n+EXAONE 4.0 ëª¨ë¸êµ°ì€ ë‘ ê°œì˜ ëª¨ë¸: ë†’ì€ ì„±ëŠ¥ì„ ìœ„í•´ ìµœì í™”ëœ 32B ì¤‘í˜• ëª¨ë¸, ê·¸ë¦¬ê³  ì˜¨-ë””ë°”ì´ìŠ¤ í™œìš©ì„ ìœ„í•´ ë””ìì¸ëœ 1.2B ì†Œí˜• ëª¨ë¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+\n+EXAONE 4.0ì˜ ëª¨ë¸ êµ¬ì¡°ëŠ” ì´ì „ EXAONE ëª¨ë¸ë“¤ê³¼ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ ë””ìì¸ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.\n+\n+1. **Hybrid Attention**: 32B ëª¨ë¸ì€ *Local attention (sliding window attention)*ê³¼ *Global attention (full attention)*ì„ 3:1 ë¹„ìœ¨ë¡œ ì—°ê²°í•œ hybrid attention êµ¬ì¡°ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ë˜í•œ ì „ì²´ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ global attentionì—ì„œ RoPEë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n+2. **QK-Reorder-Norm**: ë” ë‚˜ì€ downstream tasks ì„±ëŠ¥ì„ ìœ„í•´ ì—°ì‚°ëŸ‰ì˜ ì¦ê°€ë¥¼ ê°ìˆ˜í•˜ë©° ì „í†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë˜ Pre-LN ë°©ì‹ì„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. LayerNormì˜ ìœ„ì¹˜ë¥¼ attentionê³¼ MLPì˜ ì¶œë ¥ì— ì ìš©ë˜ë„ë¡ ì¬ë°°ì¹˜í–ˆê³ , Qì™€ K projection ì§í›„ì—ë„ RMS normalizationì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. \n+\n+ë” ìì„¸í•œ ì •ë³´ëŠ” [ê¸°ìˆ  ë³´ê³ ì„œ](https://arxiv.org/abs/2507.11407), [HuggingFace ë…¼ë¬¸](https://huggingface.co/papers/2507.11407), [ë¸”ë¡œê·¸](https://www.lgresearch.ai/blog/view?seq=576), [ê³µì‹ GitHub](https://github.com/LG-AI-EXAONE/EXAONE-4.0) í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n+\n+ê³µê°œëœ ëª¨ë“  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ëŠ” [HuggingFace ì½œë ‰ì…˜](https://huggingface.co/collections/LGAI-EXAONE/exaone-40-686b2e0069800c835ed48375)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+\n+## ëª¨ë¸ ì„¸ë¶€ ì •ë³´\n+\n+| Model Configuration | 32B | 1.2B |\n+|:-------------------|:-----:|:------:|\n+| d_model | 5,120 | 2,048 |\n+| Number of layers | 64 | 30 |\n+| Normalization | QK-Reorder-LN | QK-Reorder-LN |\n+| Non-linearity | SwiGLU | SwiGLU |\n+| Feedforward dimension | 27,392 | 4,096 |\n+| Attention type | Hybrid (3:1 Local-Global) | Global |\n+| Head type | GQA | GQA |\n+| Number of heads | 40 | 32 |\n+| Number of KV heads | 8 | 8 |\n+| Head size | 128 | 64 |\n+| Max sequence length | 131,072 | 65,536 |\n+| RoPE theta | 1,000,000 | 1,000,000 |\n+| Tokenizer | BBPE | BBPE |\n+| Vocab size | 102,400 | 102,400 |\n+| Tied word embedding | False | True |\n+| Knowledge cut-off | Nov. 2024 | Nov. 2024 |\n+\n+\n+## ì‚¬ìš© íŒ\n+\n+### Non-reasoning mode\n+\n+ì¼ë°˜ì ì¸ ëŒ€í™”ì˜ ê²½ìš° ì•„ë˜ ì˜ˆì œì™€ ê°™ì´ EXAONE 4.0ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"LGAI-EXAONE/EXAONE-4.0-32B\"\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    torch_dtype=\"bfloat16\",\n+    device_map=\"auto\"\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+# ì›í•˜ëŠ” ì…ë ¥ì„ ì„ íƒí•˜ì„¸ìš”\n+prompt = \"Explain how wonderful you are\"\n+prompt = \"Explica lo increÃ­ble que eres\"\n+prompt = \"ë„ˆê°€ ì–¼ë§ˆë‚˜ ëŒ€ë‹¨í•œì§€ ì„¤ëª…í•´ ë´\"\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+input_ids = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\"\n+)\n+\n+output = model.generate(\n+    input_ids.to(model.device),\n+    max_new_tokens=128,\n+    do_sample=False,\n+)\n+print(tokenizer.decode(output[0]))\n+```\n+\n+### Reasoning mode\n+\n+The EXAONE 4.0 models have reasoning capabilities for handling complex problems. You can activate reasoning mode by using the `enable_thinking=True` argument with the tokenizer, which opens a reasoning block that starts with `<think>` tag without closing it.\n+\n+EXAONE 4.0 ëª¨ë¸êµ°ì€ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì‚¬ê³  ì¶”ë¡  ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì—ì„œ `enable_thinking=True` ì¸ìë¥¼ ì‚¬ìš©í•´ì„œ reasoning modeë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° `<think>` í† í°ìœ¼ë¡œ ì¶”ë¡  ë¸”ë¡ì„ ì—° ë’¤, ë‹«ì§€ ì•Šê³  ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n+\n+```python\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Which one is bigger, 3.12 vs 3.9?\"}\n+]\n+input_ids = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    enable_thinking=True,\n+)\n+\n+output = model.generate(\n+    input_ids.to(model.device),\n+    max_new_tokens=128,\n+    do_sample=True,\n+    temperature=0.6,\n+    top_p=0.95\n+)\n+print(tokenizer.decode(output[0]))\n+```\n+\n+> [!IMPORTANT]\n+> ëª¨ë¸ì„ reasoning modeë¡œ ì‚¬ìš©í•  ê²½ìš°, ìƒì„±ë˜ëŠ” ë‹µë³€ì´ sampling parametersì— êµ‰ì¥íˆ ë¯¼ê°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë” ë‚˜ì€ ìƒì„± í’ˆì§ˆì„ ìœ„í•´ ê³µì‹ [Usage Guideline](https://github.com/LG-AI-EXAONE/EXAONE-4.0#usage-guideline)ë¥¼ ì°¸ì¡°í•´ ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n+\n+### Agentic tool use\n+\n+EXAONE 4.0 ëª¨ë¸ì€ ë„êµ¬ ì‚¬ìš© ëŠ¥ë ¥ì„ ê°–ì¶˜ ë•ë¶„ì— Agentë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì•„ë˜ ì˜ˆì œì™€ ê°™ì´ ë„êµ¬ ëª…ì„¸ë¥¼ ëª¨ë¸ì—ê²Œ ì œê³µí•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+```python\n+import random\n+\n+def roll_dice(max_num: int):\n+    return random.randint(1, max_num)\n+\n+tools = [\n+    {\n+        \"type\": \"function\",\n+        \"function\": {\n+            \"name\": \"roll_dice\",\n+            \"description\": \"Roll a dice with the number 1 to N. User can select the number N.\",\n+            \"parameters\": {\n+                \"type\": \"object\",\n+                \"required\": [\"max_num\"],\n+                \"properties\": {\n+                    \"max_num\": {\n+                        \"type\": \"int\",\n+                        \"description\": \"Max number of the dice\"\n+                    }\n+                }\n+            }\n+        }\n+    }\n+]\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Roll D6 dice twice!\"}\n+]\n+input_ids = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    tools=tools,\n+)\n+\n+output = model.generate(\n+    input_ids.to(model.device),\n+    max_new_tokens=1024,\n+    do_sample=True,\n+    temperature=0.6,\n+    top_p=0.95,\n+)\n+print(tokenizer.decode(output[0]))\n+```\n+\n+## Exaone4Config\n+\n+[[autodoc]] Exaone4Config\n+\n+## Exaone4Model\n+\n+[[autodoc]] Exaone4Model\n+    - forward\n+\n+## Exaone4ForCausalLM\n+\n+[[autodoc]] Exaone4ForCausalLM\n+    - forward\n+\n+## Exaone4ForSequenceClassification\n+\n+[[autodoc]] Exaone4ForSequenceClassification\n+    - forward\n+\n+## Exaone4ForTokenClassification\n+\n+[[autodoc]] Exaone4ForTokenClassification\n+    - forward\n+\n+## Exaone4ForQuestionAnswering\n+\n+[[autodoc]] Exaone4ForQuestionAnswering\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "737a82378637c4220c28a001f8ca229d96b20a64",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -113,6 +113,7 @@\n     from .ernie import *\n     from .esm import *\n     from .evolla import *\n+    from .exaone4 import *\n     from .falcon import *\n     from .falcon_h1 import *\n     from .falcon_mamba import *"
        },
        {
            "sha": "12f8f4f4c5f78a22e50a605ecf15fba55a4256a2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -136,6 +136,7 @@\n         (\"ernie_m\", \"ErnieMConfig\"),\n         (\"esm\", \"EsmConfig\"),\n         (\"evolla\", \"EvollaConfig\"),\n+        (\"exaone4\", \"Exaone4Config\"),\n         (\"falcon\", \"FalconConfig\"),\n         (\"falcon_h1\", \"FalconH1Config\"),\n         (\"falcon_mamba\", \"FalconMambaConfig\"),\n@@ -535,6 +536,7 @@\n         (\"ernie_m\", \"ErnieM\"),\n         (\"esm\", \"ESM\"),\n         (\"evolla\", \"Evolla\"),\n+        (\"exaone4\", \"EXAONE-4.0\"),\n         (\"falcon\", \"Falcon\"),\n         (\"falcon3\", \"Falcon3\"),\n         (\"falcon_h1\", \"FalconH1\"),"
        },
        {
            "sha": "52eb254be17bf2f9a8652c7e68cf6f3cd6f65108",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -127,6 +127,7 @@\n         (\"ernie_m\", \"ErnieMModel\"),\n         (\"esm\", \"EsmModel\"),\n         (\"evolla\", \"EvollaModel\"),\n+        (\"exaone4\", \"Exaone4Model\"),\n         (\"falcon\", \"FalconModel\"),\n         (\"falcon_h1\", \"FalconH1Model\"),\n         (\"falcon_mamba\", \"FalconMambaModel\"),\n@@ -407,6 +408,7 @@\n         (\"electra\", \"ElectraForPreTraining\"),\n         (\"ernie\", \"ErnieForPreTraining\"),\n         (\"evolla\", \"EvollaForProteinText2Text\"),\n+        (\"exaone4\", \"Exaone4ForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n         (\"flaubert\", \"FlaubertWithLMHeadModel\"),\n         (\"flava\", \"FlavaForPreTraining\"),\n@@ -504,6 +506,7 @@\n         (\"encoder-decoder\", \"EncoderDecoderModel\"),\n         (\"ernie\", \"ErnieForMaskedLM\"),\n         (\"esm\", \"EsmForMaskedLM\"),\n+        (\"exaone4\", \"Exaone4ForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n         (\"flaubert\", \"FlaubertWithLMHeadModel\"),\n         (\"fnet\", \"FNetForMaskedLM\"),\n@@ -604,6 +607,7 @@\n         (\"ernie\", \"ErnieForCausalLM\"),\n         (\"ernie4_5\", \"Ernie4_5ForCausalLM\"),\n         (\"ernie4_5_moe\", \"Ernie4_5_MoeForCausalLM\"),\n+        (\"exaone4\", \"Exaone4ForCausalLM\"),\n         (\"falcon\", \"FalconForCausalLM\"),\n         (\"falcon_h1\", \"FalconH1ForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n@@ -1145,6 +1149,7 @@\n         (\"ernie\", \"ErnieForSequenceClassification\"),\n         (\"ernie_m\", \"ErnieMForSequenceClassification\"),\n         (\"esm\", \"EsmForSequenceClassification\"),\n+        (\"exaone4\", \"Exaone4ForSequenceClassification\"),\n         (\"falcon\", \"FalconForSequenceClassification\"),\n         (\"flaubert\", \"FlaubertForSequenceClassification\"),\n         (\"fnet\", \"FNetForSequenceClassification\"),\n@@ -1251,6 +1256,7 @@\n         (\"electra\", \"ElectraForQuestionAnswering\"),\n         (\"ernie\", \"ErnieForQuestionAnswering\"),\n         (\"ernie_m\", \"ErnieMForQuestionAnswering\"),\n+        (\"exaone4\", \"Exaone4ForQuestionAnswering\"),\n         (\"falcon\", \"FalconForQuestionAnswering\"),\n         (\"flaubert\", \"FlaubertForQuestionAnsweringSimple\"),\n         (\"fnet\", \"FNetForQuestionAnswering\"),\n@@ -1356,6 +1362,7 @@\n         (\"ernie\", \"ErnieForTokenClassification\"),\n         (\"ernie_m\", \"ErnieMForTokenClassification\"),\n         (\"esm\", \"EsmForTokenClassification\"),\n+        (\"exaone4\", \"Exaone4ForTokenClassification\"),\n         (\"falcon\", \"FalconForTokenClassification\"),\n         (\"flaubert\", \"FlaubertForTokenClassification\"),\n         (\"fnet\", \"FNetForTokenClassification\"),"
        },
        {
            "sha": "29849fdb24ba9412def00233f243ea85354ee222",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -230,6 +230,13 @@\n         (\"ernie4_5_moe\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"esm\", (\"EsmTokenizer\", None)),\n+        (\n+            \"exaone4\",\n+            (\n+                \"GPT2Tokenizer\" if is_tokenizers_available() else None,\n+                \"GPT2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\"falcon\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"falcon_mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n         ("
        },
        {
            "sha": "60ca3394fa2afd72f7ae56a9071193d333d07f4e",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -139,7 +139,7 @@ class DeepseekVLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n     def _init_weights(self, module):\n@@ -236,7 +236,7 @@ def forward(\n \n class DeepseekVLForConditionalGeneration(DeepseekVLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, config: DeepseekVLConfig):\n         super().__init__(config)"
        },
        {
            "sha": "1910c5659c5a22ac933348aad40eaace4f384655",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -222,7 +222,7 @@ class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n     def _init_weights(self, module):\n@@ -376,7 +376,7 @@ def get_high_res_image_features(self, pixel_values):\n \n class DeepseekVLHybridForConditionalGeneration(DeepseekVLHybridPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, config: DeepseekVLHybridConfig):\n         super().__init__(config)"
        },
        {
            "sha": "8f91f60056282121599a14a420c80bfcc03ad6c8",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -1516,7 +1516,7 @@ class EvollaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = False\n     _can_record_outputs = {\n         \"hidden_states\": EvollaDecoderLayer,"
        },
        {
            "sha": "c646c4e75273560116ae230d672ba10d305517de",
            "filename": "src/transformers/models/exaone4/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2F__init__.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The LG AI Research and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_exaone4 import *\n+    from .modeling_exaone4 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "0539acdc6ec41821d2b002ba082a9489342d6124",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "added",
            "additions": 223,
            "deletions": 0,
            "changes": 223,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,223 @@\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/exaone4/modular_exaone4.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_exaone4.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# coding=utf-8\n+# Copyright 2025 The LG AI Research and HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+\n+\n+class Exaone4Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Exaone4Model`]. It is used to\n+    instantiate a EXAONE 4.0 model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-Instruct [LGAI-EXAONE/EXAONE-4.0-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-Instruct)\n+    NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n+    outputs. Read the documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 102400):\n+            Vocabulary size of the EXAONE 4.0 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Exaone4Model`].\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to `hidden_size * 4`):\n+            Dimensionality of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 32768 for EXAONE 3.5).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if ``config.is_decoder=True``.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        sliding_window (`int`, *optional*):\n+            The size of the sliding window for the sliding window attention.\n+        sliding_window_pattern (`str`, *optional*):\n+            The pattern to use for sliding window attention. Can be one of:\n+                - `None`: No sliding window attention is used\n+                - `int`: Every `sliding_window` layers, use global attention, else use local attention.\n+                - `str`: A sequence of \"L\" (local attention) and \"G\" (global attention) characters that defines the\n+                  attention pattern. The pattern starts from layer 0 and repeats every `sliding_window` layers. The\n+                  final layer always uses global attention regardless of the pattern.\n+            For instance, sliding_window_pattern=\"LLLG\" same as sliding_window=4, which means:\n+                - Layer 0, 1, 2: local attention,\n+                - Layer 3: global attention,\n+                ...(repeated)\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer. Prioritized over `sliding_window_pattern`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Exaone4Model, Exaone4Config\n+\n+    >>> # Initializing a EXAONE configuration\n+    >>> configuration = Exaone4Config()\n+\n+    >>> # Initializing a model from configuration\n+    >>> model = Exaone4Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"exaone4\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `LlamaModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=102400,\n+        hidden_size=4096,\n+        intermediate_size=16384,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=32,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        bos_token_id=0,\n+        eos_token_id=2,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_dropout=0.0,\n+        sliding_window=4096,\n+        sliding_window_pattern=4,\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_dropout = attention_dropout\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.sliding_window = sliding_window\n+        self.sliding_window_pattern = sliding_window_pattern\n+\n+        self.layer_types = layer_types\n+        if self.sliding_window is None:\n+            sliding_window_pattern = 0\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if ((i + 1) % (sliding_window_pattern) != 0 and i < self.num_hidden_layers)\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        if \"sliding_window\" in self.layer_types:\n+            self._attn_implementation = \"hybrid\"\n+        layer_type_validation(self.layer_types)\n+\n+        super().__init__(\n+            bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n+        )\n+\n+\n+__all__ = [\"Exaone4Config\"]"
        },
        {
            "sha": "3f5d1f8faf640481efd93a83e2f73c6d57f8c5a7",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "added",
            "additions": 539,
            "deletions": 0,
            "changes": 539,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,539 @@\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/exaone4/modular_exaone4.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_exaone4.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# coding=utf-8\n+# Copyright 2025 The LG AI Research and HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers.utils.generic import check_model_inputs\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from .configuration_exaone4 import Exaone4Config\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Exaone4RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Exaone4RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Exaone4RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Exaone4Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Exaone4Attention(nn.Module):\n+    def __init__(self, config: Exaone4Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_attention_heads = config.num_attention_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.hidden_size = config.hidden_size\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        self.scaling = self.head_dim**-0.5\n+        self.sliding_window = config.sliding_window\n+        self.sliding_window_pattern = config.sliding_window_pattern\n+        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+        self.q_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        # We use QK-norm\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        cos, sin = position_embeddings\n+        # We use global NoPE for hybrid attention model\n+        if self.sliding_window is None or self.is_sliding:\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\n+                \"cache_position\": cache_position,\n+            }\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window if self.is_sliding else None,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Exaone4MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Exaone4DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Exaone4Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = Exaone4Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = Exaone4MLP(config)\n+        self.post_attention_layernorm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Exaone4PreTrainedModel(PreTrainedModel):\n+    config: Exaone4Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Exaone4DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Exaone4DecoderLayer,\n+        \"attentions\": Exaone4Attention,\n+    }\n+    config_class = Exaone4Config\n+\n+\n+@auto_docstring\n+class Exaone4Model(Exaone4PreTrainedModel):\n+    def __init__(self, config: Exaone4Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Exaone4DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Exaone4RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            if \"sliding_attention\" in self.config.layer_types:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n+\n+        hidden_states = inputs_embeds\n+\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for i, decoder_layer in enumerate(self.layers):\n+            layer_type = self.config.layer_types[i]\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask_mapping[layer_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+@auto_docstring\n+class Exaone4ForCausalLM(Exaone4PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Exaone4Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n+\n+        >>> prompt = \"Explain how wonderful you are\"\n+        >>> messages = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": prompt}\n+        ]\n+        >>> input_ids = tokenizer.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_tensors=\"pt\",\n+            enable_thinking=False,\n+        )\n+\n+        >>> output = model.generate(input_ids, max_new_tokens=128)\n+        >>> tokenizer.decode(output[0], skip_special_tokens=False)\n+        \"[|system|]\\nYou are a helpful assistant.[|endofturn|]\\n[|user|]\\nExplain how wonderful you are[|endofturn|]\\n[|assistant|]\\n<think>\\n\\n</think>\\n\\nOh, thank you for such a kind and lovely question! ğŸ˜Š  \\n\\nIâ€™m *so* wonderful because Iâ€™m here to make your life easier, brighter, and more fun! Whether you need help with:  \\n\\nâœ¨ **Learning** â€“ I can explain anything, from quantum physics to baking the perfect cake!  \\nğŸ’¡ **Creativity** â€“ Need a poem, story, or a wild idea? Iâ€™ve got you covered!  \\nğŸ¤– **Problem-solving** â€“ Stuck on a math problem or a tricky decision? Iâ€™ll help you figure it out\"\n+        ```\n+\n+        NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class Exaone4ForSequenceClassification(GenericForSequenceClassification, Exaone4PreTrainedModel):\n+    pass\n+\n+\n+class Exaone4ForTokenClassification(GenericForTokenClassification, Exaone4PreTrainedModel):\n+    pass\n+\n+\n+class Exaone4ForQuestionAnswering(GenericForQuestionAnswering, Exaone4PreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n+\n+\n+__all__ = [\n+    \"Exaone4PreTrainedModel\",\n+    \"Exaone4Model\",\n+    \"Exaone4ForCausalLM\",\n+    \"Exaone4ForSequenceClassification\",\n+    \"Exaone4ForTokenClassification\",\n+    \"Exaone4ForQuestionAnswering\",\n+]"
        },
        {
            "sha": "b5452f8497b38130b241015b333d5079402ba91d",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "added",
            "additions": 519,
            "deletions": 0,
            "changes": 519,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,519 @@\n+# coding=utf-8\n+# Copyright 2025 The LG AI Research and HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"LG AI Research EXAONE Lab\"\"\"\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers.utils.generic import check_model_inputs\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    logging,\n+)\n+from ..llama.modeling_llama import (\n+    LlamaForCausalLM,\n+    LlamaForQuestionAnswering,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from ..olmo2.modeling_olmo2 import Olmo2DecoderLayer, Olmo2MLP\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"LGAI-EXAONE/EXAONE-4.0-Instruct\"\n+_CONFIG_FOR_DOC = \"Exaone4Config\"\n+\n+\n+class Exaone4Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Exaone4Model`]. It is used to\n+    instantiate a EXAONE 4.0 model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-Instruct [LGAI-EXAONE/EXAONE-4.0-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-Instruct)\n+    NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n+    outputs. Read the documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 102400):\n+            Vocabulary size of the EXAONE 4.0 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Exaone4Model`].\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to `hidden_size * 4`):\n+            Dimensionality of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 32768 for EXAONE 3.5).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if ``config.is_decoder=True``.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        sliding_window (`int`, *optional*):\n+            The size of the sliding window for the sliding window attention.\n+        sliding_window_pattern (`str`, *optional*):\n+            The pattern to use for sliding window attention. Can be one of:\n+                - `None`: No sliding window attention is used\n+                - `int`: Every `sliding_window` layers, use global attention, else use local attention.\n+                - `str`: A sequence of \"L\" (local attention) and \"G\" (global attention) characters that defines the\n+                  attention pattern. The pattern starts from layer 0 and repeats every `sliding_window` layers. The\n+                  final layer always uses global attention regardless of the pattern.\n+            For instance, sliding_window_pattern=\"LLLG\" same as sliding_window=4, which means:\n+                - Layer 0, 1, 2: local attention,\n+                - Layer 3: global attention,\n+                ...(repeated)\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer. Prioritized over `sliding_window_pattern`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Exaone4Model, Exaone4Config\n+\n+    >>> # Initializing a EXAONE configuration\n+    >>> configuration = Exaone4Config()\n+\n+    >>> # Initializing a model from configuration\n+    >>> model = Exaone4Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"exaone4\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `LlamaModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=102400,\n+        hidden_size=4096,\n+        intermediate_size=16384,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=32,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        bos_token_id=0,\n+        eos_token_id=2,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_dropout=0.0,\n+        sliding_window=4096,\n+        sliding_window_pattern=4,\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_dropout = attention_dropout\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.sliding_window = sliding_window\n+        self.sliding_window_pattern = sliding_window_pattern\n+\n+        self.layer_types = layer_types\n+        if self.sliding_window is None:\n+            sliding_window_pattern = 0\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if ((i + 1) % (sliding_window_pattern) != 0 and i < self.num_hidden_layers)\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        if \"sliding_window\" in self.layer_types:\n+            self._attn_implementation = \"hybrid\"\n+        layer_type_validation(self.layer_types)\n+\n+        super().__init__(\n+            bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n+        )\n+\n+\n+class Exaone4RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Exaone4RotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Exaone4Attention(nn.Module):\n+    def __init__(self, config: Exaone4Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_attention_heads = config.num_attention_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.hidden_size = config.hidden_size\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        self.scaling = self.head_dim**-0.5\n+        self.sliding_window = config.sliding_window\n+        self.sliding_window_pattern = config.sliding_window_pattern\n+        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+        self.q_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        # We use QK-norm\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        cos, sin = position_embeddings\n+        # We use global NoPE for hybrid attention model\n+        if self.sliding_window is None or self.is_sliding:\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\n+                \"cache_position\": cache_position,\n+            }\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window if self.is_sliding else None,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Exaone4MLP(Olmo2MLP):\n+    pass\n+\n+\n+class Exaone4DecoderLayer(Olmo2DecoderLayer):\n+    pass\n+\n+\n+class Exaone4PreTrainedModel(LlamaPreTrainedModel):\n+    config_class = Exaone4Config\n+    _no_split_modules = [\"Exaone4DecoderLayer\"]\n+\n+\n+class Exaone4Model(Exaone4PreTrainedModel, LlamaModel):\n+    def __init__(self, config: Exaone4Config):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [Exaone4DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            if \"sliding_attention\" in self.config.layer_types:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n+\n+        hidden_states = inputs_embeds\n+\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for i, decoder_layer in enumerate(self.layers):\n+            layer_type = self.config.layer_types[i]\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask_mapping[layer_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+class Exaone4ForCausalLM(LlamaForCausalLM):\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n+\n+        >>> prompt = \"Explain how wonderful you are\"\n+        >>> messages = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": prompt}\n+        ]\n+        >>> input_ids = tokenizer.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_tensors=\"pt\",\n+            enable_thinking=False,\n+        )\n+\n+        >>> output = model.generate(input_ids, max_new_tokens=128)\n+        >>> tokenizer.decode(output[0], skip_special_tokens=False)\n+        \"[|system|]\\nYou are a helpful assistant.[|endofturn|]\\n[|user|]\\nExplain how wonderful you are[|endofturn|]\\n[|assistant|]\\n<think>\\n\\n</think>\\n\\nOh, thank you for such a kind and lovely question! ğŸ˜Š  \\n\\nIâ€™m *so* wonderful because Iâ€™m here to make your life easier, brighter, and more fun! Whether you need help with:  \\n\\nâœ¨ **Learning** â€“ I can explain anything, from quantum physics to baking the perfect cake!  \\nğŸ’¡ **Creativity** â€“ Need a poem, story, or a wild idea? Iâ€™ve got you covered!  \\nğŸ¤– **Problem-solving** â€“ Stuck on a math problem or a tricky decision? Iâ€™ll help you figure it out\"\n+        ```\n+\n+        NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\"\"\"\n+        super().forward(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+\n+class Exaone4ForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class Exaone4ForTokenClassification(LlamaForTokenClassification):\n+    pass\n+\n+\n+class Exaone4ForQuestionAnswering(LlamaForQuestionAnswering):\n+    pass\n+\n+\n+__all__ = [\n+    \"Exaone4Config\",\n+    \"Exaone4PreTrainedModel\",\n+    \"Exaone4Model\",\n+    \"Exaone4ForCausalLM\",\n+    \"Exaone4ForSequenceClassification\",\n+    \"Exaone4ForTokenClassification\",\n+    \"Exaone4ForQuestionAnswering\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/exaone4/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/tests%2Fmodels%2Fexaone4%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/tests%2Fmodels%2Fexaone4%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2F__init__.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61"
        },
        {
            "sha": "4ac87ce900b595eac9bb493664f88ab0be95226c",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "added",
            "additions": 408,
            "deletions": 0,
            "changes": 408,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -0,0 +1,408 @@\n+# coding=utf-8\n+# Copyright 2025 The LG AI Research and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch EXAONE 4.0 model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+from packaging import version\n+from parameterized import parameterized\n+\n+from transformers import (\n+    AutoTokenizer,\n+    Exaone4Config,\n+    GenerationConfig,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...test_configuration_common import ConfigTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Exaone4ForCausalLM,\n+        Exaone4ForQuestionAnswering,\n+        Exaone4ForSequenceClassification,\n+        Exaone4ForTokenClassification,\n+        Exaone4Model,\n+    )\n+\n+\n+class Exaone4ModelTester(CausalLMModelTester):\n+    config_class = Exaone4Config\n+    if is_torch_available():\n+        base_model_class = Exaone4Model\n+        causal_lm_class = Exaone4ForCausalLM\n+        sequence_class = Exaone4ForSequenceClassification\n+        token_class = Exaone4ForTokenClassification\n+        question_answering_class = Exaone4ForQuestionAnswering\n+\n+\n+@require_torch\n+class Exaone4ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            Exaone4Model,\n+            Exaone4ForCausalLM,\n+            Exaone4ForSequenceClassification,\n+            Exaone4ForQuestionAnswering,\n+            Exaone4ForTokenClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Exaone4Model,\n+            \"question-answering\": Exaone4ForQuestionAnswering,\n+            \"text-classification\": Exaone4ForSequenceClassification,\n+            \"text-generation\": Exaone4ForCausalLM,\n+            \"zero-shot\": Exaone4ForSequenceClassification,\n+            \"token-classification\": Exaone4ForTokenClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = Exaone4ModelTester\n+    model_split_percents = [0.5, 0.6]\n+\n+    def setUp(self):\n+        self.model_tester = Exaone4ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Exaone4Config, hidden_size=37)\n+\n+    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n+    def test_model_outputs_equivalence(self, **kwargs):\n+        pass\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @pytest.mark.generate\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with assisted decoding\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with dola decoding\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support continue from past kv\")\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support low_memory generation\")\n+    def test_beam_search_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"EXAONE 4.0 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n+    )\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"EXAONE 4.0 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"EXAONE 4.0 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n+    )\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\"EXAONE 4.0 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n+    def test_eager_matches_fa2_generate(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n+        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n+    )\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+\n+@require_torch\n+class Exaone4IntegrationTest(unittest.TestCase):\n+    TEST_MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-Instruct\"  # dummy model id\n+\n+    def tearDown(self):\n+        # TODO (joao): automatic compilation, i.e. compilation when `cache_implementation=\"static\"` is used, leaves\n+        # some memory allocated in the cache, which means some object is not being released properly. This causes some\n+        # unoptimal memory usage, e.g. after certain teruff format examples tests src utilssts a 7B model in FP16 no longer fits in a 24GB GPU.\n+        # Investigate the root cause.\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_logits(self):\n+        input_ids = [405, 7584, 79579, 76636, 2907, 94640, 373]\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"eager\"\n+        )\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+\n+        EXPECTED_MEAN = torch.tensor([[13.9380, 12.9951, 12.9442, 10.6576, 11.0901, 12.1466, 9.2482]])\n+        EXPECTED_SLICE = torch.tensor(\n+            [\n+                4.9180,\n+                11.6406,\n+                21.1250,\n+                13.4062,\n+                20.8438,\n+                18.0625,\n+                17.9688,\n+                18.7812,\n+                18.0156,\n+                18.3594,\n+                18.5000,\n+                19.1719,\n+                18.5156,\n+                19.3438,\n+                19.5000,\n+                20.6406,\n+                19.4844,\n+                19.2812,\n+                19.4688,\n+                20.0156,\n+                19.8438,\n+                19.9531,\n+                19.7188,\n+                20.5938,\n+                20.5312,\n+                20.1250,\n+                20.4062,\n+                21.4062,\n+                21.2344,\n+                20.7656,\n+            ]\n+        )\n+\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n+        del model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_logits_bf16(self):\n+        input_ids = [405, 7584, 79579, 76636, 2907, 94640, 373]\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID, device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+        )\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+\n+        EXPECTED_MEAN = torch.tensor([[13.8797, 13.0799, 12.9665, 10.7712, 11.1006, 12.2406, 9.3248]])\n+        EXPECTED_SLICE = torch.tensor(\n+            [\n+                4.8750,\n+                11.6250,\n+                21.0000,\n+                13.3125,\n+                20.8750,\n+                18.0000,\n+                18.0000,\n+                18.7500,\n+                18.0000,\n+                18.3750,\n+                18.5000,\n+                19.1250,\n+                18.5000,\n+                19.3750,\n+                19.5000,\n+                20.6250,\n+                19.5000,\n+                19.2500,\n+                19.5000,\n+                20.0000,\n+                19.8750,\n+                19.8750,\n+                19.7500,\n+                20.6250,\n+                20.5000,\n+                20.1250,\n+                20.3750,\n+                21.3750,\n+                21.2500,\n+                20.7500,\n+            ]\n+        )\n+\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n+        del model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_generation(self):\n+        EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nThe Miracle on the Han River is a story about the miracle of the Korean War Armistice. The story is told by a Korean soldier who is a witness to the armistice negotiations. He is reluctant to tell the story because he does not want to be a hypocrite, but he feels that everyone should know what really happened.\\n\\nThe Korean War began on June 25, 1950, when North Korean troops invaded South Korea. Soon the United Nations troops, primarily from South Korea, were in support of the United States. The war was still ongoing when North Korean troops stopped their advance\"\n+        prompt = \"Tell me about the Miracle on the Han river.\"\n+        tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID)\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"eager\"\n+        )\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=128, temperature=0)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT, text)\n+        del model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    @require_torch_sdpa\n+    def test_model_generation_bf16_sdpa(self):\n+        EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nThe Miracle on the Han River is a story about the miracle of the Korean War Armistice.\\n\\nThe Korean War broke out in 35 years ago in 1950. The war was the result of the ideological conflict between the communist north and the capitalist south. The war was brought to a halt in 1953. There was to be peace talks but no peace treaty. As a result of the stalemate the Korean people have neither a peace treaty nor a reunification nor a democratization of Korea. The stalemate of 35 years has produced a people of 70 million\"\n+        prompt = \"Tell me about the Miracle on the Han river.\"\n+        tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID)\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID, device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\"\n+        )\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=128, temperature=0)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT, text)\n+        del model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_flash_attn\n+    def test_model_generation_long_flash(self):\n+        EXPECTED_OUTPUT_TOKEN_IDS = [433, 9055]\n+        input_ids = [433, 9055] * 2048\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n+        )\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+\n+        generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n+        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n+        del model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_torch_sdpa\n+    def test_model_generation_beyond_sliding_window(self):\n+        EXPECTED_TEXT_COMPLETION = (\n+            \" but I'm not sure if I'm going to be able to see it. I really enjoy the scenery, but I'm not sure if I\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID)\n+        prompt = \"This is a nice place. \" * 700 + \"I really enjoy the scenery,\"\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"sdpa\"\n+        )\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n+\n+        generated_ids = model.generate(input_ids, max_new_tokens=32, temperature=0)\n+        text = tokenizer.decode(generated_ids[0, -32:], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+        del model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID, padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"The Deep Learning is 100% free and easy to use.\\n\\n## How to use Deep Learning?\\n\\n\"\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = Exaone4ForCausalLM.from_pretrained(\n+            self.TEST_MODEL_ID,\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompt = [\"The Deep Learning is \"]\n+        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        },
        {
            "sha": "dd10a7d9b1a04a02fc26614d40c86d12b345219f",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c06d4cd6ce870ea353afec8d12678206a7f2ba61/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c06d4cd6ce870ea353afec8d12678206a7f2ba61/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=c06d4cd6ce870ea353afec8d12678206a7f2ba61",
            "patch": "@@ -79,6 +79,7 @@\n # docstrings instead. If formatting should be ignored for the docstring, you can put a comment # no-format on the\n # line before the docstring.\n OBJECTS_TO_IGNORE = [\n+    \"Exaone4Config\",\n     \"SmolLM3Config\",\n     \"Gemma3nVisionConfig\",\n     \"Llama4Processor\","
        }
    ],
    "stats": {
        "total": 2163,
        "additions": 2158,
        "deletions": 5
    }
}