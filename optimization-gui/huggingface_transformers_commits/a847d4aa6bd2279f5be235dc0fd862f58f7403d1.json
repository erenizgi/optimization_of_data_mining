{
    "author": "henrikm11",
    "message": "Fast image processor for VitMatte added and bug in slow version fixed (#37616)\n\n* added fast image processor for VitMatte including updated and new tests, fixed a bug in the slow image processor that processed images incorrectly for input format ChannelDimension.FIRST in which case the trimaps were not added in the correct dimension, this bug was also reflected in the tests through incorretly shaped trimaps being passed\n\n* final edits for fast vitmatte image processor and tests\n\n* final edits for fast vitmatte image processor and tests\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
    "files": [
        {
            "sha": "566c296229fcb8bf8a38673235b7222e1f379e6f",
            "filename": "docs/source/en/model_doc/vitmatte.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md?ref=a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
            "patch": "@@ -53,6 +53,11 @@ The model expects both the image and trimap (concatenated) as input. Use [`ViTMa\n [[autodoc]] VitMatteImageProcessor\n     - preprocess\n \n+## VitMatteImageProcessorFast\n+\n+[[autodoc]] VitMatteImageProcessorFast\n+    - preprocess\n+\n ## VitMatteForImageMatting\n \n [[autodoc]] VitMatteForImageMatting"
        },
        {
            "sha": "b4d82dd8748945205d25e9fa15ce6c473f057212",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
            "patch": "@@ -167,7 +167,7 @@\n             (\"vit_hybrid\", (\"ViTHybridImageProcessor\",)),\n             (\"vit_mae\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vit_msn\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"vitmatte\", (\"VitMatteImageProcessor\",)),\n+            (\"vitmatte\", (\"VitMatteImageProcessor\", \"VitMatteImageProcessorFast\")),\n             (\"xclip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"yolos\", (\"YolosImageProcessor\", \"YolosImageProcessorFast\")),\n             (\"zoedepth\", (\"ZoeDepthImageProcessor\",)),"
        },
        {
            "sha": "863c6b5f8f46677b530fcc0b8f79a7fb0f83457b",
            "filename": "src/transformers/models/vitmatte/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2F__init__.py?ref=a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_vitmatte import *\n     from .image_processing_vitmatte import *\n+    from .image_processing_vitmatte_fast import *\n     from .modeling_vitmatte import *\n else:\n     import sys"
        },
        {
            "sha": "464a67ef63ab7992c22e74c5c7118d37c635e18d",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
            "patch": "@@ -250,8 +250,10 @@ def preprocess(\n             ]\n \n         # concatenate images and trimaps\n+        axis = -1 if input_data_format == ChannelDimension.LAST else 0\n         images = [\n-            np.concatenate([image, np.expand_dims(trimap, axis=-1)], axis=-1) for image, trimap in zip(images, trimaps)\n+            np.concatenate([image, np.expand_dims(trimap, axis=axis)], axis=axis)\n+            for image, trimap in zip(images, trimaps)\n         ]\n \n         if do_pad:"
        },
        {
            "sha": "4355a0266433c7f6705c451738c7c7853fa76c93",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "added",
            "additions": 240,
            "deletions": 0,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
            "patch": "@@ -0,0 +1,240 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for ViTMatte.\"\"\"\n+\n+from functools import partial\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    get_image_size,\n+    make_list_of_images,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    filter_out_non_signature_kwargs,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VitMatteFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    do_pad: Optional[bool]\n+    size_divisibility: int\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast VitMatte image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to make the width and height divisible by `size_divisibility`. Can be overridden\n+            by the `do_pad` parameter in the `preprocess` method.\n+        size_divisibility (`int`, *optional*, defaults to 32):\n+            The width and height of the image will be padded to be divisible by this number.\n+    \"\"\",\n+)\n+class VitMatteImageProcessorFast(BaseImageProcessorFast):\n+    do_rescale: bool = True\n+    rescale_factor: Union[int, float] = 1 / 255\n+    do_normalize: bool = True\n+    image_mean: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_MEAN\n+    image_std: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_STD\n+    do_pad: bool = True\n+    size_divisibility: int = 32\n+    valid_kwargs = VitMatteFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[VitMatteFastImageProcessorKwargs]) -> None:\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to make the width and height divisible by `size_divisibility`. Can be overridden\n+            by the `do_pad` parameter in the `preprocess` method.\n+        size_divisibility (`int`, *optional*, defaults to 32):\n+            The width and height of the image will be padded to be divisible by this number.\n+    \"\"\",\n+    )\n+    def preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        trimaps: list[\"torch.Tensor\"],\n+        **kwargs: Unpack[VitMatteFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+\n+        # Prepare input trimaps\n+        trimaps = self._prepare_input_trimaps(trimaps=trimaps, device=device)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"resample\")\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+        kwargs.pop(\"do_resize\")\n+        kwargs.pop(\"do_center_crop\")\n+        kwargs.pop(\"size\")\n+        kwargs.pop(\"crop_size\")\n+\n+        return self._preprocess(images=images, trimaps=trimaps, **kwargs)\n+\n+    def _prepare_input_trimaps(\n+        self, trimaps: ImageInput, device: Optional[\"torch.device\"] = None\n+    ) -> list[\"torch.Tensor\"]:\n+        \"\"\"\n+        Prepare input trimaps for processing,m this can not yet deal with nested list\n+\n+        Args:\n+            trimaps ('ImageInout):\n+                The input trimaps to be process, should not be nested\n+            device('Optional['torch.device'] defaults to 'self.device'):\n+                The device to process the trimaps on\n+\n+        Returns:\n+            list['torch.Tensor']:\n+                Input trimaps converted to a list of tensors\n+        \"\"\"\n+        # from batch or single image to list, and insert channel dimension\n+        trimaps = make_list_of_images(trimaps, expected_ndims=2)\n+\n+        # passing ChannelDimension.First achieves correct functionality on grayscale/single channel\n+        process_image_fn = partial(\n+            self._process_image,\n+            input_data_format=ChannelDimension.FIRST,\n+            device=device,\n+        )\n+\n+        processed_trimaps = []\n+        for trimap in trimaps:\n+            processed_trimaps.append(torch.unsqueeze(process_image_fn(trimap), dim=0))\n+\n+        return processed_trimaps\n+\n+    def _pad_image(\n+        self,\n+        images: \"torch.tensor\",\n+        size_divisibility: int = 32,\n+    ) -> \"torch.tensor\":\n+        \"\"\"\n+        Pads an image or batched images constantly so that width and height are divisible by size_divisibility\n+\n+        Args:\n+            image (`torch,tensor`):\n+                Image to pad.\n+            size_divisibility (`int`, *optional*, defaults to 32):\n+                The width and height of the image will be padded to be divisible by this number.\n+        \"\"\"\n+        height, width = get_image_size(images, channel_dim=ChannelDimension.FIRST)\n+\n+        pad_height = 0 if height % size_divisibility == 0 else size_divisibility - height % size_divisibility\n+        pad_width = 0 if width % size_divisibility == 0 else size_divisibility - width % size_divisibility\n+\n+        if pad_width + pad_height > 0:\n+            padding = (0, 0, pad_width, pad_height)\n+            images = F.pad(images, padding)\n+\n+        return images\n+\n+    @filter_out_non_signature_kwargs()\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        trimaps: list[\"torch.Tensor\"],\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        do_pad: Optional[bool] = None,\n+        size_divisibility: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_trimaps, grouped_trimaps_index = group_images_by_shape(trimaps)\n+        processed_images_grouped = {}\n+        for shape in grouped_images:\n+            stacked_images = grouped_images[shape]\n+            stacked_trimaps = grouped_trimaps[shape]\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            stacked_trimaps = self.rescale_and_normalize(\n+                stacked_trimaps, do_rescale, rescale_factor, False, image_mean, image_std\n+            )\n+            stacked_images = torch.cat([stacked_images, stacked_trimaps], dim=1)\n+            if do_pad:\n+                stacked_images = self._pad_image(stacked_images, self.size_divisibility)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"VitMatteImageProcessorFast\"]"
        },
        {
            "sha": "f3a8a1507b3be452de74d4c7cf5f36ea01f328a4",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 165,
            "deletions": 45,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=a847d4aa6bd2279f5be235dc0fd862f58f7403d1",
            "patch": "@@ -13,13 +13,15 @@\n # limitations under the License.\n \n \n+import time\n import unittest\n import warnings\n \n import numpy as np\n+import requests\n \n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import is_flaky, require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -33,6 +35,9 @@\n \n     from transformers import VitMatteImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import VitMatteImageProcessorFast\n+\n \n class VitMatteImageProcessingTester:\n     def __init__(\n@@ -92,6 +97,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class VitMatteImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = VitMatteImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = VitMatteImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -102,18 +108,17 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"size_divisibility\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisibility\"))\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n         # create random numpy tensors\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n         for image in image_inputs:\n@@ -122,15 +127,16 @@ def test_call_numpy(self):\n         # Test not batched input (image processor does not support batched inputs)\n         image = image_inputs[0]\n         trimap = np.random.randint(0, 3, size=image.shape[:2])\n-        encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n-        # Verify that width and height can be divided by size_divisibility\n-        self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-        self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-3] == 4)\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n         # create random PyTorch tensors\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n@@ -139,16 +145,37 @@ def test_call_pytorch(self):\n \n         # Test not batched input (image processor does not support batched inputs)\n         image = image_inputs[0]\n-        trimap = np.random.randint(0, 3, size=image.shape[:2])\n-        encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n+        trimap = np.random.randint(0, 3, size=image.shape[1:])\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n-        # Verify that width and height can be divided by size_divisibility\n-        self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-        self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-3] == 4)\n+\n+        # create batched tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        image_input = torch.stack(image_inputs, dim=0)\n+        self.assertIsInstance(image_input, torch.Tensor)\n+        self.assertTrue(image_input.shape[1] == 3)\n+\n+        trimap_shape = [image_input.shape[0]] + [1] + list(image_input.shape)[2:]\n+        trimap_input = torch.randint(0, 3, trimap_shape, dtype=torch.uint8)\n+        self.assertIsInstance(trimap_input, torch.Tensor)\n+        self.assertTrue(trimap_input.shape[1] == 1)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n+\n+            # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-3] == 4)\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n         # create random PIL images\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n         for image in image_inputs:\n@@ -157,16 +184,17 @@ def test_call_pil(self):\n         # Test not batched input (image processor does not support batched inputs)\n         image = image_inputs[0]\n         trimap = np.random.randint(0, 3, size=image.size[::-1])\n-        encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n-        # Verify that width and height can be divided by size_divisibility\n-        self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-        self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-3] == 4)\n \n     def test_call_numpy_4_channels(self):\n         # Test that can process images which have an arbitrary number of channels\n-        # Initialize image_processing\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n \n         # create random numpy tensors\n         self.image_processor_tester.num_channels = 4\n@@ -175,20 +203,23 @@ def test_call_numpy_4_channels(self):\n         # Test not batched input (image processor does not support batched inputs)\n         image = image_inputs[0]\n         trimap = np.random.randint(0, 3, size=image.shape[:2])\n-        encoded_images = image_processor(\n-            images=image,\n-            trimaps=trimap,\n-            input_data_format=\"channels_first\",\n-            image_mean=0,\n-            image_std=1,\n-            return_tensors=\"pt\",\n-        ).pixel_values\n-\n-        # Verify that width and height can be divided by size_divisibility\n-        self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-        self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n-\n-    def test_padding(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            encoded_images = image_processor(\n+                images=image,\n+                trimaps=trimap,\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+                return_tensors=\"pt\",\n+            ).pixel_values\n+\n+            # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-3] == 5)\n+\n+    def test_padding_slow(self):\n         image_processing = self.image_processing_class(**self.image_processor_dict)\n         image = np.random.randn(3, 249, 491)\n         images = image_processing.pad_image(image)\n@@ -198,6 +229,17 @@ def test_padding(self):\n         images = image_processing.pad_image(image)\n         assert images.shape == (3, 256, 512)\n \n+    def test_padding_fast(self):\n+        # extra test because name is different for fast image processor\n+        image_processing = self.fast_image_processing_class(**self.image_processor_dict)\n+        image = torch.rand(3, 249, 491)\n+        images = image_processing._pad_image(image)\n+        assert images.shape == (3, 256, 512)\n+\n+        image = torch.rand(3, 249, 512)\n+        images = image_processing._pad_image(image)\n+        assert images.shape == (3, 256, 512)\n+\n     def test_image_processor_preprocess_arguments(self):\n         # vitmatte require additional trimap input for image_processor\n         # that is why we override original common test\n@@ -214,3 +256,81 @@ def test_image_processor_preprocess_arguments(self):\n             messages = \" \".join([str(w.message) for w in raised_warnings])\n             self.assertGreaterEqual(len(raised_warnings), 1)\n             self.assertIn(\"extra_argument\", messages)\n+\n+    @is_flaky()\n+    def test_fast_is_faster_than_slow(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\n+\n+        def measure_time(image_processor, images, trimaps):\n+            # Warmup\n+            for _ in range(5):\n+                _ = image_processor(images, trimaps=trimaps, return_tensors=\"pt\")\n+            all_times = []\n+            for _ in range(10):\n+                start = time.time()\n+                _ = image_processor(images, trimaps=trimaps, return_tensors=\"pt\")\n+                all_times.append(time.time() - start)\n+            # Take the average of the fastest 3 runs\n+            avg_time = sum(sorted(all_times[:3])) / 3.0\n+            return avg_time\n+\n+        dummy_images = torch.randint(0, 255, (4, 3, 400, 800), dtype=torch.uint8)\n+        dummy_trimaps = torch.randint(0, 3, (4, 400, 800), dtype=torch.uint8)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        fast_time = measure_time(image_processor_fast, dummy_images, dummy_trimaps)\n+        slow_time = measure_time(image_processor_slow, dummy_images, dummy_trimaps)\n+\n+        self.assertLessEqual(fast_time, slow_time)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        dummy_trimap = np.random.randint(0, 3, size=dummy_image.size[::-1])\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, trimaps=dummy_trimap, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, trimaps=dummy_trimap, return_tensors=\"pt\")\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        # this only checks on equal resolution, since the slow processor doesn't work otherwise\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        dummy_trimaps = [np.random.randint(0, 3, size=image.shape[1:]) for image in dummy_images]\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, trimaps=dummy_trimaps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, trimaps=dummy_trimaps, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )"
        }
    ],
    "stats": {
        "total": 462,
        "additions": 415,
        "deletions": 47
    }
}