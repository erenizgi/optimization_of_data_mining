{
    "author": "adutchengineer",
    "message": "build: Add fast image processor tvp (#39529)\n\n* build: add TvpImageProcessorFast\n\n- Introduced TvpImageProcessorFast to enhance image processing capabilities.\n- Updated image processing auto registration to include the new fast processor.\n- Modified tests to accommodate both TvpImageProcessor and TvpImageProcessorFast, ensuring comprehensive coverage for both classes.\n\n* fix: TvpImageProcessorFast with new resize method and update processing logic\n\n* build: add TvpImageProcessorFast\n\n* refactor: clean up whitespace and formatting in TvpImageProcessorFast and related tests\n\n- Removed unnecessary whitespace and ensured consistent formatting in image_processing_tvp_fast.py.\n- Updated import order in test_image_processing_tvp.py for clarity.\n- Minor adjustments to maintain code readability and consistency.\n\n* fix: Enhance TvpFastImageProcessorKwargs and update documentation\n\n- Added TvpFastImageProcessorKwargs class to define valid kwargs for TvpImageProcessorFast.\n- Updated the documentation in tvp.md to include the new class and its parameters.\n- Refined the image processing logic in image_processing_tvp_fast.py for better handling of padding and resizing.\n- Improved test cases in test_image_processing_tvp.py to ensure compatibility with the new processing logic and tensor inputs.\n\n* fix: tested now with python 3.9\n\n* fix: remove tvp kwargs from docs\n\n* simplify processing\n\n* remove import and fix tests\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "b834cb813862387c7148ca86cdab68d2a959d7f3",
    "files": [
        {
            "sha": "dd0f63e55e19c385424a68f9bd9c954bb5a058a8",
            "filename": "docs/source/en/model_doc/tvp.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b834cb813862387c7148ca86cdab68d2a959d7f3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b834cb813862387c7148ca86cdab68d2a959d7f3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md?ref=b834cb813862387c7148ca86cdab68d2a959d7f3",
            "patch": "@@ -174,6 +174,11 @@ Tips:\n [[autodoc]] TvpImageProcessor\n     - preprocess\n \n+## TvpImageProcessorFast\n+\n+[[autodoc]] TvpImageProcessorFast\n+    - preprocess\n+\n ## TvpProcessor\n \n [[autodoc]] TvpProcessor"
        },
        {
            "sha": "31699667747c454ca2c536d78d1f2eb5f8f3ede4",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b834cb813862387c7148ca86cdab68d2a959d7f3/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b834cb813862387c7148ca86cdab68d2a959d7f3/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=b834cb813862387c7148ca86cdab68d2a959d7f3",
            "patch": "@@ -176,7 +176,7 @@\n             (\"timesformer\", (\"VideoMAEImageProcessor\", None)),\n             (\"timm_wrapper\", (\"TimmWrapperImageProcessor\", None)),\n             (\"tvlt\", (\"TvltImageProcessor\", None)),\n-            (\"tvp\", (\"TvpImageProcessor\", None)),\n+            (\"tvp\", (\"TvpImageProcessor\", \"TvpImageProcessorFast\")),\n             (\"udop\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"upernet\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),"
        },
        {
            "sha": "7033f7a2e7f332115bd93eadba2761c269747709",
            "filename": "src/transformers/models/tvp/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b834cb813862387c7148ca86cdab68d2a959d7f3/src%2Ftransformers%2Fmodels%2Ftvp%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b834cb813862387c7148ca86cdab68d2a959d7f3/src%2Ftransformers%2Fmodels%2Ftvp%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2F__init__.py?ref=b834cb813862387c7148ca86cdab68d2a959d7f3",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_tvp import *\n     from .image_processing_tvp import *\n+    from .image_processing_tvp_fast import *\n     from .modeling_tvp import *\n     from .processing_tvp import *\n else:"
        },
        {
            "sha": "3eec7e2d9c3dbb831e6cc6b4e4cb84af54f64974",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "added",
            "additions": 297,
            "deletions": 0,
            "changes": 297,
            "blob_url": "https://github.com/huggingface/transformers/blob/b834cb813862387c7148ca86cdab68d2a959d7f3/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b834cb813862387c7148ca86cdab68d2a959d7f3/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=b834cb813862387c7148ca86cdab68d2a959d7f3",
            "patch": "@@ -0,0 +1,297 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for TVP.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    make_nested_list_of_images,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class TvpFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    do_flip_channel_order (`bool`, *optional*):\n+        Whether to flip the channel order of the image from RGB to BGR.\n+    do_pad (`bool`, *optional*):\n+        Whether to pad the image.\n+    pad_size (`Dict[str, int]` or `SizeDict`, *optional*):\n+        Size dictionary specifying the desired height and width for padding.\n+    constant_values (`float` or `List[float]`, *optional*):\n+        Value used to fill the padding area when `pad_mode` is `'constant'`.\n+    pad_mode (`str`, *optional*):\n+        Padding mode to use â€” `'constant'`, `'edge'`, `'reflect'`, or `'symmetric'`.\n+    \"\"\"\n+\n+    do_flip_channel_order: Optional[bool]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[SizeDict]\n+    constant_values: Optional[Union[float, list[float]]]\n+    pad_mode: Optional[str]\n+\n+\n+@auto_docstring\n+class TvpImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"longest_edge\": 448}\n+    default_to_square = False\n+    crop_size = {\"height\": 448, \"width\": 448}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_pad = True\n+    pad_size = {\"height\": 448, \"width\": 448}\n+    constant_values = 0\n+    pad_mode = \"constant\"\n+    do_normalize = True\n+    do_flip_channel_order = True\n+    valid_kwargs = TvpFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[TvpFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        videos: Union[ImageInput, list[ImageInput], list[list[ImageInput]]],\n+        **kwargs: Unpack[TvpFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess videos using the fast image processor.\n+        \"\"\"\n+        return super().preprocess(videos, **kwargs)\n+\n+    def _further_process_kwargs(\n+        self,\n+        pad_size: Optional[SizeDict] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if pad_size is not None:\n+            pad_size = SizeDict(**get_size_dict(pad_size, param_name=\"pad_size\"))\n+        kwargs[\"pad_size\"] = pad_size\n+\n+        return super()._further_process_kwargs(**kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        \"\"\"\n+        Prepare the images structure for processing.\n+\n+        Args:\n+            images (`ImageInput`):\n+                The input images to process.\n+\n+        Returns:\n+            `ImageInput`: The images with a valid nesting.\n+        \"\"\"\n+        return make_nested_list_of_images(images, **kwargs)\n+\n+    def _pad_frames(\n+        self,\n+        frames: \"torch.Tensor\",\n+        pad_size: Union[SizeDict, dict],\n+        constant_values: Union[float, list[float]],\n+        pad_mode: str,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"Pad frames to the specified size.\"\"\"\n+        height, width = pad_size.height, pad_size.width\n+\n+        if frames.shape[-2:] == (height, width):\n+            return frames\n+\n+        # Calculate padding\n+        current_height, current_width = frames.shape[-2:]\n+        pad_bottom = height - current_height\n+        pad_right = width - current_width\n+\n+        if pad_bottom < 0 or pad_right < 0:\n+            raise ValueError(\"The padding size must be greater than frame size\")\n+\n+        # Apply padding\n+        padding = [0, 0, pad_right, pad_bottom]  # [left, top, right, bottom]\n+        return F.pad(frames, padding, fill=constant_values, padding_mode=pad_mode)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to the specified size.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict` or `dict`):\n+                Size dictionary. If `size` has `longest_edge`, resize the longest edge to that value\n+                while maintaining aspect ratio. Otherwise, use the base class resize method.\n+            interpolation (`F.InterpolationMode`, *optional*):\n+                Interpolation method to use.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+\n+        # Handle longest_edge case (TVP-specific)\n+        if size.longest_edge:\n+            # Get current dimensions\n+            current_height, current_width = image.shape[-2:]\n+\n+            # Calculate new dimensions maintaining aspect ratio\n+            if current_height >= current_width:\n+                ratio = current_width * 1.0 / current_height\n+                new_height = size.longest_edge\n+                new_width = int(new_height * ratio)\n+            else:\n+                ratio = current_height * 1.0 / current_width\n+                new_width = size.longest_edge\n+                new_height = int(new_width * ratio)\n+\n+            return super().resize(\n+                image, SizeDict(height=new_height, width=new_width), interpolation=interpolation, antialias=antialias\n+            )\n+\n+        # Use base class resize method for other cases\n+        return super().resize(image, size, interpolation, antialias, **kwargs)\n+\n+    def _flip_channel_order(self, frames: \"torch.Tensor\") -> \"torch.Tensor\":\n+        \"\"\"\n+        Flip channel order from RGB to BGR.\n+\n+        The slow processor puts the red channel at the end (BGR format),\n+        but the channel order is different. We need to match the exact\n+        channel order of the slow processor:\n+\n+        Slow processor:\n+        - Channel 0: Blue (originally Red)\n+        - Channel 1: Green\n+        - Channel 2: Red (originally Blue)\n+        \"\"\"\n+        # Assuming frames are in channels_first format (..., C, H, W)\n+        frames = frames.flip(-3)\n+\n+        return frames\n+\n+    def _preprocess(\n+        self,\n+        images: list[list[\"torch.Tensor\"]],\n+        do_resize: bool,\n+        size: Union[SizeDict, dict],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: Union[SizeDict, dict],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_pad: bool,\n+        pad_size: Union[SizeDict, dict],\n+        constant_values: Union[float, list[float]],\n+        pad_mode: str,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_flip_channel_order: bool,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: Optional[bool],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess videos using the fast image processor.\n+\n+        This method processes each video frame through the same pipeline as the original\n+        TVP image processor but uses torchvision operations for better performance.\n+        \"\"\"\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_frames in grouped_images.items():\n+            # Resize if needed\n+            if do_resize:\n+                stacked_frames = self.resize(stacked_frames, size, interpolation)\n+\n+            # Center crop if needed\n+            if do_center_crop:\n+                stacked_frames = self.center_crop(stacked_frames, crop_size)\n+\n+            # Rescale and normalize using fused method for consistency\n+            stacked_frames = self.rescale_and_normalize(\n+                stacked_frames, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+\n+            # Pad if needed\n+            if do_pad:\n+                stacked_frames = self._pad_frames(stacked_frames, pad_size, constant_values, pad_mode)\n+\n+            # Flip channel order if needed (RGB to BGR)\n+            if do_flip_channel_order:\n+                stacked_frames = self._flip_channel_order(stacked_frames)\n+\n+            processed_images_grouped[shape] = stacked_frames\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+        if return_tensors == \"pt\":\n+            processed_images = [torch.stack(images, dim=0) for images in processed_images]\n+            processed_images = torch.stack(processed_images, dim=0)\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"TvpImageProcessorFast\"]"
        },
        {
            "sha": "d2f05f4473aad0bc93edf26b0fafd40e695eacd2",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 226,
            "deletions": 159,
            "changes": 385,
            "blob_url": "https://github.com/huggingface/transformers/blob/b834cb813862387c7148ca86cdab68d2a959d7f3/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b834cb813862387c7148ca86cdab68d2a959d7f3/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=b834cb813862387c7148ca86cdab68d2a959d7f3",
            "patch": "@@ -20,7 +20,7 @@\n \n from transformers.image_transforms import PaddingMode\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_video_inputs\n \n@@ -31,7 +31,7 @@\n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import TvpImageProcessor\n+    from transformers import TvpImageProcessor, TvpImageProcessorFast\n \n \n class TvpImageProcessingTester:\n@@ -124,6 +124,9 @@ def prepare_video_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class TvpImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = TvpImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = (\n+        TvpImageProcessorFast if is_vision_available() and is_torchvision_available() else None\n+    )\n \n     def setUp(self):\n         super().setUp()\n@@ -134,173 +137,237 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"pad_size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"pad_size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"longest_edge\": 40})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"longest_edge\": 40})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size={\"longest_edge\": 12})\n-        self.assertEqual(image_processor.size, {\"longest_edge\": 12})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size={\"longest_edge\": 12})\n+            self.assertEqual(image_processor.size, {\"longest_edge\": 12})\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL videos\n-        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False)\n-        for video in video_inputs:\n-            self.assertIsInstance(video, list)\n-            self.assertIsInstance(video[0], Image.Image)\n-\n-        # Test not batched input\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n-        encoded_videos = image_processing(video_inputs[0], return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        # Test batched\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs, batched=True)\n-        encoded_videos = image_processing(video_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL videos\n+            video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False)\n+            for video in video_inputs:\n+                self.assertIsInstance(video, list)\n+                self.assertIsInstance(video[0], Image.Image)\n+\n+            # Test not batched input\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n+            encoded_videos = image_processing(video_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            # Test batched\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(\n+                video_inputs, batched=True\n+            )\n+            encoded_videos = image_processing(video_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, numpify=True)\n-        for video in video_inputs:\n-            self.assertIsInstance(video, list)\n-            self.assertIsInstance(video[0], np.ndarray)\n-\n-        # Test not batched input\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n-        encoded_videos = image_processing(video_inputs[0], return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        # Test batched\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs, batched=True)\n-        encoded_videos = image_processing(video_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n+        # Test numpy with both processors\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, numpify=True)\n+            for video in video_inputs:\n+                self.assertIsInstance(video, list)\n+                self.assertIsInstance(video[0], np.ndarray)\n+\n+            # For fast processor, convert numpy to tensor\n+            if image_processing_class == self.fast_image_processing_class:\n+                # Convert numpy arrays to tensors for fast processor\n+                tensor_video_inputs = []\n+                for video in video_inputs:\n+                    tensor_video = [torch.from_numpy(frame) for frame in video]\n+                    tensor_video_inputs.append(tensor_video)\n+                test_inputs = tensor_video_inputs\n+            else:\n+                test_inputs = video_inputs\n+\n+            # Test not batched input\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n+            encoded_videos = image_processing(test_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            # Test batched\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(\n+                video_inputs, batched=True\n+            )\n+            encoded_videos = image_processing(test_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n \n     def test_call_numpy_4_channels(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, numpify=True)\n-        for video in video_inputs:\n-            self.assertIsInstance(video, list)\n-            self.assertIsInstance(video[0], np.ndarray)\n-\n-        # Test not batched input\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n-        encoded_videos = image_processing(\n-            video_inputs[0], return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n-        ).pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        # Test batched\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs, batched=True)\n-        encoded_videos = image_processing(\n-            video_inputs, return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n-        ).pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n+        # Test numpy with both processors\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, numpify=True)\n+            for video in video_inputs:\n+                self.assertIsInstance(video, list)\n+                self.assertIsInstance(video[0], np.ndarray)\n+\n+            # For fast processor, convert numpy to tensor\n+            if image_processing_class == self.fast_image_processing_class:\n+                # Convert numpy arrays to tensors for fast processor\n+                tensor_video_inputs = []\n+                for video in video_inputs:\n+                    tensor_video = [torch.from_numpy(frame) for frame in video]\n+                    tensor_video_inputs.append(tensor_video)\n+                test_inputs = tensor_video_inputs\n+            else:\n+                test_inputs = video_inputs\n+\n+            # Test not batched input\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n+            encoded_videos = image_processing(\n+                test_inputs[0], return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+            ).pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            # Test batched\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(\n+                video_inputs, batched=True\n+            )\n+            encoded_videos = image_processing(\n+                test_inputs, return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+            ).pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n         self.image_processor_tester.num_channels = 3\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, torchify=True)\n-        for video in video_inputs:\n-            self.assertIsInstance(video, list)\n-            self.assertIsInstance(video[0], torch.Tensor)\n-\n-        # Test not batched input\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n-        encoded_videos = image_processing(video_inputs[0], return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        # Test batched\n-        expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs, batched=True)\n-        encoded_videos = image_processing(video_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_videos.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_frames,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n+        # Test PyTorch tensors with both processors\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, torchify=True)\n+            for video in video_inputs:\n+                self.assertIsInstance(video, list)\n+                self.assertIsInstance(video[0], torch.Tensor)\n+\n+            # Test not batched input\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n+            encoded_videos = image_processing(video_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            # Test batched\n+            expected_height, expected_width = self.image_processor_tester.get_expected_values(\n+                video_inputs, batched=True\n+            )\n+            encoded_videos = image_processing(video_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_videos.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_frames,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_video_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        # Higher max atol for video processing, mean_atol still 5e-3 -> 1e-2\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_values, encoding_fast.pixel_values, atol=10.0, mean_atol=1e-2\n         )"
        }
    ],
    "stats": {
        "total": 690,
        "additions": 530,
        "deletions": 160
    }
}