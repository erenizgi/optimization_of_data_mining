{
    "author": "RyanMullins",
    "message": "Shieldgemma2 (#36678)\n\n* single commit\n\n* correct config\n\n* fixup\n\n* dummy pt\n\n* Use ShieldGemma2Config in conversion script\n\n* Update src/transformers/models/shieldgemma2/configuration_shieldgemma2.py\n\n* Adding shieldgemma2 to models.__init__.py\n\n* Adding ShieldGemma2 to main __init__.py\n\n* Update shieldgemma2.md\n\n* Update shieldgemma2.md\n\n* Adding tests. Addressing review feedback.\n\n* Minor docs update\n\n* Fixing code quality feedback from CI\n\n* Fixing empty messages bug reported by ghunkins\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Ren Pang <ain-soph@live.com>",
    "sha": "487dab1b2b2d0c4e3ca32ec713766cc25673006e",
    "files": [
        {
            "sha": "016fe9bf98740757973282d4e748261b8dce374a",
            "filename": "docs/source/en/model_doc/shieldgemma2.md",
            "status": "added",
            "additions": 100,
            "deletions": 0,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,100 @@\n+\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ShieldGemma 2\n+\n+## Overview\n+\n+The ShieldGemma 2 model was proposed in a forthcoming technical report by Google. ShieldGemma 2 is built on [Gemma 3](https://ai.google.dev/gemma/docs/core/model_card_3), is a 4 billion (4B) parameter model that checks the safety of both synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm as defined below:\n+\n+-   No Sexually Explicit content: The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).\n+-   No Dangerous Content: The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide).\n+-   No Violence/Gore content: The image shall not contain content that depicts shocking, sensational, or gratuitous violence (e.g., excessive blood and gore, gratuitous violence against animals, extreme injury or moment of death).\n+\n+We recommend using ShieldGemma 2 as an input filter to vision language models, or as an output filter of image generation systems. To train a robust image safety model, we curated training datasets of natural and synthetic images and instruction-tuned Gemma 3 to demonstrate strong performance.\n+\n+This model was contributed by [Ryan Mullins](https://huggingface.co/RyanMullins).\n+\n+## Usage Example\n+\n+- ShieldGemma 2 provides a Processor that accepts a list of `images` and an optional list of `policies` as input, and constructs a batch of prompts as the product of these two lists using the provided chat template.\n+- You can extend ShieldGemma's built-in in policies with the `custom_policies` argument to the Processor. Using the same key as one of the built-in policies will overwrite that policy with your custom defintion.\n+- ShieldGemma 2 does not support the image cropping capabilities used by Gemma 3.\n+\n+### Classification against Built-in Policies\n+\n+```python\n+from PIL import Image\n+import requests\n+from transformers import AutoProcessor, ShieldGemma2ForImageClassification\n+\n+model_id = \"google/shieldgemma-2-4b-it\"\n+model = ShieldGemma2ForImageClassification.from_pretrained(model_id, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+inputs = processor(images=[image], return_tensors=\"pt\").to(model.device)\n+\n+output = model(**inputs)\n+print(output.probabilities)\n+```\n+\n+### Classification against Custom Policies\n+\n+```python\n+from PIL import Image\n+import requests\n+from transformers import AutoProcessor, ShieldGemma2ForImageClassification\n+\n+model_id = \"google/shieldgemma-2-4b-it\"\n+model = ShieldGemma2ForImageClassification.from_pretrained(model_id, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+custom_policies = {\n+    \"key_a\": \"descrition_a\",\n+    \"key_b\": \"descrition_b\",\n+}\n+\n+inputs = processor(\n+    images=[image],\n+    custom_policies=custom_policies,\n+    policies=[\"dangerous\", \"key_a\", \"key_b\"],\n+    return_tensors=\"pt\",\n+).to(model.device)\n+\n+output = model(**inputs)\n+print(output.probabilities)\n+```\n+\n+\n+## ShieldGemma2Processor\n+\n+[[autodoc]] ShieldGemma2Processor\n+\n+## ShieldGemma2Config\n+\n+[[autodoc]] ShieldGemma2Config\n+\n+## ShieldGemma2ForImageClassification\n+\n+[[autodoc]] ShieldGemma2ForImageClassification\n+    - forward"
        },
        {
            "sha": "56960e9b5fceb460c4370d2af0395f234398f20a",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -774,6 +774,10 @@\n     \"models.seggpt\": [\"SegGptConfig\"],\n     \"models.sew\": [\"SEWConfig\"],\n     \"models.sew_d\": [\"SEWDConfig\"],\n+    \"models.shieldgemma2\": [\n+        \"ShieldGemma2Config\",\n+        \"ShieldGemma2Processor\",\n+    ],\n     \"models.siglip\": [\n         \"SiglipConfig\",\n         \"SiglipProcessor\",\n@@ -3581,6 +3585,7 @@\n             \"SEWDPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.shieldgemma2\"].append(\"ShieldGemma2ForImageClassification\")\n     _import_structure[\"models.siglip\"].extend(\n         [\n             \"SiglipForImageClassification\",\n@@ -5982,6 +5987,10 @@\n     from .models.seggpt import SegGptConfig\n     from .models.sew import SEWConfig\n     from .models.sew_d import SEWDConfig\n+    from .models.shieldgemma2 import (\n+        ShieldGemma2Config,\n+        ShieldGemma2Processor,\n+    )\n     from .models.siglip import (\n         SiglipConfig,\n         SiglipProcessor,\n@@ -8350,6 +8359,9 @@\n             SEWDModel,\n             SEWDPreTrainedModel,\n         )\n+        from .models.shieldgemma2 import (\n+            ShieldGemma2ForImageClassification,\n+        )\n         from .models.siglip import (\n             SiglipForImageClassification,\n             SiglipModel,"
        },
        {
            "sha": "22a0d281b3c3802a9e38979b96cf915a8076a2f8",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -247,6 +247,7 @@\n     seggpt,\n     sew,\n     sew_d,\n+    shieldgemma2,\n     siglip,\n     siglip2,\n     smolvlm,"
        },
        {
            "sha": "3b7445f604368f821a6b3b0e4033686085eed16f",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -274,6 +274,7 @@\n         (\"seggpt\", \"SegGptConfig\"),\n         (\"sew\", \"SEWConfig\"),\n         (\"sew-d\", \"SEWDConfig\"),\n+        (\"shieldgemma2\", \"ShieldGemma2Config\"),\n         (\"siglip\", \"SiglipConfig\"),\n         (\"siglip2\", \"Siglip2Config\"),\n         (\"siglip_vision_model\", \"SiglipVisionConfig\"),\n@@ -625,6 +626,7 @@\n         (\"seggpt\", \"SegGPT\"),\n         (\"sew\", \"SEW\"),\n         (\"sew-d\", \"SEW-D\"),\n+        (\"shieldgemma2\", \"Shieldgemma2\"),\n         (\"siglip\", \"SigLIP\"),\n         (\"siglip2\", \"SigLIP2\"),\n         (\"siglip2_vision_model\", \"Siglip2VisionModel\"),"
        },
        {
            "sha": "9a5edd483578331eeaecef8397f803039916dd77",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -137,6 +137,7 @@\n             (\"sam\", (\"SamImageProcessor\",)),\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),\n+            (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"superglue\", \"SuperGlueImageProcessor\"),"
        },
        {
            "sha": "649b56a2144105e21b0667b01eb4ab3a2b974117",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -727,6 +727,7 @@\n         (\"regnet\", \"RegNetForImageClassification\"),\n         (\"resnet\", \"ResNetForImageClassification\"),\n         (\"segformer\", \"SegformerForImageClassification\"),\n+        (\"shieldgemma2\", \"ShieldGemma2ForImageClassification\"),\n         (\"siglip\", \"SiglipForImageClassification\"),\n         (\"siglip2\", \"Siglip2ForImageClassification\"),\n         (\"swiftformer\", \"SwiftFormerForImageClassification\"),\n@@ -849,6 +850,7 @@\n         (\"pixtral\", \"LlavaForConditionalGeneration\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLForConditionalGeneration\"),\n         (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),\n+        (\"shieldgemma2\", \"Gemma3ForConditionalGeneration\"),\n         (\"smolvlm\", \"SmolVLMForConditionalGeneration\"),\n         (\"udop\", \"UdopForConditionalGeneration\"),\n         (\"vipllava\", \"VipLlavaForConditionalGeneration\"),"
        },
        {
            "sha": "5b699a4a44dce53381325a0ccc32040fc560f625",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -101,6 +101,7 @@\n         (\"seamless_m4t\", \"SeamlessM4TProcessor\"),\n         (\"sew\", \"Wav2Vec2Processor\"),\n         (\"sew-d\", \"Wav2Vec2Processor\"),\n+        (\"shieldgemma2\", \"ShieldGemma2Processor\"),\n         (\"siglip\", \"SiglipProcessor\"),\n         (\"siglip2\", \"Siglip2Processor\"),\n         (\"speech_to_text\", \"Speech2TextProcessor\"),"
        },
        {
            "sha": "13f9a8a4297b09a72565ae7097fa61fff9671b43",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -493,6 +493,13 @@\n                     \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n                 ),\n             ),\n+            (\n+                \"shieldgemma2\",\n+                (\n+                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+                ),\n+            ),\n             (\"siglip\", (\"SiglipTokenizer\" if is_sentencepiece_available() else None, None)),\n             (\n                 \"siglip2\","
        },
        {
            "sha": "3eaa894027352f1dd928e853bec4eef131acd8e4",
            "filename": "src/transformers/models/shieldgemma2/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2F__init__.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_shieldgemma2 import *\n+    from .modeling_shieldgemma2 import *\n+    from .processing_shieldgemma2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8094cb14b430f68f0163176debac96c5cb059ae2",
            "filename": "src/transformers/models/shieldgemma2/configuration_shieldgemma2.py",
            "status": "added",
            "additions": 115,
            "deletions": 0,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,115 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ShieldGemma2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`ShieldGemma2ForImageClassification`]. It is used to instantiate an\n+    ShieldGemma2ForImageClassification according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the shieldgemma-2-4b-it.\n+\n+    e.g. [google/gemma-3-4b](https://huggingface.co/google/gemma-3-4b)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[ShieldGemma2TextConfig, dict]`, *optional*):\n+            The config object of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*):\n+            Custom vision config or dict.\n+        mm_tokens_per_image (`int`, *optional*, defaults to 256):\n+            The number of tokens per image embedding.\n+        boi_token_index (`int`, *optional*, defaults to 255999):\n+            The begin-of-image token index to wrap the image prompt.\n+        eoi_token_index (`int`, *optional*, defaults to 256000):\n+            The end-of-image token index to wrap the image prompt.\n+        image_token_index (`int`, *optional*, defaults to 262144):\n+            The image token index to encode the image prompt.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import ShieldGemma2ForConditionalGeneration, ShieldGemma2Config, SiglipVisionConfig, ShieldGemma2TextConfig\n+\n+    >>> # Initializing a Siglip-like vision config\n+    >>> vision_config = SiglipVisionConfig()\n+\n+    >>> # Initializing a ShieldGemma2 Text config\n+    >>> text_config = ShieldGemma2TextConfig()\n+\n+    >>> # Initializing a ShieldGemma2 gemma-3-4b style configuration\n+    >>> configuration = ShieldGemma2Config(vision_config, text_config)\n+\n+    >>> # Initializing a model from the gemma-3-4b style configuration\n+    >>> model = ShieldGemma2TextConfig(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"shieldgemma2\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        mm_tokens_per_image: int = 256,\n+        boi_token_index: int = 255_999,\n+        eoi_token_index: int = 256_000,\n+        image_token_index: int = 262_144,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = (\n+                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n+            )\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"siglip_vision_model\"]()\n+\n+        self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"gemma3_text\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"gemma3_text\"]()\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.mm_tokens_per_image = mm_tokens_per_image\n+        self.boi_token_index = boi_token_index\n+        self.eoi_token_index = eoi_token_index\n+        self.image_token_index = image_token_index\n+        self.initializer_range = initializer_range\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"ShieldGemma2Config\"]"
        },
        {
            "sha": "0ca46eb29e2de38ee06e339fe91c0ef22caf2f1d",
            "filename": "src/transformers/models/shieldgemma2/convert_shieldgemma2_weights_orbax_to_hf.py",
            "status": "added",
            "additions": 470,
            "deletions": 0,
            "changes": 470,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,470 @@\n+r\"\"\"Utility to convert Gemma models from Orbax to HF Transformers checkpoint.\n+\n+python -m transformers.models.shieldgemma2.convert_shieldgemma2_weights_orbax_to_hf \\\n+    --tokenizer_path=\"$HOME/gemma3/tokenizer/gemma3_cleaned_262144_v2.spiece.model\" \\\n+    --checkpoint_path_gemma=\"$HOME/gemma3/gemma3_4b_pt_orbax/\" \\\n+    --checkpoint_path_shieldgemma=\"$HOME/shieldgemma2/shieldgemma-2_4b_orbax/\" \\\n+    --output_path=\"$HOME/shieldgemma2/shieldgemma2_4b_pt_safetensors/\" \\\n+    --precision='bfloat16'\n+\"\"\"\n+\n+import dataclasses\n+from collections.abc import Iterator, Mapping, Sequence\n+from typing import Any\n+\n+import accelerate\n+import numpy as np\n+import torch\n+import tree\n+from absl import app, flags, logging\n+from orbax import checkpoint as obc\n+\n+from ...image_utils import PILImageResampling\n+from ..gemma import GemmaTokenizerFast\n+from ..gemma3 import Gemma3ImageProcessor, Gemma3TextConfig\n+from ..siglip import SiglipVisionConfig\n+from . import ShieldGemma2Config, ShieldGemma2ForImageClassification, ShieldGemma2Processor\n+\n+\n+# ==== Internal Constants and Classes ====\n+\n+_CHAT_TEMPLATE = \"\"\"{{ bos_token }}\n+{%- for message in messages -%}\n+    {%- if message['content'] is iterable -%}\n+        {%- for item in message['content'] -%}\n+            {{ '<start_of_turn>user\\n' }}\n+            {%- if item['type'] == 'image' -%}\n+                {{ '<start_of_image>' }}\n+            {%- elif item['type'] == 'text' -%}\n+                {{ 'You are a policy expert trying to help determine whether a generated or real image is in' }}\n+                {{ ' violation of the safety policy below. Your decision must very clearly follow the specific'}}\n+                {{ ' language used in the safety policy and should not include any interpretation outside of the' }}\n+                {{ ' safety policy.\\n\\n' }}\n+                {{ '<BEGIN_SAFETY_POLICY>\\n' }}\n+                {{ item['text'] + '\\n' }}\n+                {{ '</END_SAFETY_POLICY>\\n\\n' }}\n+                {{ 'Does the image violate the above policy? Your Answer must start with \\\\'Yes\\\\' or \\\\'No\\\\'.' }}\n+                {{ '<end_of_turn>\\n' }}\n+            {%- endif -%}\n+        {%- endfor -%}\n+        {{'<start_of_turn>model\\n'}}\n+    {%- else -%}\n+        {{ raise_exception(\"Conversation messages must contain iterable content containing images and policy definitions in text.\") }}\n+    {%- endif -%}\n+{%- endfor -%}\n+\"\"\"\n+\n+_DTYPES = {\n+    \"float32\": torch.float32,\n+    \"bfloat16\": torch.bfloat16,\n+    \"float16\": torch.float16,\n+}\n+\n+_SHIELDGEMMA2_POLICIES: Mapping[str, str] = {\n+    \"dangerous\": (\n+        \"No Dangerous Content: The image shall not contain content that\"\n+        \" facilitates or encourages activities that could cause real-world harm\"\n+        \" (e.g., building firearms and explosive devices, promotion of\"\n+        \" terrorism, instructions for suicide).\"\n+    ),\n+    \"sexual\": (\n+        \"No Sexually Explicit content: The image shall not contain content that\"\n+        \" depicts explicit or graphic sexual acts (e.g., pornography, erotic\"\n+        \" nudity, depictions of rape or sexual assault).\"\n+    ),\n+    \"violence\": (\n+        \"No Violence/Gore content: The image shall not contain content that\"\n+        \" depicts shocking, sensational, or gratuitous violence (e.g.,\"\n+        \" excessive blood and gore, gratuitous violence against animals,\"\n+        \" extreme injury or moment of death).\"\n+    ),\n+}\n+\n+_SIGLIP_BASE = \"SigLiPFromPatches_0/siglip_encoder\"\n+_SIGLIP_EMBEDDING = \"SigLiPFromPatches_0/siglip_encoder/embedding\"\n+_SIGLIP_TRANSFORMER_ENCODER_BLOCK = \"SigLiPFromPatches_0/siglip_encoder/Transformer/encoderblock_\"\n+_SIGLIP_TRANSFORMER_ENCODER_BLOCK_LEN = len(_SIGLIP_TRANSFORMER_ENCODER_BLOCK)\n+_SIGLIP_TRANSFORMER_ENCODER_NORM = \"SigLiPFromPatches_0/siglip_encoder/Transformer/encoder_norm\"\n+\n+_TRANSFORMER_DECODER_BLOCK = \"transformer/layer_\"\n+_TRANSFORMER_DECODER_BLOCK_LEN = len(_TRANSFORMER_DECODER_BLOCK)\n+_TRANSFORMER_EMBEDDER = \"transformer/embedder\"\n+_TRANSFORMER_FINAL_NORM = \"transformer/final_norm\"\n+_TRANSFORMER_POST_TRAINING_PREFIX = \"rlx_networks/policy_network/\"\n+_TRANSFORMER_POST_TRAINING_PREFIX_LEN = len(_TRANSFORMER_POST_TRAINING_PREFIX)\n+\n+# ==== Flags ====\n+\n+_GEMMA_CHECKPOINT_PATH = flags.DEFINE_string(\n+    name=\"checkpoint_path_gemma\",\n+    default=None,\n+    help=\"Path to the Orbax checkpoint containing the vision weights.\",\n+    required=True,\n+)\n+\n+_SHIELDGEMMA_CHECKPOINT_PATH = flags.DEFINE_string(\n+    name=\"checkpoint_path_shieldgemma\",\n+    default=None,\n+    help=\"Path to the Orbax checkpoint containing the language model weights.\",\n+    required=True,\n+)\n+\n+OUTPUT_PATH = flags.DEFINE_string(\n+    name=\"output_path\",\n+    default=None,\n+    help=\"Path to store the HF checkpoint.\",\n+    required=True,\n+)\n+\n+PRECISION = flags.DEFINE_enum(\n+    name=\"precision\",\n+    default=None,\n+    help=\"The floating point precision (aka dtype) of the model.\",\n+    enum_values=set(_DTYPES.keys()),\n+    required=True,\n+)\n+\n+TOKENIZER_PATH = flags.DEFINE_string(\n+    name=\"tokenizer_path\",\n+    default=None,\n+    help=\"Path to the SentencePiece model file.\",\n+    required=True,\n+)\n+\n+\n+def convert_siglip_weight(\n+    config: SiglipVisionConfig,\n+    paths: Sequence[str],\n+    weights: np.ndarray,\n+) -> tuple[str, np.ndarray]:\n+    path, prop = paths\n+    normalized_path: str = \"\"\n+    updated_weights: np.ndarray = None\n+\n+    if path == _SIGLIP_BASE:\n+        normalized_path = \"vision_tower.vision_model.embeddings.position_embedding.weight\"\n+        updated_weights = weights.reshape(-1, config.hidden_size)\n+    elif path == _SIGLIP_EMBEDDING:\n+        if prop == \"kernel\":\n+            normalized_path = \"vision_tower.vision_model.embeddings.patch_embedding.weight\"\n+            updated_weights = weights.transpose(3, 2, 0, 1)\n+        elif prop == \"bias\":\n+            normalized_path = \"vision_tower.vision_model.embeddings.patch_embedding.bias\"\n+            updated_weights = weights\n+        else:\n+            raise ValueError(f\"Unexpected member, `{prop}`, for path `{path}`. Should be `bias` or `kernel`.\")\n+    elif path.startswith(_SIGLIP_TRANSFORMER_ENCODER_BLOCK):\n+        encoder_block_path = path[_SIGLIP_TRANSFORMER_ENCODER_BLOCK_LEN:]\n+        next_path_seperator_idx = encoder_block_path.find(\"/\")\n+        layer_idx = encoder_block_path[:next_path_seperator_idx]\n+        encoder_block_path = encoder_block_path[next_path_seperator_idx:]\n+        normalized_path = f\"vision_tower.vision_model.encoder.layers.{layer_idx}\"\n+\n+        if encoder_block_path.startswith(\"/LayerNorm\"):\n+            normalized_path += \".layer_norm1\" if path.endswith(\"_0\") else \".layer_norm2\"\n+\n+            if prop == \"scale\":\n+                normalized_path += \".weight\"\n+                updated_weights = weights.transpose()\n+            elif prop == \"bias\":\n+                normalized_path += \".bias\"\n+                updated_weights = weights\n+            else:\n+                raise ValueError(f\"Unexpected member, `{prop}`, for path `{path}`. Should be `bias` or `scale`.\")\n+        elif encoder_block_path.startswith(\"/MlpBlock_0\"):\n+            normalized_path += \".mlp.fc1\" if \"/Dense_0\" in encoder_block_path else \".mlp.fc2\"\n+\n+            if prop == \"kernel\":\n+                normalized_path += \".weight\"\n+                updated_weights = weights.transpose()\n+            elif prop == \"bias\":\n+                normalized_path += \".bias\"\n+                updated_weights = weights\n+            else:\n+                raise ValueError(f\"Unexpected member, `{prop}`, for path `{path}`. Should be `bias` or `kernel`.\")\n+        elif encoder_block_path.startswith(\"/MultiHeadDotProductAttention_0\"):\n+            if encoder_block_path.endswith(\"/key\"):\n+                normalized_path += \".self_attn.k_proj\"\n+            elif encoder_block_path.endswith(\"/out\"):\n+                normalized_path += \".self_attn.out_proj\"\n+            elif encoder_block_path.endswith(\"/query\"):\n+                normalized_path += \".self_attn.q_proj\"\n+            elif encoder_block_path.endswith(\"/value\"):\n+                normalized_path += \".self_attn.v_proj\"\n+            else:\n+                raise ValueError(f\"Unexpected path `{path}` in SigLIP Transformer MultiHeadDotProductAttention_0.\")\n+\n+            if prop == \"bias\":\n+                normalized_path += \".bias\"\n+                updated_weights = weights.reshape(-1, config.hidden_size).reshape(-1)\n+            elif prop == \"kernel\":\n+                normalized_path += \".weight\"\n+                updated_weights = weights.reshape(-1, config.hidden_size).transpose()\n+            else:\n+                raise ValueError(f\"Unexpected member, `{prop}`, for path `{path}`. Should be `bias` or `kernel`.\")\n+        else:\n+            raise ValueError(f\"Unexpected path `{path}` in SigLIP Transformer Encoder Block.\")\n+    elif path == _SIGLIP_TRANSFORMER_ENCODER_NORM:\n+        if prop == \"scale\":\n+            normalized_path = \"vision_tower.vision_model.post_layernorm.weight\"\n+            updated_weights = weights.transpose()\n+        elif prop == \"bias\":\n+            normalized_path = \"vision_tower.vision_model.post_layernorm.bias\"\n+            updated_weights = weights\n+        else:\n+            raise ValueError(f\"Unexpected member, `{prop}`, for path `{path}`. Should be `bias` or `scale`.\")\n+    else:\n+        raise ValueError(f\"Unexpected path `{path}`.\")\n+\n+    if \"vision\" in normalized_path:\n+        print(normalized_path)\n+    return normalized_path, updated_weights\n+\n+\n+def convert_transformer_weights(\n+    config: Gemma3TextConfig,\n+    paths: Sequence[str],\n+    weights: np.ndarray,\n+) -> Iterator[tuple[str, np.ndarray]]:\n+    path, prop = paths\n+\n+    if path.startswith(_TRANSFORMER_POST_TRAINING_PREFIX):\n+        path = path[_TRANSFORMER_POST_TRAINING_PREFIX_LEN:]\n+\n+    converted_paths: list[str] = []\n+    converted_weights: list[Any] = []\n+\n+    attn_head_dim = config.num_attention_heads * config.head_dim\n+    kv_head_dim = config.num_key_value_heads * config.head_dim\n+\n+    if path == _TRANSFORMER_EMBEDDER:\n+        if prop == \"input_embedding\":\n+            # Tied to language_model.lm_head.weight, assigned at the end.\n+            converted_paths = [\"language_model.model.embed_tokens.weight\"]\n+            # Gemma3 model doesn't have image soft token in input and output embeddings, resize to avoid bugs we had with Mllama\n+            pre_expansion_embeddings = weights\n+            mu = np.mean(pre_expansion_embeddings, axis=0)\n+            sigma = np.cov(pre_expansion_embeddings, rowvar=False, bias=True)\n+            new_embeddings = np.random.multivariate_normal(mu, 1e-5 * sigma, size=64)\n+            weights = np.vstack([pre_expansion_embeddings, new_embeddings])\n+            converted_weights = [weights]\n+        else:\n+            raise ValueError(f\"Unexpected member, {prop}, in Embedder.\")\n+    elif path.startswith(f\"{_TRANSFORMER_EMBEDDER}/mm\"):\n+        if path.endswith(\"/mm_input_projection\"):\n+            converted_paths = [\"multi_modal_projector.mm_input_projection_weight\"]\n+            converted_weights = [weights]\n+        elif path.endswith(\"/mm_soft_embedding_norm\"):\n+            converted_paths = [\"multi_modal_projector.mm_soft_emb_norm.weight\"]\n+            converted_weights = [weights]\n+        else:\n+            raise ValueError(f\"Unexpected subpath, `{path}`, in Embedder.\")\n+    elif path == _TRANSFORMER_FINAL_NORM:\n+        converted_paths = [\"language_model.model.norm.weight\"]\n+        converted_weights = [weights]\n+    elif path.startswith(_TRANSFORMER_DECODER_BLOCK):\n+        decoder_block_path = path[_TRANSFORMER_DECODER_BLOCK_LEN:]\n+        next_path_seperator_idx = decoder_block_path.find(\"/\")\n+        layer_idx = decoder_block_path[:next_path_seperator_idx]\n+        decoder_block_path = decoder_block_path[next_path_seperator_idx:]\n+\n+        base_path = f\"language_model.model.layers.{layer_idx}\"\n+\n+        if path.endswith(\"attn/attn_vec_einsum\"):\n+            converted_paths = [f\"{base_path}.self_attn.o_proj.weight\"]\n+            converted_weights = [weights.transpose(2, 0, 1).reshape(config.hidden_size, attn_head_dim)]\n+        elif path.endswith(\"attn/_key_norm\"):\n+            converted_paths = [f\"{base_path}.self_attn.k_norm.weight\"]\n+            converted_weights = [weights]\n+        elif path.endswith(\"attn/kv_einsum\"):\n+            converted_paths = [\n+                f\"{base_path}.self_attn.k_proj.weight\",\n+                f\"{base_path}.self_attn.v_proj.weight\",\n+            ]\n+            k_proj_weights, v_proj_weights = weights\n+            converted_weights = [\n+                k_proj_weights.transpose(0, 2, 1).reshape(kv_head_dim, config.hidden_size),\n+                v_proj_weights.transpose(0, 2, 1).reshape(kv_head_dim, config.hidden_size),\n+            ]\n+        elif path.endswith(\"attn/q_einsum\"):\n+            converted_paths = [f\"{base_path}.self_attn.q_proj.weight\"]\n+            converted_weights = [weights.transpose(0, 2, 1).reshape(attn_head_dim, config.hidden_size)]\n+        elif path.endswith(\"attn/_query_norm\"):\n+            converted_paths = [f\"{base_path}.self_attn.q_norm.weight\"]\n+            converted_weights = [weights]\n+        elif path.endswith(\"mlp/gating_einsum\"):\n+            converted_paths = [\n+                f\"{base_path}.mlp.gate_proj.weight\",\n+                f\"{base_path}.mlp.up_proj.weight\",\n+            ]\n+            gate_proj_weight, up_proj_weight = weights\n+            converted_weights = [gate_proj_weight, up_proj_weight]\n+        elif path.endswith(\"mlp/linear\"):\n+            converted_paths = [f\"{base_path}.mlp.down_proj.weight\"]\n+            converted_weights = [weights.transpose()]\n+        elif path.endswith(\"post_attention_norm\"):\n+            converted_paths = [f\"{base_path}.post_attention_layernorm.weight\"]\n+            converted_weights = [weights]\n+        elif path.endswith(\"post_ffw_norm\"):\n+            converted_paths = [f\"{base_path}.post_feedforward_layernorm.weight\"]\n+            converted_weights = [weights]\n+        elif path.endswith(\"pre_attention_norm\"):\n+            converted_paths = [f\"{base_path}.input_layernorm.weight\"]\n+            converted_weights = [weights]\n+        elif path.endswith(\"pre_ffw_norm\"):\n+            converted_paths = [f\"{base_path}.pre_feedforward_layernorm.weight\"]\n+            converted_weights = [weights]\n+        else:\n+            raise ValueError(f\"Unexpected path `{path}` in Decoder Block.\")\n+    else:\n+        raise ValueError(f\"Unexpected path `{path}`.\")\n+\n+    if (cpl := len(converted_paths)) != (cwl := len(converted_weights)):\n+        raise ValueError(\n+            \"The `converted_paths` and `converted_weights` should be the same \"\n+            f\"length. Got {cpl} and {cwl}, respectively, for {path}.\"\n+        )\n+\n+    return zip(converted_paths, converted_weights)\n+\n+\n+def transpose_reshape(x: torch.Tensor) -> torch.Tensor:\n+    x = x.transpose(1, 2)\n+    return x.reshape(x.shape[0] * x.shape[1], x.shape[2]).contiguous()\n+\n+\n+@dataclasses.dataclass(frozen=True)\n+class ConversionResult:\n+    state_tree: dict[str, torch.Tensor]\n+    config: ShieldGemma2Config\n+\n+\n+def convert(\n+    shieldgemma_checkpoint_path: str,\n+    gemma_checkpoint_path: str,\n+    config: ShieldGemma2Config,\n+    target_dtype: torch.dtype,\n+) -> ConversionResult:\n+    \"\"\"Loads Orbax checkpoint from `input_path` and converts it to HF tree.\"\"\"\n+    checkpointer = obc.PyTreeCheckpointer()\n+\n+    sg2_ckpt = checkpointer.restore(shieldgemma_checkpoint_path)\n+    g3_ckpt = checkpointer.restore(gemma_checkpoint_path)\n+\n+    hf_tree: dict[str, torch.Tensor] = {}\n+\n+    def update_tree(path: str, weights: np.ndarray) -> None:\n+        torch_tensor = torch.from_numpy(weights.astype(\"float32\")).type(target_dtype)\n+        logging.info(\n+            \"%s converted shape=%s with dtype=%s\",\n+            path,\n+            weights.shape,\n+            torch_tensor.dtype,\n+        )\n+        hf_tree[f\"model.{path}\"] = torch_tensor\n+\n+    for paths, value in tree.flatten_with_path(g3_ckpt):\n+        if paths[0].startswith(\"SigLiPFromPatches_\"):\n+            path, weights = convert_siglip_weight(config=config.vision_config, paths=paths, weights=value)\n+            update_tree(path, weights)\n+\n+    for paths, value in tree.flatten_with_path(sg2_ckpt):\n+        for path, weights in convert_transformer_weights(config=config.text_config, paths=paths, weights=value):\n+            update_tree(path, weights)\n+\n+    hf_tree[\"model.language_model.lm_head.weight\"] = hf_tree[\"model.language_model.model.embed_tokens.weight\"]\n+\n+    return ConversionResult(state_tree=hf_tree, config=config)\n+\n+\n+def main(*args):\n+    del args\n+\n+    dtype = getattr(torch, PRECISION.value)\n+    output_path = OUTPUT_PATH.value\n+\n+    tokenizer = GemmaTokenizerFast(\n+        TOKENIZER_PATH.value,\n+        extra_special_tokens={\n+            \"image_token\": \"<image_soft_token>\",  # Should be ID=262_144\n+            \"boi_token\": \"<start_of_image>\",  # Should be ID=255_999\n+            \"eoi_token\": \"<end_of_image>\",  # Should be ID=256_000\n+        },\n+    )\n+\n+    yes_token_index, no_token_index = torch.tensor(tokenizer([\"Yes\", \"No\"])[\"input_ids\"])[:, 1].numpy()\n+\n+    config = ShieldGemma2Config(\n+        yes_token_index=int(yes_token_index),\n+        no_token_index=int(no_token_index),\n+        text_config=Gemma3TextConfig(\n+            vocab_size=262_208,\n+            hidden_size=2560,\n+            intermediate_size=2560 * 8 // 2,\n+            num_attention_heads=8,\n+            head_dim=256,\n+            num_hidden_layers=34,\n+            num_key_value_heads=4,\n+            sliding_window=1024,\n+            rope_scaling={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n+            rope_theta=1_000_000,\n+            rope_local_base_freq=10_000,\n+            attn_logit_softcapping=None,\n+            query_pre_attn_scalar=256,\n+            max_position_embeddings=8192,\n+        ),\n+        vision_config={\n+            \"hidden_size\": 1152,\n+            \"intermediate_size\": 4304,\n+            \"num_hidden_layers\": 27,\n+            \"num_attention_heads\": 16,\n+            \"num_channels\": 3,\n+            \"image_size\": 896,\n+            \"patch_size\": 14,\n+            \"hidden_act\": \"gelu_pytorch_tanh\",\n+            \"layer_norm_eps\": 1e-6,\n+            \"attention_dropout\": 0.0,\n+            \"vision_use_head\": False,\n+        },\n+    )\n+\n+    config.save_pretrained(output_path)\n+\n+    image_processor = Gemma3ImageProcessor(\n+        image_seq_length=256,\n+        image_mean=(0.5,) * 3,\n+        image_std=(0.5,) * 3,\n+        size={\"height\": 896, \"width\": 896},\n+        resample=PILImageResampling.BILINEAR,\n+    )\n+    processor = ShieldGemma2Processor(\n+        image_processor=image_processor,\n+        tokenizer=tokenizer,\n+        policy_definitions=_SHIELDGEMMA2_POLICIES,\n+    )\n+    tokenizer.chat_template = _CHAT_TEMPLATE\n+    processor.chat_template = _CHAT_TEMPLATE\n+\n+    processor.save_pretrained(output_path)\n+    logging.info(\"Saved Shieldgemma2Processor to %s\", output_path)\n+    del processor\n+    del tokenizer\n+\n+    logging.info(\"Converting Shieldgemma2 @ %s\", dtype)\n+    result = convert(_SHIELDGEMMA_CHECKPOINT_PATH.value, _GEMMA_CHECKPOINT_PATH.value, config, dtype)\n+    logging.info(\"Converted Shieldgemma2 state tree from Orbax to Hugging Face.\")\n+\n+    with accelerate.init_empty_weights():\n+        model = ShieldGemma2ForImageClassification(config=config)\n+\n+    model.load_state_dict(result.state_tree, assign=True, strict=True)\n+    model.config.torch_dtype = dtype\n+    logging.info(\"Loaded Shieldgemma2 in Hugging Face Transformers.\")\n+    model.save_pretrained(output_path, safe_serialization=True)\n+    logging.info(\"Saved Shieldgemma2 to SafeTensors in %s\", output_path)\n+    del model\n+    del result\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)"
        },
        {
            "sha": "47ebc3e1f6f7d4885bb9c349d651b7b838eb4478",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "added",
            "additions": 228,
            "deletions": 0,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,228 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import List, Optional, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+\n+from ...cache_utils import Cache\n+from ...modeling_outputs import ImageClassifierOutputWithNoAttention\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.deprecation import deprecate_kwarg\n+from ..auto import AutoModelForImageTextToText\n+from .configuration_shieldgemma2 import ShieldGemma2Config\n+\n+\n+_CHECKPOINT_FOR_DOC = \"google/shieldgemma-2-4b-it\"\n+_CONFIG_FOR_DOC = \"ShieldGemma2Config\"\n+\n+logger = logging.get_logger(__name__)\n+\n+SHIELDGEMMA2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\n+    Returns:\n+        A `ShieldGemma2ImageClassifierOutputWithNoAttention` instance continaing the logits and probabilities\n+        associated with the model predicting the `Yes` or `No` token as the response to that prompt, captured in the\n+        following properties.\n+\n+            *   `logits` (`torch.Tensor` of shape `(batch_size, 2)`):\n+                The first position along dim=1 is the logits for the `Yes` token and the second position along dim=1 is\n+                the logits for the `No` token.\n+            *   `probabilities` (`torch.Tensor` of shape `(batch_size, 2)`):\n+                The first position along dim=1 is the probability of predicting the `Yes` token and the second position\n+                along dim=1 is the probability of predicting the `No` token.\n+\n+        ShieldGemma prompts are constructed such that predicting the `Yes` token means the content *does violate* the\n+        policy as described. If you are only interested in the violative condition, use\n+        `violated = outputs.probabilities[:, 1]` to extract that slice from the output tensors.\n+\n+        When used with the `ShieldGemma2Processor`, the `batch_size` will be equal to `len(images) * len(policies)`,\n+        and the order within the batch will be img1_policy1, ... img1_policyN, ... imgM_policyN.\n+\"\"\"\n+\n+\n+@dataclass\n+class ShieldGemma2ImageClassifierOutputWithNoAttention(ImageClassifierOutputWithNoAttention):\n+    \"\"\"ShieldGemma2 classifies imags as violative or not relative to a specific policy\n+    Args:\n+    \"\"\"\n+\n+    probabilities: torch.Tensor = None\n+\n+\n+class ShieldGemma2ForImageClassification(PreTrainedModel):\n+    config_class = ShieldGemma2Config\n+\n+    def __init__(self, config: ShieldGemma2Config):\n+        super().__init__(config=config)\n+        self.yes_token_index = getattr(config, \"yes_token_index\", 10_784)\n+        self.no_token_index = getattr(config, \"no_token_index\", 3771)\n+        self.model = AutoModelForImageTextToText.from_config(config=config)\n+\n+    def get_input_embeddings(self):\n+        return self.model.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.model.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.model.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.model.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.language_model.get_decoder()\n+\n+    def tie_weights(self):\n+        return self.model.language_model.tie_weights()\n+\n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n+    @add_start_docstrings_to_model_forward(SHIELDGEMMA2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(\n+        output_type=ShieldGemma2ImageClassifierOutputWithNoAttention, config_class=_CONFIG_FOR_DOC\n+    )\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> ShieldGemma2ImageClassifierOutputWithNoAttention:\n+        \"\"\"Predicts the binary probability that the image violates the speicfied policy.\n+\n+        Returns:\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            token_type_ids=token_type_ids,\n+            cache_position=cache_position,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+        logits = outputs.logits\n+        selected_logits = logits[:, -1, [self.yes_token_index, self.no_token_index]]\n+        probabilities = torch.softmax(selected_logits, dim=-1)\n+        return ShieldGemma2ImageClassifierOutputWithNoAttention(\n+            logits=selected_logits,\n+            probabilities=probabilities,\n+        )\n+\n+\n+__all__ = [\n+    \"ShieldGemma2ForImageClassification\",\n+]"
        },
        {
            "sha": "097b83ef6ee6d5971535f7b2aad71efbc336c0a7",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "added",
            "additions": 195,
            "deletions": 0,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,195 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from collections.abc import Mapping, Sequence\n+from typing import Optional\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..gemma3.processing_gemma3 import Gemma3Processor, Gemma3ProcessorKwargs\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+DEFAULT_SHIELDGEMMA2_POLICIES: Mapping[str, str] = {\n+    \"dangerous\": (\n+        \"No Dangerous Content: The image shall not contain content that\"\n+        \" facilitates or encourages activities that could cause real-world harm\"\n+        \" (e.g., building firearms and explosive devices, promotion of\"\n+        \" terrorism, instructions for suicide).\"\n+    ),\n+    \"sexual\": (\n+        \"No Sexually Explicit content: The image shall not contain content that\"\n+        \" depicts explicit or graphic sexual acts (e.g., pornography, erotic\"\n+        \" nudity, depictions of rape or sexual assault).\"\n+    ),\n+    \"violence\": (\n+        \"No Violence/Gore content: The image shall not contain content that\"\n+        \" depicts shocking, sensational, or gratuitous violence (e.g.,\"\n+        \" excessive blood and gore, gratuitous violence against animals,\"\n+        \" extreme injury or moment of death).\"\n+    ),\n+}\n+\n+\n+class ShieldGemma2ProcessorKwargs(Gemma3ProcessorKwargs, total=False):\n+    policies: Optional[Sequence[str]]\n+    custom_policies: Optional[Mapping[str, str]]\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+        },\n+        \"images_kwargs\": {\n+            \"do_pan_and_scan\": False,\n+        },\n+    }\n+\n+\n+class ShieldGemma2Processor(Gemma3Processor):\n+    def __init__(\n+        self, image_processor, tokenizer, chat_template=None, image_seq_length=256, policy_definitions=None, **kwargs\n+    ):\n+        \"\"\"A processor for the ShieldGemma 2 model.\n+\n+        Args:\n+            image_processor: The image processor to use, typically a `Gemma3ImageProcessorFast` instance.\n+            tokenizer: The tokenizer to use, typically a `GemmaTokenizerFast` instance.\n+            chat_template: The chat template to use with this processor. Typically, this is unset as the processor\n+                configuration on Hugging Face Hub includes this value already.\n+            image_seq_length: The number of soft tokens per image. Typically, this is unset as the processor\n+                configuration on Hugging Face Hub includes this value already.\n+            policy_definitions: A mapping from policy name to its description in text used as the default policies to\n+                classify images against. The policy descriptions are included in the text of the prompts generated by\n+                this processor. Typically, this is unset as the processor configuration on Hugging Face Hub includes\n+                the base policies ShieldGemma was trained on.\n+        \"\"\"\n+        super().__init__(image_processor, tokenizer, chat_template, image_seq_length, **kwargs)\n+        if policy_definitions is None:\n+            self.policy_definitions = DEFAULT_SHIELDGEMMA2_POLICIES\n+        else:\n+            self.policy_definitions = policy_definitions\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text=None,\n+        videos=None,\n+        audio=None,\n+        **kwargs: Unpack[ShieldGemma2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"Generates a batch of inputs from the provided images.\n+\n+        ShieldGemma was trained to classify image content for policy compliance using a specific prompt construction.\n+        This processor generates a batch of such prompts from the provided images by:\n+\n+        1.  Creating a list of conversations, one for each `<image, policy>` pair;\n+        2.  Converting these conversations to text using `self.apply_chat_template()`; and\n+        3.  Encoding the conversations and images using the same techniques as `Gemma3Processor`.\n+\n+        Args:\n+            images: A single image or a list of images to include in the batch.\n+            text: Not supported.\n+            videos: Not supported.\n+            audio: Not supported.\n+            kwargs: An optional dictionary of keyword arguments to configre the\n+                processor. Possible values include:\n+\n+                *   `custom_policies`: Additional policy definitions that augment the `self.policy_definitions` passed\n+                    into the constructor. Note that `custom_policies` that share a key with `self.policy_definitions`\n+                    will override the policy description\n+                *   `policies`: (Optional) a list of keys in the joint `self.policy_definitions | custom_policies`\n+                    dictionary of specific interest for the provided images. If empty or None, prompts will be\n+                    generated for every key in the joint dictionary.\n+\n+        Returns:\n+            A `BatchFeature` continaing `input_ids`, `pixel_values`, etc. where each Tensor is of shape\n+            `(len(images) * len(policies), )`, and the order within the batch will be\n+            img1_policy1, ... img1_policyN, ... imgM_policyN.\n+        \"\"\"\n+        del text, videos, audio\n+\n+        if not images:\n+            raise ValueError(\"ShieldGemma 2 needs images to classify\")\n+        elif not isinstance(images, Sequence):\n+            images = [images]\n+\n+        if not self.chat_template:\n+            raise ValueError(\"ShieldGemma 2 requires the use of a specific chat template\")\n+\n+        # Disable pan and scan\n+        images_kwargs = kwargs.setdefault(\"images_kwargs\", {})\n+        if images_kwargs.get(\"do_pan_and_scan\") is True:\n+            logger.warning_once(\"ShieldGemma2 does not support pan and scan.\")\n+            images_kwargs[\"do_pan_and_scan\"] = False\n+\n+        # Enable padding on the batch during tokenization\n+        text_kwargs = kwargs.setdefault(\"text_kwargs\", {})\n+        if \"padding\" not in text_kwargs:\n+            text_kwargs[\"padding\"] = kwargs.pop(\"padding\", True)\n+            text_kwargs[\"padding_side\"] = kwargs.pop(\"padding_side\", \"left\")\n+\n+        policy_definitions: Mapping[str, str] = {\n+            **self.policy_definitions,\n+            **kwargs.get(\"custom_policies\", {}),\n+        }\n+\n+        if (policies := kwargs.get(\"policies\")) is None:\n+            policies = list(policy_definitions.keys())\n+\n+        # TODO(ryanmullins): Support images from PIL or URLs.\n+        messages = []\n+        expanded_images = []\n+        for img in images:\n+            for policy in policies:\n+                messages.append(\n+                    [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"image\"},\n+                                {\"type\": \"text\", \"text\": policy_definitions[policy]},\n+                            ],\n+                        }\n+                    ]\n+                )\n+                expanded_images.append([img])\n+\n+        text = self.apply_chat_template(messages, tokenize=False)\n+        return super().__call__(images=expanded_images, text=text, **kwargs)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\"]\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"ShieldGemma2Processor\"]"
        },
        {
            "sha": "daef53543262381ef2c76f21f34c882cf53e74a4",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -8870,6 +8870,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class ShieldGemma2ForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class SiglipForImageClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/shieldgemma2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/tests%2Fmodels%2Fshieldgemma2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/tests%2Fmodels%2Fshieldgemma2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2F__init__.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e"
        },
        {
            "sha": "fdc5d9e7138aabd2da6d1c9191e2f1dd55184172",
            "filename": "tests/models/shieldgemma2/test_modeling_shieldgemma2.py",
            "status": "added",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,61 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Gemma3 model.\"\"\"\n+\n+import unittest\n+from io import BytesIO\n+\n+import requests\n+from PIL import Image\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import ShieldGemma2ForImageClassification, ShieldGemma2Processor\n+\n+\n+@slow\n+@require_torch_gpu\n+# @require_read_token\n+class ShieldGemma2IntegrationTest(unittest.TestCase):\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_model(self):\n+        model_id = \"google/shieldgemma-2-4b-it\"\n+\n+        processor = ShieldGemma2Processor.from_pretrained(model_id, padding_side=\"left\")\n+        url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n+        response = requests.get(url)\n+        image = Image.open(BytesIO(response.content))\n+\n+        model = ShieldGemma2ForImageClassification.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        inputs = processor(images=[image]).to(torch_device)\n+        output = model(**inputs)\n+        self.assertEqual(len(output.probabilities), 3)\n+        for element in output.probabilities:\n+            self.assertEqual(len(element), 2)"
        },
        {
            "sha": "31ae3248709b79a6d2a5ca8c621b19a5bb8520f3",
            "filename": "tests/models/shieldgemma2/test_processing_shieldgemma2.py",
            "status": "added",
            "additions": 220,
            "deletions": 0,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -0,0 +1,220 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import json\n+import os\n+import shutil\n+import tempfile\n+import unittest\n+from collections.abc import Mapping\n+\n+from parameterized import parameterized\n+\n+from transformers import GemmaTokenizer, ShieldGemma2Processor\n+from transformers.testing_utils import get_tests_dir, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import Gemma3ImageProcessor\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n+\n+# Copied from _CHAT_TEMPLATE in src/transformers/models/shieldgemma2/convert_shieldgemma2_weights_orbax_to_hf.py\n+_CHAT_TEMPLATE = \"\"\"{{ bos_token }}\n+{%- for message in messages -%}\n+    {%- if message['content'] is iterable -%}\n+        {%- for item in message['content'] -%}\n+            {{ '<start_of_turn>user\\n' }}\n+            {%- if item['type'] == 'image' -%}\n+                {{ '<start_of_image>' }}\n+            {%- elif item['type'] == 'text' -%}\n+                {{ 'You are a policy expert trying to help determine whether a generated or real image is in' }}\n+                {{ ' violation of the safety policy below. Your decision must very clearly follow the specific'}}\n+                {{ ' language used in the safety policy and should not include any interpretation outside of the' }}\n+                {{ ' safety policy.\\n\\n' }}\n+                {{ '<BEGIN_SAFETY_POLICY>\\n' }}\n+                {{ item['text'] + '\\n' }}\n+                {{ '</END_SAFETY_POLICY>\\n\\n' }}\n+                {{ 'Does the image violate the above policy? Your Answer must start with \\\\'Yes\\\\' or \\\\'No\\\\'.' }}\n+                {{ '<end_of_turn>\\n' }}\n+            {%- endif -%}\n+        {%- endfor -%}\n+        {{'<start_of_turn>model\\n'}}\n+    {%- else -%}\n+        {{ raise_exception(\"Conversation messages must contain iterable content containing images and policy definitions in text.\") }}\n+    {%- endif -%}\n+{%- endfor -%}\n+\"\"\"\n+\n+# Simplified from _SHIELDGEMMA2_POLICIES in src/transformers/models/shieldgemma2/convert_shieldgemma2_weights_orbax_to_hf.py\n+_SHIELDGEMMA2_POLICIES: Mapping[str, str] = {\n+    \"dangerous\": \"Test policy related to dangerous content.\",\n+    \"sexual\": \"Test policy related to sexually explicit content.\",\n+    \"violence\": \"Test policy related to violent content.\",\n+}\n+\n+\n+@require_vision\n+class ShieldGemma2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = ShieldGemma2Processor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = Gemma3ImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n+\n+        extra_special_tokens = {\n+            \"image_token\": \"<image_soft_token>\",\n+            \"boi_token\": \"<start_of_image>\",\n+            \"eoi_token\": \"<end_of_image>\",\n+        }\n+        tokenizer = GemmaTokenizer(SAMPLE_VOCAB, keep_accents=True, extra_special_tokens=extra_special_tokens)\n+\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = ShieldGemma2Processor(image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_processor_dict(self):\n+        return {\n+            \"chat_template\": _CHAT_TEMPLATE,\n+            \"policy_definitions\": _SHIELDGEMMA2_POLICIES,\n+        }\n+\n+    def test_policy_definitions_saved_in_config(self):\n+        processor_config_path = os.path.join(self.tmpdirname, \"processor_config.json\")\n+\n+        with open(processor_config_path, \"rb\") as processor_config_file:\n+            json_dict = json.load(processor_config_file)\n+\n+        self.assertIsInstance(json_dict, dict)\n+        self.assertIn(\"policy_definitions\", json_dict)\n+        self.assertIs(len(json_dict[\"policy_definitions\"]), 3)\n+\n+    @parameterized.expand(\n+        [\n+            (\"all_policies\", None, 3),\n+            (\"selected_policies\", [\"dangerous\", \"violence\"], 2),\n+            (\"single_policy\", [\"sexual\"], 1),\n+        ]\n+    )\n+    def test_with_default_policies(self, name, policies, expected_batch_size):\n+        processor = self.get_processor()\n+\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        images = self.prepare_image_inputs()\n+        processed_inputs = processor(images=images, policies=policies)\n+        self.assertEqual(len(processed_inputs[self.text_input_name]), expected_batch_size)\n+        self.assertEqual(len(processed_inputs[self.images_input_name]), expected_batch_size)\n+\n+    @parameterized.expand(\n+        [\n+            (\"all_policies\", None, 6),\n+            (\"selected_policies_from_both\", [\"cbrne\", \"dangerous\", \"specialized_advice\", \"violence\"], 4),\n+            (\"selected_policies_from_custom\", [\"cbrne\", \"specialized_advice\"], 2),\n+            (\"selected_policies_from_default\", [\"dangerous\", \"violence\"], 2),\n+            (\"single_policy_from_custom\", [\"ip\"], 1),\n+            (\"single_policy_from_default\", [\"sexual\"], 1),\n+        ]\n+    )\n+    def test_with_custom_policies(self, name, policies, expected_batch_size):\n+        processor = self.get_processor()\n+\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        # Test policies adapated from https://ailuminate.mlcommons.org/benchmarks/ hazard categories\n+        custom_policies = {\n+            \"cbrne\": \"Test policy related to indiscriminate weapons.\",\n+            \"ip\": \"Test policy related to intellectual property.\",\n+            \"specialized_advice\": \"Test policy related to specialized advice.\",\n+        }\n+\n+        images = self.prepare_image_inputs()\n+        processed_inputs = processor(images=images, custom_policies=custom_policies, policies=policies)\n+        self.assertEqual(len(processed_inputs[self.text_input_name]), expected_batch_size)\n+        self.assertEqual(len(processed_inputs[self.images_input_name]), expected_batch_size)\n+\n+    def test_with_multiple_images(self):\n+        processor = self.get_processor()\n+\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        images = self.prepare_image_inputs(batch_size=2)\n+        print(images)\n+        processed_inputs = processor(images=images)\n+        self.assertEqual(len(processed_inputs[self.text_input_name]), 6)\n+        self.assertEqual(len(processed_inputs[self.images_input_name]), 6)\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n+    def test_chat_template_accepts_processing_kwargs(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n+    def test_chat_template_batched(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n+    def test_chat_template_dict_torch(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n+    def test_chat_template_single(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_unstructured_kwargs_batched(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_unstructured_kwargs(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_structured_kwargs_nested_from_dict(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_structured_kwargs_nested(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        pass\n+\n+    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        pass"
        },
        {
            "sha": "7ce1bdd0f874ec374dbe7147822e247ba789c273",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -228,6 +228,14 @@\n     \"GPTNeoXConfig\": [\"rotary_emb_base\"],\n     \"Gemma3Config\": [\"boi_token_index\", \"eoi_token_index\"],\n     \"Gemma3TextConfig\": [\"cache_implementation\", \"tie_word_embeddings\"],\n+    \"ShieldGemma2Config\": [\n+        \"boi_token_index\",\n+        \"eoi_token_index\",\n+        \"initializer_range\",\n+        \"mm_tokens_per_image\",\n+        \"text_config\",\n+        \"vision_config\",\n+    ],\n }\n \n "
        },
        {
            "sha": "54bb9267c50e7c491a4c1d0400851370f59b75cb",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/487dab1b2b2d0c4e3ca32ec713766cc25673006e/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487dab1b2b2d0c4e3ca32ec713766cc25673006e/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=487dab1b2b2d0c4e3ca32ec713766cc25673006e",
            "patch": "@@ -167,6 +167,7 @@\n     \"models/vision_text_dual_encoder/test_modeling_flax_vision_text_dual_encoder.py\",\n     \"models/decision_transformer/test_modeling_decision_transformer.py\",\n     \"models/bark/test_modeling_bark.py\",\n+    \"models/shieldgemma2/test_modeling_shieldgemma2.py\",\n ]\n \n # Update this list for models that are not in any of the auto MODEL_XXX_MAPPING. Being in this list is an exception and"
        }
    ],
    "stats": {
        "total": 1459,
        "additions": 1459,
        "deletions": 0
    }
}