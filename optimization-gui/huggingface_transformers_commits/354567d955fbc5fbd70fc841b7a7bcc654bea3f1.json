{
    "author": "AlphaOrOmega",
    "message": "Adding superglue fast image processing (#41394)\n\n* Default implementation - no time improvement\n\n* Improved implementation - apparently 2 times faster with only simple function refactor\n\n* elementary torch first approach, still need further implementation of torch first method\n\n* torch-first approach finished\n\n* refactor processor\n\n* refactor test\n\n* partial doc update\n\n* EfficientLoFTRImageProcessorFast based implementation\n\n* EfficientLoFTRImageProcessorFast based implementation\n\n* Logic checked - Test Passed - Validated execution speed\n\n* use modular for efficientloftr\n\n* fix import\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
    "files": [
        {
            "sha": "9bee8e535fdfa9bb3b1b5064cef59afeadc159f2",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -88,16 +88,16 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     import torch\n     from PIL import Image\n     import requests\n-    \n+\n     processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n     model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n-    \n+\n     # SuperGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n     with torch.inference_mode():\n         outputs = model(**inputs)\n-    \n+\n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image\n     keypoints1 = outputs.keypoints1  # Keypoints in second image\n@@ -112,7 +112,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # Process outputs for visualization\n     image_sizes = [[(image.height, image.width) for image in images]]\n     processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-    \n+\n     for i, output in enumerate(processed_outputs):\n         print(f\"For the image pair {i}\")\n         for keypoint0, keypoint1, matching_score in zip(\n@@ -147,6 +147,13 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     - post_process_keypoint_matching\n     - visualize_keypoint_matching\n \n+## SuperGlueImageProcessorFast\n+\n+[[autodoc]] SuperGlueImageProcessorFast\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n+\n ## SuperGlueForKeypointMatching\n \n [[autodoc]] SuperGlueForKeypointMatching"
        },
        {
            "sha": "ba40af484c946484f1f1ca0a5aa91d5e0660ceca",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -171,7 +171,7 @@\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n-            (\"superglue\", (\"SuperGlueImageProcessor\", None)),\n+            (\"superglue\", (\"SuperGlueImageProcessor\", \"SuperGlueImageProcessorFast\")),\n             (\"superpoint\", (\"SuperPointImageProcessor\", \"SuperPointImageProcessorFast\")),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "b551851f1fc4ed2868e82b80f662f577051b4a60",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -1,30 +1,17 @@\n-# coding=utf-8\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Image processor class for EfficientLoFTR.\"\"\"\n-\n-from typing import TYPE_CHECKING, Optional, Union\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/efficientloftr/modular_efficientloftr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_efficientloftr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Optional, Union\n \n import torch\n from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     ImageInput,\n     ImageType,\n@@ -35,17 +22,9 @@\n     is_valid_image,\n )\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TensorType,\n-    auto_docstring,\n-)\n+from ...utils import TensorType, auto_docstring\n from .image_processing_efficientloftr import EfficientLoFTRImageProcessorKwargs\n-\n-\n-if TYPE_CHECKING:\n-    from .modeling_efficientloftr import KeypointMatchingOutput\n-\n-import torchvision.transforms.v2.functional as F\n+from .modeling_efficientloftr import KeypointMatchingOutput\n \n \n def _is_valid_image(image):\n@@ -299,7 +278,7 @@ def _get_color(self, score):\n         r = int(255 * (1 - score))\n         g = int(255 * score)\n         b = 0\n-        return (r, g, b)\n+        return r, g, b\n \n \n __all__ = [\"EfficientLoFTRImageProcessorFast\"]"
        },
        {
            "sha": "62d0a537abcc5f8f8fa0021db612a381563fa0b9",
            "filename": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
            "status": "added",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -0,0 +1,8 @@\n+from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n+\n+\n+class EfficientLoFTRImageProcessorFast(SuperGlueImageProcessorFast):\n+    pass\n+\n+\n+__all__ = [\"EfficientLoFTRImageProcessorFast\"]"
        },
        {
            "sha": "360f7cbd39cbb6df28986461b6638632c74a351d",
            "filename": "src/transformers/models/superglue/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fsuperglue%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fsuperglue%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2F__init__.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_superglue import *\n     from .image_processing_superglue import *\n+    from .image_processing_superglue_fast import *\n     from .modeling_superglue import *\n else:\n     import sys"
        },
        {
            "sha": "d1540d01dda1aa4f69b0417e5589111b0e2599cc",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import requires\n \n@@ -133,6 +134,15 @@ def _is_valid_image(image):\n     raise ValueError(error_message)\n \n \n+class SuperGlueImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: bool\n+\n+\n @requires(backends=(\"torch\",))\n class SuperGlueImageProcessor(BaseImageProcessor):\n     r\"\"\""
        },
        {
            "sha": "f41e4d35aaee5cc745ea79b4eac0e7dc53f4d918",
            "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
            "status": "added",
            "additions": 292,
            "deletions": 0,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -0,0 +1,292 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_type,\n+    is_pil_image,\n+    is_valid_image,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_superglue import SuperGlueImageProcessorKwargs\n+from .modeling_superglue import KeypointMatchingOutput\n+\n+\n+def _is_valid_image(image):\n+    return is_pil_image(image) or (\n+        is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+    )\n+\n+\n+def flatten_pair_images(images):\n+    # Handle the pair validation and flattening similar to slow processor\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image) or isinstance(image, torch.Tensor)) for image in images):\n+            # Single pair of images - keep as is, they'll be processed by the base class\n+            return images\n+        elif all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) or isinstance(image, torch.Tensor) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            # Multiple pairs - flatten them\n+            images = [image for image_pair in images for image in image_pair]\n+            return images\n+    raise ValueError(\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+@auto_docstring\n+class SuperGlueImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = SuperGlueImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[SuperGlueImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[SuperGlueImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        # we need to handle image pairs validation and flattening\n+        return flatten_pair_images(images)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            processed_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Convert back to pairs format\n+        image_pairs = [processed_images[i : i + 2] for i in range(0, len(processed_images), 2)]\n+\n+        # Stack each pair into a single tensor to match slow processor format\n+        stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n+\n+        # Return in same format as slow processor\n+        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n+\n+        return BatchFeature(data={\"pixel_values\": image_pairs})\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images:\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        from ...image_utils import to_numpy_array\n+        from .image_processing_superglue import validate_and_format_image_pairs\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = torch.zeros((max(height0, height1), width0 + width1, 3), dtype=torch.uint8)\n+            plot_image[:height0, :width0] = torch.from_numpy(image_pair[0])\n+            plot_image[:height1, width0:] = torch.from_numpy(image_pair[1])\n+\n+            plot_image_pil = Image.fromarray(plot_image.numpy())\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return r, g, b\n+\n+\n+__all__ = [\"SuperGlueImageProcessorFast\"]"
        },
        {
            "sha": "53897ff1abe52012f8ee8c90499c58f1f5117aa6",
            "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -15,19 +15,14 @@\n import unittest\n \n import numpy as np\n-import pytest\n-from packaging import version\n \n from tests.models.superglue.test_image_processing_superglue import (\n     SuperGlueImageProcessingTest,\n     SuperGlueImageProcessingTester,\n )\n from transformers.testing_utils import (\n     require_torch,\n-    require_torch_accelerator,\n     require_vision,\n-    slow,\n-    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -103,46 +98,6 @@ def setUp(self) -> None:\n         super().setUp()\n         self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)\n \n-    def test_slow_fast_equivalence(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n-    def test_slow_fast_equivalence_batched(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n-            self.skipTest(\n-                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n-            )\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):\n         \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n@@ -173,25 +128,3 @@ def test_fast_is_faster_than_slow(self):\n         self.assertLessEqual(\n             fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n         )\n-\n-    @slow\n-    @require_torch_accelerator\n-    @require_vision\n-    @pytest.mark.torch_compile_test\n-    def test_can_compile_fast_image_processor(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if self.fast_image_processing_class is None:\n-            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        torch.compiler.reset()\n-        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n-        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n-        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-\n-        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n-        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(\n-            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n-        )"
        },
        {
            "sha": "fc93e2004e5448ed460598d5830b97f9d51b2f55",
            "filename": "tests/models/lightglue/test_image_processing_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -90,6 +90,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class LightGlueImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n     image_processing_class = LightGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = None\n \n     def setUp(self) -> None:\n         super().setUp()"
        },
        {
            "sha": "6f4d206572af1153b7a6c82f9d807ee2188ee08e",
            "filename": "tests/models/superglue/test_image_processing_superglue.py",
            "status": "modified",
            "additions": 89,
            "deletions": 2,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/354567d955fbc5fbd70fc841b7a7bcc654bea3f1/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py?ref=354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
            "patch": "@@ -11,12 +11,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import time\n import unittest\n \n+import numpy as np\n+import pytest\n+from packaging import version\n from parameterized import parameterized\n \n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import (\n     ImageProcessingTestMixin,\n@@ -33,6 +43,9 @@\n if is_vision_available():\n     from transformers import SuperGlueImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SuperGlueImageProcessorFast\n+\n \n def random_array(size):\n     return np.random.randint(255, size=size)\n@@ -119,6 +132,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class SuperGlueImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SuperGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SuperGlueImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n@@ -397,3 +411,76 @@ def check_post_processed_output(post_processed_output, image_pair_size):\n             tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n \n             check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+\n+    @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n+    def test_fast_is_faster_than_slow(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast speed test as one of the image processors is not defined\")\n+\n+        # Create image pairs for speed test\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # Time slow processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        slow_time = time.time() - start_time\n+\n+        # Time fast processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        fast_time = time.time() - start_time\n+\n+        # Fast should be faster (or at least not significantly slower)\n+        self.assertLessEqual(\n+            fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )"
        }
    ],
    "stats": {
        "total": 534,
        "additions": 426,
        "deletions": 108
    }
}