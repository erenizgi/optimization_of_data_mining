{
    "author": "MilkClouds",
    "message": "added warning to Trainer when label_names is not specified for PeftModel (#32085)\n\n* feat: added warning to Trainer when label_names is not specified for PeftModel\n\n* Update trainer.py\n\n* feat: peft detectw ith `_is_peft_model`\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Applied formatting in trainer.py\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>",
    "sha": "0cd5e2dfd04d120c53a3c8c2ae90425ab5d015b8",
    "files": [
        {
            "sha": "a8e7fe996b4aaab3befaf64736d4daa6f3a407da",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cd5e2dfd04d120c53a3c8c2ae90425ab5d015b8/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cd5e2dfd04d120c53a3c8c2ae90425ab5d015b8/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=0cd5e2dfd04d120c53a3c8c2ae90425ab5d015b8",
            "patch": "@@ -776,6 +776,12 @@ def __init__(\n         # returned to 0 every time flos need to be logged\n         self.current_flos = 0\n         self.hp_search_backend = None\n+        if _is_peft_model(self.model) and self.args.label_names is None:\n+            logger.warning(\n+                f\"No label_names provided for model class `{self.model.__class__.__name__}`.\"\n+                \" Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`.\"\n+                \" Note that empty label_names list will be used instead.\"\n+            )\n         default_label_names = find_labels(self.model.__class__)\n         self.label_names = default_label_names if self.args.label_names is None else self.args.label_names\n         self.can_return_loss = can_return_loss(self.model.__class__)\n@@ -1243,10 +1249,10 @@ def create_optimizer(self):\n                 for module in opt_model.modules():\n                     if isinstance(module, nn.Embedding):\n                         skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())\n-                        logger.info(f\"skipped {module}: {skipped/2**20}M params\")\n+                        logger.info(f\"skipped {module}: {skipped / 2**20}M params\")\n                         manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n                         logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n-                logger.info(f\"skipped: {skipped/2**20}M params\")\n+                logger.info(f\"skipped: {skipped / 2**20}M params\")\n \n         if is_sagemaker_mp_enabled():\n             self.optimizer = smp.DistributedOptimizer(self.optimizer)"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 8,
        "deletions": 2
    }
}