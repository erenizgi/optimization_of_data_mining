{
    "author": "avihu111",
    "message": "Granite speech speedup + model saving bugfix (#39028)\n\n* ensure the query is updated during training\n\navoid unused parameters that DDP does not like\n\n* avoid a crash when `kwargs` contain `padding=True`\n\ntrainers often pass this argument automatically\n\n* minor\n\n* Remove mel_spec lazy init, and rename to mel_filters.\nthis ensures save_pretrained will not crash when saving the processor during training\nhttps://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595\n\n* minor - most feature extractors has a `sampling_rate` property\n\n* speedup relative position embeddings\n\n* fix several issues in model saving/loading:\n- avoid modifying `self._hf_peft_config_loaded` when saving\n- adapter_config automatically points to the original base model - a finetuned version should point to the model save dir.\n- fixing model weights names, that are changed by adding an adapter.\n\n* minor\n\n* minor\n\n* minor\n\n* fixing a crash without peft active\n\n* add todo to replace einsum",
    "sha": "22b0a898787f9e34c2b9b4ac1e53d2497c44ff39",
    "files": [
        {
            "sha": "6e61f732b77b02d119def826999ba70affcaa6cd",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 30,
            "deletions": 9,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/22b0a898787f9e34c2b9b4ac1e53d2497c44ff39/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22b0a898787f9e34c2b9b4ac1e53d2497c44ff39/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=22b0a898787f9e34c2b9b4ac1e53d2497c44ff39",
            "patch": "@@ -159,8 +159,12 @@ def forward(self, hidden_states: torch.Tensor, attention_dists: torch.Tensor) ->\n         # shaw's relative positional embedding\n         dist = attention_dists.to(hidden_states.device)\n         rel_pos_emb = self.rel_pos_emb(dist)\n-        rel_pos_emb_expanded = rel_pos_emb.view([1, 1, 1] + list(rel_pos_emb.shape))\n-        pos_attn = torch.sum(query_states.unsqueeze(-2) * rel_pos_emb_expanded, dim=-1) * self.scale\n+        # alternative computation of `pos_attn` - for readability\n+        # rel_pos_emb_expanded = rel_pos_emb.view([1, 1, 1] + list(rel_pos_emb.shape))\n+        # pos_attn = torch.sum(query_states.unsqueeze(-2) * rel_pos_emb_expanded, dim=-1) * self.scale\n+        # einsum implementation of pos_attn - gives x30 speedup over the alternative\n+        # TODO (@avihu111) find a fast alternative to einsum\n+        pos_attn = torch.einsum(\"b m h c d, c r d -> b m h c r\", query_states, rel_pos_emb) * self.scale\n \n         if remainder > 0:\n             # masked attention in the extended block\n@@ -541,17 +545,34 @@ def generate(self, *args, **kwargs) -> torch.LongTensor:\n                 self.disable_adapters()\n         return super().generate(*args, input_features=input_features, **kwargs)\n \n-    def save_pretrained(self, *args, **kwargs):\n+    def save_pretrained(self, save_directory, *args, **kwargs):\n         # overwrite save_pretrained to first save the adapter if we have one\n-        # NOTE - this will use the base model path we are exporting in the lora\n-        # adapter, which may not necessarily be the best behavior, but for now\n-        # we keep this for portability, since using the local dir causes problems\n-        # if the model is loaded from outside of the current working dir.\n         if is_peft_available and self._hf_peft_config_loaded:\n-            super().save_pretrained(*args, **kwargs)\n+            adapter_name = self._get_adapter_name()\n+            self.peft_config[adapter_name].base_model_name_or_path = save_directory\n+            super().save_pretrained(save_directory, *args, **kwargs)\n         # Then save the base model afterwards\n+        prev_val = self._hf_peft_config_loaded\n         self._hf_peft_config_loaded = False\n-        super().save_pretrained(*args, **kwargs)\n+        super().save_pretrained(save_directory, *args, **kwargs)\n+        self._hf_peft_config_loaded = prev_val\n+\n+    @staticmethod\n+    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n+        # save the model with the original weights format\n+        return key.replace(\".base_layer\", \"\"), False\n+\n+    def _fix_state_dict_keys_on_save(self, state_dict):\n+        if is_peft_available and self._hf_peft_config_loaded:\n+            # state dict is only adapter, should keep the same\n+            return state_dict\n+        # rename back the base model state dict\n+        return {\n+            self._fix_state_dict_key_on_save(key)[0]: value for key, value in state_dict.items() if \".lora_\" not in key\n+        }\n+\n+    def _get_adapter_name(self):\n+        return list(self.peft_config.keys())[0]\n \n \n __all__ = ["
        }
    ],
    "stats": {
        "total": 39,
        "additions": 30,
        "deletions": 9
    }
}