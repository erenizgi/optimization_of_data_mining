{
    "author": "Kuangdd01",
    "message": "fix Glm4v batch videos forward (#39172)\n\n* changes for video\n\n* update modular\n\n* change get_video_features\n\n* update video token replacement\n\n* update modular\n\n* add test and fix typo\n\n* lint\n\n* fix order\n\n* lint\n\n* fix\n\n* remove dependency\n\n* lint\n\n* lint\n\n* remove todo\n\n* resize video for test\n\n* lint..\n\n* fix test\n\n* new a processor for video_test\n\n* fix test",
    "sha": "520b9dcb42cef21662c304583368ff6645116a45",
    "files": [
        {
            "sha": "8ab64acffffc4e6fe6f8282f2567e46f2b3ed13e",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=520b9dcb42cef21662c304583368ff6645116a45",
            "patch": "@@ -1052,6 +1052,7 @@ def get_rope_index(\n                 device=input_ids.device,\n             )\n             image_index, video_index = 0, 0\n+            video_group_index = 0\n             attention_mask = attention_mask.to(total_input_ids.device)\n             for i, input_ids in enumerate(total_input_ids):\n                 input_ids = input_ids[attention_mask[i] == 1]\n@@ -1081,7 +1082,6 @@ def get_rope_index(\n \n                 llm_pos_ids_list = []\n                 video_frame_num = 1\n-\n                 for modality_type, start_idx, end_idx in input_type_group:\n                     st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n \n@@ -1125,7 +1125,11 @@ def get_rope_index(\n                             w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(1, llm_grid_h, -1).flatten()\n                             llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + st_idx)\n \n-                        video_index += 1\n+                        video_group_index += 1\n+\n+                        if video_group_index >= video_grid_thw[video_index][0]:\n+                            video_index += 1\n+                            video_group_index = 0\n \n                         video_frame_num += 1\n \n@@ -1174,7 +1178,13 @@ def get_video_features(\n                 The temporal, height and width of feature shape of each video in LLM.\n         \"\"\"\n         pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n-        video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        # reshape video_grid_thw -> [b, 3] -> [1, h, w] * frames\n+        temp_frames_hw = []\n+        for t, h, w in video_grid_thw:\n+            repeated_row = torch.tensor([1, h.item(), w.item()]).unsqueeze(0).repeat(t, 1)\n+            temp_frames_hw.append(repeated_row)\n+        flattened_video_grid_thw = torch.cat(temp_frames_hw, dim=0)\n+        video_embeds = self.visual(pixel_values_videos, grid_thw=flattened_video_grid_thw)\n         split_sizes = (video_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n         video_embeds = torch.split(video_embeds, split_sizes)\n         return video_embeds"
        },
        {
            "sha": "7a5b6efa2ef85f8d62b1f361d3face0f475ae795",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 42,
            "deletions": 8,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=520b9dcb42cef21662c304583368ff6645116a45",
            "patch": "@@ -1064,6 +1064,7 @@ def get_rope_index(\n                 device=input_ids.device,\n             )\n             image_index, video_index = 0, 0\n+            video_group_index = 0\n             attention_mask = attention_mask.to(total_input_ids.device)\n             for i, input_ids in enumerate(total_input_ids):\n                 input_ids = input_ids[attention_mask[i] == 1]\n@@ -1093,7 +1094,6 @@ def get_rope_index(\n \n                 llm_pos_ids_list = []\n                 video_frame_num = 1\n-\n                 for modality_type, start_idx, end_idx in input_type_group:\n                     st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n \n@@ -1137,7 +1137,11 @@ def get_rope_index(\n                             w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(1, llm_grid_h, -1).flatten()\n                             llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + st_idx)\n \n-                        video_index += 1\n+                        video_group_index += 1\n+\n+                        if video_group_index >= video_grid_thw[video_index][0]:\n+                            video_index += 1\n+                            video_group_index = 0\n \n                         video_frame_num += 1\n \n@@ -1173,6 +1177,30 @@ def get_rope_index(\n \n             return position_ids, mrope_position_deltas\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n+        # reshape video_grid_thw -> [b, 3] -> [1, h, w] * frames\n+        temp_frames_hw = []\n+        for t, h, w in video_grid_thw:\n+            repeated_row = torch.tensor([1, h.item(), w.item()]).unsqueeze(0).repeat(t, 1)\n+            temp_frames_hw.append(repeated_row)\n+        flattened_video_grid_thw = torch.cat(temp_frames_hw, dim=0)\n+        video_embeds = self.visual(pixel_values_videos, grid_thw=flattened_video_grid_thw)\n+        split_sizes = (video_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        video_embeds = torch.split(video_embeds, split_sizes)\n+        return video_embeds\n+\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -1664,32 +1692,38 @@ def __call__(\n             video_index = 0\n             for i in range(len(text)):\n                 while self.video_token in text[i]:\n-                    num_frames = len(video_grid_thw)\n+                    num_frames = video_grid_thw[video_index][0]\n                     video_structure = \"\"\n \n                     if hasattr(timestamps, \"tolist\"):\n                         timestamps_list = timestamps.tolist()[0]\n                     else:\n                         timestamps_list = timestamps[0] if isinstance(timestamps[0], list) else timestamps\n+\n                     unique_timestamps = []\n                     for idx in range(0, len(timestamps_list)):\n                         unique_timestamps.append(timestamps_list[idx])\n+\n                     selected_timestamps = unique_timestamps[:num_frames]\n                     while len(selected_timestamps) < num_frames:\n                         selected_timestamps.append(selected_timestamps[-1] if selected_timestamps else 0)\n+\n                     for frame_idx in range(num_frames):\n                         timestamp_sec = selected_timestamps[frame_idx]\n                         frame_structure = f\"<|begin_of_image|>{self.image_token}<|end_of_image|>{timestamp_sec}\"\n                         video_structure += frame_structure\n+\n                     text[i] = text[i].replace(self.video_token, video_structure, 1)\n+                    num_image_tokens = (\n+                        video_grid_thw[video_index].prod() // merge_length // video_grid_thw[video_index][0]\n+                    )\n+                    for frame_idx in range(num_frames):\n+                        if self.image_token in text[i]:\n+                            text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n+\n                     video_index += 1\n \n-                for frame_idx in range(len(video_grid_thw)):\n-                    if self.image_token in text[i]:\n-                        num_image_tokens = video_grid_thw[frame_idx].prod() // merge_length\n-                        text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n-\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])"
        },
        {
            "sha": "c71804fc111af0ea735cfaf73b9472b032f88d7a",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=520b9dcb42cef21662c304583368ff6645116a45",
            "patch": "@@ -167,32 +167,38 @@ def __call__(\n             video_index = 0\n             for i in range(len(text)):\n                 while self.video_token in text[i]:\n-                    num_frames = len(video_grid_thw)\n+                    num_frames = video_grid_thw[video_index][0]\n                     video_structure = \"\"\n \n                     if hasattr(timestamps, \"tolist\"):\n                         timestamps_list = timestamps.tolist()[0]\n                     else:\n                         timestamps_list = timestamps[0] if isinstance(timestamps[0], list) else timestamps\n+\n                     unique_timestamps = []\n                     for idx in range(0, len(timestamps_list)):\n                         unique_timestamps.append(timestamps_list[idx])\n+\n                     selected_timestamps = unique_timestamps[:num_frames]\n                     while len(selected_timestamps) < num_frames:\n                         selected_timestamps.append(selected_timestamps[-1] if selected_timestamps else 0)\n+\n                     for frame_idx in range(num_frames):\n                         timestamp_sec = selected_timestamps[frame_idx]\n                         frame_structure = f\"<|begin_of_image|>{self.image_token}<|end_of_image|>{timestamp_sec}\"\n                         video_structure += frame_structure\n+\n                     text[i] = text[i].replace(self.video_token, video_structure, 1)\n+                    num_image_tokens = (\n+                        video_grid_thw[video_index].prod() // merge_length // video_grid_thw[video_index][0]\n+                    )\n+                    for frame_idx in range(num_frames):\n+                        if self.image_token in text[i]:\n+                            text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n+\n                     video_index += 1\n \n-                for frame_idx in range(len(video_grid_thw)):\n-                    if self.image_token in text[i]:\n-                        num_image_tokens = video_grid_thw[frame_idx].prod() // merge_length\n-                        text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n-\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])"
        },
        {
            "sha": "68915bf58622f5361ff8795e63008a45e9dfad79",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/520b9dcb42cef21662c304583368ff6645116a45/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=520b9dcb42cef21662c304583368ff6645116a45",
            "patch": "@@ -249,10 +249,6 @@ def _preprocess(\n         processed_grids = reorder_videos(processed_grids, grouped_videos_index)\n         pixel_values_videos = torch.cat(processed_videos, dim=0)\n         video_grid_thw = torch.tensor(processed_grids)\n-        total_frames = video_grid_thw[0][0].item()\n-        h = video_grid_thw[0][1].item()\n-        w = video_grid_thw[0][2].item()\n-        video_grid_thw = [[1, h, w] for _ in range(total_frames)]\n         data = {\n             \"pixel_values_videos\": pixel_values_videos,\n             \"video_grid_thw\": video_grid_thw,"
        },
        {
            "sha": "1e962e4c334ea68b4109663c41dd8fc898403e36",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 59,
            "deletions": 1,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/520b9dcb42cef21662c304583368ff6645116a45/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/520b9dcb42cef21662c304583368ff6645116a45/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=520b9dcb42cef21662c304583368ff6645116a45",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch GLM-4.1V model.\"\"\"\n \n+import copy\n import gc\n import unittest\n \n@@ -236,7 +237,26 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    # RoPE index doesn't match when using embeddings\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"image_grid_thw\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+            with torch.no_grad():\n+                model(**inputs)[0]\n+\n     def test_inputs_embeds_matches_input_ids(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -350,6 +370,44 @@ def test_small_model_integration_test_batch(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n+    @slow\n+    def test_small_model_integration_test_with_video(self):\n+        processor = AutoProcessor.from_pretrained(\"THUDM/GLM-4.1V-9B-Thinking\", max_image_size={\"longest_edge\": 50176})\n+        model = Glm4vForConditionalGeneration.from_pretrained(\n+            \"THUDM/GLM-4.1V-9B-Thinking\", torch_dtype=torch.float16, device_map=\"auto\"\n+        )\n+        questions = [\"Describe this video.\"] * 2\n+        video_urls = [\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"\n+        ] * 2\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"video\": video_url,\n+                        },\n+                        {\"type\": \"text\", \"text\": question},\n+                    ],\n+                }\n+            ]\n+            for question, video_url in zip(questions, video_urls)\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\", padding=True\n+        ).to(torch_device)\n+        output = model.generate(**inputs, max_new_tokens=30)\n+        EXPECTED_DECODED_TEXT = [\n+            \"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is a room with a wooden floor, maybe a traditional Japanese room with tatami\",\n+            \"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is a room with a wooden floor, maybe a traditional Japanese room with tatami\"\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n     @slow\n     def test_small_model_integration_test_expand(self):\n         model = Glm4vForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "b629e61eb50c61b8d7ad5b78d16904fa9b66c20e",
            "filename": "tests/models/glm4v/test_video_processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/520b9dcb42cef21662c304583368ff6645116a45/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/520b9dcb42cef21662c304583368ff6645116a45/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py?ref=520b9dcb42cef21662c304583368ff6645116a45",
            "patch": "@@ -228,7 +228,7 @@ def test_call_pytorch(self):\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n \n-    @unittest.skip(\"Skip for now, the test needs adjustment fo GLM-4.1V\")\n+    @unittest.skip(\"Skip for now, the test needs adjustment for GLM-4.1V\")\n     def test_call_numpy_4_channels(self):\n         for video_processing_class in self.video_processor_list:\n             # Test that can process videos which have an arbitrary number of channels"
        }
    ],
    "stats": {
        "total": 150,
        "additions": 127,
        "deletions": 23
    }
}