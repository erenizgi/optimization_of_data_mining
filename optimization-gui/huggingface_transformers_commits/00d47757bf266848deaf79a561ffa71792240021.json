{
    "author": "LysandreJik",
    "message": "Reorder serving docs (#39634)\n\n* Slight reorg\n\n* LLMs + draft VLMs\n\n* Actual VLM examples\n\n* Initial responses\n\n* Reorder\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/tiny_agents.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/open_webui.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/cursor.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Responses API\n\n* Address Pedro's comments\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>",
    "sha": "00d47757bf266848deaf79a561ffa71792240021",
    "files": [
        {
            "sha": "f82d6c4a6d031395f23248855ce4473693a26aba",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -89,6 +89,18 @@\n     - local: chat_extras\n       title: Tools and RAG\n     title: Chat with models\n+  - sections:\n+      - local: serving\n+        title: Serving LLMs, VLMs, and other chat-based models\n+      - local: jan\n+        title: Jan\n+      - local: cursor\n+        title: Cursor\n+      - local: tiny_agents\n+        title: Tiny-Agents CLI and MCP tools\n+      - local: open_webui\n+        title: Open WebUI\n+    title: Serving\n   - sections:\n     - local: perf_torch_compile\n       title: torch.compile\n@@ -103,8 +115,6 @@\n     title: Agents\n   - local: tools\n     title: Tools\n-  - local: serving\n-    title: Serving\n   - local: transformers_as_backend\n     title: Inference server backends\n   title: Inference"
        },
        {
            "sha": "18ebe803edfbd16b21ccf8a06d95e136e400da24",
            "filename": "docs/source/en/cursor.md",
            "status": "added",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fcursor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fcursor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcursor.md?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -0,0 +1,42 @@\n+# Using Cursor as a client of transformers serve\n+\n+This example shows how to use `transformers serve` as a local LLM provider for [Cursor](https://cursor.com/), the popular IDE. In this particular case, requests to `transformers serve` will come from an external IP (Cursor's server IPs), which requires some additional setup. Furthermore, some of Cursor's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons.\n+\n+To launch a server with CORS enabled, run\n+\n+```shell\n+transformers serve --enable-cors\n+```\n+\n+You'll also need to expose your server to external IPs. A potential solution is to use [`ngrok`](https://ngrok.com/), which has a permissive free tier. After setting up your `ngrok` account and authenticating on your server machine, you run\n+\n+```shell\n+ngrok http [port]\n+```\n+\n+where `port` is the port used by `transformers serve` (`8000` by default). On the terminal where you launched `ngrok`, you'll see a https address in the \"Forwarding\" row, as in the image below. This is the address to send requests to.\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_ngrok.png\"/>\n+</h3>\n+\n+You're now ready to set things up on the app side! In Cursor, while you can't set a new provider, you can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set your `transformers serve` endpoint, follow this order:\n+1. Unselect ALL models in the list above (e.g. `gpt4`, ...);\n+2. Add and select the model you want to use (e.g. `Qwen/Qwen3-4B`)\n+3. Add some random text to OpenAI API Key. This field won't be used, but it can’t be empty;\n+4. Add the https address from `ngrok` to the \"Override OpenAI Base URL\" field, appending `/v1` to the address (i.e. `https://(...).ngrok-free.app/v1`);\n+5. Hit \"Verify\".\n+\n+After you follow these steps, your \"Models\" tab should look like the image below. Your server should also have received a few requests from the verification step.\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor.png\"/>\n+</h3>\n+\n+You are now ready to use your local model in Cursor! For instance, if you toggle the AI Pane, you can select the model you added and ask it questions about your local files.\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor_chat.png\"/>\n+</h3>\n+\n+"
        },
        {
            "sha": "ff580496c81be13783563b2041604454b114c213",
            "filename": "docs/source/en/jan.md",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fjan.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fjan.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fjan.md?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -0,0 +1,32 @@\n+# Jan: using the serving API as a local LLM provider\n+\n+This example shows how to use `transformers serve` as a local LLM provider for the [Jan](https://jan.ai/) app. Jan is a ChatGPT-alternative graphical interface, fully running on your machine. The requests to `transformers serve` come directly from the local app -- while this section focuses on Jan, you can extrapolate some instructions to other apps that make local requests.\n+\n+## Running models locally\n+\n+To connect `transformers serve` with Jan, you'll need to set up a new model provider (\"Settings\" > \"Model Providers\"). Click on \"Add Provider\", and set a new name. In your new model provider page, all you need to set is the \"Base URL\" to the following pattern:\n+\n+```shell\n+http://[host]:[port]/v1\n+```\n+\n+where `host` and `port` are the `transformers serve` CLI parameters (`localhost:8000` by default). After setting this up, you should be able to see some models in the \"Models\" section, hitting \"Refresh\". Make sure you add some text in the \"API key\" text field too -- this data is not actually used, but the field can't be empty. Your custom model provider page should look like this:\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_jan_model_providers.png\"/>\n+</h3>\n+\n+You are now ready to chat!\n+\n+> [!TIP]\n+> You can add any `transformers`-compatible model to Jan through `transformers serve`. In the custom model provider you created, click on the \"+\" button in the \"Models\" section and add its Hub repository name, e.g. `Qwen/Qwen3-4B`.\n+\n+## Running models on a separate machine\n+\n+To conclude this example, let's look into a more advanced use-case. If you have a beefy machine to serve models with, but prefer using Jan on a different device, you need to add port forwarding. If you have `ssh` access from your Jan machine into your server, this can be accomplished by typing the following to your Jan machine's terminal\n+\n+```\n+ssh -N -f -L 8000:localhost:8000 your_server_account@your_server_IP -p port_to_ssh_into_your_server\n+```\n+\n+Port forwarding is not Jan-specific: you can use it to connect `transformers serve` running in a different machine with an app of your choice."
        },
        {
            "sha": "9042131631e7a99b60dee05de7e20e645eb65e55",
            "filename": "docs/source/en/open_webui.md",
            "status": "added",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fopen_webui.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fopen_webui.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fopen_webui.md?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -0,0 +1,22 @@\n+#  Audio transcriptions with WebUI and `transformers serve`\n+\n+This guide shows how to do audio transcription for chat purposes, using `transformers serve` and [Open WebUI](https://openwebui.com/). This guide assumes you have Open WebUI installed on your machine and ready to run. Please refer to the examples above to use the text functionalities of `transformer serve` with Open WebUI -- the instructions are the same.\n+\n+To start, let's launch the server. Some of Open WebUI's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons, so you need to enable it:\n+\n+```shell\n+transformers serve --enable-cors\n+```\n+\n+Before you can speak into Open WebUI, you need to update its settings to use your server for speech to text (STT) tasks. Launch Open WebUI, and navigate to the audio tab inside the admin settings. If you're using Open WebUI with the default ports, [this link (default)](http://localhost:3000/admin/settings/audio) or [this link (python deployment)](http://localhost:8080/admin/settings/audio) will take you there. Do the following changes there:\n+1. Change the type of \"Speech-to-Text Engine\" to \"OpenAI\";\n+2. Update the address to your server's address -- `http://localhost:8000/v1` by default;\n+3. Type your model of choice into the \"STT Model\" field, e.g. `openai/whisper-large-v3` ([available models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)).\n+\n+If you've done everything correctly, the audio tab should look like this\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_openwebui_stt_settings.png\"/>\n+</h3>\n+\n+You're now ready to speak! Open a new chat, utter a few words after hitting the microphone button, and you should see the corresponding text on the chat input after the model transcribes it."
        },
        {
            "sha": "218cb18682e31296f01f625d3a009f41727ca226",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 251,
            "deletions": 90,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -18,8 +18,17 @@ rendered properly in your Markdown viewer.\n \n Transformer models can be efficiently deployed using libraries such as vLLM, Text Generation Inference (TGI), and others. These libraries are designed for production-grade user-facing services, and can scale to multiple servers and millions of concurrent users. Refer to [Transformers as Backend for Inference Servers](./transformers_as_backends) for usage examples.\n \n+> [!TIP]\n+> Responses API is now supported as an experimental API! Read more about it [here](#responses-api).\n+\n Apart from that you can also serve transformer models easily using the `transformers serve` CLI. This is ideal for experimentation purposes, or to run models locally for personal and private use.\n \n+In this document, we dive into the different supported endpoints and modalities; we also cover the setup of several user interfaces that can be used on top of `transformers serve` in the following guides:\n+- [Jan (text and MCP user interface)](./jan.md)\n+- [Cursor (IDE)](./cursor.md)\n+- [Open WebUI (text, image, speech user interface)](./open_webui.md)\n+- [Tiny-Agents (text and MCP CLI tool)](./tiny_agents.md)\n+\n ## Serve CLI\n \n > [!WARNING]\n@@ -45,7 +54,14 @@ The simplest way to interact with the server is through our `transformers chat`\n transformers chat localhost:8000 --model-name-or-path Qwen/Qwen3-4B\n ```\n \n-or by sending an HTTP request with `cURL`, e.g.\n+or by sending an HTTP request, like we'll see below.\n+\n+## Chat Completions - text-based\n+\n+See below for examples for text-based requests. Both LLMs and VLMs should handle \n+\n+<hfoptions id=\"chat-completion-http\">\n+<hfoption id=\"curl\">\n \n ```shell\n curl -X POST http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 1000, \"stream\": true, \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\"}'\n@@ -61,150 +77,295 @@ data: {\"object\": \"chat.completion.chunk\", \"id\": \"req_0\", \"created\": 1751377863,\n (...)\n ```\n \n-The server is also an MCP client, so it can interact with MCP tools in agentic use cases. This, of course, requires the use of an LLM that is designed to use tools.\n-\n-> [!TIP]\n-> At the moment, MCP tool usage in `transformers` is limited to the `qwen` family of models.\n+</hfoption>\n+<hfoption id=\"python - huggingface_hub\">\n \n-<!-- TODO: example with a minimal python example, and explain that it is possible to pass a full generation config in the request -->\n+```python\n+import asyncio\n+from huggingface_hub import AsyncInferenceClient\n \n+messages = [{\"role\": \"user\", \"content\": \"What is the Transformers library known for?\"}]\n+client = AsyncInferenceClient(\"http://localhost:8000\")\n \n-### Usage example 1: chat with local requests (feat. Jan)\n+async def responses_api_test_async():\n+    async for chunk in (await client.chat_completion(messages, model=\"Qwen/Qwen2.5-0.5B-Instruct\", max_tokens=256, stream=True)):\n+        token = chunk.choices[0].delta.content\n+        if token:\n+            print(token, end='')\n \n-This example shows how to use `transformers serve` as a local LLM provider for the [Jan](https://jan.ai/) app. Jan is a ChatGPT-alternative graphical interface, fully running on your machine. The requests to `transformers serve` come directly from the local app -- while this section focuses on Jan, you can extrapolate some instructions to other apps that make local requests.\n+asyncio.run(responses_api_test_async())\n+asyncio.run(client.close())\n+```\n \n-To connect `transformers serve` with Jan, you'll need to set up a new model provider (\"Settings\" > \"Model Providers\"). Click on \"Add Provider\", and set a new name. In your new model provider page, all you need to set is the \"Base URL\" to the following pattern:\n+From which you should get an iterative string printed:\n \n ```shell\n-http://[host]:[port]/v1\n+The Transformers library is primarily known for its ability to create and manipulate large-scale language models [...]\n ```\n \n-where `host` and `port` are the `transformers serve` CLI parameters (`localhost:8000` by default). After setting this up, you should be able to see some models in the \"Models\" section, hitting \"Refresh\". Make sure you add some text in the \"API key\" text field too -- this data is not actually used, but the field can't be empty. Your custom model provider page should look like this:\n+</hfoption>\n+<hfoption id=\"python - openai\">\n \n-<h3 align=\"center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_jan_model_providers.png\"/>\n-</h3>\n+```python\n+from openai import OpenAI\n \n-You are now ready to chat!\n+client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"<random_string>\")\n \n-> [!TIP]\n-> You can add any `transformers`-compatible model to Jan through `transformers serve`. In the custom model provider you created, click on the \"+\" button in the \"Models\" section and add its Hub repository name, e.g. `Qwen/Qwen3-4B`.\n+completion = client.chat.completions.create(\n+    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n+    messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": \"What is the Transformers library known for?\"\n+        }\n+    ],\n+    stream=True\n+)\n+\n+for chunk in completion:\n+    token = chunk.choices[0].delta.content\n+    if token:\n+        print(token, end='')\n+```\n \n-To conclude this example, let's look into a more advanced use-case. If you have a beefy machine to serve models with, but prefer using Jan on a different device, you need to add port forwarding. If you have `ssh` access from your Jan machine into your server, this can be accomplished by typing the following to your Jan machine's terminal\n+From which you should get an iterative string printed:\n \n+```shell\n+The Transformers library is primarily known for its ability to create and manipulate large-scale language models [...]\n ```\n-ssh -N -f -L 8000:localhost:8000 your_server_account@your_server_IP -p port_to_ssh_into_your_server\n-```\n-\n-Port forwarding is not Jan-specific: you can use it to connect `transformers serve` running in a different machine with an app of your choice.\n \n+</hfoption>\n+</hfoptions>\n \n-### Usage example 2: chat with external requests (feat. Cursor)\n+## Chat Completions - VLMs\n \n-This example shows how to use `transformers serve` as a local LLM provider for [Cursor](https://cursor.com/), the popular IDE. Unlike in the previous example, requests to `transformers serve` will come from an external IP (Cursor's server IPs), which requires some additional setup. Furthermore, some of Cursor's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons.\n+The Chat Completion API also supports images; see below for examples for text-and-image-based requests.\n \n-To launch a server with CORS enabled, run\n+<hfoptions id=\"chat-completion-http-images\">\n+<hfoption id=\"curl\">\n \n ```shell\n-transformers serve --enable-cors\n+curl http://localhost:8000/v1/chat/completions \\\n+  -H \"Content-Type: application/json\" \\\n+  -d '{\n+    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n+    \"stream\": true,\n+    \"messages\": [\n+      {\n+        \"role\": \"user\",\n+        \"content\": [\n+          {\n+            \"type\": \"text\",\n+            \"text\": \"What is in this image?\"\n+          },\n+          {\n+            \"type\": \"image_url\",\n+            \"image_url\": {\n+              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n+            }\n+          }\n+        ]\n+      }\n+    ],\n+    \"max_tokens\": 300\n+  }'\n+\n ```\n \n-You'll also need to expose your server to external IPs. A potential solution is to use [`ngrok`](https://ngrok.com/), which has a permissive free tier. After setting up your `ngrok` account and authenticating on your server machine, you run\n+from which you'll receive multiple chunks in the Completions API format\n \n ```shell\n-ngrok http [port]\n+data: {\"id\":\"req_0\",\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0}],\"created\":1753366665,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct@main\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"\"}\n+\n+data: {\"id\":\"req_0\",\"choices\":[{\"delta\":{\"content\":\"The \"},\"index\":0}],\"created\":1753366701,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct@main\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"\"}\n+\n+data: {\"id\":\"req_0\",\"choices\":[{\"delta\":{\"content\":\"image \"},\"index\":0}],\"created\":1753366701,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct@main\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"\"}\n ```\n \n-where `port` is the port used by `transformers serve` (`8000` by default). On the terminal where you launched `ngrok`, you'll see an https address in the \"Forwarding\" row, as in the image below. This is the address to send requests to.\n+</hfoption>\n+<hfoption id=\"python - huggingface_hub\">\n+\n+```python\n+import asyncio\n+from huggingface_hub import AsyncInferenceClient\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+            {\n+                \"type\": \"image_url\",\n+                \"image_url\": {\n+                    \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\",\n+                }\n+            },\n+        ],\n+    }\n+]\n+client = AsyncInferenceClient(\"http://localhost:8000\")\n+\n+async def responses_api_test_async():\n+    async for chunk in (await client.chat_completion(messages, model=\"Qwen/Qwen2.5-VL-7B-Instruct\", max_tokens=256, stream=True)):\n+        token = chunk.choices[0].delta.content\n+        if token:\n+            print(token, end='')\n+\n+asyncio.run(responses_api_test_async())\n+asyncio.run(client.close())\n+```\n \n-<h3 align=\"center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_ngrok.png\"/>\n-</h3>\n+From which you should get an iterative string printed:\n \n-You're now ready to set things up on the app side! In Cursor, while you can't set a new provider, you can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set your `transformers serve` endpoint, follow this order:\n-1. Unselect ALL models in the list above (e.g. `gpt4`, ...);\n-2. Add and select the model you want to use (e.g. `Qwen/Qwen3-4B`)\n-3. Add some random text to OpenAI API Key. This field won't be used, but it can’t be empty;\n-4. Add the https address from `ngrok` to the \"Override OpenAI Base URL\" field, appending `/v1` to the address (i.e. `https://(...).ngrok-free.app/v1`);\n-5. Hit \"Verify\".\n+```xmp\n+The image depicts an astronaut in a space suit standing on what appears to be the surface of the moon, given the barren, rocky landscape and the dark sky in the background. The astronaut is holding a large egg that has cracked open, revealing a small creature inside. The scene is imaginative and playful, combining elements of space exploration with a whimsical twist involving the egg and the creature.\n+```\n \n-After you follow these steps, your \"Models\" tab should look like the image below. Your server should also have received a few requests from the verification step.\n+</hfoption>\n+<hfoption id=\"python - openai\">\n \n-<h3 align=\"center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor.png\"/>\n-</h3>\n+```python\n+from openai import OpenAI\n \n-You are now ready to use your local model in Cursor! For instance, if you toggle the AI Pane, you can select the model you added and ask it questions about your local files.\n+client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"<random_string>\")\n \n-<h3 align=\"center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor_chat.png\"/>\n-</h3>\n+completion = client.chat.completions.create(\n+    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n+    messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ],\n+    stream=True\n+)\n+\n+for chunk in completion:\n+    token = chunk.choices[0].delta.content\n+    if token:\n+        print(token, end='')\n+```\n \n+From which you should get an iterative string printed:\n \n-### Usage example 3: `tiny-agents` CLI and MCP Tools\n+```xmp\n+The image depicts an astronaut in a space suit standing on what appears to be the surface of the moon, given the barren, rocky landscape and the dark sky in the background. The astronaut is holding a large egg that has cracked open, revealing a small creature inside. The scene is imaginative and playful, combining elements of space exploration with a whimsical twist involving the egg and the creature.\n+```\n+\n+</hfoption>\n+</hfoptions>\n \n-To showcase the use of MCP tools, let's see how to integrate the `transformers serve` server with the [`tiny-agents`](https://huggingface.co/blog/python-tiny-agents) CLI.\n+## Responses API\n+\n+The Responses API is the newest addition to the supported APIs of `transformers serve`.\n \n > [!TIP]\n-> Many Hugging Face Spaces can be used as MCP servers, as in this example. You can find all compatible Spaces [here](https://huggingface.co/spaces?filter=mcp-server).\n+> This API is still experimental: expect bug patches and additition of new features in the coming weeks.\n+> If you run into any issues, please let us know and we'll work on fixing them ASAP.\n \n-The first step to use MCP tools is to let the model know which tools are available. As an example, let's consider a `tiny-agents` configuration file with a reference to an [image generation MCP server](https://evalstate-flux1-schnell.hf.space/).\n+Instead of the previous `/v1/chat/completions` path, the Responses API lies behind the `/v1/responses` path.\n+See below for examples interacting with our Responses endpoint with `curl`, as well as the Python OpenAI client.\n \n-```json\n-{\n-    \"model\": \"Menlo/Jan-nano\",\n-    \"endpointUrl\": \"http://localhost:8000\",\n-    \"servers\": [\n-        {\n-            \"type\": \"sse\",\n-            \"url\": \"https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse\"\n-        }\n-    ]\n-}\n-```\n+So far, this endpoint only supports text and therefore only LLMs. VLMs to come!\n \n-You can then launch your `tiny-agents` chat interface with the following command.\n+<hfoptions id=\"responses\">\n+<hfoption id=\"curl\">\n \n-```bash\n-tiny-agents run path/to/your/config.json\n+```shell\n+curl http://localhost:8000/v1/responses \\\n+  -H \"Content-Type: application/json\" \\\n+  -d '{\n+    \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+    \"stream\": true,\n+    \"input\": \"Tell me a three sentence bedtime story about a unicorn.\"\n+  }'\n ```\n \n-If you have `transformers serve` running in the background, you're ready to use MCP tools from a local model! For instance, here's the example of a chat session with `tiny-agents`:\n+from which you'll receive multiple chunks in the Responses API format\n+\n+```shell\n+data: {\"response\":{\"id\":\"resp_req_0\",\"created_at\":1754059817.783648,\"model\":\"Qwen/Qwen2.5-0.5B-Instruct@main\",\"object\":\"response\",\"output\":[],\"parallel_tool_calls\":false,\"tool_choice\":\"auto\",\"tools\":[],\"status\":\"queued\",\"text\":{\"format\":{\"type\":\"text\"}}},\"sequence_number\":0,\"type\":\"response.created\"}\n+\n+data: {\"response\":{\"id\":\"resp_req_0\",\"created_at\":1754059817.783648,\"model\":\"Qwen/Qwen2.5-0.5B-Instruct@main\",\"object\":\"response\",\"output\":[],\"parallel_tool_calls\":false,\"tool_choice\":\"auto\",\"tools\":[],\"status\":\"in_progress\",\"text\":{\"format\":{\"type\":\"text\"}}},\"sequence_number\":1,\"type\":\"response.in_progress\"}\n+\n+data: {\"item\":{\"id\":\"msg_req_0\",\"content\":[],\"role\":\"assistant\",\"status\":\"in_progress\",\"type\":\"message\"},\"output_index\":0,\"sequence_number\":2,\"type\":\"response.output_item.added\"}\n+\n+data: {\"content_index\":0,\"item_id\":\"msg_req_0\",\"output_index\":0,\"part\":{\"annotations\":[],\"text\":\"\",\"type\":\"output_text\"},\"sequence_number\":3,\"type\":\"response.content_part.added\"}\n+\n+data: {\"content_index\":0,\"delta\":\"\",\"item_id\":\"msg_req_0\",\"output_index\":0,\"sequence_number\":4,\"type\":\"response.output_text.delta\"}\n \n-```bash\n-Agent loaded with 1 tools:\n- • flux1_schnell_infer\n-»  Generate an image of a cat on the moon\n-<Tool req_0_tool_call>flux1_schnell_infer {\"prompt\": \"a cat on the moon\", \"seed\": 42, \"randomize_seed\": true, \"width\": 1024, \"height\": 1024, \"num_inference_steps\": 4}\n+data: {\"content_index\":0,\"delta\":\"Once \",\"item_id\":\"msg_req_0\",\"output_index\":0,\"sequence_number\":5,\"type\":\"response.output_text.delta\"}\n \n-Tool req_0_tool_call\n-[Binary Content: Image image/webp, 57732 bytes]\n-The task is complete and the content accessible to the User\n-Image URL: https://evalstate-flux1-schnell.hf.space/gradio_api/file=/tmp/gradio/3dbddc0e53b5a865ed56a4e3dbdd30f3f61cf3b8aabf1b456f43e5241bd968b8/image.webp\n-380576952\n+data: {\"content_index\":0,\"delta\":\"upon \",\"item_id\":\"msg_req_0\",\"output_index\":0,\"sequence_number\":6,\"type\":\"response.output_text.delta\"}\n \n-I have generated an image of a cat on the moon using the Flux 1 Schnell Image Generator. The image is 1024x1024 pixels and was created with 4 inference steps. Let me know if you would like to make any changes or need further assistance!\n+data: {\"content_index\":0,\"delta\":\"a \",\"item_id\":\"msg_req_0\",\"output_index\":0,\"sequence_number\":7,\"type\":\"response.output_text.delta\"}\n ```\n \n-### Usage example 4: speech to text transcription (feat. Open WebUI)\n+</hfoption>\n+<hfoption id=\"python - openai\">\n \n-This guide shows how to do audio transcription for chat purposes, using `transformers serve` and [Open WebUI](https://openwebui.com/). This guide assumes you have Open WebUI installed on your machine and ready to run. Please refer to the examples above to use the text functionalities of `transformer serve` with Open WebUI -- the instructions are the same.\n+```python\n+from openai import OpenAI\n \n-To start, let's launch the server. Some of Open WebUI's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons, so you need to enable it:\n+client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"<KEY>\")\n+\n+response = client.responses.create(\n+    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n+    instructions=\"You are a helpful assistant.\",\n+    input=\"Hello!\",\n+    stream=True,\n+    metadata={\"foo\": \"bar\"},\n+)\n+\n+for event in response:\n+    print(event)\n+```\n+\n+From which you should get events printed out successively.\n \n ```shell\n-transformers serve --enable-cors\n+ResponseCreatedEvent(response=Response(id='resp_req_0', created_at=1754060400.3718212, error=None, incomplete_details=None, instructions='You are a helpful assistant.', metadata={'foo': 'bar'}, model='Qwen/Qwen2.5-0.5B-Instruct@main', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status='queued', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created')\n+ResponseInProgressEvent(response=Response(id='resp_req_0', created_at=1754060400.3718212, error=None, incomplete_details=None, instructions='You are a helpful assistant.', metadata={'foo': 'bar'}, model='Qwen/Qwen2.5-0.5B-Instruct@main', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=1, type='response.in_progress')\n+ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_req_0', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n+ResponseContentPartAddedEvent(content_index=0, item_id='msg_req_0', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=3, type='response.content_part.added')\n+ResponseTextDeltaEvent(content_index=0, delta='', item_id='msg_req_0', output_index=0, sequence_number=4, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='', item_id='msg_req_0', output_index=0, sequence_number=5, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='Hello! ', item_id='msg_req_0', output_index=0, sequence_number=6, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='How ', item_id='msg_req_0', output_index=0, sequence_number=7, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='can ', item_id='msg_req_0', output_index=0, sequence_number=8, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='I ', item_id='msg_req_0', output_index=0, sequence_number=9, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='assist ', item_id='msg_req_0', output_index=0, sequence_number=10, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='you ', item_id='msg_req_0', output_index=0, sequence_number=11, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='', item_id='msg_req_0', output_index=0, sequence_number=12, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='', item_id='msg_req_0', output_index=0, sequence_number=13, type='response.output_text.delta')\n+ResponseTextDeltaEvent(content_index=0, delta='today?', item_id='msg_req_0', output_index=0, sequence_number=14, type='response.output_text.delta')\n+ResponseTextDoneEvent(content_index=0, item_id='msg_req_0', output_index=0, sequence_number=15, text='Hello! How can I assist you today?', type='response.output_text.done')\n+ResponseContentPartDoneEvent(content_index=0, item_id='msg_req_0', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=None), sequence_number=16, type='response.content_part.done')\n+ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_req_0', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=None)], role='assistant', status='completed', type='message', annotations=[]), output_index=0, sequence_number=17, type='response.output_item.done')\n+ResponseCompletedEvent(response=Response(id='resp_req_0', created_at=1754060400.3718212, error=None, incomplete_details=None, instructions='You are a helpful assistant.', metadata={'foo': 'bar'}, model='Qwen/Qwen2.5-0.5B-Instruct@main', object='response', output=[ResponseOutputMessage(id='msg_req_0', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=None)], role='assistant', status='completed', type='message', annotations=[])], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=18, type='response.completed')\n ```\n \n-Before you can speak into Open WebUI, you need to update its settings to use your server for speech to text (STT) tasks. Launch Open WebUI, and navigate to the audio tab inside the admin settings. If you're using Open WebUI with the default ports, [this link (default)](http://localhost:3000/admin/settings/audio) or [this link (python deployment)](http://localhost:8080/admin/settings/audio) will take you there. Do the following changes there:\n-1. Change the type of \"Speech-to-Text Engine\" to \"OpenAI\";\n-2. Update the address to your server's address -- `http://localhost:8000/v1` by default;\n-3. Type your model of choice into the \"STT Model\" field, e.g. `openai/whisper-large-v3` ([available models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)).\n+</hfoption>\n+</hfoptions>\n+\n+\n+## MCP integration\n+\n+The `transformers serve` server is also an MCP client, so it can interact with MCP tools in agentic use cases. This, of course, requires the use of an LLM that is designed to use tools.\n+\n+> [!TIP]\n+> At the moment, MCP tool usage in `transformers` is limited to the `qwen` family of models.\n+\n+<!-- TODO: example with a minimal python example, and explain that it is possible to pass a full generation config in the request -->\n+\n \n-If you've done everything correctly, the audio tab should look like this\n \n-<h3 align=\"center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_openwebui_stt_settings.png\"/>\n-</h3>\n \n-You're now ready to speak! Open a new chat, utter a few words after hitting the microphone button, and you should see the corresponding text on the chat input after the model transcribes it."
        },
        {
            "sha": "dc53d05a4bffe106b4360ffc4e14ce1230cffc9f",
            "filename": "docs/source/en/tiny_agents.md",
            "status": "added",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Ftiny_agents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/docs%2Fsource%2Fen%2Ftiny_agents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftiny_agents.md?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -0,0 +1,45 @@\n+### `tiny-agents` CLI and MCP Tools\n+\n+To showcase the use of MCP tools, let's see how to integrate the `transformers serve` server with the [`tiny-agents`](https://huggingface.co/blog/python-tiny-agents) CLI.\n+\n+> [!TIP]\n+> Many Hugging Face Spaces can be used as MCP servers, as in this example. You can find all compatible Spaces [here](https://huggingface.co/spaces?filter=mcp-server).\n+\n+The first step to use MCP tools is to let the model know which tools are available. As an example, let's consider a `tiny-agents` configuration file with a reference to an [image generation MCP server](https://evalstate-flux1-schnell.hf.space/).\n+\n+```json\n+{\n+    \"model\": \"Menlo/Jan-nano\",\n+    \"endpointUrl\": \"http://localhost:8000\",\n+    \"servers\": [\n+        {\n+            \"type\": \"sse\",\n+            \"url\": \"https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse\"\n+        }\n+    ]\n+}\n+```\n+\n+You can then launch your `tiny-agents` chat interface with the following command.\n+\n+```bash\n+tiny-agents run path/to/your/config.json\n+```\n+\n+If you have `transformers serve` running in the background, you're ready to use MCP tools from a local model! For instance, here's the example of a chat session with `tiny-agents`:\n+\n+```bash\n+Agent loaded with 1 tools:\n+ • flux1_schnell_infer\n+»  Generate an image of a cat on the moon\n+<Tool req_0_tool_call>flux1_schnell_infer {\"prompt\": \"a cat on the moon\", \"seed\": 42, \"randomize_seed\": true, \"width\": 1024, \"height\": 1024, \"num_inference_steps\": 4}\n+\n+Tool req_0_tool_call\n+[Binary Content: Image image/webp, 57732 bytes]\n+The task is complete and the content accessible to the User\n+Image URL: https://evalstate-flux1-schnell.hf.space/gradio_api/file=/tmp/gradio/3dbddc0e53b5a865ed56a4e3dbdd30f3f61cf3b8aabf1b456f43e5241bd968b8/image.webp\n+380576952\n+\n+I have generated an image of a cat on the moon using the Flux 1 Schnell Image Generator. The image is 1024x1024 pixels and was created with 4 inference steps. Let me know if you would like to make any changes or need further assistance!\n+```\n+"
        },
        {
            "sha": "dbfd87cdf69a4f48d361ea3e1f75f8a99b68caf8",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d47757bf266848deaf79a561ffa71792240021/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d47757bf266848deaf79a561ffa71792240021/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=00d47757bf266848deaf79a561ffa71792240021",
            "patch": "@@ -696,7 +696,6 @@ def get_gen_models(self) -> list[dict[str, any]]:\n             \"HuggingFaceTB/SmolVLM-Instruct\",\n             \"ibm-granite/granite-vision-3.2-2b\",\n             \"Qwen/Qwen2.5-VL-7B-Instruct\",\n-            \"OpenGVLab/InternVL3-1B\",\n         ]\n \n         if HF_HUB_OFFLINE:\n@@ -844,13 +843,18 @@ def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n                         if content[\"type\"] == \"text\":\n                             parsed_message[\"content\"].append(content)\n                         elif content[\"type\"] == \"image_url\":\n-                            image_data = re.sub(\"^data:image/.+;base64,\", \"\", content[\"image_url\"][\"url\"])\n-                            image = Image.open(BytesIO(base64.b64decode(image_data)))\n+                            if \"base64\" in content[\"image_url\"][\"url\"]:\n+                                image_data = re.sub(\"^data:image/.+;base64,\", \"\", content[\"image_url\"][\"url\"])\n+                                image = Image.open(BytesIO(base64.b64decode(image_data)))\n \n-                            file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n-                            image.save(file.name)\n+                                file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n+                                url = file.name\n \n-                            parsed_message[\"content\"].append({\"type\": \"image\", \"url\": file.name})\n+                                image.save(file.name)\n+                            else:\n+                                url = content[\"image_url\"][\"url\"]\n+\n+                            parsed_message[\"content\"].append({\"type\": \"image\", \"url\": url})\n             processor_inputs.append(parsed_message)\n         return processor_inputs\n "
        }
    ],
    "stats": {
        "total": 512,
        "additions": 414,
        "deletions": 98
    }
}