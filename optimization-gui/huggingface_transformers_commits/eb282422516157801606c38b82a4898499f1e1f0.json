{
    "author": "yonigozlan",
    "message": "Add MLlama fast image processor (#41391)\n\n* Merge conflict\n\n* add fast processor\n\n* add fast processor\n\n* make style\n\n* add new convert rgb\n\n* use nested group by shape in mllama fast, add support for multiple inputs in group by shape\n\n* refactor after review\n\n---------\n\nCo-authored-by: Vincent <phamvinh257@gmail.com>",
    "sha": "eb282422516157801606c38b82a4898499f1e1f0",
    "files": [
        {
            "sha": "235c022952e119b628c201d17884e6fe5fe164a7",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -67,7 +67,7 @@ processor = AutoProcessor.from_pretrained(model_id)\n messages = [\n     [\n         {\n-            \"role\": \"user\", \n+            \"role\": \"user\",\n             \"content\": [\n                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n                 {\"type\": \"text\", \"text\": \"What does the image show?\"}\n@@ -113,6 +113,10 @@ print(processor.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] MllamaImageProcessor\n \n+## MllamaImageProcessorFast\n+\n+[[autodoc]] MllamaImageProcessorFast\n+\n ## MllamaForConditionalGeneration\n \n [[autodoc]] MllamaForConditionalGeneration"
        },
        {
            "sha": "a145754d3209e490902d9b6398e38e794f0bbf62",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -221,19 +221,19 @@ def is_fast(self) -> bool:\n \n     def pad(\n         self,\n-        images: \"torch.Tensor\",\n+        images: list[\"torch.Tensor\"],\n         pad_size: SizeDict = None,\n         fill_value: Optional[int] = 0,\n         padding_mode: Optional[str] = \"constant\",\n         return_mask: bool = False,\n         disable_grouping: Optional[bool] = False,\n         **kwargs,\n-    ) -> \"torch.Tensor\":\n+    ) -> Union[tuple[\"torch.Tensor\", \"torch.Tensor\"], \"torch.Tensor\"]:\n         \"\"\"\n         Pads images to `(pad_size[\"height\"], pad_size[\"width\"])` or to the largest size in the batch.\n \n         Args:\n-            images (`torch.Tensor`):\n+            images (`list[torch.Tensor]`):\n                 Images to pad.\n             pad_size (`SizeDict`, *optional*):\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n@@ -248,7 +248,7 @@ def pad(\n                 Whether to disable grouping of images by size.\n \n         Returns:\n-            `torch.Tensor`: The resized image.\n+            `Union[tuple[torch.Tensor, torch.Tensor], torch.Tensor]`: The padded images and pixel masks if `return_mask` is `True`.\n         \"\"\"\n         if pad_size is not None:\n             if not (pad_size.height and pad_size.width):"
        },
        {
            "sha": "c0975b5dfc5903146c6532e2325192acfa8e425c",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 83,
            "deletions": 18,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -797,25 +797,61 @@ def flip_channel_order(\n     return image\n \n \n+def split_to_tiles(images: \"torch.Tensor\", num_tiles_height: int, num_tiles_width: int) -> \"torch.Tensor\":\n+    # Split image into number of required tiles (width x height)\n+    batch_size, num_channels, height, width = images.size()\n+    images = images.view(\n+        batch_size,\n+        num_channels,\n+        num_tiles_height,\n+        height // num_tiles_height,\n+        num_tiles_width,\n+        width // num_tiles_width,\n+    )\n+    # Permute dimensions to reorder the axes\n+    image = images.permute(0, 2, 4, 1, 3, 5).contiguous()\n+    # Reshape into the desired output shape (batch_size * 4, num_channels, width/2, height/2)\n+    image = image.view(\n+        batch_size,\n+        num_tiles_width * num_tiles_height,\n+        num_channels,\n+        height // num_tiles_height,\n+        width // num_tiles_width,\n+    )\n+    return image\n+\n+\n def _cast_tensor_to_float(x):\n     if x.is_floating_point():\n         return x\n     return x.float()\n \n \n-def _group_images_by_shape(nested_images, is_nested: bool = False):\n-    \"\"\"Helper function to flatten a single level of nested image structures and group by shape.\"\"\"\n+def _group_images_by_shape(nested_images, *paired_inputs, is_nested: bool = False):\n+    \"\"\"Helper function to flatten a single level of nested image and batch structures and group by shape.\"\"\"\n     grouped_images = defaultdict(list)\n     grouped_images_index = {}\n-    nested_images = [nested_images] if not is_nested else nested_images\n-    for i, sublist in enumerate(nested_images):\n-        for j, image in enumerate(sublist):\n+    paired_grouped_values = [defaultdict(list) for _ in paired_inputs]\n+\n+    # Normalize inputs to consistent nested structure\n+    normalized_images = [nested_images] if not is_nested else nested_images\n+    normalized_paired = []\n+    for paired_input in paired_inputs:\n+        normalized_paired.append([paired_input] if not is_nested else paired_input)\n+\n+    # Process each image and group by shape\n+    for i, (sublist, *paired_sublists) in enumerate(zip(normalized_images, *normalized_paired)):\n+        for j, (image, *paired_values) in enumerate(zip(sublist, *paired_sublists)):\n             key = (i, j) if is_nested else j\n             shape = image.shape[1:]\n+\n+            # Add to grouped structures\n             grouped_images[shape].append(image)\n+            for paired_index, paired_value in enumerate(paired_values):\n+                paired_grouped_values[paired_index][shape].append(paired_value)\n             grouped_images_index[key] = (shape, len(grouped_images[shape]) - 1)\n \n-    return grouped_images, grouped_images_index\n+    return grouped_images, *paired_grouped_values, grouped_images_index\n \n \n def _reconstruct_nested_structure(indices, processed_images):\n@@ -844,13 +880,35 @@ def _reconstruct_nested_structure(indices, processed_images):\n     return result\n \n \n+def _disable_grouping_output_nested(images, *paired_inputs):\n+    \"\"\"Build the disable_grouping output tuple for a single-level nested structure.\"\"\"\n+    outer_range = range(len(images))\n+    inner_ranges = [range(len(images[i])) for i in outer_range]\n+\n+    # Precompute all (i, j) pairs\n+    ij_pairs = [(i, j) for i in outer_range for j in inner_ranges[i]]\n+\n+    images_dict = {(i, j): images[i][j].unsqueeze(0) for (i, j) in ij_pairs}\n+    paired_dicts = [{(i, j): paired_list[i][j].unsqueeze(0) for (i, j) in ij_pairs} for paired_list in paired_inputs]\n+    index_map = {(i, j): ((i, j), 0) for (i, j) in ij_pairs}\n+    return images_dict, *paired_dicts, index_map\n+\n+\n+def _disable_grouping_output_flat(images, *paired_inputs):\n+    \"\"\"Build the disable_grouping output tuple for a flat list structure.\"\"\"\n+    idx_range = range(len(images))\n+    images_dict = {i: images[i].unsqueeze(0) for i in idx_range}\n+    paired_dicts = [{i: paired_list[i].unsqueeze(0) for i in idx_range} for paired_list in paired_inputs]\n+    index_map = {i: (i, 0) for i in idx_range}\n+    return images_dict, *paired_dicts, index_map\n+\n+\n def group_images_by_shape(\n     images: Union[list[\"torch.Tensor\"], \"torch.Tensor\"],\n-    disable_grouping: bool,\n+    *paired_inputs,\n+    disable_grouping: Optional[bool],\n     is_nested: bool = False,\n-) -> tuple[\n-    dict[tuple[int, int], list[\"torch.Tensor\"]], dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]\n-]:\n+) -> tuple[dict, ...]:\n     \"\"\"\n     Groups images by shape.\n     Returns a dictionary with the shape as key and a list of images with that shape as value,\n@@ -862,15 +920,22 @@ def group_images_by_shape(\n     Args:\n         images (Union[list[\"torch.Tensor\"], \"torch.Tensor\"]):\n             A list of images or a single tensor\n+        *paired_inputs (Any):\n+            Zero or more lists that mirror the structure of `images` (flat list, or list of lists when\n+            `is_nested=True`). Each element is paired 1:1 with the corresponding image so it can be grouped by the\n+            same shape key. These paired values are grouped alongside `images` but are not stacked in the output, so\n+            they do not need to be tensors.\n         disable_grouping (bool):\n             Whether to disable grouping. If None, will be set to True if the images are on CPU, and False otherwise.\n             This choice is based on empirical observations, as detailed here: https://github.com/huggingface/transformers/pull/38157\n         is_nested (bool, *optional*, defaults to False):\n             Whether the images are nested.\n \n     Returns:\n-        tuple[dict[tuple[int, int], list[\"torch.Tensor\"]], dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]]:\n-            - A dictionary with shape as key and list of images with that shape as value\n+        tuple[dict, ...]:\n+            - A dictionary with shape as key and list/batch of images with that shape as value\n+            - Zero or more dictionaries (one per argument in `*paired_inputs`) grouped consistently with `images`; these carry\n+              the corresponding per-item values and are not stacked\n             - A dictionary mapping original indices to (shape, index) tuples\n     \"\"\"\n     # If disable grouping is not explicitly provided, we favor disabling it if the images are on CPU, and enabling it otherwise.\n@@ -880,19 +945,19 @@ def group_images_by_shape(\n \n     if disable_grouping:\n         if is_nested:\n-            return {(i, j): images[i][j].unsqueeze(0) for i in range(len(images)) for j in range(len(images[i]))}, {\n-                (i, j): ((i, j), 0) for i in range(len(images)) for j in range(len(images[i]))\n-            }\n+            return _disable_grouping_output_nested(images, *paired_inputs)\n         else:\n-            return {i: images[i].unsqueeze(0) for i in range(len(images))}, {i: (i, 0) for i in range(len(images))}\n+            return _disable_grouping_output_flat(images, *paired_inputs)\n \n     # Handle single level nested structure\n-    grouped_images, grouped_images_index = _group_images_by_shape(images, is_nested)\n+    grouped_images, *paired_grouped_values, grouped_images_index = _group_images_by_shape(\n+        images, *paired_inputs, is_nested=is_nested\n+    )\n \n     # Stack images with the same shape\n     grouped_images = {shape: torch.stack(images_list, dim=0) for shape, images_list in grouped_images.items()}\n \n-    return grouped_images, grouped_images_index\n+    return grouped_images, *paired_grouped_values, grouped_images_index\n \n \n def reorder_images("
        },
        {
            "sha": "6793af9c564fc96c44be7bc79a3d8737fe3ef7e3",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -134,7 +134,7 @@\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n-            (\"mllama\", (\"MllamaImageProcessor\", None)),\n+            (\"mllama\", (\"MllamaImageProcessor\", \"MllamaImageProcessorFast\")),\n             (\"mm-grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\", \"MobileNetV1ImageProcessorFast\")),\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),"
        },
        {
            "sha": "91bf7388309060d32fed3268e5ab2df79e78ea96",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -43,7 +43,6 @@\n \n \n if is_vision_available():\n-    import PIL\n     from PIL import Image\n \n \n@@ -142,7 +141,7 @@ def convert_to_rgb(image: ImageInput) -> ImageInput:\n         image (Image):\n             The image to convert.\n     \"\"\"\n-    if not isinstance(image, PIL.Image.Image):\n+    if not isinstance(image, Image.Image):\n         return image\n \n     # `image.convert(\"RGB\")` would only work for .jpg images, as it creates a wrong background"
        },
        {
            "sha": "ef44786f7c663aa0d264e18b562803a506957560",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 24,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -28,6 +28,7 @@\n     group_images_by_shape,\n     reorder_images,\n )\n+from ...image_transforms import split_to_tiles\n from ...image_utils import ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import (\n@@ -92,30 +93,6 @@ def get_max_res_without_distortion(\n     return new_height, new_width\n \n \n-def split_to_tiles(images: torch.Tensor, num_tiles_height: int, num_tiles_width: int) -> torch.Tensor:\n-    # Split image into number of required tiles (width x height)\n-    batch_size, num_channels, height, width = images.size()\n-    images = images.view(\n-        batch_size,\n-        num_channels,\n-        num_tiles_height,\n-        height // num_tiles_height,\n-        num_tiles_width,\n-        width // num_tiles_width,\n-    )\n-    # Permute dimensions to reorder the axes\n-    image = images.permute(0, 2, 4, 1, 3, 5).contiguous()\n-    # Reshape into the desired output shape (batch_size * 4, num_channels, width/2, height/2)\n-    image = image.view(\n-        batch_size,\n-        num_tiles_width * num_tiles_height,\n-        num_channels,\n-        height // num_tiles_height,\n-        width // num_tiles_width,\n-    )\n-    return image\n-\n-\n @lru_cache(maxsize=1)\n def find_supported_resolutions(max_num_chunks: int, patch_size: SizeDict) -> torch.Tensor:\n     \"\"\""
        },
        {
            "sha": "76609a5f2009ffb6b3440e9ab5146808a09a60dc",
            "filename": "src/transformers/models/mllama/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fmllama%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fmllama%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2F__init__.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_mllama import *\n     from .image_processing_mllama import *\n+    from .image_processing_mllama_fast import *\n     from .modeling_mllama import *\n     from .processing_mllama import *\n else:"
        },
        {
            "sha": "94ddf741fe359618a7bd90c75d5f8e16c4211a82",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 27,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -43,7 +43,6 @@\n \n \n if is_vision_available():\n-    import PIL\n     from PIL import Image\n \n \n@@ -407,30 +406,6 @@ def pack_images(\n     return stacked_images, all_num_tiles\n \n \n-def pack_aspect_ratios(aspect_ratios: list[list[tuple[int, int]]], pad_value: int = 1) -> np.ndarray:\n-    \"\"\"\n-    Stack a list of aspect ratios into a numpy array.\n-\n-    Args:\n-        aspect_ratios (`list[list[tuple[int, int]]]`):\n-            A list of aspect ratios.\n-        pad_value (`int`, *optional*, defaults to 1):\n-            The value to pad the aspect ratios with.\n-\n-    Returns:\n-        `np.ndarray`:\n-            The aspect ratios stacked into a numpy array with shape (batch_size, max_num_images, 2).\n-    \"\"\"\n-    batch_size = len(aspect_ratios)\n-    max_num_images = max(len(row) for row in aspect_ratios)\n-\n-    aspect_ratios_stacked = np.full((batch_size, max_num_images, 2), pad_value, dtype=np.int64)\n-    for i, row in enumerate(aspect_ratios):\n-        if len(row) > 0:\n-            aspect_ratios_stacked[i, : len(row)] = np.array(row)\n-    return aspect_ratios_stacked\n-\n-\n def convert_aspect_ratios_to_ids(aspect_ratios: list[list[tuple[int, int]]], max_image_tiles: int) -> np.ndarray:\n     \"\"\"\n     Convert aspect ratio tuples to unique ids.\n@@ -511,7 +486,7 @@ def convert_to_rgb(image: ImageInput) -> ImageInput:\n         image (Image):\n             The image to convert.\n     \"\"\"\n-    if not isinstance(image, PIL.Image.Image):\n+    if not isinstance(image, Image.Image):\n         return image\n \n     # `image.convert(\"RGB\")` would only work for .jpg images, as it creates a wrong background\n@@ -718,7 +693,7 @@ def preprocess(\n             # iterate over images in a batch sample\n             for image in images:\n                 # default PIL images to channels_last\n-                if input_data_format is None and isinstance(image, PIL.Image.Image):\n+                if input_data_format is None and isinstance(image, Image.Image):\n                     input_data_format = ChannelDimension.LAST\n \n                 # convert to numpy array for processing"
        },
        {
            "sha": "da812a32d7624601755ff29c6c7efb1f097f161a",
            "filename": "src/transformers/models/mllama/image_processing_mllama_fast.py",
            "status": "added",
            "additions": 402,
            "deletions": 0,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama_fast.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -0,0 +1,402 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from PIL import Image\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    ImageInput,\n+    SizeDict,\n+    TensorType,\n+    Unpack,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_transforms import split_to_tiles\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    PILImageResampling,\n+    make_nested_list_of_images,\n+)\n+from ...utils import auto_docstring\n+from .image_processing_mllama import (\n+    MllamaImageProcessorKwargs,\n+    get_all_supported_aspect_ratios,\n+    get_image_size_fit_to_canvas,\n+    get_optimal_tiled_canvas,\n+)\n+\n+\n+def _validate_size(size: SizeDict) -> None:\n+    if not (size.height and size.width):\n+        raise ValueError(f\"Argument `size` must be a dictionary with keys 'height' and 'width'. Got: {size}\")\n+    if size.height != size.width:\n+        raise ValueError(f\"Argument `size` must have the same height and width, got {size}\")\n+\n+\n+def _validate_mllama_preprocess_arguments(do_resize, size, do_pad, max_image_tiles):\n+    if not do_pad:\n+        raise ValueError(\"MllamaImageProcessor doesn't support `do_pad=False` mode.\")\n+    if not do_resize:\n+        raise ValueError(\"MllamaImageProcessor doesn't support `do_resize=False` mode.\")\n+    if max_image_tiles is None or max_image_tiles <= 0:\n+        raise ValueError(f\"MllamaImageProcessor `max_image_tiles` must be a positive integer, got {max_image_tiles}.\")\n+    _validate_size(size)\n+\n+\n+def build_aspect_ratio_mask(aspect_ratios: list[tuple[int, int]], max_image_tiles: int) -> \"torch.Tensor\":\n+    \"\"\"\n+    Builds a mask for the aspect ratios of the images.\n+\n+    Args:\n+        aspect_ratios (`List[List[Tuple[int, int]]]`):\n+            A list of lists containing aspect ratios for each image in the batch.\n+            Each aspect ratio is represented as a tuple of (width, height) in terms of number of tiles.\n+        max_image_tiles (`int`):\n+            The maximum number of tiles any image can be split into.\n+\n+    Returns:\n+        `torch.Tensor`: A 3D torch.Tensor of shape (batch_size, max_num_images, max_image_tiles).\n+            The mask contains 1s for valid tiles and 0s for padding.\n+    \"\"\"\n+    batch_size = len(aspect_ratios)\n+    max_num_images = max(len(row) for row in aspect_ratios)\n+\n+    aspect_ratio_mask = torch.zeros((batch_size, max_num_images, max_image_tiles), dtype=torch.long)\n+\n+    # Set the first tile to 1 for all aspect ratios\n+    # because in original implementation aspect ratios are padded with (1, 1),\n+    # but original code examples are not built to handle batches, so we might remove it later\n+    aspect_ratio_mask[:, :, 0] = 1\n+\n+    # Set the aspect ratio mask for the rest of the tiles\n+    for i, sample_aspect_ratios in enumerate(aspect_ratios):\n+        for j, (num_tiles_w, num_tiles_h) in enumerate(sample_aspect_ratios):\n+            aspect_ratio_mask[i, j, : num_tiles_w * num_tiles_h] = 1\n+\n+    return aspect_ratio_mask\n+\n+\n+def pad_batches_and_tiles(\n+    batch_images: list[list[\"torch.Tensor\"]],\n+    max_image_tiles: int,\n+) -> tuple[\"torch.Tensor\", list[list[int]]]:\n+    \"\"\"\n+    Stack a list of lists of images with variable lengths into a torch.Tensor, applying zero padding as needed.\n+    Each list in the input represents a batch sample, and each image within a list is expected to be\n+    pre-split into tiles. The resulting array will have a shape of\n+    (batch_size, max_num_images, max_image_tiles, channels, tile_height, tile_width).\n+\n+    Args:\n+        batch_images (`List[List[torch.Tensor]]`):\n+            A list of lists of image tiles. Each inner list represents\n+            a batch sample containing multiple images, where each image is pre-split into tiles.\n+            The shape of each tile array is (num_tiles, channels, tile_height, tile_width).\n+        max_image_tiles (int):\n+            The maximum number of tiles any image was potantially split.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, List[List[int]]]`: A tuple containing:\n+            - stacked_images (`torch.Tensor`):\n+                A numpy array of stacked images with shape\n+                (batch_size, max_num_images, max_image_tiles, channels, tile_height, tile_width).\n+            - all_num_tiles (`List[List[int]]`):\n+                A list of lists containing the number of tiles\n+                for each image in each batch sample.\n+    \"\"\"\n+\n+    # Determine output shape\n+    batch_size = len(batch_images)\n+    max_num_images = max(len(images) for images in batch_images)\n+    shapes = [image.shape for images in batch_images for image in images]\n+    _, channels, tile_height, tile_width = shapes[0]\n+\n+    # Initialize the stacked images array with zeros\n+    stacked_images = torch.zeros(\n+        (batch_size, max_num_images, max_image_tiles, channels, tile_height, tile_width),\n+        dtype=torch.float32,\n+    )\n+\n+    # Fill the stacked images array with the tiled images from the batch\n+    all_num_tiles = []\n+    for i, images in enumerate(batch_images):\n+        num_sample_tiles = []\n+        for j, image in enumerate(images):\n+            num_tiles = image.shape[0]\n+            stacked_images[i, j, :num_tiles] = image\n+            num_sample_tiles.append(num_tiles)\n+        all_num_tiles.append(num_sample_tiles)\n+\n+    return stacked_images, all_num_tiles\n+\n+\n+def convert_aspect_ratios_to_ids(aspect_ratios: list[list[tuple[int, int]]], max_image_tiles: int) -> \"torch.Tensor\":\n+    \"\"\"\n+    Convert aspect ratio tuples to unique ids.\n+\n+    For batch padding we use 0, because there might be different number of images in each batch.\n+    The aspect ratio ids start from 1, with 1 corresponding to the first supported aspect ratio.\n+\n+    Args:\n+        aspect_ratios (`List[List[Tuple[int, int]]]`):\n+            A list of aspect ratios for each image in the batch.\n+        max_image_tiles (`int`):\n+            The maximum number of tiles any image can be split into.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            The aspect ratios ids as a numpy array with shape (batch_size, max_num_images).\n+            Each id corresponds to the index of the aspect ratio in the list of supported aspect ratios,\n+            offset by 1 (so 0 can be used for padding).\n+    \"\"\"\n+\n+    batch_size = len(aspect_ratios)\n+    max_num_images = max(len(row) for row in aspect_ratios)\n+    supported_aspect_ratios = get_all_supported_aspect_ratios(max_image_tiles)\n+\n+    aspect_ratios_ids = torch.zeros((batch_size, max_num_images), dtype=torch.long)\n+    for i, sample_aspect_ratios in enumerate(aspect_ratios):\n+        for j, (num_tiles_h, num_tiles_w) in enumerate(sample_aspect_ratios):\n+            aspect_ratios_ids[i, j] = supported_aspect_ratios.index((num_tiles_h, num_tiles_w)) + 1\n+    return aspect_ratios_ids\n+\n+\n+# Copied from transformers.models.idefics2.image_processing_idefics2.convert_to_rgb\n+def convert_to_rgb(image: ImageInput) -> ImageInput:\n+    \"\"\"\n+    Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image\n+    as is.\n+    Args:\n+        image (Image):\n+            The image to convert.\n+    \"\"\"\n+    if not isinstance(image, Image.Image):\n+        return image\n+\n+    # `image.convert(\"RGB\")` would only work for .jpg images, as it creates a wrong background\n+    # for transparent images. The call to `alpha_composite` handles this case\n+    if image.mode == \"RGB\":\n+        return image\n+\n+    image_rgba = image.convert(\"RGBA\")\n+    background = Image.new(\"RGBA\", image_rgba.size, (255, 255, 255))\n+    alpha_composite = Image.alpha_composite(background, image_rgba)\n+    alpha_composite = alpha_composite.convert(\"RGB\")\n+    return alpha_composite\n+\n+\n+@auto_docstring\n+class MllamaImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_pad = True\n+    max_image_tiles = 4\n+    valid_kwargs = MllamaImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[MllamaImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[MllamaImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(self, images: ImageInput, expected_ndims: int = 3) -> ImageInput:\n+        \"\"\"\n+        Prepare a nested images structure for processing.\n+        \"\"\"\n+        images = self.fetch_images(images)\n+        return make_nested_list_of_images(images, expected_ndims=expected_ndims)\n+\n+    def convert_to_rgb(\n+        self,\n+        image: ImageInput,\n+    ) -> ImageInput:\n+        \"\"\"\n+        Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image\n+        as is.\n+        Args:\n+            image (ImageInput):\n+                The image to convert.\n+\n+        Returns:\n+            ImageInput: The converted image.\n+        \"\"\"\n+        return convert_to_rgb(image)\n+\n+    def pad(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: dict[str, int],\n+        aspect_ratio: tuple[int, int],\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pad an image to the `size` x `aspect_ratio`. For example, if size is {height: 224, width: 224} and aspect ratio is\n+        (1, 2), the image will be padded to 224x448.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+            aspect_ratio (`Tuple[int, int]`):\n+                The aspect ratio of the image.\n+\n+        Returns:\n+            `torch.Tensor`: The padded image.\n+        \"\"\"\n+\n+        image_height, image_width = image.shape[-2:]\n+        num_tiles_height, num_tiles_width = aspect_ratio\n+        padded_height = num_tiles_height * size.height\n+        padded_width = num_tiles_width * size.width\n+        pad_size = (0, 0, padded_width - image_width, padded_height - image_height)\n+\n+        image = F.pad(\n+            image,\n+            pad_size,\n+            fill=0,\n+        )\n+\n+        return image\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        max_image_tiles: int,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+    ) -> Union[\"torch.Tensor\", tuple[int, int]]:\n+        \"\"\"\n+        Resizes an image to fit within a tiled canvas while maintaining its aspect ratio.\n+        The optimal canvas size is calculated based on the maximum number of tiles and the tile size.\n+\n+        The function first determines the best tile arrangement for the image, then resizes the image\n+        to fit within this canvas. The resized image and the number of tiles along the height and width\n+        dimensions are returned.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+            max_image_tiles (`int`):\n+                The maximum number of tiles to split the image into.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                Resampling filter to use when resizing the image.\n+\n+        Returns:\n+            `Union[np.ndarray, Tuple[int, int]]`: The resized image and a tuple containing the number of tiles\n+            along the height and width dimensions.\n+        \"\"\"\n+\n+        image_height, image_width = image.shape[-2:]\n+        tile_size = size.height\n+\n+        canvas_height, canvas_width = get_optimal_tiled_canvas(\n+            image_height=image_height,\n+            image_width=image_width,\n+            max_image_tiles=max_image_tiles,\n+            tile_size=tile_size,\n+        )\n+        num_tiles_height = canvas_height // tile_size\n+        num_tiles_width = canvas_width // tile_size\n+\n+        new_height, new_width = get_image_size_fit_to_canvas(\n+            image_height=image_height,\n+            image_width=image_width,\n+            canvas_height=canvas_height,\n+            canvas_width=canvas_width,\n+            tile_size=tile_size,\n+        )\n+\n+        image = F.resize(image, (new_height, new_width), interpolation=interpolation, antialias=antialias)\n+\n+        return image, (num_tiles_height, num_tiles_width)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        max_image_tiles: Optional[int],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: Optional[bool],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        split_images_grouped = {}\n+        aspect_ratio_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            stacked_images, aspect_ratio = self.resize(\n+                image=stacked_images, size=size, interpolation=interpolation, max_image_tiles=max_image_tiles\n+            )\n+            stacked_images = self.pad(\n+                image=stacked_images,\n+                size=size,\n+                aspect_ratio=aspect_ratio,\n+            )\n+            num_tiles_height, num_tiles_width = aspect_ratio\n+            aspect_ratio_grouped[shape] = [aspect_ratio] * len(stacked_images)\n+            # same aspect ratio for all images in the batch\n+            split_images = split_to_tiles(stacked_images, num_tiles_height, num_tiles_width)\n+\n+            # Fused rescale and normalize\n+            split_images = self.rescale_and_normalize(\n+                split_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            split_images_grouped[shape] = split_images\n+\n+        split_images = reorder_images(split_images_grouped, grouped_images_index, is_nested=True)\n+        aspect_ratios = reorder_images(aspect_ratio_grouped, grouped_images_index, is_nested=True)\n+\n+        split_images, num_tiles = pad_batches_and_tiles(split_images, max_image_tiles)\n+\n+        aspect_ratio_ids = convert_aspect_ratios_to_ids(aspect_ratios, max_image_tiles=max_image_tiles)\n+        aspect_ratio_mask = build_aspect_ratio_mask(aspect_ratios, max_image_tiles=max_image_tiles)\n+\n+        encoded_inputs = BatchFeature(\n+            data={\n+                \"pixel_values\": split_images,\n+                \"aspect_ratio_ids\": aspect_ratio_ids,\n+                \"aspect_ratio_mask\": aspect_ratio_mask,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+        encoded_inputs[\"num_tiles\"] = num_tiles\n+\n+        return encoded_inputs\n+\n+\n+__all__ = [\"MllamaImageProcessorFast\"]"
        },
        {
            "sha": "fddc8fb94bff87e9766dbae7c7b3ac1d7af793b4",
            "filename": "tests/models/mllama/test_image_processing_mllama.py",
            "status": "modified",
            "additions": 215,
            "deletions": 199,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb282422516157801606c38b82a4898499f1e1f0/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb282422516157801606c38b82a4898499f1e1f0/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py?ref=eb282422516157801606c38b82a4898499f1e1f0",
            "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin\n \n@@ -28,6 +28,9 @@\n \n     from transformers import MllamaImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import MllamaImageProcessorFast\n+\n \n if is_torch_available():\n     import torch\n@@ -151,6 +154,7 @@ def expected_output_image_shape(self, images):\n @require_vision\n class MllamaImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = MllamaImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = MllamaImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -161,120 +165,127 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"max_image_tiles\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"max_image_tiles\"))\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for sample_images in image_inputs:\n-            for image in sample_images:\n-                self.assertIsInstance(image, np.ndarray)\n-\n-        expected_output_image_shape = (\n-            max(len(images) for images in image_inputs),\n-            self.image_processor_tester.max_image_tiles,\n-            self.image_processor_tester.num_channels,\n-            self.image_processor_tester.size[\"height\"],\n-            self.image_processor_tester.size[\"width\"],\n-        )\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for sample_images in image_inputs:\n+                for image in sample_images:\n+                    self.assertIsInstance(image, np.ndarray)\n+\n+            expected_output_image_shape = (\n+                max(len(images) for images in image_inputs),\n+                self.image_processor_tester.max_image_tiles,\n+                self.image_processor_tester.num_channels,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            )\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for images in image_inputs:\n-            for image in images:\n-                self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for images in image_inputs:\n+                for image in images:\n+                    self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n \n     def test_call_channels_last(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-\n-        # a white 1x1 pixel RGB image\n-        image_inputs = [[np.full(shape=(1, 1, 3), fill_value=1.0, dtype=float)]]\n-        encoded_images = image_processing(\n-            image_inputs, return_tensors=\"pt\", input_data_format=\"channels_last\"\n-        ).pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+\n+            # a white 1x1 pixel RGB image\n+            image_inputs = [[np.full(shape=(1, 1, 3), fill_value=1.0, dtype=float)]]\n+            encoded_images = image_processing(\n+                image_inputs, return_tensors=\"pt\", input_data_format=\"channels_last\"\n+            ).pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n \n     def test_ambiguous_channel_pil_image(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n \n-        image_inputs = [[Image.new(\"RGB\", (1, 1))], [Image.new(\"RGB\", (100, 1))]]\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+            image_inputs = [[Image.new(\"RGB\", (1, 1))], [Image.new(\"RGB\", (100, 1))]]\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n \n     def test_resize_impractical_aspect_ratio(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # Ensure that no error is raised even if the aspect ratio is impractical\n-        image_inputs = [[Image.new(\"RGB\", (9999999, 1))]]\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # Ensure that no error is raised even if the aspect ratio is impractical\n+            image_inputs = [[Image.new(\"RGB\", (9999999, 1))]]\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-\n-        for images in image_inputs:\n-            for image in images:\n-                self.assertIsInstance(image, torch.Tensor)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test batched\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            tuple(encoded_images.shape),\n-            (self.image_processor_tester.batch_size, *expected_output_image_shape),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for images in image_inputs:\n+                for image in images:\n+                    self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+            )\n \n     def test_call_numpy_4_channels(self):\n         self.skipTest(\"4 channels input is not supported yet\")\n@@ -284,100 +295,105 @@ def get_empty_tiles(pixel_values):\n             # image has shape batch_size, max_num_images, max_image_tiles, num_channels, height, width\n             # we want to get a binary mask of shape batch_size, max_num_images, max_image_tiles\n             # of empty tiles, i.e. tiles that are completely zero\n-            return np.all(pixel_values == 0, axis=(3, 4, 5))\n-\n-        image_processor_dict = {**self.image_processor_dict, \"size\": {\"height\": 50, \"width\": 50}, \"max_image_tiles\": 4}\n-        image_processor = self.image_processing_class(**image_processor_dict)\n-\n-        # image fits 2x2 tiles grid (width x height)\n-        image = Image.new(\"RGB\", (80, 95))\n-        inputs = image_processor(image, return_tensors=\"np\")\n-        pixel_values = inputs.pixel_values\n-        empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n-        self.assertEqual(empty_tiles, [False, False, False, False])\n-        aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n-        self.assertEqual(aspect_ratio_ids, 6)\n-        aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n-        self.assertEqual(aspect_ratio_mask, [1, 1, 1, 1])\n-\n-        # image fits 3x1 grid (width x height)\n-        image = Image.new(\"RGB\", (101, 50))\n-        inputs = image_processor(image, return_tensors=\"np\")\n-        pixel_values = inputs.pixel_values\n-        empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n-        self.assertEqual(empty_tiles, [False, False, False, True])\n-        aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n-        self.assertEqual(aspect_ratio_ids, 3)\n-        num_tiles = inputs.aspect_ratio_mask[0, 0].sum()\n-        self.assertEqual(num_tiles, 3)\n-        aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n-        self.assertEqual(aspect_ratio_mask, [1, 1, 1, 0])\n-\n-        # image fits 1x1 grid (width x height)\n-        image = Image.new(\"RGB\", (20, 39))\n-        inputs = image_processor(image, return_tensors=\"np\")\n-        pixel_values = inputs.pixel_values\n-        empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n-        self.assertEqual(empty_tiles, [False, True, True, True])\n-        aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n-        self.assertEqual(aspect_ratio_ids, 1)\n-        aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n-        self.assertEqual(aspect_ratio_mask, [1, 0, 0, 0])\n-\n-        # image fits 2x1 grid (width x height)\n-        image = Image.new(\"RGB\", (51, 20))\n-        inputs = image_processor(image, return_tensors=\"np\")\n-        pixel_values = inputs.pixel_values\n-        empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n-        self.assertEqual(empty_tiles, [False, False, True, True])\n-        aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n-        self.assertEqual(aspect_ratio_ids, 2)\n-        aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n-        self.assertEqual(aspect_ratio_mask, [1, 1, 0, 0])\n-\n-        # image is greater than 2x2 tiles grid (width x height)\n-        image = Image.new(\"RGB\", (150, 150))\n-        inputs = image_processor(image, return_tensors=\"np\")\n-        pixel_values = inputs.pixel_values\n-        empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n-        self.assertEqual(empty_tiles, [False, False, False, False])\n-        aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n-        self.assertEqual(aspect_ratio_ids, 6)  # (2 - 1) * 4 + 2 = 6\n-        aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n-        self.assertEqual(aspect_ratio_mask, [1, 1, 1, 1])\n-\n-        # batch of images\n-        image1 = Image.new(\"RGB\", (80, 95))\n-        image2 = Image.new(\"RGB\", (101, 50))\n-        image3 = Image.new(\"RGB\", (23, 49))\n-        inputs = image_processor([[image1], [image2, image3]], return_tensors=\"np\")\n-        pixel_values = inputs.pixel_values\n-        empty_tiles = get_empty_tiles(pixel_values).tolist()\n-        expected_empty_tiles = [\n-            # sample 1 with 1 image 2x2 grid\n-            [\n-                [False, False, False, False],\n-                [True, True, True, True],  # padding\n-            ],\n-            # sample 2\n-            [\n-                [False, False, False, True],  # 3x1\n-                [False, True, True, True],  # 1x1\n-            ],\n-        ]\n-        self.assertEqual(empty_tiles, expected_empty_tiles)\n-        aspect_ratio_ids = inputs.aspect_ratio_ids.tolist()\n-        expected_aspect_ratio_ids = [[6, 0], [3, 1]]\n-        self.assertEqual(aspect_ratio_ids, expected_aspect_ratio_ids)\n-        aspect_ratio_mask = inputs.aspect_ratio_mask.tolist()\n-        expected_aspect_ratio_mask = [\n-            [\n-                [1, 1, 1, 1],\n-                [1, 0, 0, 0],\n-            ],\n-            [\n-                [1, 1, 1, 0],\n-                [1, 0, 0, 0],\n-            ],\n-        ]\n-        self.assertEqual(aspect_ratio_mask, expected_aspect_ratio_mask)\n+            return torch.all(pixel_values == 0, dim=(3, 4, 5))\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor_dict = {\n+                **self.image_processor_dict,\n+                \"size\": {\"height\": 50, \"width\": 50},\n+                \"max_image_tiles\": 4,\n+            }\n+            image_processor = image_processing_class(**image_processor_dict)\n+\n+            # image fits 2x2 tiles grid (width x height)\n+            image = Image.new(\"RGB\", (80, 95))\n+            inputs = image_processor(image, return_tensors=\"pt\")\n+            pixel_values = inputs.pixel_values\n+            empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n+            self.assertEqual(empty_tiles, [False, False, False, False])\n+            aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n+            self.assertEqual(aspect_ratio_ids, 6)\n+            aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n+            self.assertEqual(aspect_ratio_mask, [1, 1, 1, 1])\n+\n+            # image fits 3x1 grid (width x height)\n+            image = Image.new(\"RGB\", (101, 50))\n+            inputs = image_processor(image, return_tensors=\"pt\")\n+            pixel_values = inputs.pixel_values\n+            empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n+            self.assertEqual(empty_tiles, [False, False, False, True])\n+            aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n+            self.assertEqual(aspect_ratio_ids, 3)\n+            num_tiles = inputs.aspect_ratio_mask[0, 0].sum()\n+            self.assertEqual(num_tiles, 3)\n+            aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n+            self.assertEqual(aspect_ratio_mask, [1, 1, 1, 0])\n+\n+            # image fits 1x1 grid (width x height)\n+            image = Image.new(\"RGB\", (20, 39))\n+            inputs = image_processor(image, return_tensors=\"pt\")\n+            pixel_values = inputs.pixel_values\n+            empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n+            self.assertEqual(empty_tiles, [False, True, True, True])\n+            aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n+            self.assertEqual(aspect_ratio_ids, 1)\n+            aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n+            self.assertEqual(aspect_ratio_mask, [1, 0, 0, 0])\n+\n+            # image fits 2x1 grid (width x height)\n+            image = Image.new(\"RGB\", (51, 20))\n+            inputs = image_processor(image, return_tensors=\"pt\")\n+            pixel_values = inputs.pixel_values\n+            empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n+            self.assertEqual(empty_tiles, [False, False, True, True])\n+            aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n+            self.assertEqual(aspect_ratio_ids, 2)\n+            aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n+            self.assertEqual(aspect_ratio_mask, [1, 1, 0, 0])\n+\n+            # image is greater than 2x2 tiles grid (width x height)\n+            image = Image.new(\"RGB\", (150, 150))\n+            inputs = image_processor(image, return_tensors=\"pt\")\n+            pixel_values = inputs.pixel_values\n+            empty_tiles = get_empty_tiles(pixel_values)[0, 0].tolist()\n+            self.assertEqual(empty_tiles, [False, False, False, False])\n+            aspect_ratio_ids = inputs.aspect_ratio_ids[0, 0]\n+            self.assertEqual(aspect_ratio_ids, 6)  # (2 - 1) * 4 + 2 = 6\n+            aspect_ratio_mask = inputs.aspect_ratio_mask[0, 0].tolist()\n+            self.assertEqual(aspect_ratio_mask, [1, 1, 1, 1])\n+\n+            # batch of images\n+            image1 = Image.new(\"RGB\", (80, 95))\n+            image2 = Image.new(\"RGB\", (101, 50))\n+            image3 = Image.new(\"RGB\", (23, 49))\n+            inputs = image_processor([[image1], [image2, image3]], return_tensors=\"pt\")\n+            pixel_values = inputs.pixel_values\n+            empty_tiles = get_empty_tiles(pixel_values).tolist()\n+            expected_empty_tiles = [\n+                # sample 1 with 1 image 2x2 grid\n+                [\n+                    [False, False, False, False],\n+                    [True, True, True, True],  # padding\n+                ],\n+                # sample 2\n+                [\n+                    [False, False, False, True],  # 3x1\n+                    [False, True, True, True],  # 1x1\n+                ],\n+            ]\n+            self.assertEqual(empty_tiles, expected_empty_tiles)\n+            aspect_ratio_ids = inputs.aspect_ratio_ids.tolist()\n+            expected_aspect_ratio_ids = [[6, 0], [3, 1]]\n+            self.assertEqual(aspect_ratio_ids, expected_aspect_ratio_ids)\n+            aspect_ratio_mask = inputs.aspect_ratio_mask.tolist()\n+            expected_aspect_ratio_mask = [\n+                [\n+                    [1, 1, 1, 1],\n+                    [1, 0, 0, 0],\n+                ],\n+                [\n+                    [1, 1, 1, 0],\n+                    [1, 0, 0, 0],\n+                ],\n+            ]\n+            self.assertEqual(aspect_ratio_mask, expected_aspect_ratio_mask)"
        }
    ],
    "stats": {
        "total": 991,
        "additions": 715,
        "deletions": 276
    }
}