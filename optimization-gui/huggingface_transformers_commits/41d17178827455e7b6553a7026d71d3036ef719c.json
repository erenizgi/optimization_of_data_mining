{
    "author": "Cyrilvallez",
    "message": "New DynamicSlidingWindowLayer & associated Cache (#40039)\n\n* start adding the layer\n\n* style\n\n* improve\n\n* modular\n\n* fix\n\n* fix\n\n* improve\n\n* generate integration\n\n* comment\n\n* remove old one\n\n* remove\n\n* fix\n\n* fix\n\n* fix\n\n* fix all recompiles\n\n* fix\n\n* doc\n\n* fix\n\n* add text config check\n\n* fix encoderdecoder cache\n\n* add it for all models with sliding/hybrid support\n\n* revert\n\n* start fixing\n\n* prophetnet\n\n* fsmt\n\n* fix ddp_data\n\n* add test for mistral\n\n* improve mistral test and add gemma2 test\n\n* docstrings",
    "sha": "41d17178827455e7b6553a7026d71d3036ef719c",
    "files": [
        {
            "sha": "4887c88826e5e4be520fd09740c6fd31023b29a6",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 331,
            "deletions": 254,
            "changes": 585,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -36,23 +36,26 @@ class CacheLayerMixin(ABC):\n     def __init__(self):\n         self.keys, self.values = None, None\n \n+    def __repr__(self):\n+        return f\"{self.__class__.__name__}\"\n+\n+    @abstractmethod\n+    def lazy_initialization(self, key_states: torch.Tensor): ...\n+\n     @abstractmethod\n     def update(\n         self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = None\n     ) -> tuple[torch.Tensor, torch.Tensor]: ...\n \n     @abstractmethod\n-    def lazy_initialization(self, key_states: torch.Tensor): ...\n+    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...\n \n     @abstractmethod\n-    def get_seq_length(self, cache_position=None) -> int: ...\n+    def get_seq_length(self) -> int: ...\n \n     @abstractmethod\n     def get_max_cache_shape(self) -> int: ...\n \n-    @abstractmethod\n-    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...\n-\n     def offload(self):\n         \"\"\"Offload this layer's data to CPU device.\"\"\"\n         if self.keys is not None:\n@@ -70,23 +73,21 @@ def reset(self) -> None:\n         if self.keys is not None:\n             self.keys.zero_()\n             self.values.zero_()\n+        # This attribute is set on several Layers\n+        if hasattr(self, \"cumulative_length\"):\n+            self.cumulative_length = 0\n \n-    def reorder_cache(self, beam_idx: torch.LongTensor) -> tuple[torch.Tensor, torch.Tensor]:\n+    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n         \"\"\"Reorders this layer's cache for beam search.\"\"\"\n-        if self.keys.numel():\n-            device = self.keys.device\n-            self.keys = self.keys.index_select(0, beam_idx.to(device))\n-        if self.values.numel():\n-            device = self.values.device\n-            self.values = self.values.index_select(0, beam_idx.to(device))\n+        if self.get_seq_length() > 0:\n+            self.keys = self.keys.index_select(0, beam_idx.to(self.keys.device))\n+            self.values = self.values.index_select(0, beam_idx.to(self.values.device))\n \n \n class DynamicLayer(CacheLayerMixin):\n     \"\"\"\n     A cache layer that grows dynamically as more tokens are generated. This is the default for generative models.\n-    It stores the Key and Value states as tensors with shape `[batch_size, num_heads, seq_len, head_dim]`.\n-\n-    See `CacheLayerMixin` for details on common methods that are implemented by all cache layers.\n+    It stores the key and value states as tensors of shape `[batch_size, num_heads, seq_len, head_dim]`.\n     \"\"\"\n \n     is_sliding = False\n@@ -103,18 +104,15 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Updates the cache with the new `key_states` and `value_states`.\n+        Update the key and value caches in-place, and return the necesary kes and value states.\n \n-        Parameters:\n-            key_states (`torch.Tensor`):\n-                The new key states to cache.\n-            value_states (`torch.Tensor`):\n-                The new value states to cache.\n-            cache_kwargs (`dict[str, Any]`, *optional*):\n-                Additional arguments for the cache subclass. No additional arguments are used in `DynamicLayer`.\n+        Args:\n+            key_states (`torch.Tensor`): The new key states to cache.\n+            value_states (`torch.Tensor`): The new value states to cache.\n+            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n \n-        Return:\n-            A tuple containing the updated key and value states.\n+        Returns:\n+            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n         if self.keys is None:\n@@ -124,7 +122,15 @@ def update(\n         self.values = torch.cat([self.values, value_states], dim=-2)\n         return self.keys, self.values\n \n-    def get_seq_length(self, cache_position=None) -> int:\n+    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, kv_offset\n+\n+    def get_seq_length(self) -> int:\n         \"\"\"Returns the sequence length of the cached states.\"\"\"\n         if self.keys is None or self.keys.numel() == 0:\n             return 0\n@@ -134,75 +140,119 @@ def get_max_cache_shape(self) -> int:\n         \"\"\"Returns the maximum sequence length of the cache object. DynamicLayer does not have a maximum length.\"\"\"\n         return -1\n \n-    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n-        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n-        if self.keys is not None and self.keys.numel():\n-            self.keys = self.keys.index_select(0, beam_idx.to(self.keys.device))\n-            self.values = self.values.index_select(0, beam_idx.to(self.values.device))\n-\n     def crop(self, max_length: int) -> None:\n         \"\"\"\n-        Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be\n-        negative to remove `max_length` tokens.\n+        Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be negative\n+        to remove `max_length` tokens.\n         \"\"\"\n         if max_length < 0:\n             max_length = self.get_seq_length() - abs(max_length)\n \n         if self.get_seq_length() <= max_length:\n             return\n \n-        if self.keys is not None and self.keys.numel():\n-            self.keys = self.keys[..., :max_length, :]\n-            self.values = self.values[..., :max_length, :]\n+        self.keys = self.keys[..., :max_length, :]\n+        self.values = self.values[..., :max_length, :]\n \n     def batch_repeat_interleave(self, repeats: int) -> None:\n         \"\"\"Repeat the cache `repeats` times in the batch dimension.\"\"\"\n-        if self.keys is not None and self.keys.numel():\n+        if self.get_seq_length() > 0:\n             self.keys = self.keys.repeat_interleave(repeats, dim=0)\n             self.values = self.values.repeat_interleave(repeats, dim=0)\n \n     def batch_select_indices(self, indices: torch.Tensor) -> None:\n         \"\"\"Only keep the `indices` in the batch dimension of the cache.\"\"\"\n-        if self.keys is not None and self.keys.numel():\n+        if self.get_seq_length() > 0:\n             self.keys = self.keys[indices, ...]\n             self.values = self.values[indices, ...]\n \n-    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n-        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n-        kv_offset = 0\n-        query_length = cache_position.shape[0]\n-        past_seen_tokens = self.get_seq_length()\n-        kv_length = query_length + past_seen_tokens\n-        return kv_length, kv_offset\n \n-    @classmethod\n-    def from_tensors(cls, keys: torch.Tensor, values: torch.Tensor) -> \"DynamicLayer\":\n+class DynamicSlidingWindowLayer(DynamicLayer):\n+    \"\"\"\n+    A cache layer that grows dynamically as more tokens are generated, up until the sliding window size.\n+    It stores the key and value states as tensors of shape `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n+    \"\"\"\n+\n+    is_sliding = True\n+\n+    def __init__(self, sliding_window: int):\n+        super().__init__()\n+        self.sliding_window = sliding_window\n+        self.cumulative_length = 0\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Build a `DynamicLayer` instance from pre-existing key/value tensors.\n+        Update the key and value caches in-place, and return the necesary kes and value states.\n \n         Args:\n-            keys (`torch.Tensor`):\n-                Key cache tensor of shape ``[batch_size, num_heads, seq_len, head_dim]``.\n-            values (`torch.Tensor`):\n-                Value cache tensor of shape ``[batch_size, num_heads, seq_len, head_dim]``.\n+            key_states (`torch.Tensor`): The new key states to cache.\n+            value_states (`torch.Tensor`): The new value states to cache.\n+            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n \n         Returns:\n-            `DynamicLayer`: The newly constructed layer whose internal cache directly references\n-            the supplied tensors.\n+            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n+        \"\"\"\n+        # Lazy initialization\n+        if self.keys is None:\n+            self.lazy_initialization(key_states)\n+\n+        self.cumulative_length += key_states.shape[-2]\n+\n+        # Compute the full states\n+        full_key_states = torch.cat([self.keys, key_states], dim=-2)\n+        full_value_states = torch.cat([self.values, value_states], dim=-2)\n+        # Only cache the last `self.sliding_window - 1` tokens (or all of them if lower than that)\n+        self.keys = full_key_states[:, :, -self.sliding_window + 1 :, :]\n+        self.values = full_value_states[:, :, -self.sliding_window + 1 :, :]\n+\n+        # Return the full states\n+        return full_key_states, full_value_states\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n+        query_length = cache_position.shape[0]\n+        first_cache_position = cache_position[0]\n+\n+        kv_offset = torch.clamp(first_cache_position - self.sliding_window + 1, min=0)\n+\n+        if self.get_seq_length() >= self.sliding_window:\n+            kv_length = self.sliding_window - 1 + query_length\n+        else:\n+            kv_length = self.get_seq_length() + query_length\n+\n+        return kv_length, kv_offset\n+\n+    def get_seq_length(self) -> int:\n+        \"\"\"Returns the sequence length of the cached states.\"\"\"\n+        return self.cumulative_length\n+\n+    def get_max_cache_shape(self) -> int:\n+        \"\"\"Return the maximum cache shape of the cache\"\"\"\n+        return self.sliding_window\n+\n+    def crop(self, max_length: int) -> None:\n         \"\"\"\n-        layer = cls()\n-        layer.dtype, layer.device = keys.dtype, keys.device\n-        layer.keys = keys\n-        layer.values = values\n-        return layer\n+        Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be\n+        negative to remove `max_length` tokens.\n+        \"\"\"\n+        if self.get_seq_length() >= self.sliding_window:\n+            raise ValueError(\n+                \"Cannot `crop` a `DynamicSlidingWindowLayer` after it has seen more tokens than its\"\n+                \"sliding window (otherwise some states are lost)\"\n+            )\n+        super().crop(max_length)\n+        self.cumulative_length = self.keys.shape[-2]\n \n \n class StaticLayer(CacheLayerMixin):\n     \"\"\"\n-    A static cache layer that stores the Key and Value states as static tensors with shape `[batch_size, num_heads, seq_len, head_dim]`.\n-    It allocates its full backing tensors up-front and mutates them in-place. Built for `torch.compile` support.\n-\n-    See `CacheLayerMixin` for details on common methods that are implemented by all cache layers.\n+    A static cache layer that stores the key and value states as static tensors of shape `[batch_size, num_heads, max_cache_len), head_dim]`.\n+    It lazily allocates its full backing tensors, and then mutates them in-place. Built for `torch.compile` support.\n     \"\"\"\n \n     is_compileable = True\n@@ -259,15 +309,15 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the static cache tensors in place.\n+        Update the key and value caches in-place, and return the necesary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n             value_states (`torch.Tensor`): The new value states to cache.\n             cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n \n         Returns:\n-            tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value states.\n+            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n         if self.keys is None:\n@@ -290,38 +340,28 @@ def update(\n             self.values[:, :, cache_position] = value_states\n         return self.keys, self.values\n \n-    def get_max_cache_shape(self) -> int:\n-        \"\"\"Return the maximum cache shape of the cache\"\"\"\n-        return self.max_cache_len\n-\n-    def get_seq_length(self, cache_position=None) -> int:\n-        \"\"\"Returns the sequence length of the cached states.\"\"\"\n-        if cache_position is not None:\n-            return int(cache_position[-1] + 1)\n-        # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n-        # limit the check to the first batch member and head dimension.\n-        seq_length = (self.keys[0, 0].any(dim=-1)).sum() if self.keys is not None else 0\n-        return seq_length\n-\n-    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n-        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n-        dev = self.keys.device\n-        beam_idx_dev = beam_idx.to(dev)\n-        self.keys = self.keys.index_select(0, beam_idx_dev)\n-        self.values = self.values.index_select(0, beam_idx_dev)\n-\n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n         kv_offset = 0\n         kv_length = self.max_cache_len\n         return kv_length, kv_offset\n \n+    def get_seq_length(self) -> int:\n+        \"\"\"Returns the sequence length of the cached states.\"\"\"\n+        # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n+        # limit the check to the first batch member and head dimension.\n+        return (self.keys[0, 0].any(dim=-1)).sum() if self.keys is not None else 0\n+\n+    def get_max_cache_shape(self) -> int:\n+        \"\"\"Return the maximum cache shape of the cache\"\"\"\n+        return self.max_cache_len\n+\n \n class SlidingWindowLayer(StaticLayer):\n     \"\"\"\n-    A static cache layer that implements sliding window attention caching.\n-\n-    See `CacheLayerMixin` for details on common methods that are implemented by all cache layers.\n+    A static cache layer that stores the key and value states as static tensors of shape\n+    `[batch_size, num_heads, min(max_cache_len, sliding_window), head_dim]`. It lazily allocates its full backing\n+    tensors, and then mutates them in-place. Built for `torch.compile` support.\n     \"\"\"\n \n     is_sliding = True\n@@ -345,15 +385,15 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the sliding window cache tensors in place.\n+        Update the key and value caches in-place, and return the necesary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n             value_states (`torch.Tensor`): The new value states to cache.\n             cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n \n         Returns:\n-            tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value states.\n+            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n         if self.keys is None:\n@@ -407,20 +447,14 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         kv_length = max(query_length, self.max_cache_len)\n         return kv_length, kv_offset\n \n-    def reset(self) -> None:\n-        super().reset()\n-        self.cumulative_length = 0\n-\n-    def get_seq_length(self, cache_position=None) -> int:\n+    def get_seq_length(self) -> int:\n         \"\"\"Returns the sequence length of the cached states.\"\"\"\n         return self.cumulative_length\n \n \n class ChunkedSlidingLayer(SlidingWindowLayer):\n     \"\"\"\n     An extended SlidingWindowLayer that supports prefill chunking, originally implemented for Llama 4.\n-\n-    See `SlidingWindowLayer` for details on common methods that are implemented by all cache layers.\n     \"\"\"\n \n     def update(\n@@ -429,6 +463,17 @@ def update(\n         value_states: torch.Tensor,\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Update the key and value caches in-place, and return the necesary kes and value states.\n+\n+        Args:\n+            key_states (`torch.Tensor`): The new key states to cache.\n+            value_states (`torch.Tensor`): The new value states to cache.\n+            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n+\n+        Returns:\n+            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n+        \"\"\"\n         # Lazy initialization\n         if self.keys is None:\n             self.lazy_initialization(key_states)\n@@ -474,6 +519,7 @@ def update(\n         return full_key_states, full_value_states\n \n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n         query_length = cache_position.shape[0]\n         first_cache_position = cache_position[0]\n         sliding_window = self.max_cache_len\n@@ -495,7 +541,7 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n class QuantizedLayer(DynamicLayer):\n     \"\"\"\n     A quantized layer similar to what is described in the [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://huggingface.co/papers/2402.02750).\n-    It allows the model to generate longer sequence length without allocating too much memory for Key and Value cache by\n+    It allows the model to generate longer sequence length without allocating too much memory for the key and value caches by\n     applying quantization.\n \n     The cache has two types of storage, one for original precision and one for the quantized cache. A `residual length`\n@@ -527,18 +573,15 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Updates the cache with the new `key_states` and `value_states`.\n+        Update the key and value caches in-place, and return the necesary kes and value states.\n \n-        Parameters:\n-            key_states (`torch.Tensor`):\n-                The new key states to cache.\n-            value_states (`torch.Tensor`):\n-                The new value states to cache.\n-            cache_kwargs (`dict[str, Any]`, *optional*):\n-                Additional arguments for the cache subclass. No additional arguments are used in `DynamicLayer`.\n+        Args:\n+            key_states (`torch.Tensor`): The new key states to cache.\n+            value_states (`torch.Tensor`): The new value states to cache.\n+            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n \n-        Return:\n-            A tuple containing the updated key and value states.\n+        Returns:\n+            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         self.cumulative_length += key_states.shape[-2]\n \n@@ -564,16 +607,16 @@ def update(\n \n         return keys_to_return, values_to_return\n \n-    def get_seq_length(self, cache_position=None) -> int:\n-        \"\"\"Returns the sequence length of the cached states.\"\"\"\n-        return self.cumulative_length\n-\n     @abstractmethod\n     def _quantize(self, tensor, axis): ...\n \n     @abstractmethod\n     def _dequantize(self, q_tensor): ...\n \n+    def get_seq_length(self) -> int:\n+        \"\"\"Returns the sequence length of the cached states.\"\"\"\n+        return self.cumulative_length\n+\n \n class QuantoQuantizedLayer(QuantizedLayer):\n     def __init__(\n@@ -675,7 +718,7 @@ def _dequantize(self, qtensor):\n         return tensor\n \n \n-LAYER_CLASS_MAP: dict[str, type[CacheLayerMixin]] = {\n+STATIC_LAYER_CLASS_MAPPING: dict[str, type[CacheLayerMixin]] = {\n     \"full_attention\": StaticLayer,\n     \"sliding_attention\": SlidingWindowLayer,\n     \"chunked_attention\": ChunkedSlidingLayer,\n@@ -814,11 +857,11 @@ def early_initialization(\n         for layer in self.layers:\n             layer.lazy_initialization(fake_keys_tensor)\n \n-    def get_seq_length(self, layer_idx: int = 0, cache_position=None) -> int:\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cache for the given layer.\"\"\"\n         if layer_idx >= len(self.layers):\n             return 0\n-        return self.layers[layer_idx].get_seq_length(cache_position)\n+        return self.layers[layer_idx].get_seq_length()\n \n     def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n         \"\"\"\n@@ -924,45 +967,85 @@ def __len__(self):\n class DynamicCache(Cache):\n     \"\"\"\n     A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n-\n-    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n-    `[batch_size, num_heads, seq_len, head_dim]`.\n+    It stores the key and value states as a list of `CacheLayer`, one for each layer. The expected shape for each tensor\n+    in the `CacheLayer`s is `[batch_size, num_heads, seq_len, head_dim]`.\n+    If a config is passed, it will additionally check for sliding or hybrid cache structure, greatly reducing the\n+    memory requirement of the cached tensors to `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Example:\n \n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+    ```python\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n \n-        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = DynamicCache()\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        DynamicCache()\n-        ```\n+    >>> # Prepare a cache class and pass it to model's forward\n+    >>> past_key_values = DynamicCache(config=model.config)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    DynamicCache()\n+    ```\n     \"\"\"\n \n-    # Specialized constructor for DDP cache data, needed for BC\n-    def __init__(self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None):\n-        # `ddp_cache_data` was originally added for compatibility with `torch.distributed` (DDP). See #36212\n-        # and #36373 for more information. In a nutshell, it is `map(gather_map, zip(*caches))`, i.e. each item in the\n-        # iterable contains the key and value states for a layer gathered across replicas by torch.distributed\n-        # (shape=[global batch size, num_heads, seq_len, head_dim]).\n+    def __init__(\n+        self,\n+        ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None,\n+        config: Optional[PretrainedConfig] = None,\n+    ):\n+        \"\"\"\n+        Create a `DynamicCache`. Specialized constructor for DDP cache data, needed for BC.\n+\n+        Args:\n+            ddp_cache_data (`Iterable[tuple[torch.Tensor, torch.Tensor]]`, *optional*):\n+                It was originally added for compatibility with `torch.distributed` (DDP). In a nutshell, it is\n+                `map(gather_map, zip(*caches))`, i.e. each item in the iterable contains the key and value states\n+                for a layer gathered across replicas by torch.distributed (shape=[global batch size, num_heads, seq_len, head_dim]).\n+                Note: it needs to be the 1st arg as well to work correctly\n+            config (`PretrainedConfig`, *optional*):\n+                The config of the model for which this Cache will be used. If passed, it will be used to check for sliding\n+                or hybrid layer structure, greatly reducing the memory requirement of the cached tensors to\n+                `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n+        \"\"\"\n+        layers = []\n+        # If a config is passed, use it to infer the layer types and initialize accordingly\n+        if config is not None:\n+            config = config.get_text_config()\n+            sliding_window = getattr(config, \"sliding_window\", None) or getattr(config, \"attention_chunk_size\", None)\n+            layer_types = getattr(config, \"layer_types\", None)\n+            if layer_types is None:\n+                layer_types = [\n+                    \"sliding_attention\" if sliding_window is not None else \"full_attention\"\n+                    for _ in range(config.num_hidden_layers)\n+                ]\n+\n+            for layer_type in layer_types:\n+                if layer_type in (\"sliding_attention\", \"chunked_attention\"):\n+                    layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n+                else:\n+                    layers.append(DynamicLayer())\n+\n+        # In this case, use the passed data to already fill in the Cache\n         if ddp_cache_data is not None:\n-            layers = []\n-            for key_states, value_states in ddp_cache_data:\n-                layers.append(DynamicLayer.from_tensors(key_states, value_states))\n-            super().__init__(layers=layers)\n-        else:\n+            # Init all the layers with the data\n+            for layer_idx, (key_states, value_states) in enumerate(ddp_cache_data):\n+                # If the config was not passed above, initialize a DynamicLayer for each entry of the ddp_data\n+                if config is None:\n+                    layers.append(DynamicLayer())\n+                # Update the layer with the data\n+                _, _ = layers[layer_idx].update(key_states, value_states)\n+\n+        # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n+        if len(layers) == 0:\n             super().__init__(layer_class_to_replicate=DynamicLayer)\n+        else:\n+            super().__init__(layers=layers)\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n         \"\"\"\n         Converts the `Cache` instance into the its equivalent in the legacy cache format. Used for\n         backward compatibility.\n@@ -973,7 +1056,7 @@ def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n         return legacy_cache\n \n     @classmethod\n-    def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]) -> \"Cache\":\n+    def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.Tensor, torch.Tensor]]) -> \"DynamicCache\":\n         \"\"\"\n         Converts a cache in the legacy cache format into an equivalent `Cache`. Used for\n         backward compatibility.\n@@ -991,7 +1074,7 @@ def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.FloatTensor, torch\n if is_torch_greater_or_equal(\"2.3\"):\n \n     def _get_cache_dict(cache: DynamicCache):\n-        if any(not isinstance(layer, DynamicLayer) for layer in cache.layers):\n+        if any(not isinstance(layer, (DynamicLayer, DynamicSlidingWindowLayer)) for layer in cache.layers):\n             raise RuntimeError(\"This pytree flattening function should only be applied to DynamicCache\")\n \n         if not is_torch_greater_or_equal_than_2_6:\n@@ -1054,22 +1137,22 @@ class StaticCache(Cache):\n \n     Example:\n \n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n+    ```python\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n \n-        >>> inputs = tokenizer(text=\"My name is Llama\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is Llama\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n-        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = StaticCache(max_cache_len=max_generated_length, config=model.config)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        StaticCache()\n-        ```\n+    >>> # Prepare a cache class and pass it to model's forward\n+    >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n+    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n+    >>> past_key_values = StaticCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    StaticCache()\n+    ```\n     \"\"\"\n \n     # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n@@ -1089,21 +1172,22 @@ class OffloadedStaticCache(Cache):\n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Example:\n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, OffloadedStaticCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+    ```python\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, OffloadedStaticCache\n+\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n \n-        >>> inputs = tokenizer(text=\"My name is GPT2\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is GPT2\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class with offloading\n-        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = OffloadedStaticCache(max_cache_len=max_generated_length, config=model.config)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache with offloaded layers\n-        OffloadedStaticCache()\n-        ```\n+    >>> # Prepare a cache class with offloading\n+    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n+    >>> past_key_values = OffloadedStaticCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache with offloaded layers\n+    OffloadedStaticCache()\n+    ```\n     \"\"\"\n \n     # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n@@ -1119,22 +1203,22 @@ class SlidingWindowCache(Cache):\n \n     Example:\n \n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, SlidingWindowCache\n+    ```python\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, SlidingWindowCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n \n-        >>> inputs = tokenizer(text=\"My name is Mistral\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is Mistral\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n-        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = SlidingWindowCache(max_cache_len=max_generated_length, config=model.config)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        SlidingWindowCache()\n-        ```\n+    >>> # Prepare a cache class and pass it to model's forward\n+    >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n+    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n+    >>> past_key_values = SlidingWindowCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    SlidingWindowCache()\n+    ```\n     \"\"\"\n \n     # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n@@ -1154,22 +1238,22 @@ class HybridCache(Cache):\n \n     Example:\n \n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n+    ```python\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n \n-        >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n-        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = HybridCache(max_cache_len=max_generated_length, config=model.config)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        HybridCache()\n-        ```\n+    >>> # Prepare a cache class and pass it to model's forward\n+    >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n+    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n+    >>> past_key_values = HybridCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    HybridCache()\n+    ```\n     \"\"\"\n \n     # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n@@ -1182,7 +1266,7 @@ def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n                     init_kwargs[\"sliding_window\"] = config.sliding_window\n                 elif layer_type == \"chunked_attention\":\n                     init_kwargs[\"sliding_window\"] = config.attention_chunk_size\n-                layers.append(LAYER_CLASS_MAP[layer_type](**init_kwargs))\n+                layers.append(STATIC_LAYER_CLASS_MAPPING[layer_type](**init_kwargs))\n         else:\n             # In this case, fall back to StaticCache\n             layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n@@ -1214,7 +1298,7 @@ def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n                     init_kwargs[\"sliding_window\"] = config.sliding_window\n                 elif layer_type == \"chunked_attention\":\n                     init_kwargs[\"sliding_window\"] = config.attention_chunk_size\n-                layers.append(LAYER_CLASS_MAP[layer_type](**init_kwargs))\n+                layers.append(STATIC_LAYER_CLASS_MAPPING[layer_type](**init_kwargs))\n         else:\n             # In this case, fall back to StaticCache\n             layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n@@ -1276,21 +1360,21 @@ class QuantoQuantizedCache(QuantizedCache):\n \n     Example:\n \n-        ```python\n-        >>> # Run pip install quanto first if you don't have it yet\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n+    ```python\n+    >>> # Run pip install quanto first if you don't have it yet\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n \n-        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = QuantoQuantizedCache(config=model.config, nbits=4)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        QuantoQuantizedCache()\n-        ```\n+    >>> # Prepare a cache class and pass it to model's forward\n+    >>> past_key_values = QuantoQuantizedCache(config=model.config, nbits=4)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    QuantoQuantizedCache()\n+    ```\n     \"\"\"\n \n     def __init__(\n@@ -1321,21 +1405,21 @@ class HQQQuantizedCache(QuantizedCache):\n \n     Example:\n \n-        ```python\n-        >>> # Run pip install hqq first if you don't have it yet\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n+    ```python\n+    >>> # Run pip install hqq first if you don't have it yet\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n \n-        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n+    >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = HQQQuantizedCache(config=model.config, nbits=4, axis_key=1, axis_value=1)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        HQQQuantizedCache()\n-        ```\n+    >>> # Prepare a cache class and pass it to model's forward\n+    >>> past_key_values = HQQQuantizedCache(config=model.config, nbits=4, axis_key=1, axis_value=1)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    HQQQuantizedCache()\n+    ```\n     \"\"\"\n \n     def __init__(\n@@ -1359,32 +1443,27 @@ class EncoderDecoderCache(Cache):\n \n     Example:\n \n-        ```python\n-        >>> from transformers import AutoProcessor, AutoModelForCausalLM, DynamicCache, EncoderDecoderCache\n+    ```python\n+    >>> from transformers import AutoProcessor, AutoModelForCausalLM, DynamicCache, EncoderDecoderCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"openai/whisper-small\")\n-        >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"openai/whisper-small\")\n+    >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n \n-        >>> inputs = processor(audio=YOUR-AUDIO, return_tensors=\"pt\")\n+    >>> inputs = processor(audio=YOUR-AUDIO, return_tensors=\"pt\")\n \n-        >>> # Prepare cache classes for encoder and decoder and pass it to model's forward\n-        >>> self_attention_cache = DynamicCache()\n-        >>> cross_attention_cache = DynamicCache()\n-        >>> past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        EncoderDecoderCache()\n-        ```\n+    >>> # Prepare cache classes for encoder and decoder and pass it to model's forward\n+    >>> self_attention_cache = DynamicCache()\n+    >>> cross_attention_cache = DynamicCache()\n+    >>> past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n+    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+    >>> outputs.past_key_values # access cache filled with key/values from generation\n+    EncoderDecoderCache()\n+    ```\n     \"\"\"\n \n-    # Override @property from Cache -> this will be set in __init__ on the instances\n-    is_compileable = False\n-\n     def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n         self.self_attention_cache = self_attention_cache\n         self.cross_attention_cache = cross_attention_cache\n-        # Override @property from Cache\n-        self.is_compileable = getattr(self.self_attention_cache, \"is_compileable\", False)\n \n         self.is_updated = {}\n         for layer_idx in range(len(cross_attention_cache)):\n@@ -1456,23 +1535,13 @@ def from_legacy_cache(\n                     cache.is_updated[layer_idx] = True\n         return cache\n \n-    def get_seq_length(self, layer_idx: Optional[int] = 0, cache_position=None) -> int:\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n-        # check if empty list because in case of static cache it will be a tensors and we can't check `if not torch.Tensor`\n-        return self.self_attention_cache.get_seq_length(layer_idx, cache_position)\n+        return self.self_attention_cache.get_seq_length(layer_idx)\n \n     def reset(self):\n-        if hasattr(self.self_attention_cache, \"reset\"):\n-            self.self_attention_cache.reset()\n-        if hasattr(self.cross_attention_cache, \"reset\"):\n-            self.cross_attention_cache.reset()\n-        elif not hasattr(self.self_attention_cache, \"reset\") and not hasattr(self.cross_attention_cache, \"reset\"):\n-            raise ValueError(\n-                \"Neither self nor cross-attention cache have valid `.reset()` methods. `.reset()` should \"\n-                \"only be called on compatible cache classes, such as `StaticCache` or `SlidingWindowCache`. \"\n-                f\"Got {self.self_attention_cache.__str__()} for the self attention cache and \"\n-                f\"{self.cross_attention_cache.__str__()} for the cross attention cache.\"\n-            )\n+        self.self_attention_cache.reset()\n+        self.cross_attention_cache.reset()\n         for layer_idx in self.is_updated:\n             self.is_updated[layer_idx] = False\n \n@@ -1533,13 +1602,21 @@ def get_max_cache_shape(self) -> int:\n     def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n         return self.self_attention_cache.get_mask_sizes(cache_position, layer_idx)\n \n+    @property\n+    def is_sliding(self):\n+        return self.self_attention_cache.is_sliding\n+\n+    @property\n+    def is_compileable(self) -> bool:\n+        return self.self_attention_cache.is_compileable\n+\n \n ### Deprecated classes\n \n \n class SinkCache(Cache):\n     \"\"\"\n-    Is its now a `custom_generate` repository on the Hub: https://huggingface.co/transformers-community/sink_cache.\n+    It is now a `custom_generate` repository on the Hub: https://huggingface.co/transformers-community/sink_cache.\n     See [these docs](https://huggingface.co/docs/transformers/generation_strategies#custom-decoding-methods) for\n     general `custom_generate`usage.\n     \"\"\""
        },
        {
            "sha": "8c4d5369c19aaf7ee699c64413786ac5c0cf3b31",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -1813,7 +1813,7 @@ def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n             # Support for BC tuple cache format\n             if isinstance(cache, tuple):\n                 past_length = cache[0][0].shape[2]\n-            elif hasattr(cache, \"get_seq_length\") and cache.get_seq_length() is not None:\n+            elif hasattr(cache, \"get_seq_length\"):\n                 past_length = cache.get_seq_length()\n \n             cache_position = cache_position[past_length:]\n@@ -1959,6 +1959,16 @@ def _prepare_cache_for_generation(\n         generation_config.cache_implementation = generation_config.cache_implementation or getattr(\n             self.config.get_text_config(decoder=True), \"cache_implementation\", None\n         )\n+\n+        # assisted decoding and contrastive search need to roll-back the Cache, which is not supported if\n+        # it has sliding layers - so if we use any of those 2, do not pass the config to DynamicCache, which\n+        # will result in creating a Cache with only full layers even if model uses sliding window\n+        generation_mode = generation_config.get_generation_mode(assistant_model)\n+        dynamic_cache_kwargs = (\n+            {\"config\": self.config}\n+            if generation_mode not in (GenerationMode.ASSISTED_GENERATION, GenerationMode.CONTRASTIVE_SEARCH)\n+            else {}\n+        )\n         if generation_config.cache_implementation is not None:\n             if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n                 if generation_config.cache_implementation == \"static\" and not self._can_compile_fullgraph:\n@@ -2001,15 +2011,15 @@ def _prepare_cache_for_generation(\n             elif generation_config.cache_implementation == \"offloaded\":\n                 model_kwargs[cache_name] = OffloadedCache()\n             elif generation_config.cache_implementation == \"dynamic\":\n-                model_kwargs[cache_name] = DynamicCache()\n+                model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n         # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n         # keeps copying the cache thus using much more memory\n         else:\n             model_kwargs[cache_name] = (\n-                DynamicCache()\n+                DynamicCache(**dynamic_cache_kwargs)\n                 if not requires_cross_attention_cache\n-                else EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else EncoderDecoderCache(DynamicCache(**dynamic_cache_kwargs), DynamicCache(**dynamic_cache_kwargs))\n             )\n \n     def _supports_logits_to_keep(self) -> bool:"
        },
        {
            "sha": "e3928960740ddcf9999a01eeb987e6d20049f594",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -364,7 +364,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "6b63ef4e2fbc841baa00b8cc38b8463ded777861",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -744,7 +744,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "321b5876e674c893a63257a0f1e92fd450cd45cd",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -363,7 +363,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "068e39a1c0ac6603975a5871cd8edd8b6f47ba77",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -396,7 +396,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "673d79a17b11c4df66328be8f1416c39aa90fae3",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -371,7 +371,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "7624d18780b9a748109265dd0917df04b5c6a77e",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -405,7 +405,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "83996ccb72317464fcd95df23dbe71af3697f586",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -702,7 +702,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "da702ba2504c80a2919c13ee80a94b0b145ae495",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -512,7 +512,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "e595b0197eb77e15233f8db683bc95664f9b72cf",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -556,7 +556,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "c8053f2a250e8edf81b9359bb42948c92a2a376e",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -628,7 +628,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "54b87c35da306d8bd47ea29285e42a146c314a69",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -547,7 +547,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "af37a0f96c53ca0409be9a3ac486305601862626",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -475,7 +475,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "58085b1352e0573476b37cd0cb3edbd613f9fac7",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -1186,7 +1186,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "28c79dbef36dd75e975ae0831b5bc19aa4b6dcf5",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -362,7 +362,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "00715c35acd52175200fd96a3384f5173be5763c",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -371,7 +371,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "9304b5a0256988fefa8ed7c88b9a5837166f5102",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -384,7 +384,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "7aec2662293f8f1d6d17fa90e73367769ce95461",
            "filename": "src/transformers/models/fsmt/configuration_fsmt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -190,7 +190,10 @@ def __init__(\n         self.activation_function = activation_function\n \n         self.decoder = DecoderConfig(\n-            vocab_size=tgt_vocab_size, bos_token_id=eos_token_id, is_encoder_decoder=is_encoder_decoder\n+            vocab_size=tgt_vocab_size,\n+            bos_token_id=eos_token_id,\n+            is_encoder_decoder=is_encoder_decoder,\n+            num_hidden_layers=encoder_layers,\n         )\n         if \"decoder\" in common_kwargs:\n             del common_kwargs[\"decoder\"]"
        },
        {
            "sha": "6fb960cbfa7d7e9123ad41263f58c3c4531d1361",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -406,7 +406,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "e0124339852e7d9318ef18c0432ed6c699c51a8f",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -403,7 +403,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "abd3872ec125f716cb7994e4656b25eaa11f8448",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -507,7 +507,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "b4c5e351f9f5ef9ef7f21f3440134f4744d7c4df",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -585,7 +585,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "35ba8d414a17eb76bc01650f52e818c57f6e73b8",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -378,7 +378,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "c77e1c3369953b44fb07ba71af20bd3d924d401c",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -382,7 +382,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "0d6dcb355e7320f011b71e510af291756dc14476",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -496,7 +496,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "297a6b9220d7958cb4c49b6cc5619ac17c379965",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -462,7 +462,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "d3cade9d1497ba055c950b4a6010841d512ad33a",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -391,7 +391,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "15090f2317e6f2a40ffdc2ba39c5dddb528dcbe8",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -363,7 +363,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "0facccb23704fd52233f7b89bdcbcbd61ac16037",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -368,7 +368,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "2cdd6ae865d36c7a74786dd58da9e6b15d176372",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -510,7 +510,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "f7e6ee8c628f8f69f0de60ebe4c50c6060dc4b56",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -341,7 +341,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "470c552ce647b409fdbe89a574c20b52b7a38192",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -132,7 +132,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "b6b5883f4a775a0713b9b33e8a8c90c3554be703",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -432,7 +432,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "ffcf8224353fd643068f76bf6152e40b21fe8ab9",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -303,7 +303,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "e254a298a738663e339e3d3c8b3a3f4045d215c7",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -328,7 +328,7 @@ def forward(\n \n         # Handle past_key_values and cache setup\n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "475ee8bb381055b07e72ad7425e58c78d8602101",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -505,7 +505,7 @@ def forward(\n \n         # Handle past_key_values and cache setup\n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "ba1774d54328afdbdf076724ac4605636b369c89",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -345,7 +345,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "07df928b049764301e0a2f256f2da512d110188b",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -350,7 +350,7 @@ def forward(\n             inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "9cfef814304691b870fe27bf4cd12b2f0f129407",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -373,7 +373,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "45503e6bf207be646ddba31871caa2c96b7ce429",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -1664,7 +1664,7 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "09fd03763a1bba2533578004919f7f6d8852cde4",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -1514,7 +1514,7 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "733a150451835f8731aa98e875c10d47035dd063",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -979,7 +979,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "a6da708da4c500bf6e8411031fcb3abf0537b78d",
            "filename": "src/transformers/models/prophetnet/configuration_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconfiguration_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconfiguration_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconfiguration_prophetnet.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -167,7 +167,7 @@ def __init__(\n \n     @property\n     def num_hidden_layers(self) -> int:\n-        return self.num_encoder_layers + self.num_decoder_layers\n+        return self.num_encoder_layers\n \n     @num_hidden_layers.setter\n     def num_hidden_layers(self, value):"
        },
        {
            "sha": "dbe813648cab82c872ddf4a8f88f0f0ffcd33667",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -2008,7 +2008,7 @@ def prepare_inputs_for_generation(\n         if attention_mask is None:\n             attention_mask = input_ids.new_ones(input_ids.shape)\n \n-        if past_key_values:\n+        if past_key_values is not None and past_key_values.get_seq_length() > 0:\n             input_ids = input_ids[:, -1:]\n         # first step, decoder_cached_states are empty\n         return {"
        },
        {
            "sha": "1fd40d0562df23124ac23ac4209045266a85440e",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -345,7 +345,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "bc8f2e25ec061e9412d62bca4f60627450fd4f78",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -132,7 +132,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "0d0380f37c2fb0c311f2c4abbeedd108f88b27a3",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -1574,7 +1574,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -2154,7 +2154,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "4d2c00c51ae80dfcf3c7d564b0083755304a88b6",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -853,7 +853,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "158a2634d9c242852a521caa03db8dfa1788cb9f",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -828,7 +828,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "4cae17faece42b3f96b4eca91498f086bda3998b",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -825,7 +825,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "5f84ad175bbbf53f5fe048522610b1746c2a39ee",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -371,7 +371,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "d8c216974e141d9078a520f99e0f6d2d14b1b208",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -455,7 +455,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "2733e938ab9b04c25950957f6ad3adebefe711a9",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -375,7 +375,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "ab05f9643b4d63cf23d1d3c1f6ab679387138076",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -344,7 +344,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "2fe6a6be7e608c19c647f2082206aad7d663c0ef",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -170,7 +170,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "90b4672c9da000232fcaa5f462d30d1ccad8e1dc",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -813,8 +813,8 @@ def forward(\n \n         if not self.training and use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(\n-                self_attention_cache=DynamicCache(),\n-                cross_attention_cache=DynamicCache(),\n+                self_attention_cache=DynamicCache(config=self.config),\n+                cross_attention_cache=DynamicCache(config=self.config),\n             )\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "ed407be82a5781b7567916131d7a5d15ca413786",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -676,8 +676,8 @@ def forward(\n \n         if not self.training and use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(\n-                self_attention_cache=DynamicCache(),\n-                cross_attention_cache=DynamicCache(),\n+                self_attention_cache=DynamicCache(config=self.config),\n+                cross_attention_cache=DynamicCache(config=self.config),\n             )\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "5d778d8cb2ec60ce23d50c882c85f14e5e384c80",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 55,
            "deletions": 2,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -20,7 +20,8 @@\n from parameterized import parameterized\n from pytest import mark\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, is_torch_available, pipeline\n+from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, Gemma2Config, is_torch_available, pipeline\n+from transformers.cache_utils import DynamicLayer, DynamicSlidingWindowLayer\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -472,7 +473,59 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         input_size = inputs.input_ids.shape[-1]\n         self.assertTrue(input_size > model.config.sliding_window)\n \n-        out = model.generate(**inputs, max_new_tokens=20)[:, input_size:]\n+        # It should by Hybrid by default from hub config, but let's make sure!\n+        out = model.generate(**inputs, max_new_tokens=20, cache_implementation=\"hybrid\")[:, input_size:]\n         output_text = tokenizer.batch_decode(out)\n \n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n+\n+    @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"flex_attention\",), (\"eager\",)])\n+    @require_read_token\n+    def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str):\n+        \"\"\"\n+        Same as above, but explicitly setting the cache to Dynamic, as it's otherwise static by default for\n+        the model on the hub\n+        \"\"\"\n+        if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n+            self.skipTest(\"FlashAttention2 is required for this test.\")\n+\n+        if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n+            self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n+\n+        model_id = \"google/gemma-2-2b\"\n+        EXPECTED_COMPLETIONS = [\n+            \" the people, the food, the culture, the history, the music, the art, the architecture\",\n+            \", green, yellow, orange, purple, pink, brown, black, white, gray, silver\",\n+        ]\n+\n+        input_text = [\n+            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n+        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n+        ).to(torch_device)\n+\n+        # Make sure prefill is larger than sliding window\n+        input_size = inputs.input_ids.shape[-1]\n+        self.assertTrue(input_size > model.config.sliding_window)\n+\n+        out = model.generate(**inputs, max_new_tokens=20, cache_implementation=\"dynamic\", return_dict_in_generate=True)\n+        output_text = tokenizer.batch_decode(out.sequences[:, input_size:])\n+\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n+\n+        # Let's check that the dynamic cache has hybrid layers!\n+        dynamic_cache = out.past_key_values\n+        self.assertTrue(isinstance(dynamic_cache, DynamicCache))\n+        for layer, layer_type in zip(dynamic_cache.layers, model.config.layer_types):\n+            if layer_type == \"sliding_attention\":\n+                self.assertTrue(isinstance(layer, DynamicSlidingWindowLayer))\n+                self.assertEqual(layer.keys.shape[-2], model.config.sliding_window - 1)\n+            else:\n+                self.assertTrue(isinstance(layer, DynamicLayer))\n+                # max_new_tokens - 1 because last token generated is not cached\n+                self.assertEqual(layer.keys.shape[-2], input_size + 20 - 1)"
        },
        {
            "sha": "a17f464370b817c52160a49ff3d6d982f8b38047",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 45,
            "deletions": 1,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -18,8 +18,10 @@\n \n import pytest\n from packaging import version\n+from parameterized import parameterized\n \n-from transformers import AutoTokenizer, MistralConfig, is_torch_available, set_seed\n+from transformers import AutoTokenizer, DynamicCache, MistralConfig, is_torch_available, set_seed\n+from transformers.cache_utils import DynamicSlidingWindowLayer\n from transformers.testing_utils import (\n     DeviceProperties,\n     Expectations,\n@@ -337,6 +339,48 @@ def test_compile_static_cache(self):\n         static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n \n+    @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"flex_attention\",), (\"eager\",)])\n+    @require_flash_attn\n+    @slow\n+    def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str):\n+        \"\"\"Test that we can correctly generate beyond the sliding window. This is non-trivial as Mistral will use\n+        a DynamicCache with only sliding layers.\"\"\"\n+\n+        model_id = \"mistralai/Mistral-7B-v0.1\"\n+        EXPECTED_COMPLETIONS = [\n+            \"This is a nice place. This is a nice place. This is a nice place. This is\",\n+            \", green, yellow, orange, purple, pink, brown, black, white, gray, silver\",\n+        ]\n+\n+        input_text = [\n+            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n+        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = MistralForCausalLM.from_pretrained(\n+            model_id, attn_implementation=attn_implementation, device_map=torch_device, torch_dtype=torch.float16\n+        )\n+\n+        # Make sure prefill is larger than sliding window\n+        input_size = inputs.input_ids.shape[-1]\n+        self.assertTrue(input_size > model.config.sliding_window)\n+\n+        # Should already be Dynamic by default, but let's make sure!\n+        out = model.generate(**inputs, max_new_tokens=20, cache_implementation=\"dynamic\", return_dict_in_generate=True)\n+        output_text = tokenizer.batch_decode(out.sequences[:, input_size:])\n+\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n+\n+        # Let's check that the dynamic cache has hybrid layers!\n+        dynamic_cache = out.past_key_values\n+        self.assertTrue(isinstance(dynamic_cache, DynamicCache))\n+        for layer in dynamic_cache.layers:\n+            self.assertTrue(isinstance(layer, DynamicSlidingWindowLayer))\n+            self.assertEqual(layer.keys.shape[-2], model.config.sliding_window - 1)\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "8bc551c2cb72e4dc0ec4a73f546bfa7644cfcedb",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/41d17178827455e7b6553a7026d71d3036ef719c/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41d17178827455e7b6553a7026d71d3036ef719c/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=41d17178827455e7b6553a7026d71d3036ef719c",
            "patch": "@@ -549,6 +549,7 @@ def prepare_config_and_inputs(self):\n             vocab_size=self.vocab_size,\n             d_model=self.d_model,\n             decoder_layers=self.decoder_layers,\n+            num_hidden_layers=self.decoder_layers,\n             decoder_ffn_dim=self.decoder_ffn_dim,\n             encoder_attention_heads=self.encoder_attention_heads,\n             decoder_attention_heads=self.decoder_attention_heads,"
        }
    ],
    "stats": {
        "total": 828,
        "additions": 508,
        "deletions": 320
    }
}