{
    "author": "ydshieh",
    "message": "Reduce timing on CircleCI - part 1 (Use @slow for IntegrationTests) (#42206)\n\n* fix 1\n\n* fix 2: bark\n\n* fix 2: mamba\n\n* fix 4: Speech2TextModelIntegrationTests\n\n* fix 5: Aria\n\n* fix 6: RTDetrModelIntegrationTest\n\n* fix 7: PLBartBaseIntegrationTest\n\n* fix 8: XLMRobertaModelIntegrationTest\n\n* fix 9: TvpModelIntegrationTests\n\n* fix 10: LlavaForConditionalGenerationIntegrationTest\n\n* fix 11: RTDetrV2ModelIntegrationTest\n\n* fix 12: HieraModelIntegrationTest\n\n* fix 13: Olmo2IntegrationTest\n\n* fix 14: BarkModelIntegrationTests\n\n* fix 15: Rag\n\n* fix 16: JambaModelIntegrationTest\n\n* run\n\n* fix 17: ImageGPTModelTest\n\n* fix 18: MBartEnroIntegrationTest\n\n* revert\n\n* style\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "cd416f3c5c6974b5afd211759624283777a45c6e",
    "files": [
        {
            "sha": "126da4be3b0035afeedfe9ac62a493515026f09e",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -226,6 +226,7 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n \n @unittest.skipIf(SKIP, reason=\"A10 doesn't have enough GPU memory for this tests\")\n @require_torch\n+@slow\n class AriaForConditionalGenerationIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n@@ -234,7 +235,6 @@ def setUp(self):\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n@@ -272,7 +272,6 @@ def test_small_model_integration_test(self):\n         ).get_expectation()\n         self.assertEqual(decoded_output, expected_output)\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_single(self):\n@@ -304,7 +303,6 @@ def test_small_model_integration_test_llama_single(self):\n             f\"Expected: {repr(EXPECTED_DECODED_TEXT)}\\nActual: {repr(decoded_output)}\",\n         )\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n@@ -346,7 +344,6 @@ def test_small_model_integration_test_llama_batched(self):\n         decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n@@ -383,7 +380,6 @@ def test_small_model_integration_test_batch(self):\n         decoded_output = self.processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched_regression(self):\n@@ -417,7 +413,6 @@ def test_small_model_integration_test_llama_batched_regression(self):\n         decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_vision\n     @require_bitsandbytes\n@@ -510,7 +505,6 @@ def test_tokenizer_integration(self):\n         self.assertEqual(slow_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n         self.assertEqual(fast_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n \n-    @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_generation_no_images(self):"
        },
        {
            "sha": "aebab17f624ad37d36b3f30519ebe87d9f418318",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -903,6 +903,7 @@ def test_resize_embeddings_untied(self):\n \n \n @require_torch\n+@slow\n class BarkModelIntegrationTests(unittest.TestCase):\n     @cached_property\n     def model(self):\n@@ -940,7 +941,6 @@ def test_model_can_generate(self):\n         # Bark has custom generate without inheriting GenerationMixin. This test could prevent regression.\n         self.assertTrue(self.model.can_generate())\n \n-    @slow\n     def test_generate_semantic(self):\n         input_ids = self.inputs\n \n@@ -957,7 +957,6 @@ def test_generate_semantic(self):\n             )\n         self.assertListEqual(output_ids[0, : len(expected_output_ids)].tolist(), expected_output_ids)\n \n-    @slow\n     def test_generate_semantic_early_stop(self):\n         input_ids = self.inputs\n         min_eos_p = 0.01\n@@ -1000,7 +999,6 @@ def test_generate_semantic_early_stop(self):\n         self.assertLess(len(output_ids[0, :].tolist()), len(output_ids_without_min_eos_p[0, :].tolist()))\n         self.assertListEqual(output_ids[0, : len(expected_output_ids)].tolist(), expected_output_ids)\n \n-    @slow\n     def test_generate_coarse(self):\n         input_ids = self.inputs\n \n@@ -1029,7 +1027,6 @@ def test_generate_coarse(self):\n \n         self.assertListEqual(output_ids[0, : len(expected_output_ids)].tolist(), expected_output_ids)\n \n-    @slow\n     def test_generate_fine(self):\n         input_ids = self.inputs\n \n@@ -1079,23 +1076,20 @@ def test_generate_fine(self):\n \n         self.assertListEqual(output_ids[0, :, : len(expected_output_ids[0])].tolist(), expected_output_ids)\n \n-    @slow\n     def test_generate_end_to_end(self):\n         input_ids = self.inputs\n \n         with torch.no_grad():\n             self.model.generate(**input_ids)\n             self.model.generate(**{key: val for (key, val) in input_ids.items() if key != \"history_prompt\"})\n \n-    @slow\n     def test_generate_end_to_end_with_args(self):\n         input_ids = self.inputs\n \n         with torch.no_grad():\n             self.model.generate(**input_ids, do_sample=True, temperature=0.6, penalty_alpha=0.6)\n             self.model.generate(**input_ids, do_sample=True, temperature=0.6, num_beams=4)\n \n-    @slow\n     def test_generate_batching(self):\n         args = {\"do_sample\": False, \"temperature\": None}\n \n@@ -1126,7 +1120,6 @@ def test_generate_batching(self):\n         outputs, _ = self.model.generate(**s1, **args, return_output_lengths=True)\n         self.assertTrue((outputs == output1).all().item())\n \n-    @slow\n     def test_generate_end_to_end_with_sub_models_args(self):\n         input_ids = self.inputs\n \n@@ -1157,7 +1150,6 @@ def test_generate_end_to_end_with_sub_models_args(self):\n         )\n \n     @require_torch_accelerator\n-    @slow\n     def test_generate_end_to_end_with_offload(self):\n         input_ids = self.inputs\n "
        },
        {
            "sha": "93cd58488e25f46022227cc39d9f0c3d4a033b25",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -524,12 +524,12 @@ def prepare_img():\n \n @require_torch\n @require_vision\n+@slow\n class HieraModelIntegrationTest(unittest.TestCase):\n     @cached_property\n     def default_image_processor(self):\n         return AutoImageProcessor.from_pretrained(\"facebook/hiera-tiny-224-in1k-hf\") if is_vision_available() else None\n \n-    @slow\n     def test_inference_image_classification_head(self):\n         model = HieraForImageClassification.from_pretrained(\"facebook/hiera-tiny-224-in1k-hf\").to(torch_device)\n \n@@ -583,7 +583,6 @@ def test_inference_interpolate_pos_encoding(self):\n \n         torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n-    @slow\n     def test_inference_for_pretraining(self):\n         # make random mask reproducible\n         torch.manual_seed(2)"
        },
        {
            "sha": "08cf69199ee11dd657b3d97dee15e706faab5447",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -249,6 +249,9 @@ def _check_scores(self, batch_size, scores, generated_length, config):\n         self.assertEqual(len(scores), generated_length)\n         self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n \n+    # After #33632, this test still passes, but many subsequential tests fail with `device-side assert triggered`.\n+    # We need to put `@slow` whenever `run_test_using_subprocess` is used.\n+    @slow\n     @run_test_using_subprocess\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         super().test_beam_search_generate_dict_outputs_use_cache()"
        },
        {
            "sha": "010eb99240948628d2b0c07b22f72cf87b0f24d6",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -556,6 +556,7 @@ def test_multi_gpu_data_parallel_forward(self):\n \n \n @require_torch\n+@slow\n class JambaModelIntegrationTest(unittest.TestCase):\n     model = None\n     tokenizer = None\n@@ -574,7 +575,6 @@ def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id)\n         cls.device_properties = get_device_properties()\n \n-    @slow\n     def test_simple_generate(self):\n         # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4.\n         #\n@@ -599,7 +599,6 @@ def test_simple_generate(self):\n         output_sentence = self.tokenizer.decode(out[0, :])\n         self.assertEqual(output_sentence, expected_sentence)\n \n-    @slow\n     def test_simple_batched_generate_with_padding(self):\n         # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4.\n         #"
        },
        {
            "sha": "c8df5a0b93fb47ff5a0e14830c4924d3f83903fa",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -282,14 +282,14 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n \n \n @require_torch\n+@slow\n class LlavaForConditionalGenerationIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"llava-hf/bakLlava-v1-hf\")\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n-    @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -315,7 +315,6 @@ def test_small_model_integration_test(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n-    @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_single(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -346,7 +345,6 @@ def test_small_model_integration_test_llama_single(self):\n \n         self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -398,7 +396,6 @@ def test_small_model_integration_test_llama_batched(self):\n         decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -446,7 +443,6 @@ def test_small_model_integration_test_batch(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n-    @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched_regression(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -503,7 +499,6 @@ def test_small_model_integration_test_llama_batched_regression(self):\n         decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     @require_torch\n     @require_vision\n     @require_bitsandbytes\n@@ -577,7 +572,6 @@ def test_tokenizer_integration(self):\n         self.assertEqual(slow_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n         self.assertEqual(fast_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n \n-    @slow\n     @require_bitsandbytes\n     def test_generation_no_images(self):\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n@@ -592,7 +586,6 @@ def test_generation_no_images(self):\n         # Make sure that `generate` works\n         _ = model.generate(**inputs, max_new_tokens=20)\n \n-    @slow\n     @require_bitsandbytes\n     def test_generation_siglip_backbone(self):\n         model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n@@ -621,7 +614,6 @@ def test_generation_siglip_backbone(self):\n         decoded_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n         self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n-    @slow\n     def test_pixtral(self):\n         model_id = \"mistral-community/pixtral-12b\"\n         model = LlavaForConditionalGeneration.from_pretrained(model_id)\n@@ -651,7 +643,6 @@ def test_pixtral(self):\n         # check that both inputs are handled correctly and generate the same output\n         self.assertEqual(output, EXPECTED_GENERATION)\n \n-    @slow\n     @require_bitsandbytes\n     def test_pixtral_4bit(self):\n         model_id = \"mistral-community/pixtral-12b\"\n@@ -680,7 +671,6 @@ def test_pixtral_4bit(self):\n         EXPECTED_GENERATION = EXPECTED_GENERATIONS.get_expectation()\n         self.assertTrue(output in EXPECTED_GENERATION)\n \n-    @slow\n     @require_bitsandbytes\n     def test_pixtral_batched(self):\n         model_id = \"mistral-community/pixtral-12b\""
        },
        {
            "sha": "d4738501bcd94ffedae4b7b7c8dfd630e5cef6f0",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -398,6 +398,7 @@ def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n \n+@slow\n @require_torch\n class MambaIntegrationTests(unittest.TestCase):\n     def setUp(self):\n@@ -445,7 +446,6 @@ def test_simple_generate_cuda_kernels_tiny(self, device):\n         self.assertEqual(output_sentence, expected_output)\n \n     @parameterized.expand([(torch_device,), (\"cpu\",)])\n-    @slow\n     def test_simple_generate_cuda_kernels_small(self, device):\n         expected_output = \"Hello my name is\\n\\nI am a\\n\\nI am a\"\n \n@@ -458,7 +458,6 @@ def test_simple_generate_cuda_kernels_small(self, device):\n         self.assertEqual(output_sentence, expected_output)\n \n     @parameterized.expand([(torch_device,), (\"cpu\",)])\n-    @slow\n     def test_simple_generate_cuda_kernels_mid(self, device):\n         expected_output = \"Hello my name is John and I am a\\n\\nI am a single father of a beautiful daughter. I am a\"\n \n@@ -471,7 +470,6 @@ def test_simple_generate_cuda_kernels_mid(self, device):\n         self.assertEqual(output_sentence, expected_output)\n \n     @parameterized.expand([(torch_device,), (\"cpu\",)])\n-    @slow\n     def test_simple_generate_cuda_kernels_big(self, device):\n         expected_output = \"Hello my name is John and I am a new member of this forum. I am a retired Marine and I am a member of the Marine Corps League. I am a\"\n \n@@ -483,7 +481,6 @@ def test_simple_generate_cuda_kernels_big(self, device):\n \n         self.assertEqual(output_sentence, expected_output)\n \n-    @slow\n     @pytest.mark.torch_compile_test\n     def test_compile_mamba_cache(self):\n         expected_output = \"Hello my name is John and I am a\\n\\nI am a single father of a beautiful daughter. I am a\""
        },
        {
            "sha": "8de6b16c7721fccf12972cd49b574e32c4ccecf1",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -418,6 +418,7 @@ def model(self):\n @require_torch\n @require_sentencepiece\n @require_tokenizers\n+@slow\n class MBartEnroIntegrationTest(AbstractSeq2SeqIntegrationTest):\n     checkpoint_name = \"facebook/mbart-large-en-ro\"\n     src_text = [\n@@ -432,7 +433,6 @@ class MBartEnroIntegrationTest(AbstractSeq2SeqIntegrationTest):\n     ]\n     expected_src_tokens = [8274, 127873, 25916, 7, 8622, 2071, 438, 67485, 53, 187895, 23, 51712, 2, 250004]\n \n-    @slow\n     def test_enro_generate_one(self):\n         batch: BatchEncoding = self.tokenizer(\n             [\"UN Chief Says There Is No Military Solution in Syria\"], return_tensors=\"pt\"\n@@ -442,7 +442,6 @@ def test_enro_generate_one(self):\n         self.assertEqual(self.tgt_text[0], decoded[0])\n         # self.assertEqual(self.tgt_text[1], decoded[1])\n \n-    @slow\n     def test_enro_generate_batch(self):\n         batch: BatchEncoding = self.tokenizer(self.src_text, return_tensors=\"pt\", padding=True, truncation=True).to(\n             torch_device\n@@ -489,6 +488,7 @@ def test_mbart_fast_forward(self):\n @require_torch\n @require_sentencepiece\n @require_tokenizers\n+@slow\n class MBartCC25IntegrationTest(AbstractSeq2SeqIntegrationTest):\n     checkpoint_name = \"facebook/mbart-large-cc25\"\n     src_text = [\n@@ -507,7 +507,6 @@ def test_cc25_generate(self):\n         decoded = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n         self.assertEqual(self.tgt_text[0], decoded[0])\n \n-    @slow\n     def test_fill_mask(self):\n         inputs = self.tokenizer([\"One of the best <mask> I ever read!\"], return_tensors=\"pt\").to(torch_device)\n         outputs = self.model.generate("
        },
        {
            "sha": "12b60f8e4d1de2c01f361ff7d452f869f8528f76",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -190,14 +190,14 @@ def test_model(self):\n \n \n @require_torch\n+@slow\n class Olmo2IntegrationTest(unittest.TestCase):\n     def setUp(self):\n         cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n-    @slow\n     def test_model_1b_logits_bfloat16(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = Olmo2ForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\").to(torch_device, torch.bfloat16)\n@@ -220,7 +220,6 @@ def test_model_1b_logits_bfloat16(self):\n         EXPECTED_SLICE = torch.tensor(expectations.get_expectation(), device=torch_device)\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n-    @slow\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = Olmo2ForCausalLM.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\").to(torch_device, dtype=torch.bfloat16)\n@@ -242,7 +241,6 @@ def test_model_7b_logits(self):\n         EXPECTED_SLICE = torch.tensor(expectations.get_expectation(), device=torch_device)\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n-    @slow\n     def test_model_7b_greedy_generation(self):\n         EXPECTED_TEXT_COMPLETION = \"\"\"Simply put, the theory of relativity states that 1) the speed of light is constant, 2) the speed of light is the fastest speed possible, and 3) the speed of light is the same for all observers, regardless of their relative motion. The theory of relativity is based on the idea that the speed of light is constant. This means that\"\"\"\n         prompt = \"Simply put, the theory of relativity states that \"\n@@ -285,7 +283,6 @@ def test_simple_encode_decode(self):\n         self.assertEqual(rust_tokenizer.encode(\" Hello\"), [22691])\n \n     @pytest.mark.torch_export_test\n-    @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")"
        },
        {
            "sha": "f89b95ec984564da5ec39ea793197ae0c82367d2",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -437,6 +437,7 @@ def test_plbart_fast_forward(self):\n @require_torch\n @require_sentencepiece\n @require_tokenizers\n+@slow\n class PLBartBaseIntegrationTest(AbstractSeq2SeqIntegrationTest):\n     checkpoint_name = \"uclanlp/plbart-base\"\n     src_text = [\"Is 0 the first Fibonacci number ?\", \"Find the sum of all prime numbers .\"]\n@@ -452,7 +453,6 @@ def test_base_generate(self):\n         decoded = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n         self.assertEqual(self.tgt_text[0], decoded[0])\n \n-    @slow\n     def test_fill_mask(self):\n         inputs = self.tokenizer([\"Is 0 the <mask> Fibonacci <mask> ?\"], return_tensors=\"pt\").to(torch_device)\n         src_lan = self.tokenizer._convert_lang_code_special_format(\"en_XX\")"
        },
        {
            "sha": "1bcfa7f4e808b4c0a978cef0d699937f529c71d1",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -680,6 +680,7 @@ def config_and_inputs(self):\n @require_sentencepiece\n @require_tokenizers\n @require_torch_non_multi_accelerator\n+@slow\n class RagModelIntegrationTests(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n@@ -751,7 +752,6 @@ def get_rag_config(self):\n             dataset_revision=\"b24a417\",\n         )\n \n-    @slow\n     def test_rag_sequence_inference(self):\n         rag_config = self.get_rag_config()\n         rag_decoder_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n@@ -790,7 +790,6 @@ def test_rag_sequence_inference(self):\n         expected_loss = torch.tensor([36.7368]).to(torch_device)\n         _assert_tensors_equal(expected_loss, output.loss, atol=TOLERANCE)\n \n-    @slow\n     def test_rag_token_inference(self):\n         rag_config = self.get_rag_config()\n         rag_decoder_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n@@ -829,7 +828,6 @@ def test_rag_token_inference(self):\n         expected_loss = torch.tensor([36.3557]).to(torch_device)\n         _assert_tensors_equal(expected_loss, output.loss, atol=TOLERANCE)\n \n-    @slow\n     def test_rag_token_generate_beam(self):\n         rag_config = self.get_rag_config()\n         rag_decoder_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n@@ -868,7 +866,6 @@ def test_rag_token_generate_beam(self):\n         self.assertEqual(output_text_1, EXPECTED_OUTPUT_TEXT_1)\n         self.assertEqual(output_text_2, EXPECTED_OUTPUT_TEXT_2)\n \n-    @slow\n     def test_rag_sequence_generate_beam(self):\n         rag_config = self.get_rag_config()\n         rag_decoder_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n@@ -908,7 +905,7 @@ def test_rag_sequence_generate_beam(self):\n         self.assertEqual(output_text_2, EXPECTED_OUTPUT_TEXT_2)\n \n     @property\n-    def test_data_questions(self):\n+    def questions_data(self):\n         return [\n             \"who got the first nobel prize in physics\",\n             \"when is the next deadpool movie being released\",\n@@ -920,7 +917,6 @@ def test_data_questions(self):\n             \"how many episodes are there in dragon ball z\",\n         ]\n \n-    @slow\n     def test_rag_sequence_generate_batch(self):\n         tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n         retriever = RagRetriever.from_pretrained(\n@@ -934,7 +930,7 @@ def test_rag_sequence_generate_batch(self):\n         )\n \n         input_dict = tokenizer(\n-            self.test_data_questions,\n+            self.questions_data,\n             return_tensors=\"pt\",\n             padding=True,\n             truncation=True,\n@@ -963,7 +959,6 @@ def test_rag_sequence_generate_batch(self):\n         ]\n         self.assertListEqual(outputs, EXPECTED_OUTPUTS)\n \n-    @slow\n     def test_rag_sequence_generate_batch_from_context_input_ids(self):\n         tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n         retriever = RagRetriever.from_pretrained(\n@@ -977,7 +972,7 @@ def test_rag_sequence_generate_batch_from_context_input_ids(self):\n         )\n \n         input_dict = tokenizer(\n-            self.test_data_questions,\n+            self.questions_data,\n             return_tensors=\"pt\",\n             padding=True,\n             truncation=True,\n@@ -1016,7 +1011,6 @@ def test_rag_sequence_generate_batch_from_context_input_ids(self):\n         ]\n         self.assertListEqual(outputs, EXPECTED_OUTPUTS)\n \n-    @slow\n     def test_rag_token_generate_batch(self):\n         tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n         retriever = RagRetriever.from_pretrained(\n@@ -1030,7 +1024,7 @@ def test_rag_token_generate_batch(self):\n             rag_token.half()\n \n         input_dict = tokenizer(\n-            self.test_data_questions,\n+            self.questions_data,\n             return_tensors=\"pt\",\n             padding=True,\n             truncation=True,"
        },
        {
            "sha": "ff4b5a3f40b9d194ece14222e0dd6506a60d53c4",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -663,6 +663,7 @@ def prepare_img():\n \n @require_torch\n @require_vision\n+@slow\n class RTDetrModelIntegrationTest(unittest.TestCase):\n     @cached_property\n     def default_image_processor(self):"
        },
        {
            "sha": "d652af75b47941014f06be5b1340610790b1668b",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -667,6 +667,7 @@ def prepare_img():\n \n @require_torch\n @require_vision\n+@slow\n class RTDetrV2ModelIntegrationTest(unittest.TestCase):\n     @cached_property\n     def default_image_processor(self):"
        },
        {
            "sha": "ef58d0017213d2b85474a724d7b3c10c039f10a6",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -28,6 +28,7 @@\n     require_torch,\n     require_torch_fp16,\n     require_torchaudio,\n+    slow,\n     torch_device,\n )\n \n@@ -605,6 +606,7 @@ def test_generate_without_input_ids(self):\n @require_torchaudio\n @require_sentencepiece\n @require_tokenizers\n+@slow\n @unittest.skip(\"@eustlb broken in a weird way. To investigate later.\")\n class Speech2TextModelIntegrationTests(unittest.TestCase):\n     @classmethod"
        },
        {
            "sha": "ade0a31217ffc11fc5460a58690e5182fca35d42",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -963,6 +963,7 @@ def _mock_init_weights(self, module):\n @require_torch\n @require_sentencepiece\n @require_tokenizers\n+@slow\n class SpeechT5ForTextToSpeechIntegrationTests(unittest.TestCase):\n     @cached_property\n     def default_model(self):"
        },
        {
            "sha": "70284b68ab082ac2da5170fbd40265b6d78bb143",
            "filename": "tests/models/tvp/test_modeling_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -17,7 +17,7 @@\n from functools import cached_property\n \n from transformers import ResNetConfig, TimmBackboneConfig, TvpConfig\n-from transformers.testing_utils import require_timm, require_torch, require_vision, torch_device\n+from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_modeling_common import (\n@@ -234,6 +234,7 @@ def prepare_img():\n \n @require_vision\n @require_torch\n+@slow\n class TvpModelIntegrationTests(unittest.TestCase):\n     @cached_property\n     def default_image_processor(self):"
        },
        {
            "sha": "be0db6be28b3835718810539bfca41b02fe2733f",
            "filename": "tests/models/xlm_roberta/test_modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd416f3c5c6974b5afd211759624283777a45c6e/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py?ref=cd416f3c5c6974b5afd211759624283777a45c6e",
            "patch": "@@ -33,8 +33,8 @@\n @require_sentencepiece\n @require_tokenizers\n @require_torch\n+@slow\n class XLMRobertaModelIntegrationTest(unittest.TestCase):\n-    @slow\n     def test_xlm_roberta_base(self):\n         model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\", attn_implementation=\"eager\")\n         input_ids = torch.tensor([[0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2]])\n@@ -69,7 +69,6 @@ def test_xlm_roberta_base_sdpa(self):\n         # compare the actual values for a slice of last dim\n         torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n-    @slow\n     def test_xlm_roberta_large(self):\n         model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n         input_ids = torch.tensor([[0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2]])"
        }
    ],
    "stats": {
        "total": 83,
        "additions": 26,
        "deletions": 57
    }
}