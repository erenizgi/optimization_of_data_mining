{
    "author": "seopp",
    "message": "ğŸŒ [i18n-KO] Translated `tokenizer.md` to Korean (#39532)\n\n* docs: ko: tokenizer.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Yijun Lee <yijun-lee@users.noreply.github.com>\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* fix: resolve suggestions\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>",
    "sha": "f72311796b2bf60e0c697545b45cf4ff26665e94",
    "files": [
        {
            "sha": "99f7ea8da813899d53198055747682862c2456eb",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f72311796b2bf60e0c697545b45cf4ff26665e94/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/f72311796b2bf60e0c697545b45cf4ff26665e94/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=f72311796b2bf60e0c697545b45cf4ff26665e94",
            "patch": "@@ -336,8 +336,8 @@\n       title: PEFT\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Pipelines\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Processors\n+    - local: main_classes/tokenizer\n+      title: í† í¬ë‚˜ì´ì €\n     - local: main_classes/quantization\n       title: ì–‘ìí™”\n     - local: in_translation"
        },
        {
            "sha": "f87edbebeb004ad88c63c6960d07ebebe2645100",
            "filename": "docs/source/ko/main_classes/tokenizer.md",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/f72311796b2bf60e0c697545b45cf4ff26665e94/docs%2Fsource%2Fko%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f72311796b2bf60e0c697545b45cf4ff26665e94/docs%2Fsource%2Fko%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Ftokenizer.md?ref=f72311796b2bf60e0c697545b45cf4ff26665e94",
            "patch": "@@ -0,0 +1,82 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# í† í¬ë‚˜ì´ì €[[tokenizer]]\n+\n+í† í¬ë‚˜ì´ì €ëŠ” ëª¨ë¸ì˜ ì…ë ¥ì„ ì¤€ë¹„í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” ëª¨ë“  ëª¨ë¸ì„ ìœ„í•œ í† í¬ë‚˜ì´ì €ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ í† í¬ë‚˜ì´ì €ëŠ” ë‘ ê°€ì§€ ë²„ì „ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤. ì™„ì „í•œ íŒŒì´ì¬ êµ¬í˜„ê³¼ Rust ë¼ì´ë¸ŒëŸ¬ë¦¬ [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)ì— ê¸°ë°˜í•œ \"Fast\" êµ¬í˜„ì…ë‹ˆë‹¤. \"Fast\" êµ¬í˜„ì€ ë‹¤ìŒì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤:\n+\n+1. íŠ¹íˆ ë°°ì¹˜ í† í°í™”ë¥¼ ìˆ˜í–‰í•  ë•Œ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.\n+2. ì›ë³¸ ë¬¸ìì—´(ë¬¸ì ë° ë‹¨ì–´)ê³¼ í† í° ê³µê°„ ì‚¬ì´ë¥¼ ë§¤í•‘í•˜ëŠ” ì¶”ê°€ì ì¸ ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. (ì˜ˆ: íŠ¹ì • ë¬¸ìë¥¼ í¬í•¨í•˜ëŠ” í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ ì–»ê±°ë‚˜, íŠ¹ì • í† í°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì ë²”ìœ„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë“±).\n+\n+ê¸°ë³¸ í´ë˜ìŠ¤ì¸ [`PreTrainedTokenizer`]ì™€ [`PreTrainedTokenizerFast`]ëŠ” ë¬¸ìì—´ ì…ë ¥ì„ ì¸ì½”ë”©í•˜ëŠ” ë©”ì†Œë“œë¥¼ êµ¬í˜„í•˜ë©°(ì•„ë˜ ì°¸ì¡°), ë¡œì»¬ íŒŒì¼ì´ë‚˜ ë””ë ‰í† ë¦¬, ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €(HuggingFaceì˜ AWS S3 ì €ì¥ì†Œì—ì„œ ë‹¤ìš´ë¡œë“œëœ)ë¡œë¶€í„° íŒŒì´ì¬ ë° \"Fast\" í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê±°ë‚˜ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ë‘ í´ë˜ìŠ¤ëŠ” ê³µí†µ ë©”ì†Œë“œë¥¼ í¬í•¨í•˜ëŠ” [`~tokenization_utils_base.PreTrainedTokenizerBase`]ì™€ [`~tokenization_utils_base.SpecialTokensMixin`]ì— ì˜ì¡´í•©ë‹ˆë‹¤.\n+\n+[`PreTrainedTokenizer`]ì™€ [`PreTrainedTokenizerFast`]ëŠ” ëª¨ë“  í† í¬ë‚˜ì´ì €ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë©”ì†Œë“œë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤:\n+\n+- í† í°í™”(ë¬¸ìì—´ì„ í•˜ìœ„ ë‹¨ì–´ í† í° ë¬¸ìì—´ë¡œ ë¶„í• ), í† í° ë¬¸ìì—´ì„ IDë¡œ ë³€í™˜ ë° ê·¸ ë°˜ëŒ€ ê³¼ì •, ê·¸ë¦¬ê³  ì¸ì½”ë”©/ë””ì½”ë”©(ì¦‰, í† í°í™” ë° ì •ìˆ˜ë¡œ ë³€í™˜)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+- êµ¬ì¡°(BPE, SentencePiece ë“±)ì— êµ¬ì• ë°›ì§€ ì•Šê³  ì–´íœ˜ì— ìƒˆë¡œìš´ í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n+- íŠ¹ìˆ˜ í† í°(ë§ˆìŠ¤í¬, ë¬¸ì¥ ì‹œì‘ ë“±) ê´€ë¦¬: í† í°ì„ ì¶”ê°€í•˜ê³ , ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í† í¬ë‚˜ì´ì €ì˜ ì†ì„±ì— í• ë‹¹í•˜ë©°, í† í°í™” ê³¼ì •ì—ì„œ ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n+\n+[`BatchEncoding`]ì€ [`~tokenization_utils_base.PreTrainedTokenizerBase`]ì˜ ì¸ì½”ë”© ë©”ì†Œë“œ(`__call__`, `encode_plus`, `batch_encode_plus`)ì˜ ì¶œë ¥ì„ ë‹´ê³  ìˆìœ¼ë©°, íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ê°€ ìˆœìˆ˜ íŒŒì´ì¬ í† í¬ë‚˜ì´ì €ì¸ ê²½ìš° ì´ í´ë˜ìŠ¤ëŠ” í‘œì¤€ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ë™ì‘í•˜ë©°, ì´ëŸ¬í•œ ë©”ì†Œë“œë“¤ë¡œ ê³„ì‚°ëœ ë‹¤ì–‘í•œ ëª¨ë¸ ì…ë ¥(`input_ids`, `attention_mask` ë“±)ì„ ê°–ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ê°€ \"Fast\" í† í¬ë‚˜ì´ì €ì¼ ê²½ìš°(ì¦‰, HuggingFace [tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/tokenizers) ê¸°ë°˜ì¼ ê²½ìš°), ì´ í´ë˜ìŠ¤ëŠ” ì¶”ê°€ì ìœ¼ë¡œ ì›ë³¸ ë¬¸ìì—´(ë¬¸ì ë° ë‹¨ì–´)ê³¼ í† í° ê³µê°„ ì‚¬ì´ë¥¼ ë§¤í•‘í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ê³ ê¸‰ ì •ë ¬ ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤ (ì˜ˆ: íŠ¹ì • ë¬¸ìë¥¼ í¬í•¨í•˜ëŠ” í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ ì–»ê±°ë‚˜, íŠ¹ì • í† í°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì ë²”ìœ„ë¥¼ ì–»ëŠ” ë“±).\n+\n+\n+# ë©€í‹°ëª¨ë‹¬ í† í¬ë‚˜ì´ì €[[multimodal-tokenizer]]\n+\n+ê·¸ ì™¸ì—ë„ ê° í† í¬ë‚˜ì´ì €ëŠ” \"ë©€í‹°ëª¨ë‹¬\" í† í¬ë‚˜ì´ì €ê°€ ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í† í¬ë‚˜ì´ì €ê°€ ëª¨ë“  ê´€ë ¨ íŠ¹ìˆ˜ í† í°ì„ í† í¬ë‚˜ì´ì € ì†ì„±ì˜ ì¼ë¶€ë¡œ ì €ì¥í•˜ì—¬ ë” ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, LLaVAì™€ ê°™ì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜¤ë©´, `tokenizer.image_token_id`ì— ì ‘ê·¼í•˜ì—¬ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜ ì´ë¯¸ì§€ í† í°ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ëª¨ë“  ìœ í˜•ì˜ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€ íŠ¹ìˆ˜ í† í°ì„ í™œì„±í™”í•˜ë ¤ë©´, ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê³  í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ê°€ íŠ¹ìˆ˜ í† í°ì€ ë°˜ë“œì‹œ íŠ¹ì • ëª¨ë‹¬ë¦¬í‹°ì™€ ê´€ë ¨ë  í•„ìš”ëŠ” ì—†ìœ¼ë©°, ëª¨ë¸ì´ ìì£¼ ì ‘ê·¼í•´ì•¼ í•˜ëŠ” ì–´ë–¤ ê²ƒì´ë“  ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì½”ë“œì—ì„œ `output_dir`ì— ì €ì¥ëœ í† í¬ë‚˜ì´ì €ëŠ” ì„¸ ê°œì˜ ì¶”ê°€ íŠ¹ìˆ˜ í† í°ì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n+\n+```python\n+vision_tokenizer = AutoTokenizer.from_pretrained(\n+    \"llava-hf/llava-1.5-7b-hf\",\n+    extra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n+)\n+print(vision_tokenizer.image_token, vision_tokenizer.image_token_id)\n+(\"<image>\", 32000)\n+```\n+\n+## PreTrainedTokenizer[[transformers.PreTrainedTokenizer]]\n+\n+[[autodoc]] PreTrainedTokenizer\n+    - __call__\n+    - add_tokens\n+    - add_special_tokens\n+    - apply_chat_template\n+    - batch_decode\n+    - decode\n+    - encode\n+    - push_to_hub\n+    - all\n+\n+\n+## PreTrainedTokenizerFast[[transformers.PreTrainedTokenizerFast]]\n+\n+[`PreTrainedTokenizerFast`]ëŠ” [tokenizers](https://huggingface.co/docs/tokenizers) ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì˜ì¡´í•©ë‹ˆë‹¤. ğŸ¤— tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì–»ì€ í† í¬ë‚˜ì´ì €ëŠ”\n+ğŸ¤— transformersë¡œ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ í•˜ëŠ”ì§€ ì•Œì•„ë³´ë ¤ë©´ [Using tokenizers from ğŸ¤— tokenizers](../fast_tokenizers) í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+[[autodoc]] PreTrainedTokenizerFast\n+    - __call__\n+    - add_tokens\n+    - add_special_tokens\n+    - apply_chat_template\n+    - batch_decode\n+    - decode\n+    - encode\n+    - push_to_hub\n+    - all\n+\n+## BatchEncoding[[transformers.BatchEncoding]]\n+\n+[[autodoc]] BatchEncoding\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 84,
        "deletions": 2
    }
}