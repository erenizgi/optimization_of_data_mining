{
    "author": "wenhuach21",
    "message": "Add AutoRound quantization support (#37393)\n\n* add auto-round support\n\n* Update src/transformers/quantizers/auto.py\n\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>\n\n* fix style issue\n\nSigned-off-by: wenhuach <wenhuach87@gmail.com>\n\n* tiny change\n\n* tiny change\n\n* refine ut and doc\n\n* revert unnecessary change\n\n* tiny change\n\n* try to fix style issue\n\n* try to fix style issue\n\n* try to fix style issue\n\n* try to fix style issue\n\n* try to fix style issue\n\n* try to fix style issue\n\n* try to fix style issue\n\n* fix doc issue\n\n* Update tests/quantization/autoround/test_auto_round.py\n\n* fix comments\n\n* Update tests/quantization/autoround/test_auto_round.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update tests/quantization/autoround/test_auto_round.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* update doc\n\n* Update src/transformers/quantizers/quantizer_auto_round.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* update\n\n* update\n\n* fix\n\n* try to fix style issue\n\n* Update src/transformers/quantizers/auto.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update docs/source/en/quantization/auto_round.md\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update docs/source/en/quantization/auto_round.md\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update docs/source/en/quantization/auto_round.md\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* update\n\n* fix style issue\n\n* update doc\n\n* update doc\n\n* Refine the doc\n\n* refine doc\n\n* revert one change\n\n* set sym to True by default\n\n* Enhance the unit test's robustness.\n\n* update\n\n* add torch dtype\n\n* tiny change\n\n* add awq convert test\n\n* fix typo\n\n* update\n\n* fix packing format issue\n\n* use one gpu\n\n---------\n\nSigned-off-by: wenhuach <wenhuach87@gmail.com>\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Shen, Haihao <haihao.shen@intel.com>",
    "sha": "b3492ff9f7ddac6592f7d1861ef19127d6297129",
    "files": [
        {
            "sha": "504ed9704ce161d5a072b63e484ed791677e7d98",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -84,6 +84,9 @@ RUN python3 -m pip install --no-cache-dir compressed-tensors\n # Add AMD Quark for quantization testing\n RUN python3 -m pip install --no-cache-dir amd-quark\n \n+# Add AutoRound for quantization testing\n+RUN python3 -m pip install --no-cache-dir \"auto-round>=0.5.0\"\n+\n # Add transformers in editable mode\n RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch]\n "
        },
        {
            "sha": "c137d51b5a504a7b188ae8cdae375a67779e1553",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -92,3 +92,7 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n ## QuarkConfig\n \n [[autodoc]] QuarkConfig\n+\n+## AutoRoundConfig\n+\n+[[autodoc]] AutoRoundConfig"
        },
        {
            "sha": "63c9e0b209518acd1e76d634bdcd89a23d748aa8",
            "filename": "docs/source/en/quantization/auto_round.md",
            "status": "added",
            "additions": 286,
            "deletions": 0,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -0,0 +1,286 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# AutoRound\n+\n+[AutoRound](https://github.com/intel/auto-round) is an advanced quantization algorithm that delivers strong accuracy, even at 2-bit precision. \n+It leverages sign gradient descent to fine-tune both rounding values and min-max clipping thresholds in just 200 steps. Designed for broad compatibility, it seamlessly supports a wide range of LLMs and is actively expanding to cover more VLMs as well. \n+It also supports quantization and inference across multiple hardware platforms, including CPU, XPU, and CUDA.\n+\n+AutoRound also offers a variety of useful features, including mixed-bit tuning and inference, lm-head quantization, support for exporting to formats like GPTQ/AWQ/GGUF, and flexible tuning recipes. \n+For a comprehensive overview and the latest updates, check out the AutoRound [README](https://github.com/intel/auto-round).\n+\n+AutoRound was originally developed as part of the [Intel Neural Compressor](https://github.com/intel/neural-compressor), serving as a general-purpose model compression library for deep learning. \n+It has since evolved into a standalone library focused specifically on low-precision optimization for large language models (LLMs). \n+AutoRound remains fully integrated with the Intel Neural Compressor, and you can explore the repository for more details.\n+\n+\n+## Installation\n+\n+```bash\n+pip install auto-round\n+```\n+\n+## Supported Quantization Configurations\n+\n+AutoRound supports several quantization configurations:\n+\n+- **Int8 Weight Only**\n+- **Int4 Weight Only**\n+- **Int3 Weight Only**\n+- **Int2 Weight Only**\n+- **Mixed bits Weight only**\n+\n+## Hardware Compatibility\n+\n+CPU, XPU, and CUDA for both quantization and inference.\n+\n+## Quantization and Serialization (offline)\n+\n+Currently, only offline mode is supported to generate quantized models.\n+\n+<hfoptions id=\"quantization\">\n+<hfoption id=\"quantization cmd\">\n+\n+### Command Line Usage\n+```bash\n+auto-round \\\n+    --model facebook/opt-125m \\\n+    --bits 4 \\\n+    --group_size 128 \\\n+    --output_dir ./tmp_autoround\n+```\n+\n+AutoRound also offer another two recipes, `auto-round-best` and `auto-round-light`, designed for optimal accuracy and improved speed, respectively. \n+For 2 bits, we recommend using `auto-round-best` or `auto-round`.\n+</hfoption>\n+\n+<hfoption id=\"quantization auto-round api\">\n+\n+### AutoRound API Usage\n+This setting offers a better trade-off between accuracy and tuning cost, and is recommended in all scenarios.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from auto_round import AutoRound\n+\n+model_name = \"facebook/opt-125m\"\n+model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+bits, group_size, sym = 4, 128, True\n+# mixed bits config\n+# layer_config = {\"model.decoder.layers.6.self_attn.out_proj\": {\"bits\": 2, \"group_size\": 32}}\n+autoround = AutoRound(\n+    model,\n+    tokenizer,\n+    bits=bits,\n+    group_size=group_size,\n+    sym=sym,\n+    # enable_torch_compile=True,\n+    # layer_config=layer_config,\n+)\n+\n+output_dir = \"./tmp_autoround\"\n+# format= 'auto_round'(default), 'auto_gptq', 'auto_awq'\n+autoround.quantize_and_save(output_dir, format='auto_round') \n+```\n+\n+</hfoption>\n+\n+<hfoption id=\"quantization auto-round-best\">\n+\n+### AutoRoundBest recipe\n+This setting provides the best accuracy in most scenarios but is 4â€“5Ã— slower than the standard AutoRound recipe. It is especially recommended for 2-bit quantization and is a good choice if sufficient resources are available.\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from auto_round import AutoRound\n+\n+model_name = \"facebook/opt-125m\"\n+model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+bits, group_size, sym = 4, 128, True\n+autoround = AutoRound(\n+    model,\n+    tokenizer,\n+    bits=bits,\n+    group_size=group_size,\n+    sym=sym,\n+    nsamples=512,\n+    iters=1000,\n+    low_gpu_mem_usage=True\n+)\n+\n+output_dir = \"./tmp_autoround\"\n+autoround.quantize_and_save(output_dir, format='auto_round') \n+```\n+</hfoption>\n+\n+<hfoption id=\"quantization auto-round-light\">\n+\n+### AutoRoundLight recipe\n+This setting offers the best speed (2 - 3X faster than AutoRound), but it may cause a significant accuracy drop for small models and 2-bit quantization. It is recommended for 4-bit settings and models larger than 3B.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from auto_round import AutoRound\n+\n+model_name = \"facebook/opt-125m\"\n+model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+bits, group_size, sym = 4, 128, True\n+autoround = AutoRound(\n+    model,\n+    tokenizer,\n+    bits=bits,\n+    group_size=group_size,\n+    sym=sym,\n+    iters=50,\n+    lr=5e-3,\n+)\n+\n+output_dir = \"./tmp_autoround\"\n+autoround.quantize_and_save(output_dir, format='auto_round') \n+```\n+\n+</hfoption>\n+\n+</hfoptions>\n+\n+W4G128 Average Accuracy of 13 tasks (mmlu-pro, if_eval, gsm8k, etc) and Time Cost Results (Testing was conducted on the Nvidia A100 80G using the version of PyTorch 2.6.0 with enable_torch_compile):\n+\n+| Model   | Qwen2.5-0.5B-Instruct | Falcon3-3B    | Qwen2.5-7B-Instruct | Meta-Llama-3.1-8B-Instruct | Falcon3-10B   | Qwen2.5-72B-Instruct |\n+|---------|--------------------|---------------|------------------|----------------------------|---------------|-------------------|\n+| 16bits  | 0.4192             | 0.5203        | 0.6470           | 0.6212                     | 0.6151        | 0.7229            |\n+| Best    | **0.4137**(7m)     | **0.5142**(23m) | 0.6426(58m)      | **0.6116**(65m)            | **0.6092**(81m) | 0.7242(575m)      |\n+| Default | 0.4129(2m)         | 0.5133(6m)    | 0.6441(13m)      | 0.6106(13m)                | 0.6080(18m)   | **0.7252**(118m)  |\n+| Light   | 0.4052(2m)         | 0.5108(3m)    | **0.6453**(5m)   | 0.6104(6m)                 | 0.6063(6m)    | 0.7243(37m)       |\n+\n+## Inference\n+\n+AutoRound automatically selects the best available backend based on the installed libraries and prompts the user to install additional libraries when a better backend is found.\n+<hfoptions id=\"inference\">\n+<hfoption id=\"inference cpu\">\n+\n+### CPU\n+\n+Supports 2, 4, and 8 bits. We recommend using intel-extension-for-pytorch (IPEX) for 4 bits inference.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+text = \"There is a girl who likes adventure,\"\n+inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n+```\n+\n+<hfoption>\n+\n+<hfoption id=\"inference xpu\">\n+\n+### XPU\n+\n+Supports 4 bits only. We recommend using intel-extension-for-pytorch (IPEX) for inference.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"xpu\", torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+text = \"There is a girl who likes adventure,\"\n+inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n+```\n+\n+<hfoption>\n+\n+<hfoption id=\"inference cuda\">\n+\n+### CUDA\n+\n+Supports 2, 3, 4, and 8 bits. We recommend using GPTQModel for 4 and 8 bits inference.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+text = \"There is a girl who likes adventure,\"\n+inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n+```\n+\n+<hfoption>\n+\n+<hfoption id=\"inference backend\">\n+\n+### Specify Inference Backend\n+\n+AutoRound automatically selects the backend for each layer based on compatibility. In general, the priority order is Marlin > ExLLaMAV2 > Triton, but the final choice depends on factors such as group size, bit width, packing format, hardware device, and other implementation details. For more details, please refer to [backends](https://github.com/intel/auto-round?tab=readme-ov-file#specify-backend),\n+\n+The backend may not always be the most suitable for certain devices. \n+You can specify your preferred backend such as \"ipex\" for CPU and CPU, \"marlin/exllamav2/triton\" for CUDA, according to your needs or hardware compatibility. Please note that additional corresponding libraries may be required.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig\n+\n+model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n+quantization_config = AutoRoundConfig(backend=\"ipex\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", quantization_config=quantization_config, torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+text = \"There is a girl who likes adventure,\"\n+inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n+```\n+\n+<hfoption>\n+\n+\n+<hfoption id=\"format convert\">\n+\n+### Convert GPTQ/AWQ to AutoRound\n+\n+Most GPTQ/AWQ models can be converted to the AutoRound format for better compatibility and support with Intel devices. Please note that the quantization config will be changed if the model is serialized.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig\n+\n+model_name = \"ybelkada/opt-125m-gptq-4bit\"\n+quantization_config = AutoRoundConfig()\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", quantization_config=quantization_config, torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+text = \"There is a girl who likes adventure,\"\n+inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n+```\n+\n+<hfoption>\n+\n+<hfoptions>\n+\n+## Issues\n+\n+If you encounter any issues with the transformers integration, please open an issue on\n+the [transformers](https://github.com/huggingface/transformers/issues) repository.  \n+If you encounter any issues with auto-round, please open an issue on\n+the [AutoRound](https://github.com/intel/auto-round/issues) repository.\n+\n+\n+## Acknowledgement\n+Special thanks to open-source low precision libraries such as AutoGPTQ, AutoAWQ, GPTQModel, Triton, Marlin, and ExLLaMAV2 for providing low-precision CUDA kernels, which are leveraged in AutoRound.\n+\n+## Contribution\n+Contributions to [AutoRound](https://github.com/intel/auto-round/pulls) are welcome and greatly appreciated!\n+Whether it's fixing bugs, improving documentation, adding new features, or suggesting improvements, your help is always valued.\n\\ No newline at end of file"
        },
        {
            "sha": "be7590918606d0ad15e32882c53469aeb3818b09",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -22,25 +22,26 @@ Transformers supports many quantization methods, each with their pros and cons,\n \n Use the Space below to help you pick a quantization method depending on your hardware and number of bits to quantize to.\n \n-| Quantization Method                           | On the fly quantization | CPU             | CUDA GPU | ROCm GPU  | Metal (Apple Silicon)              | Intel GPU       | Torch compile() | Bits          | PEFT Fine Tuning | Serializable with ðŸ¤—Transformers | ðŸ¤—Transformers Support  | Link to library                             |\n-|-----------------------------------------------|----------------------|-----------------|----------|-----------|------------------------------------|-----------------|-----------------|---------------|------------------|-----------------------------|-------------------------|---------------------------------------------|\n-| [AQLM](./aqlm)                             | ðŸ”´                   | ðŸŸ¢              |     ðŸŸ¢     | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/2         | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/Vahe1994/AQLM            |\n-| [AWQ](./awq)                               | ðŸ”´                   | ðŸŸ¢              | ðŸŸ¢        | ðŸŸ¢        | ðŸ”´                                 | ðŸŸ¢              | ?               | 4             | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/casper-hansen/AutoAWQ    |\n-| [bitsandbytes](./bitsandbytes)             | ðŸŸ¢                   | ðŸŸ¡ |     ðŸŸ¢     | ðŸŸ¡ | ðŸ”´                    | ðŸŸ¡ | ðŸ”´ | 4/8         | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n-| [compressed-tensors](./compressed_tensors) | ðŸ”´                   | ðŸŸ¢              |     ðŸŸ¢     | ðŸŸ¢        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 1/8         | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/neuralmagic/compressed-tensors |\n-| [EETQ](./eetq)                             | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ?               | 8             | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/NetEase-FuXi/EETQ        |\n-| [GGUF / GGML (llama.cpp)](../gguf)         | ðŸŸ¢                   | ðŸŸ¢              | ðŸŸ¢        | ðŸ”´        | ðŸŸ¢                                 | ðŸ”´              | ðŸ”´              | 1/8         | ðŸ”´               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n-| [GPTQModel](./gptq)                        | ðŸ”´                   | ðŸŸ¢ | ðŸŸ¢        | ðŸŸ¢        | ðŸŸ¢                                 | ðŸŸ¢ | ðŸ”´              | 2/3/4/8 | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/ModelCloud/GPTQModel        |\n-| [AutoGPTQ](./gptq)                         | ðŸ”´                   | ðŸ”´              | ðŸŸ¢        | ðŸŸ¢        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 2/3/4/8 | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n-| [HIGGS](./higgs)                           | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 2/4         | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/HanGuo97/flute           |       \n-| [HQQ](./hqq)                               | ðŸŸ¢                   | ðŸŸ¢              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/8         | ðŸŸ¢               | ðŸ”´                          | ðŸŸ¢                      | https://github.com/mobiusml/hqq/            |\n-| [optimum-quanto](./quanto)                 | ðŸŸ¢                   | ðŸŸ¢              | ðŸŸ¢        | ðŸ”´        | ðŸŸ¢                                 | ðŸ”´              | ðŸŸ¢              | 2/4/8     | ðŸ”´               | ðŸ”´                          | ðŸŸ¢                      | https://github.com/huggingface/optimum-quanto       |\n-| [FBGEMM_FP8](./fbgemm_fp8)                 | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 8             | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/pytorch/FBGEMM       |\n-| [torchao](./torchao)                       | ðŸŸ¢                   | ðŸŸ¢               | ðŸŸ¢        | ðŸ”´        | ðŸŸ¡ | ðŸ”´              |                 | 4/8         |                  | ðŸŸ¢ðŸ”´                        | ðŸŸ¢                      | https://github.com/pytorch/ao       |\n-| [VPTQ](./vptq)                             | ðŸ”´                   | ðŸ”´              |     ðŸŸ¢     | ðŸŸ¡        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/8         | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/microsoft/VPTQ            |\n-| [FINEGRAINED_FP8](./finegrained_fp8)                 | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 8             | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      |        |\n-| [SpQR](./spqr)                          | ðŸ”´                       |  ðŸ”´   | ðŸŸ¢        | ðŸ”´              |    ðŸ”´    | ðŸ”´         |         ðŸŸ¢              | 3              |              ðŸ”´                     | ðŸŸ¢           | ðŸŸ¢                      | https://github.com/Vahe1994/SpQR/       |\n-| [Quark](./quark)                           | ðŸ”´                       | ðŸŸ¢ | ðŸŸ¢      | ðŸŸ¢      | ðŸŸ¢                   | ðŸŸ¢       | ?               | 2/4/6/8/9/16 | ðŸ”´                | ðŸ”´                               | ðŸŸ¢                       | https://quark.docs.amd.com/latest/                      |\n+| Quantization Method                       | On the fly quantization | CPU             | CUDA GPU | ROCm GPU  | Metal (Apple Silicon)              | Intel GPU       | Torch compile() | Bits         | PEFT Fine Tuning | Serializable with ðŸ¤—Transformers | ðŸ¤—Transformers Support  | Link to library                             |\n+|-------------------------------------------|----------------------|-----------------|----------|-----------|------------------------------------|-----------------|-----------------|--------------|------------------|-----------------------------|-------------------------|---------------------------------------------|\n+| [AQLM](./aqlm)                            | ðŸ”´                   | ðŸŸ¢              |     ðŸŸ¢     | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/2          | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/Vahe1994/AQLM            |\n+| [AutoRound](./auto_round)                 | ðŸ”´                   | ðŸŸ¢               | ðŸŸ¢          |   ðŸ”´        |   ðŸ”´                                |   ðŸŸ¢              |   ðŸ”´               | 2/3/4/8      |    ðŸ”´              |       ðŸŸ¢                      |    ðŸŸ¢                       |      https://github.com/intel/auto-round                                       |\n+| [AWQ](./awq)                              | ðŸ”´                   | ðŸŸ¢              | ðŸŸ¢        | ðŸŸ¢        | ðŸ”´                                 | ðŸŸ¢              | ?               | 4            | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/casper-hansen/AutoAWQ    |\n+| [bitsandbytes](./bitsandbytes)            | ðŸŸ¢                   | ðŸŸ¡ |     ðŸŸ¢     | ðŸŸ¡ | ðŸ”´                    | ðŸŸ¡ | ðŸ”´ | 4/8          | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n+| [compressed-tensors](./compressed_tensors) | ðŸ”´                   | ðŸŸ¢              |     ðŸŸ¢     | ðŸŸ¢        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 1/8          | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/neuralmagic/compressed-tensors |\n+| [EETQ](./eetq)                            | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ?               | 8            | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/NetEase-FuXi/EETQ        |\n+| [GGUF / GGML (llama.cpp)](../gguf)        | ðŸŸ¢                   | ðŸŸ¢              | ðŸŸ¢        | ðŸ”´        | ðŸŸ¢                                 | ðŸ”´              | ðŸ”´              | 1/8          | ðŸ”´               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n+| [GPTQModel](./gptq)                       | ðŸ”´                   | ðŸŸ¢ | ðŸŸ¢        | ðŸŸ¢        | ðŸŸ¢                                 | ðŸŸ¢ | ðŸ”´              | 2/3/4/8      | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/ModelCloud/GPTQModel        |\n+| [AutoGPTQ](./gptq)                        | ðŸ”´                   | ðŸ”´              | ðŸŸ¢        | ðŸŸ¢        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 2/3/4/8      | ðŸŸ¢               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n+| [HIGGS](./higgs)                          | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 2/4          | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/HanGuo97/flute           |       \n+| [HQQ](./hqq)                              | ðŸŸ¢                   | ðŸŸ¢              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/8          | ðŸŸ¢               | ðŸ”´                          | ðŸŸ¢                      | https://github.com/mobiusml/hqq/            |\n+| [optimum-quanto](./quanto)                | ðŸŸ¢                   | ðŸŸ¢              | ðŸŸ¢        | ðŸ”´        | ðŸŸ¢                                 | ðŸ”´              | ðŸŸ¢              | 2/4/8        | ðŸ”´               | ðŸ”´                          | ðŸŸ¢                      | https://github.com/huggingface/optimum-quanto       |\n+| [FBGEMM_FP8](./fbgemm_fp8)                | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 8            | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/pytorch/FBGEMM       |\n+| [torchao](./torchao)                      | ðŸŸ¢                   | ðŸŸ¢               | ðŸŸ¢        | ðŸ”´        | ðŸŸ¡ | ðŸ”´              |                 | 4/8          |                  | ðŸŸ¢ðŸ”´                        | ðŸŸ¢                      | https://github.com/pytorch/ao       |\n+| [VPTQ](./vptq)                            | ðŸ”´                   | ðŸ”´              |     ðŸŸ¢     | ðŸŸ¡        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/8          | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      | https://github.com/microsoft/VPTQ            |\n+| [FINEGRAINED_FP8](./finegrained_fp8)      | ðŸŸ¢                   | ðŸ”´              | ðŸŸ¢        | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 8            | ðŸ”´               | ðŸŸ¢                          | ðŸŸ¢                      |        |\n+| [SpQR](./spqr)                            | ðŸ”´                     |  ðŸ”´   | ðŸŸ¢        | ðŸ”´              |    ðŸ”´    | ðŸ”´         |         ðŸŸ¢              | 3            |              ðŸ”´                     | ðŸŸ¢           | ðŸŸ¢                      | https://github.com/Vahe1994/SpQR/       |\n+| [Quark](./quark)                          | ðŸ”´                     | ðŸŸ¢ | ðŸŸ¢      | ðŸŸ¢      | ðŸŸ¢                   | ðŸŸ¢       | ?               | 2/4/6/8/9/16 | ðŸ”´                | ðŸ”´                               | ðŸŸ¢                       | https://quark.docs.amd.com/latest/                      |\n \n ## Resources\n "
        },
        {
            "sha": "e75357c39f9c296fa99249ce38e4633a86157707",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -259,6 +259,7 @@\n     ],\n     \"utils.quantization_config\": [\n         \"AqlmConfig\",\n+        \"AutoRoundConfig\",\n         \"AwqConfig\",\n         \"BitNetConfig\",\n         \"BitsAndBytesConfig\",\n@@ -754,6 +755,7 @@\n     # bitsandbytes config\n     from .utils.quantization_config import (\n         AqlmConfig,\n+        AutoRoundConfig,\n         AwqConfig,\n         BitNetConfig,\n         BitsAndBytesConfig,"
        },
        {
            "sha": "1d326d5030713abb6e3fd5524f602a7e9d638e9e",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -19,6 +19,7 @@\n from ..utils import logging\n from ..utils.quantization_config import (\n     AqlmConfig,\n+    AutoRoundConfig,\n     AwqConfig,\n     BitNetConfig,\n     BitsAndBytesConfig,\n@@ -39,6 +40,7 @@\n )\n from .base import HfQuantizer\n from .quantizer_aqlm import AqlmHfQuantizer\n+from .quantizer_auto_round import AutoRoundQuantizer\n from .quantizer_awq import AwqQuantizer\n from .quantizer_bitnet import BitNetHfQuantizer\n from .quantizer_bnb_4bit import Bnb4BitHfQuantizer\n@@ -75,6 +77,7 @@\n     \"vptq\": VptqHfQuantizer,\n     \"spqr\": SpQRHfQuantizer,\n     \"fp8\": FineGrainedFP8HfQuantizer,\n+    \"auto-round\": AutoRoundQuantizer,\n }\n \n AUTO_QUANTIZATION_CONFIG_MAPPING = {\n@@ -95,6 +98,7 @@\n     \"vptq\": VptqConfig,\n     \"spqr\": SpQRConfig,\n     \"fp8\": FineGrainedFP8Config,\n+    \"auto-round\": AutoRoundConfig,\n }\n \n logger = logging.get_logger(__name__)\n@@ -195,10 +199,16 @@ def merge_quantization_configs(\n             warning_msg = \"\"\n \n         if isinstance(quantization_config, dict):\n-            quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n+            # Convert the config based on the type of quantization_config_from_args (e.g., AutoRoundConfig), which takes priority before automatic configuration dispatch.\n+            if isinstance(quantization_config_from_args, AutoRoundConfig):\n+                quantization_config = AutoRoundConfig.from_dict(quantization_config)\n+            else:\n+                quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n \n         if (\n-            isinstance(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config, CompressedTensorsConfig))\n+            isinstance(\n+                quantization_config, (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig)\n+            )\n             and quantization_config_from_args is not None\n         ):\n             # special case for GPTQ / AWQ / FbgemmFp8 config collision"
        },
        {
            "sha": "f2d95abb64abf2ac7b5a84b21c89ff93e83059b5",
            "filename": "src/transformers/quantizers/quantizer_auto_round.py",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -0,0 +1,81 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING, Optional\n+\n+from .base import HfQuantizer\n+\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+\n+from ..utils import is_auto_round_available, is_torch_available, logging\n+from ..utils.quantization_config import QuantizationConfigMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class AutoRoundQuantizer(HfQuantizer):\n+    \"\"\"\n+    Quantizer of the AutoRound method. (https://arxiv.org/pdf/2309.05516)\n+    \"\"\"\n+\n+    # AutoRound requires data calibration - we support only inference\n+    requires_calibration = True\n+    required_packages = [\"auto_round\"]\n+\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+\n+    def validate_environment(self, *args, **kwargs):\n+        self.device_map = kwargs.get(\"device_map\", None)\n+        if not is_auto_round_available():\n+            raise ImportError(\n+                \"Loading an AutoRound quantized model requires auto-round library (`pip install 'auto-round>=0.5'`)\"\n+            )\n+\n+    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        if torch_dtype is None:\n+            torch_dtype = torch.bfloat16\n+            logger.info(\"Loading the model in `torch.bfloat16`. To overwrite it, set `torch_dtype` manually.\")\n+        return torch_dtype\n+\n+    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        if model.__class__.main_input_name != \"input_ids\":\n+            logger.warning(\"AutoRound offers only limited support for models that are not strictly text-based.\")\n+        from auto_round.inference.convert_model import convert_hf_model, infer_target_device\n+\n+        if self.pre_quantized:\n+            target_device = infer_target_device(self.device_map)\n+            model, used_backends = convert_hf_model(model, target_device)\n+            self.used_backends = used_backends\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        if self.pre_quantized:\n+            from auto_round.inference.convert_model import post_init\n+\n+            post_init(model, self.used_backends)\n+        else:\n+            raise ValueError(\"AutoRound only sports pre-quantized models.\")\n+\n+    @property\n+    def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n+        return False\n+\n+    def is_serializable(self, safe_serialization=None):\n+        ## for gptq/awq models, the quantization config will be changed\n+        return True"
        },
        {
            "sha": "a2ff5330acf8e5f6064d5d4c35cd91fb1abf6b9f",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -70,6 +70,7 @@\n     is_aqlm_available,\n     is_auto_awq_available,\n     is_auto_gptq_available,\n+    is_auto_round_available,\n     is_av_available,\n     is_bitsandbytes_available,\n     is_bitsandbytes_multi_backend_available,\n@@ -1297,6 +1298,13 @@ def require_auto_awq(test_case):\n     return unittest.skipUnless(is_auto_awq_available(), \"test requires autoawq\")(test_case)\n \n \n+def require_auto_round(test_case):\n+    \"\"\"\n+    Decorator for auto_round dependency\n+    \"\"\"\n+    return unittest.skipUnless(is_auto_round_available(), \"test requires autoround\")(test_case)\n+\n+\n def require_optimum_quanto(test_case):\n     \"\"\"\n     Decorator for quanto dependency"
        },
        {
            "sha": "bec8ca022ff741314c44a54b41e4baabb457baa6",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -123,6 +123,7 @@\n     is_aqlm_available,\n     is_auto_awq_available,\n     is_auto_gptq_available,\n+    is_auto_round_available,\n     is_av_available,\n     is_bitsandbytes_available,\n     is_bitsandbytes_multi_backend_available,"
        },
        {
            "sha": "558cb5c7120cde93c5e7df11f48386c5d81ad8e6",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -107,7 +107,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n HQQ_MIN_VERSION = \"0.2.1\"\n VPTQ_MIN_VERSION = \"0.0.4\"\n TORCHAO_MIN_VERSION = \"0.4.0\"\n-\n+AUTOROUND_MIN_VERSION = \"0.5.0\"\n \n _accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n _apex_available = _is_package_available(\"apex\")\n@@ -159,6 +159,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _optimum_available = _is_package_available(\"optimum\")\n _auto_gptq_available = _is_package_available(\"auto_gptq\")\n _gptqmodel_available = _is_package_available(\"gptqmodel\")\n+_auto_round_available, _auto_round_version = _is_package_available(\"auto_round\", return_version=True)\n # `importlib.metadata.version` doesn't work with `awq`\n _auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n _quark_available = _is_package_available(\"quark\")\n@@ -1101,6 +1102,10 @@ def is_auto_awq_available():\n     return _auto_awq_available\n \n \n+def is_auto_round_available(min_version: str = AUTOROUND_MIN_VERSION):\n+    return _auto_round_available and version.parse(_auto_round_version) >= version.parse(min_version)\n+\n+\n def is_optimum_quanto_available():\n     # `importlib.metadata.version` doesn't work with `optimum.quanto`, need to put `optimum_quanto`\n     return _is_optimum_quanto_available"
        },
        {
            "sha": "9f17b3881ada393af808b79e35f99654c358ef72",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -63,6 +63,7 @@ class QuantizationMethod(str, Enum):\n     SPQR = \"spqr\"\n     FP8 = \"fp8\"\n     QUARK = \"quark\"\n+    AUTOROUND = \"auto-round\"\n \n \n class AWQLinearVersion(str, Enum):\n@@ -204,6 +205,75 @@ def update(self, **kwargs):\n         return unused_kwargs\n \n \n+@dataclass\n+class AutoRoundConfig(QuantizationConfigMixin):\n+    \"\"\"This is a wrapper class about all possible attributes and features that you can play with a model that has been\n+    loaded AutoRound quantization.\n+\n+    Args:\n+        bits (`int`, *optional*, defaults to 4):\n+            The number of bits to quantize to, supported numbers are (2, 3, 4, 8).\n+        group_size (`int`, *optional*, defaults to 128): Group-size value\n+        sym (`bool`, *optional*, defaults to `True`): Symmetric quantization or not\n+        backend (`str`, *optional*, defaults to `\"auto\"`): The kernel to use, e.g., ipex,marlin, exllamav2, triton, etc. Ref. https://github.com/intel/auto-round?tab=readme-ov-file#specify-backend\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        bits: int = 4,\n+        group_size: int = 128,\n+        sym: bool = True,\n+        backend: str = \"auto\",\n+        **kwargs,\n+    ):\n+        self.bits = bits\n+        self.group_size = group_size\n+        self.sym = sym\n+        self.backend = backend\n+        self.packing_format = \"auto_round:gptq\"\n+        if kwargs is not None:\n+            for key, value in kwargs.items():\n+                setattr(self, key, value)\n+        self.quant_method = QuantizationMethod.AUTOROUND\n+        self.post_init()\n+\n+    def post_init(self):\n+        r\"\"\"Safety checker that arguments are correct.\"\"\"\n+        if self.bits not in [2, 3, 4, 8]:\n+            raise ValueError(f\"Only support quantization to [2,3,4,8] bits but found {self.bits}\")\n+        if self.group_size != -1 and self.group_size <= 0:\n+            raise ValueError(\"group_size must be greater than 0 or equal to -1\")\n+\n+    def get_loading_attributes(self):\n+        loading_attibutes_dict = {\"backend\": self.backend}\n+        return loading_attibutes_dict\n+\n+    def to_dict(self):\n+        config_dict = super().to_dict()\n+        return config_dict\n+\n+    @classmethod\n+    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n+        quant_method = config_dict[\"quant_method\"]\n+        if \"auto-round\" not in quant_method and \"gptq\" not in quant_method and \"awq\" not in quant_method:\n+            raise NotImplementedError(\n+                \"Failed to convert to auto_round format. Only `gptqv1`, `awq`, and `auto-round` formats are supported.\"\n+            )\n+\n+        if \"gptq\" in quant_method and \"meta\" in config_dict:\n+            raise NotImplementedError(\"Failed to convert gptq format to auto_round format. Only supports `gptqv1`\")\n+\n+        if \"awq\" in quant_method and config_dict.get(\"version\", \"gemm\") != \"gemm\":\n+            raise NotImplementedError(\n+                \"Failed to convert awq format to auto_round format. Only supports awq format with gemm version\"\n+            )\n+\n+        if \"auto-round\" not in quant_method:\n+            config_dict[\"packing_format\"] = f\"auto_round:{quant_method}\"\n+\n+        return super().from_dict(config_dict, return_unused_kwargs=return_unused_kwargs, **kwargs)\n+\n+\n @dataclass\n class HqqConfig(QuantizationConfigMixin):\n     \"\"\""
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/autoround/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/tests%2Fquantization%2Fautoround%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/tests%2Fquantization%2Fautoround%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoround%2F__init__.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129"
        },
        {
            "sha": "fd77597a77d15bd2b5c64085809dc373acc716c9",
            "filename": "tests/quantization/autoround/test_auto_round.py",
            "status": "added",
            "additions": 209,
            "deletions": 0,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3492ff9f7ddac6592f7d1861ef19127d6297129/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3492ff9f7ddac6592f7d1861ef19127d6297129/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py?ref=b3492ff9f7ddac6592f7d1861ef19127d6297129",
            "patch": "@@ -0,0 +1,209 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import gc\n+import tempfile\n+import unittest\n+\n+from transformers import AutoModelForCausalLM, AutoRoundConfig, AutoTokenizer\n+from transformers.testing_utils import (\n+    require_accelerate,\n+    require_auto_round,\n+    require_intel_extension_for_pytorch,\n+    require_torch_gpu,\n+    require_torch_multi_gpu,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_auto_round\n+@require_accelerate\n+class AutoRoundTest(unittest.TestCase):\n+    model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n+    input_text = \"There is a girl who likes adventure,\"\n+    EXPECTED_OUTPUTS = set()\n+    ## Different backends may produce slight variations in output\n+    EXPECTED_OUTPUTS.add(\n+        \"There is a girl who likes adventure, and she has been exploring the world \"\n+        \"for many years. She travels to different countries and cultures, trying new \"\n+        \"things every day. One of her favorite places to visit is a small village in \"\n+        \"the mountains where\"\n+    )\n+    EXPECTED_OUTPUTS.add(\n+        \"There is a girl who likes adventure, and she has been exploring the world for many years. She has visited every country in Europe and has even traveled to some of the most remote parts of Africa. She enjoys hiking through the mountains and discovering\"\n+    )\n+\n+    device_map = \"cuda\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Setup quantized model\n+        \"\"\"\n+        torch.cuda.synchronize()\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name, device_map=cls.device_map, torch_dtype=torch.float16\n+        )\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_quantized_model(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=40, do_sample=False)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+    def test_raise_if_non_quantized(self):\n+        model_id = \"facebook/opt-125m\"\n+        quantization_config = AutoRoundConfig(bits=4)\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n+\n+    def test_quantized_model_bf16(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with bf16\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        quantization_config = AutoRoundConfig(backend=\"triton\")\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            torch_dtype=torch.bfloat16,\n+            device_map=self.device_map,\n+            quantization_config=quantization_config,\n+        )\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=40, do_sample=False)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+    @require_intel_extension_for_pytorch\n+    def test_quantized_model_on_cpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\")\n+\n+        quantized_model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=\"auto\")\n+        output = quantized_model.generate(**input_ids, max_new_tokens=40, do_sample=False)\n+\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+    def test_save_pretrained(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+\n+        ## some backends like marlin/ipex will repack the weight that caused the weight shape changed\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            quantization_config = AutoRoundConfig(backend=\"triton\")\n+            quantized_model = AutoModelForCausalLM.from_pretrained(\n+                self.model_name,\n+                device_map=self.device_map,\n+                torch_dtype=torch.float16,\n+                quantization_config=quantization_config,\n+            )\n+\n+            quantized_model.save_pretrained(tmpdirname)\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"cuda\")\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=40, do_sample=False)\n+            self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+    @require_torch_multi_gpu\n+    def test_quantized_model_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with multiple GPUs\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        quantization_config = AutoRoundConfig(backend=\"triton\")\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, device_map=\"auto\", quantization_config=quantization_config, torch_dtype=\"auto\"\n+        )\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=40, do_sample=False)\n+\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+    def test_convert_from_gptq(self):\n+        \"\"\"\n+        Simple test that checks if auto-round work properly wth gptq format\n+        \"\"\"\n+        model_name = \"ybelkada/opt-125m-gptq-4bit\"\n+\n+        quantization_config = AutoRoundConfig()\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_name, device_map=\"cuda\", quantization_config=quantization_config, torch_dtype=\"auto\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+        text = \"There is a girl who likes adventure,\"\n+        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+        tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0])\n+\n+    @require_intel_extension_for_pytorch\n+    def test_convert_from_awq_cpu(self):\n+        \"\"\"\n+        Simple test that checks if auto-round work properly wth awq format\n+        \"\"\"\n+        model_name = \"casperhansen/opt-125m-awq\"\n+\n+        quantization_config = AutoRoundConfig()\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_name, device_map=\"cpu\", quantization_config=quantization_config, torch_dtype=\"auto\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+        text = \"There is a girl who likes adventure,\"\n+        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+        tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0])\n+\n+    def test_mixed_bits(self):\n+        \"\"\"\n+        Simple test that checks if auto-round work properly wth mixed bits\n+        \"\"\"\n+        model_name = \"facebook/opt-125m\"\n+        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+        layer_config = {\n+            \"model.decoder.layers.0.self_attn.k_proj\": {\"bits\": 8},\n+            \"model.decoder.layers.6.self_attn.out_proj\": {\"bits\": 2, \"group_size\": 32},\n+        }\n+\n+        bits, group_size, sym = 4, 128, True\n+        from auto_round import AutoRound\n+\n+        autoround = AutoRound(model, tokenizer, bits=bits, group_size=group_size, sym=sym, layer_config=layer_config)\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            autoround.quantize_and_save(output_dir=tmpdirname)\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, torch_dtype=torch.float16, device_map=\"cuda\")\n+            text = \"There is a girl who likes adventure,\"\n+            inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n+            tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0])"
        }
    ],
    "stats": {
        "total": 724,
        "additions": 702,
        "deletions": 22
    }
}