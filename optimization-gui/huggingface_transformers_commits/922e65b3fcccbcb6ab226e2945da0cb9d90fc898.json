{
    "author": "ydshieh",
    "message": "Fix non FA2 tests after FA2 installed in CI docker image (#40430)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "922e65b3fcccbcb6ab226e2945da0cb9d90fc898",
    "files": [
        {
            "sha": "5d5e129f7e5a0d5bbba31ffaf5bf14d429942e44",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=922e65b3fcccbcb6ab226e2945da0cb9d90fc898",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch GLM-4.1V model.\"\"\"\n \n import copy\n-import gc\n import unittest\n \n from transformers import (\n@@ -25,6 +24,7 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -309,8 +309,7 @@ def setUp(self):\n         ]\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     def test_small_model_integration_test(self):\n@@ -376,10 +375,8 @@ def test_small_model_integration_test_with_video(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", dtype=torch.float16, device_map=\"auto\"\n         )\n-        questions = [\"Describe this video.\"] * 2\n-        video_urls = [\n-            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"\n-        ] * 2\n+        questions = [\"Describe this video.\"]\n+        video_urls = [\"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"]\n         messages = [\n             [\n                 {\n@@ -400,8 +397,7 @@ def test_small_model_integration_test_with_video(self):\n         ).to(torch_device)\n         output = model.generate(**inputs, max_new_tokens=30)\n         EXPECTED_DECODED_TEXT = [\n-            \"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is a room with a wooden floor, maybe a traditional Japanese room with tatami\",\n-            \"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is a room with a wooden floor, maybe a traditional Japanese room with tatami\"\n+            \"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is an indoor tennis court. There are two players: one in the foreground wearing\"\n         ]  # fmt: skip\n         self.assertEqual(\n             processor.batch_decode(output, skip_special_tokens=True),\n@@ -509,8 +505,8 @@ def test_small_model_integration_test_batch_flashatt2(self):\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture has a stocky build, thick fur, and a face that's\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\"\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\",\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),"
        },
        {
            "sha": "200926caa2947e2b8815ee78abe59833559bd669",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=922e65b3fcccbcb6ab226e2945da0cb9d90fc898",
            "patch": "@@ -345,16 +345,26 @@ def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str\n         \"\"\"Test that we can correctly generate beyond the sliding window. This is non-trivial as Mistral will use\n         a DynamicCache with only sliding layers.\"\"\"\n \n+        # Impossible to test it with this model (even with < 100 tokens), probably due to the compilation of a large model.\n+        if attn_implementation == \"flex_attention\":\n+            self.skipTest(\n+                reason=\"`flex_attention` gives `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\"\n+            )\n+\n         model_id = \"mistralai/Mistral-7B-v0.1\"\n         EXPECTED_COMPLETIONS = [\n-            \"This is a nice place. This is a nice place. This is a nice place. This is\",\n+            \"scenery, scenery, scenery, scenery, scenery,\",\n             \", green, yellow, orange, purple, pink, brown, black, white, gray, silver\",\n         ]\n \n         input_text = [\n-            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"This is a nice place. \" * 682 + \"I really enjoy the scenery,\",  # This has 4101 tokens, 15 more than 4096\n             \"A list of colors: red, blue\",  # This will almost all be padding tokens\n         ]\n+\n+        if attn_implementation == \"eager\":\n+            input_text = input_text[:1]\n+\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n         inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n@@ -364,14 +374,14 @@ def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str\n         )\n \n         # Make sure prefill is larger than sliding window\n-        input_size = inputs.input_ids.shape[-1]\n+        batch_size, input_size = inputs.input_ids.shape\n         self.assertTrue(input_size > model.config.sliding_window)\n \n         # Should already be Dynamic by default, but let's make sure!\n         out = model.generate(**inputs, max_new_tokens=20, cache_implementation=\"dynamic\", return_dict_in_generate=True)\n-        output_text = tokenizer.batch_decode(out.sequences[:, input_size:])\n+        output_text = tokenizer.batch_decode(out.sequences[:batch_size, input_size:])\n \n-        self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS[:batch_size])\n \n         # Let's check that the dynamic cache has hybrid layers!\n         dynamic_cache = out.past_key_values"
        },
        {
            "sha": "ec15a32966dc5f7662e8827c59cd3acacb658cac",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=922e65b3fcccbcb6ab226e2945da0cb9d90fc898",
            "patch": "@@ -317,7 +317,7 @@ def test_3b_generation(self):\n         model_id = \"Qwen/Qwen2.5-3B\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         model = Qwen2ForCausalLM.from_pretrained(\n-            model_id, use_sliding_window=True, max_window_layers=28, sliding_window=2048\n+            model_id, use_sliding_window=True, max_window_layers=28, sliding_window=2048, dtype=torch.float16\n         ).to(torch_device)\n         # we need a long text to test sliding window\n         # fmt: off\n@@ -710,24 +710,27 @@ def test_3b_generation(self):\n         # fmt: on\n \n         input_ids = tokenizer(LONG_TEXT, return_tensors=\"pt\").input_ids.to(torch_device)\n-        generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]\n+        generated_ids = model.generate(input_ids, max_new_tokens=20)[:, input_ids.shape[1] :]\n \n-        torch.testing.assert_close(generated_ids.cpu(), torch.tensor([[  576,  4570, 71818,   374,  6509,   825,   315,   279,  1429, 88228, 21984,   315,   279, 11220,  4948,  8584,   304,   279,   467, 19859, 4180,  4168,    13,  1084, 14230, 16170,   315,  3349, 19256,   304, 279,  2266,   315, 13444, 14550,   448, 50867,   429,   525,   330, 4145,     1,   476,   330, 88845,     1,   323,  1246,  3425,   264]], dtype=torch.long))  # fmt: skip\n+        torch.testing.assert_close(generated_ids.cpu(), torch.tensor([[279, 467, 19859, 4180, 4168, 572, 264, 882, 315, 2244, 2297, 304, 5616, 13, 576, 66827, 66846, 572, 304, 17704]], dtype=torch.long))  # fmt: skip\n         self.assertEqual(\n             tokenizer.decode(generated_ids[0]),\n-            \"\"\" The Guanzi is considered one of the most foundational texts of the developing political economy in the Warring States period. It addresses principles of price regulation in the context of effectively dealing with commodities that are \"light\" or \"heavy\" and how whether a\"\"\",\n+            \" the Warring States period was a time of great change in China. The Zhou dynasty was in decline\",\n         )\n         model.config._attn_implementation = \"eager\"\n-        new_generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]\n+        new_generated_ids = model.generate(input_ids, max_new_tokens=20)[:, input_ids.shape[1] :]\n         with self.subTest(\"Eager matches sdpa\"):\n             torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)\n \n-        model.config._attn_implementation = \"flex_attention\"\n-        new_generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]\n-        with self.subTest(\"Eager matches Flex attention\"):\n-            torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)\n+        # `flex_attention` gives `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\n+        # Impossible to test it with this model (even with < 100 tokens), probably due to the compilation of a large model.\n+\n+        # model.config._attn_implementation = \"flex_attention\"\n+        # new_generated_ids = model.generate(input_ids, max_new_tokens=20)[:, input_ids.shape[1] :]\n+        # with self.subTest(\"Eager matches Flex attention\"):\n+        #     torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)\n \n         model.config._attn_implementation = \"flash_attention_2\"\n-        new_generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]\n+        new_generated_ids = model.generate(input_ids, max_new_tokens=20)[:, input_ids.shape[1] :]\n         with self.subTest(\"Eager matches flash attention\"):\n             torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "6de8d1b0bd6931aaf3d5f8d44897f691e7af63e2",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/922e65b3fcccbcb6ab226e2945da0cb9d90fc898/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=922e65b3fcccbcb6ab226e2945da0cb9d90fc898",
            "patch": "@@ -146,7 +146,7 @@ def test_model_600m_generation(self):\n     @require_flash_attn\n     @pytest.mark.flash_attn_test\n     def test_model_600m_long_prompt(self):\n-        EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n+        EXPECTED_OUTPUT_TOKEN_IDS = [198, 198]\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n         model = Qwen3ForCausalLM.from_pretrained(\n@@ -309,7 +309,7 @@ def test_600m_generation(self):\n         model_id = \"Qwen/Qwen3-0.6B-Base\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         model = Qwen3ForCausalLM.from_pretrained(\n-            model_id, use_sliding_window=True, max_window_layers=28, sliding_window=2048\n+            model_id, use_sliding_window=True, max_window_layers=28, sliding_window=2048, dtype=torch.float16\n         ).to(torch_device)\n         # we need a long text to test sliding window\n         # fmt: off\n@@ -714,10 +714,13 @@ def test_600m_generation(self):\n         with self.subTest(\"Eager matches sdpa\"):\n             torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)\n \n-        model.config._attn_implementation = \"flex_attention\"\n-        new_generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]\n-        with self.subTest(\"Eager matches Flex attention\"):\n-            torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)\n+        # `flex_attention` gives `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\n+        # Impossible to test it with this model (even with < 100 tokens), probably due to the compilation of a large model.\n+\n+        # model.config._attn_implementation = \"flex_attention\"\n+        # new_generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]\n+        # with self.subTest(\"Eager matches Flex attention\"):\n+        #     torch.testing.assert_close(generated_ids, new_generated_ids, rtol=1e-4, atol=1e-4)\n \n         model.config._attn_implementation = \"flash_attention_2\"\n         new_generated_ids = model.generate(input_ids, max_new_tokens=50)[:, input_ids.shape[1] :]"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 44,
        "deletions": 32
    }
}