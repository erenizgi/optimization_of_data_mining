{
    "author": "Cyrilvallez",
    "message": "[loading] `tie_weights` is finally symmetric (#42507)\n\n* symetric tie weights\n\n* remove bart ones i added the other day\n\n* remove open ai skip i added the other day\n\n* style\n\n* clarify comments\n\n* unfortunately cannot raise now\n\n* typo\n\n* add flag\n\n* typo\n\n* add tests\n\n* more tests",
    "sha": "9f587100bc1e7a21bd22218a058ae14f7ea9b05c",
    "files": [
        {
            "sha": "bd14f895aa133caae242a33fda92aff83f0a00f2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 66,
            "deletions": 20,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9f587100bc1e7a21bd22218a058ae14f7ea9b05c",
            "patch": "@@ -2283,37 +2283,83 @@ def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping\n         If `recompute_mapping=True`, it will re-check all internal submodels and their config to determine the params\n         that need to be tied. This is the default when `model.tie_weights()` is called on its own, outside of\n         `__init__`, and `from_pretrained`, in case the config values were changed somewhere.\n+\n+        Note that during `from_pretrained`, tying is *symmetric*: if the mapping says \"tie target -> source\" but\n+        `source` is missing in the checkpoint while `target` exists, we *swap* source and target so we can still\n+        tie everything to the parameter that actually exists.\n         \"\"\"\n         # In this case, the keys stored in `all_tied_weights_keys` are already correct\n         if not recompute_mapping:\n             tied_keys = self.all_tied_weights_keys\n         else:\n             tied_keys = self.get_expanded_tied_weights_keys(all_submodels=True)\n \n-        for target_param_name, source_param_name in tied_keys.items():\n-            source_param = self.get_parameter_or_buffer(source_param_name)\n-            if \".\" in target_param_name:\n-                parent_name, name = target_param_name.rsplit(\".\", 1)\n-                parent = self.get_submodule(parent_name)\n-            else:\n-                name = target_param_name\n-                parent = self\n-            setattr(parent, name, source_param)\n-            self._adjust_bias(parent, source_param)\n+        tied_keys = list(tied_keys.items())\n+        for i, (target_param_name, source_param_name) in enumerate(tied_keys):\n+            # Usually we tie a single target to a single source, but when both are missing we may later tie\n+            # both the source and target to a third \"backup\" parameter that is present in the checkpoint, so we use\n+            # a list here\n+            target_param_names = [target_param_name]\n+\n+            # This is `from_pretrained` -> let's check symmetrically in case the source key is not present\n             if missing_keys is not None:\n+                remove_from_missing = True\n                 source_is_there = source_param_name not in missing_keys\n                 target_is_there = target_param_name not in missing_keys\n-                # If we tied correctly, remove the target from the missing keys\n-                if source_is_there:\n-                    missing_keys.discard(target_param_name)\n-                # If the source is not present, but the target is, the checkpoint is corrupted\n-                # TODO: maybe we could simply tie in the opposite direction here instead of error?\n-                elif target_is_there:\n-                    raise ValueError(\n-                        f\"This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie \"\n-                        f\"{source_param_name} (which should be present and is not), to {target_param_name} (which is \"\n-                        f\"present).\"\n+                # Both are already present -> it means the config is wrong and do not reflect the actual\n+                # checkpoint -> let's raise a warning and do nothing\n+                if source_is_there and target_is_there:\n+                    logger.warning(\n+                        f\"The tied weights mapping and config for this model specifies to tie {source_param_name} to \"\n+                        f\"{target_param_name}, but both are present in the checkpoints, so we will NOT tie them. \"\n+                        \"You should update the config with `tie_word_embeddings=False` to silence this warning\"\n                     )\n+                    # Skip to next iteration\n+                    continue\n+                # We're missing the source but we have the target -> we swap them, tying the parameter that exists\n+                elif not source_is_there and target_is_there:\n+                    target_param_name, source_param_name = source_param_name, target_param_name\n+                    target_param_names = [target_param_name]\n+                # Both are missing -> check other keys in case more than 2 keys are tied to the same weight\n+                elif not source_is_there and not target_is_there:\n+                    for target_backup, source_backup in tied_keys[i + 1 :]:\n+                        # In case of more than 2 keys tied to the same weight, they are guaranteed to all have\n+                        # the same source thanks to `get_expanded_tied_weights_keys` so this check is enough\n+                        if source_backup == source_param_name:\n+                            target_backup_is_there = target_backup not in missing_keys\n+                            # If the target is present, we found the correct weight to tie into (we know the source is missing)\n+                            if target_backup_is_there:\n+                                source_param_name = target_backup\n+                                # Append the source as well, since both are missing we'll tie both\n+                                target_param_names.append(source_param_name)\n+                                break\n+                    # If we did not break from the loop, it was impossible to find a source key -> let's raise\n+                    else:\n+                        # TODO Cyril: here ideally we want to raise instead of warning, but will break our CI as we have\n+                        # tests loading model from empty dicts to perform init checks - since we don't raise, add a flag\n+                        # to NOT remove from missing keys as it's actually still missing\n+                        remove_from_missing = False\n+                        logger.warning(\n+                            f\"This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie \"\n+                            f\"{source_param_name} to {target_param_name}, but both are absent from the checkpoint, \"\n+                            \"and we could not find another related tied weight for those keys\"\n+                        )\n+\n+            # Perform the actual tying\n+            source_param = self.get_parameter_or_buffer(source_param_name)\n+            for target_param_name in target_param_names:\n+                if \".\" in target_param_name:\n+                    parent_name, name = target_param_name.rsplit(\".\", 1)\n+                    parent = self.get_submodule(parent_name)\n+                else:\n+                    name = target_param_name\n+                    parent = self\n+                # Tie the weights\n+                setattr(parent, name, source_param)\n+                self._adjust_bias(parent, source_param)\n+                # Remove from missing if necesary\n+                if missing_keys is not None and remove_from_missing:\n+                    missing_keys.discard(target_param_name)\n \n     def _adjust_bias(self, output_embeddings, input_embeddings):\n         if getattr(output_embeddings, \"bias\", None) is not None and hasattr(output_embeddings, \"weight\"):"
        },
        {
            "sha": "d663821578a223d2fb2d0062c8a7b9bb5210c9d0",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=9f587100bc1e7a21bd22218a058ae14f7ea9b05c",
            "patch": "@@ -897,21 +897,6 @@ def __init__(self, config: BartConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n-        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n-        if self.config.tie_word_embeddings:\n-            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n-            # need check here, see issue #36247\n-            if missing_keys is not None:\n-                if \"shared.weight\" in missing_keys and \"decoder.embed_tokens.weight\" not in missing_keys:\n-                    self.encoder.embed_tokens.weight = self.decoder.embed_tokens.weight\n-                    self.shared.weight = self.decoder.embed_tokens.weight\n-                    missing_keys.discard(\"encoder.embed_token.weight\")\n-                    missing_keys.discard(\"shared.weight\")\n-\n-        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n-        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n-\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1049,21 +1034,6 @@ def __init__(self, config: BartConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n-        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n-        if self.config.tie_word_embeddings:\n-            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n-            # need check here, see issue #36247\n-            if missing_keys is not None:\n-                if \"model.shared.weight\" in missing_keys and \"model.decoder.embed_tokens.weight\" not in missing_keys:\n-                    self.model.encoder.embed_tokens.weight = self.model.decoder.embed_tokens.weight\n-                    self.model.shared.weight = self.model.decoder.embed_tokens.weight\n-                    missing_keys.discard(\"model.encoder.embed_token.weight\")\n-                    missing_keys.discard(\"model.shared.weight\")\n-\n-        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n-        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1242,21 +1212,6 @@ def __init__(self, config: BartConfig, **kwargs):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n-        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n-        if self.config.tie_word_embeddings:\n-            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n-            # need check here, see issue #36247\n-            if missing_keys is not None:\n-                if \"model.shared.weight\" in missing_keys and \"model.decoder.embed_tokens.weight\" not in missing_keys:\n-                    self.model.encoder.embed_tokens.weight = self.model.decoder.embed_tokens.weight\n-                    self.model.shared.weight = self.model.decoder.embed_tokens.weight\n-                    missing_keys.discard(\"model.encoder.embed_token.weight\")\n-                    missing_keys.discard(\"model.shared.weight\")\n-\n-        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n-        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1388,21 +1343,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n-        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n-        if self.config.tie_word_embeddings:\n-            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n-            # need check here, see issue #36247\n-            if missing_keys is not None:\n-                if \"model.shared.weight\" in missing_keys and \"model.decoder.embed_tokens.weight\" not in missing_keys:\n-                    self.model.encoder.embed_tokens.weight = self.model.decoder.embed_tokens.weight\n-                    self.model.shared.weight = self.model.decoder.embed_tokens.weight\n-                    missing_keys.discard(\"model.encoder.embed_token.weight\")\n-                    missing_keys.discard(\"model.shared.weight\")\n-\n-        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n-        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c27f35656d7eabb55431f31ba32735bbbedfd80f",
            "filename": "tests/models/openai/test_modeling_openai.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py?ref=9f587100bc1e7a21bd22218a058ae14f7ea9b05c",
            "patch": "@@ -269,13 +269,6 @@ def test_model_from_pretrained(self):\n         model = OpenAIGPTModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(\"Tied weights mapping is reversed, so this is supposed to error out\")\n-    def test_correct_missing_keys(self):\n-        # openai defines `_tied_weights_keys = {\"transformer.tokens_embed.weight\": \"lm_head.weight\"}` instead\n-        # of the usual `_tied_weights_keys = {\"lm_head.weight\": \"transformer.tokens_embed.weight\"}`, so removing\n-        # the head parameters actually removes the source weight, so this test is supposed to fail\n-        pass\n-\n \n @require_torch\n class OPENAIGPTModelLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "b56728ce112b6163000a4baea7b8e71a404961ce",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 173,
            "deletions": 9,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f587100bc1e7a21bd22218a058ae14f7ea9b05c/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=9f587100bc1e7a21bd22218a058ae14f7ea9b05c",
            "patch": "@@ -92,6 +92,8 @@\n     is_torch_npu_available,\n )\n \n+from ..test_modeling_common import compare_state_dicts\n+\n \n sys.path.append(str(Path(__file__).parent.parent.parent / \"utils\"))\n \n@@ -184,10 +186,35 @@ def __init__(self, config):\n         def forward(self, x):\n             return self.linear_2(self.linear(x))\n \n-        def tie_weights(self, missing_keys=None, **kwargs):\n-            self.linear_2.weight = self.linear.weight\n-            if missing_keys is not None:\n-                missing_keys.discard(\"linear_2.weight\")\n+    class BaseModelWithMultipleTiedWeights(PreTrainedModel):\n+        config_class = PreTrainedConfig\n+        _tied_weights_keys = {\"linear_2.weight\": \"linear.weight\", \"linear_3.weight\": \"linear.weight\"}\n+\n+        def __init__(self, config):\n+            super().__init__(config)\n+            self.linear = nn.Linear(5, 5)\n+            self.linear_2 = nn.Linear(5, 5)\n+            self.linear_3 = nn.Linear(5, 5)\n+            self.post_init()\n+\n+        def forward(self, x):\n+            return self.linear_2(self.linear(x))\n+\n+    class BaseModelWithMultipleMixedTiedWeights(PreTrainedModel):\n+        config_class = PreTrainedConfig\n+        # Here the tied keys both refer to `linear.weight`, but they are inconsistent in the mapping, i.e. they\n+        # are provided as a \"circular\" dependency\n+        _tied_weights_keys = {\"linear_2.weight\": \"linear.weight\", \"linear_3.weight\": \"linear_2.weight\"}\n+\n+        def __init__(self, config):\n+            super().__init__(config)\n+            self.linear = nn.Linear(5, 5)\n+            self.linear_2 = nn.Linear(5, 5)\n+            self.linear_3 = nn.Linear(5, 5)\n+            self.post_init()\n+\n+        def forward(self, x):\n+            return self.linear_2(self.linear(x))\n \n     class ModelWithHead(PreTrainedModel):\n         base_model_prefix = \"base\"\n@@ -258,11 +285,6 @@ def __init__(self, config):\n         def forward(self, x):\n             return self.decoder(self.base(x))\n \n-        def tie_weights(self, missing_keys=None, **kwargs):\n-            self.decoder.weight = self.base.linear.weight\n-            if missing_keys is not None:\n-                missing_keys.discard(\"decoder.weight\")\n-\n     class Prepare4dCausalAttentionMaskModel(nn.Module):\n         def forward(self, inputs_embeds):\n             batch_size, seq_length, _ = inputs_embeds.shape\n@@ -1372,6 +1394,148 @@ def test_tied_weights_reload(self):\n             # Should only complain about the missing bias\n             self.assertSetEqual(load_info[\"missing_keys\"], {\"decoder.bias\"})\n \n+    def test_tied_weights_can_load_symmetrically(self):\n+        \"\"\"Test that we can correctly load and tie weights even though the wrong key was saved.\"\"\"\n+        model = BaseModelWithTiedWeights(PreTrainedConfig())\n+        # Just to be sure it's actually tied\n+        self.assertIs(model.linear.weight, model.linear_2.weight, msg=\"Weights are not tied!\")\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            # Save the config\n+            with open(os.path.join(tmp_dir, \"config.json\"), \"w\") as f:\n+                f.write(json.dumps(model.config.to_dict()))\n+\n+            state_dict = model.state_dict()\n+            # Save using the wrong key\n+            state_dict.pop(\"linear.weight\")\n+            safe_save_file(state_dict, os.path.join(tmp_dir, \"model.safetensors\"))\n+\n+            new_model, load_info = BaseModelWithTiedWeights.from_pretrained(tmp_dir, output_loading_info=True)\n+            # Assert no missing keys\n+            self.assertSetEqual(load_info[\"missing_keys\"], set(), msg=f\"{load_info['missing_keys']} are missing!\")\n+            # It's still the same weight\n+            self.assertIs(new_model.linear.weight, new_model.linear_2.weight, msg=\"Weights are not tied!\")\n+\n+            # Make sure both state dict are the same\n+            compare_state_dicts(model.state_dict(), new_model.state_dict())\n+\n+    def test_tied_weights_can_load_symmetrically_multiple_keys(self):\n+        \"\"\"Test that we can correctly load and tie weights even though the wrong key was saved, when we\n+        have more than 1 target to the same source.\"\"\"\n+        # First class is consistent in how they provide the source, second is not -> make sure it works in both cases\n+        for model_class in [BaseModelWithMultipleTiedWeights, BaseModelWithMultipleMixedTiedWeights]:\n+            with self.subTest(model_class.__name__):\n+                model = model_class(PreTrainedConfig())\n+                # Just to be sure it's actually tied\n+                self.assertIs(model.linear.weight, model.linear_2.weight, msg=\"Weights are not tied!\")\n+                self.assertIs(model.linear.weight, model.linear_3.weight, msg=\"Weights are not tied!\")\n+                with tempfile.TemporaryDirectory() as tmp_dir:\n+                    # Save the config\n+                    with open(os.path.join(tmp_dir, \"config.json\"), \"w\") as f:\n+                        f.write(json.dumps(model.config.to_dict()))\n+\n+                    state_dict = model.state_dict()\n+                    # Keep only 1 of the 3 tied keys, but not the source (which is `linear.weight`)\n+                    state_dict.pop(\"linear.weight\")\n+                    state_dict.pop(\"linear_3.weight\")\n+                    safe_save_file(state_dict, os.path.join(tmp_dir, \"model.safetensors\"))\n+\n+                    new_model, load_info = BaseModelWithMultipleTiedWeights.from_pretrained(\n+                        tmp_dir, output_loading_info=True\n+                    )\n+                    # Assert no missing keys\n+                    self.assertSetEqual(\n+                        load_info[\"missing_keys\"], set(), msg=f\"{load_info['missing_keys']} are missing!\"\n+                    )\n+                    # It's still the same weight\n+                    self.assertIs(new_model.linear.weight, new_model.linear_2.weight, msg=\"Weights are not tied!\")\n+                    self.assertIs(new_model.linear.weight, new_model.linear_3.weight, msg=\"Weights are not tied!\")\n+\n+                    # Make sure both state dict are the same\n+                    compare_state_dicts(model.state_dict(), new_model.state_dict())\n+\n+                    # Now, do the same but try to keep `linear_2.weight` in the saved key instead of `linear_3.weight`\n+                    # to make sure it does not matter\n+                    state_dict = model.state_dict()\n+                    # Keep only 1 of the 3 tied keys, but not the source (which is `linear.weight`)\n+                    state_dict.pop(\"linear.weight\")\n+                    state_dict.pop(\"linear_2.weight\")\n+                    safe_save_file(state_dict, os.path.join(tmp_dir, \"model.safetensors\"))\n+\n+                    new_model, load_info = BaseModelWithMultipleTiedWeights.from_pretrained(\n+                        tmp_dir, output_loading_info=True\n+                    )\n+                    # Assert no missing keys\n+                    self.assertSetEqual(\n+                        load_info[\"missing_keys\"], set(), msg=f\"{load_info['missing_keys']} are missing!\"\n+                    )\n+                    # It's still the same weight\n+                    self.assertIs(new_model.linear.weight, new_model.linear_2.weight, msg=\"Weights are not tied!\")\n+                    self.assertIs(new_model.linear.weight, new_model.linear_3.weight, msg=\"Weights are not tied!\")\n+\n+                    # Make sure both state dict are the same\n+                    compare_state_dicts(model.state_dict(), new_model.state_dict())\n+\n+    def test_tied_weights_are_not_tied_if_both_present(self):\n+        \"\"\"Test that if both the source and target of tied weights are present, we do NOT tie them, and instead\n+        raise a warning\"\"\"\n+        model = BaseModelWithTiedWeights(PreTrainedConfig())\n+        # Just to be sure it's actually tied\n+        self.assertIs(model.linear.weight, model.linear_2.weight, msg=\"Weights are not tied!\")\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            # Save the config\n+            with open(os.path.join(tmp_dir, \"config.json\"), \"w\") as f:\n+                f.write(json.dumps(model.config.to_dict()))\n+\n+            state_dict = model.state_dict()\n+            # Clone every param to make sure nothing is tied -> we save everything\n+            state_dict = {k: v.clone() for k, v in state_dict.items()}\n+            safe_save_file(state_dict, os.path.join(tmp_dir, \"model.safetensors\"))\n+\n+            logger = logging.get_logger(\"transformers.modeling_utils\")\n+            with CaptureLogger(logger) as cl:\n+                new_model, load_info = BaseModelWithTiedWeights.from_pretrained(tmp_dir, output_loading_info=True)\n+\n+            # We should have raised a warning here saying that we will NOT tie the weights\n+            self.assertIn(\"both are present in the checkpoints, so we will NOT tie them.\", cl.out)\n+            # Assert no missing keys\n+            self.assertSetEqual(load_info[\"missing_keys\"], set(), msg=f\"{load_info['missing_keys']} are missing!\")\n+            # It should not be the same weight anymore\n+            self.assertIsNot(\n+                new_model.linear.weight, new_model.linear_2.weight, msg=\"Weights are tied but they should not!\"\n+            )\n+\n+            # Make sure both state dict are the same (the values are still the same, it's just not tied)\n+            compare_state_dicts(model.state_dict(), new_model.state_dict())\n+\n+    def test_tied_weights_are_missing_if_both_absent(self):\n+        \"\"\"Test that if both the source and target of tied weights are absent, we do tie them, but they are missing\"\"\"\n+        model = BaseModelWithTiedWeights(PreTrainedConfig())\n+        # Just to be sure it's actually tied\n+        self.assertIs(model.linear.weight, model.linear_2.weight, msg=\"Weights are not tied!\")\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            # Save the config\n+            with open(os.path.join(tmp_dir, \"config.json\"), \"w\") as f:\n+                f.write(json.dumps(model.config.to_dict()))\n+\n+            state_dict = model.state_dict()\n+            # Remove both from the state dict\n+            state_dict.pop(\"linear.weight\")\n+            state_dict.pop(\"linear_2.weight\")\n+            safe_save_file(state_dict, os.path.join(tmp_dir, \"model.safetensors\"))\n+\n+            logger = logging.get_logger(\"transformers.modeling_utils\")\n+            with CaptureLogger(logger) as cl:\n+                new_model, load_info = BaseModelWithTiedWeights.from_pretrained(tmp_dir, output_loading_info=True)\n+\n+            # We should have raised a warning here saying that we will NOT tie the weights\n+            self.assertIn(\n+                \"This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie\", cl.out\n+            )\n+            # Assert both are in the missing keys\n+            self.assertSetEqual(load_info[\"missing_keys\"], {\"linear.weight\", \"linear_2.weight\"})\n+            # They should still be tied though\n+            self.assertIs(new_model.linear.weight, new_model.linear_2.weight, msg=\"Weights are not tied!\")\n+\n     def test_unexpected_keys_warnings(self):\n         model = ModelWithHead(PreTrainedConfig())\n         logger = logging.get_logger(\"transformers.modeling_utils\")"
        }
    ],
    "stats": {
        "total": 335,
        "additions": 239,
        "deletions": 96
    }
}