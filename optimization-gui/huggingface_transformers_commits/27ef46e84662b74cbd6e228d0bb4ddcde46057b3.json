{
    "author": "simonlevine",
    "message": "Omit creation of positional IDs within ESM if applicable (#38089)\n\n* omit pos emb creation\n\n* rft\n\n---------\n\nCo-authored-by: sgottreich <sgottreich@absci.com>",
    "sha": "27ef46e84662b74cbd6e228d0bb4ddcde46057b3",
    "files": [
        {
            "sha": "9b57da6182bc31ad97a4ce1d09a86bb0b0eb0680",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/27ef46e84662b74cbd6e228d0bb4ddcde46057b3/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27ef46e84662b74cbd6e228d0bb4ddcde46057b3/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=27ef46e84662b74cbd6e228d0bb4ddcde46057b3",
            "patch": "@@ -172,9 +172,10 @@ def __init__(self, config):\n         )\n \n         self.padding_idx = config.pad_token_id\n-        self.position_embeddings = nn.Embedding(\n-            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n-        )\n+        if self.position_embedding_type == \"absolute\":\n+            self.position_embeddings = nn.Embedding(\n+                config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n+            )\n         self.token_dropout = config.token_dropout\n         self.mask_token_id = config.mask_token_id\n "
        }
    ],
    "stats": {
        "total": 7,
        "additions": 4,
        "deletions": 3
    }
}