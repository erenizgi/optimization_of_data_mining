{
    "author": "OmarManzoor",
    "message": "Add sdpa for Beit (#34941)\n\n* Add sdpa for Beit\n\n* Updates\n\n* [run-slow] beit\n\n* Update inference benchmarks\n\n* Update\n\n* Fix - add missed to super().forward()\n\n* Updates\n\n* Fix missing import",
    "sha": "747f361da19eb4d06042b593291dfae6e5a05e05",
    "files": [
        {
            "sha": "25b0eafb26a039126f7fbfebbc67e7f0ad66d268",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -71,6 +71,43 @@ alt=\"drawing\" width=\"600\"/>\n \n <small> BEiT pre-training. Taken from the <a href=\"https://arxiv.org/abs/2106.08254\">original paper.</a> </small>\n \n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```\n+from transformers import BeitForImageClassification\n+model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+On a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.5.1, OS Ubuntu 20.04) with `float16` and \n+`microsoft/beit-base-patch16-224` model, we saw the following improvements during training and inference:\n+\n+#### Training\n+\n+| num_training_steps | batch_size | image_size   | is_cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) | Mem saving (%) |\n+|--------------------|------------|--------------|---------|----------------------------|---------------------------|-------------|----------------------|--------------------|----------------|\n+| 50                 | 2          | (1048, 640)  | True    | 0.984                      | 0.746                     | 31.975      | 6738.915            | 4319.886          | 55.998         |\n+\n+#### Inference\n+\n+|   Image batch size |   Eager (s/iter) | Eager CI, %   |   Eager memory (MB) |   SDPA (s/iter) | SDPA CI, %   |   SDPA memory (MB) |   SDPA speedup | SDPA memory saved (%) |\n+|-------------------:|-----------------:|:--------------|--------------------:|----------------:|:-------------|-------------------:|---------------:|----------------------:|\n+|                  1 |            0.012 | Â±0.3%         |         3.76657e+08 |           0.011 | Â±0.5%        |        3.75739e+08 |          1.05  |                 0.244 |\n+|                  4 |            0.013 | Â±0.1%         |         4.03147e+08 |           0.011 | Â±0.2%        |        3.90554e+08 |          1.178 |                 3.225 |\n+|                 16 |            0.045 | Â±0.1%         |         4.96697e+08 |           0.035 | Â±0.1%        |        4.51232e+08 |          1.304 |                10.076 |\n+|                 32 |            0.088 | Â±0.1%         |         6.24417e+08 |           0.066 | Â±0.1%        |        5.33488e+08 |          1.325 |                17.044 |\n+\n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with BEiT."
        },
        {
            "sha": "cb1dc675caa55e2cd4af13cac5296a68b0e8cb2e",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -48,6 +48,46 @@ The original code for vision can be found [here](https://github.com/facebookrese\n - For Data2VecText, preprocessing is identical to [`RobertaModel`], including tokenization.\n - For Data2VecVision, preprocessing is identical to [`BeitModel`], including feature extraction.\n \n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+The SDPA implementation is currently available for the Data2VecAudio and Data2VecVision models.\n+\n+```\n+from transformers import Data2VecVisionForImageClassification\n+model = Data2VecVisionForImageClassification.from_pretrained(\"facebook/data2vec-vision-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+For the Data2VecVision model, on a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.5.1, OS Ubuntu 20.04)\n+with `float16` and `facebook/data2vec-vision-base` model, we saw the following improvements during training and\n+inference:\n+\n+#### Training\n+\n+| num_training_steps | batch_size | image_size   | is_cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) | Mem saving (%) |\n+|--------------------|------------|--------------|---------|----------------------------|---------------------------|-------------|----------------------|--------------------|----------------|\n+| 50                 | 2          | (1048, 640)  | True    | 0.996                      | 0.754                     | 32.147      | 6722.198            | 4264.653          | 57.626         |\n+\n+#### Inference\n+\n+|   Image batch size |   Eager (s/iter) | Eager CI, %   |   Eager memory (MB) |   SDPA (s/iter) | SDPA CI, %   |   SDPA memory (MB) |   SDPA speedup |   SDPA memory saved |\n+|-------------------:|-----------------:|:--------------|--------------------:|----------------:|:-------------|-------------------:|---------------:|--------------------:|\n+|                  1 |            0.011 | Â±0.3%         |         3.76143e+08 |           0.01  | Â±0.3%        |        3.74397e+08 |          1.101 |               0.466 |\n+|                  4 |            0.014 | Â±0.1%         |         4.02756e+08 |           0.012 | Â±0.2%        |        3.91373e+08 |          1.219 |               2.909 |\n+|                 16 |            0.046 | Â±0.3%         |         4.96482e+08 |           0.035 | Â±0.2%        |        4.51017e+08 |          1.314 |              10.081 |\n+|                 32 |            0.088 | Â±0.1%         |         6.23903e+08 |           0.067 | Â±0.1%        |        5.32974e+08 |          1.33  |              17.061 |\n+\n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Data2Vec."
        },
        {
            "sha": "cbb498070d69e55bb803ef77fddfb1305ae06e8a",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -221,6 +221,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Aria](https://huggingface.co/docs/transformers/model_doc/aria#transformers.AriaForConditionalGeneration)\n * [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)\n * [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n+* [Beit](https://huggingface.co/docs/transformers/model_doc/beit#transformers.BeitModel)\n * [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)\n * [BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt#transformers.BioGptModel)\n * [CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert#transformers.CamembertModel)\n@@ -230,6 +231,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)\n * [Cohere2](https://huggingface.co/docs/transformers/model_doc/cohere2#transformers.Cohere2Model)\n * [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n+* [data2vec_vision](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel)\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)\n * [Dinov2](https://huggingface.co/docs/transformers/en/model_doc/dinov2)"
        },
        {
            "sha": "601e2801d675874035f6060d4fdf8f3eb7150ec9",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 70,
            "deletions": 1,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -361,6 +361,68 @@ def forward(\n         return outputs\n \n \n+class BeitSdpaSelfAttention(BeitSelfAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        relative_position_bias: Optional[\"BeitRelativePositionBias\"] = None,\n+        interpolate_pos_encoding: bool = False,\n+        resolution: Optional[Tuple[int]] = None,\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`BeitSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n+                \"support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, \"\n+                \"but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+                relative_position_bias=relative_position_bias,\n+                interpolate_pos_encoding=interpolate_pos_encoding,\n+                resolution=resolution,\n+            )\n+\n+        mixed_query_layer = self.query(hidden_states)\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        attn_bias = None\n+        if self.relative_position_bias is not None:\n+            height, width = resolution\n+            window_size = (height // self.config.patch_size, width // self.config.patch_size)\n+            attn_bias = self.relative_position_bias(\n+                window_size, interpolate_pos_encoding, dim_size=hidden_states.shape[1]\n+            )\n+\n+        # Add shared relative position bias if provided.\n+        if relative_position_bias is not None:\n+            if attn_bias is None:\n+                attn_bias = relative_position_bias\n+            else:\n+                attn_bias += relative_position_bias\n+\n+        scaling = 1 / math.sqrt(self.attention_head_size)\n+        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attn_bias,\n+            dropout_p=self.config.attention_probs_dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+            scale=scaling,\n+        )\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(*new_context_layer_shape)\n+        return context_layer, None\n+\n+\n class BeitSelfOutput(nn.Module):\n     \"\"\"\n     The residual connection is defined in BeitLayer instead of here (as is the case with other models), due to the\n@@ -379,10 +441,16 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, gamma\n         return hidden_states\n \n \n+BEIT_SELF_ATTENTION_CLASSES = {\n+    \"eager\": BeitSelfAttention,\n+    \"sdpa\": BeitSdpaSelfAttention,\n+}\n+\n+\n class BeitAttention(nn.Module):\n     def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> None:\n         super().__init__()\n-        self.attention = BeitSelfAttention(config, window_size=window_size)\n+        self.attention = BEIT_SELF_ATTENTION_CLASSES[config._attn_implementation](config, window_size=window_size)\n         self.output = BeitSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -700,6 +768,7 @@ class BeitPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BeitLayer\"]\n     _keys_to_ignore_on_load_unexpected = [r\".*relative_position_index.*\"]\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "770162285bf33b12865d74a430dcffea2a82c6f1",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 74,
            "deletions": 2,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -362,6 +362,69 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.beit.modeling_beit.BeitSdpaSelfAttention with Beit->Data2VecVision\n+class Data2VecVisionSdpaSelfAttention(Data2VecVisionSelfAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        relative_position_bias: Optional[\"Data2VecVisionRelativePositionBias\"] = None,\n+        interpolate_pos_encoding: bool = False,\n+        resolution: Optional[Tuple[int]] = None,\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`Data2VecVisionSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n+                \"support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, \"\n+                \"but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+                relative_position_bias=relative_position_bias,\n+                interpolate_pos_encoding=interpolate_pos_encoding,\n+                resolution=resolution,\n+            )\n+\n+        mixed_query_layer = self.query(hidden_states)\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        attn_bias = None\n+        if self.relative_position_bias is not None:\n+            height, width = resolution\n+            window_size = (height // self.config.patch_size, width // self.config.patch_size)\n+            attn_bias = self.relative_position_bias(\n+                window_size, interpolate_pos_encoding, dim_size=hidden_states.shape[1]\n+            )\n+\n+        # Add shared relative position bias if provided.\n+        if relative_position_bias is not None:\n+            if attn_bias is None:\n+                attn_bias = relative_position_bias\n+            else:\n+                attn_bias += relative_position_bias\n+\n+        scaling = 1 / math.sqrt(self.attention_head_size)\n+        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attn_bias,\n+            dropout_p=self.config.attention_probs_dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+            scale=scaling,\n+        )\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(*new_context_layer_shape)\n+        return context_layer, None\n+\n+\n # Copied from transformers.models.beit.modeling_beit.BeitSelfOutput with Beit->Data2VecVision\n class Data2VecVisionSelfOutput(nn.Module):\n     \"\"\"\n@@ -381,11 +444,19 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, gamma\n         return hidden_states\n \n \n-# Copied from transformers.models.beit.modeling_beit.BeitAttention with Beit->Data2VecVision\n+DATA2VEC_VISION_SELF_ATTENTION_CLASSES = {\n+    \"eager\": Data2VecVisionSelfAttention,\n+    \"sdpa\": Data2VecVisionSdpaSelfAttention,\n+}\n+\n+\n+# Copied from tests.models.beit.modeling_beit.BeitAttention with Beit->Data2VecVision, BEIT->DATA2VEC_VISION\n class Data2VecVisionAttention(nn.Module):\n     def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] = None) -> None:\n         super().__init__()\n-        self.attention = Data2VecVisionSelfAttention(config, window_size=window_size)\n+        self.attention = DATA2VEC_VISION_SELF_ATTENTION_CLASSES[config._attn_implementation](\n+            config, window_size=window_size\n+        )\n         self.output = Data2VecVisionSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -711,6 +782,7 @@ class Data2VecVisionPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Data2VecVisionLayer\"]\n     _keys_to_ignore_on_load_unexpected = [r\".*relative_position_index.*\"]\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "e54273f7839965170c0ef751effd2107389b93ba",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 212,
            "deletions": 3,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -14,18 +14,35 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch BEiT model.\"\"\"\n \n+import inspect\n+import tempfile\n import unittest\n \n+import numpy as np\n from datasets import load_dataset\n from packaging import version\n+from parameterized import parameterized\n \n from transformers import BeitConfig\n-from transformers.testing_utils import require_torch, require_torch_multi_gpu, require_vision, slow, torch_device\n-from transformers.utils import cached_property, is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_multi_gpu,\n+    require_torch_sdpa,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    cached_property,\n+    is_torch_available,\n+    is_torch_bf16_available_on_device,\n+    is_torch_fp16_available_on_device,\n+    is_vision_available,\n+)\n \n from ...test_backbone_common import BackboneTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor, sdpa_kernel\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -74,6 +91,8 @@ def __init__(\n         scope=None,\n         out_indices=[1, 2, 3, 4],\n         out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"],\n+        attn_implementation=\"eager\",\n+        mask_ratio=0.5,\n     ):\n         self.parent = parent\n         self.vocab_size = vocab_size\n@@ -100,6 +119,8 @@ def __init__(\n         # in BeiT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n+        self.num_masks = int(mask_ratio * self.seq_length)\n+        self.attn_implementation = attn_implementation\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n@@ -131,6 +152,7 @@ def get_config(self):\n             initializer_range=self.initializer_range,\n             out_indices=self.out_indices,\n             out_features=self.out_features,\n+            attn_implementation=self.attn_implementation,\n         )\n \n     def create_and_check_model(self, config, pixel_values, labels, pixel_labels):\n@@ -387,6 +409,193 @@ def test_model_from_pretrained(self):\n         model = BeitModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        # The common test modifies the num_hidden_layers to be 1. However, for Beit we want to\n+        # avoid that because the num_hidden_layers is generally assumed to be 4. Also, the code\n+        # related to attention masks in the original common tests is not required as the Beit\n+        # model does not handle attention masks. Furthermore, some extra code like modifying\n+        # the norm layers eps values for specialized configs and checking for the 'noise'\n+        # has been omitted to simply the test.\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self.all_model_classes[0]._supports_sdpa:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n+            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n+            self.skipTest(\n+                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n+            )\n+\n+        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n+        if torch_dtype == \"float16\":\n+            torch_dtype = torch.float16\n+        elif torch_dtype == \"bfloat16\":\n+            torch_dtype = torch.bfloat16\n+        elif torch_dtype == \"float32\":\n+            torch_dtype = torch.float32\n+\n+        atols = {\n+            (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-6,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-6,\n+            (\"cuda\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+        rtols = {\n+            (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-4,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-4,\n+            (\"cuda\", True, torch.bfloat16): 3e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+\n+        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n+            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            config.rms_norm_eps = 1.0\n+            config.layer_norm_eps = 1.0\n+            config.norm_eps = 1.0\n+            config.norm_epsilon = 1.0\n+            config.layer_norm_epsilon = 1.0\n+\n+            model = model_class(config)\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype, use_mask_token=True)\n+                model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n+\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch_dtype,\n+                    attn_implementation=\"eager\",\n+                    use_mask_token=True,\n+                )\n+                model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n+\n+                # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n+                for x in model_eager.modules():\n+                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n+                        x.eps = 1.0\n+                for x in model_sdpa.modules():\n+                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n+                        x.eps = 1.0\n+\n+                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n+                # but it would be nicer to have an efficient way to use parameterized.expand\n+                fail_cases = []\n+                for padding_side in [\"left\", \"right\"]:\n+                    for use_mask in [False, True]:\n+                        for output_attentions in [True, False]:\n+                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+                            if not (self.has_attentions and can_output_attn) and output_attentions:\n+                                continue\n+                            # TODO: if we can also check with `batch_size=1` without being flaky?\n+                            for batch_size in [7]:\n+                                dummy_input = inputs_dict[model.main_input_name]\n+\n+                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n+                                    dummy_input = dummy_input.to(torch_dtype)\n+\n+                                dummy_input = dummy_input[:batch_size]\n+                                for enable_kernels in [False, True]:\n+                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, enable_kernels={enable_kernels}\"\n+                                    processed_inputs = {\n+                                        model.main_input_name: dummy_input,\n+                                        \"output_hidden_states\": True,\n+                                    }\n+\n+                                    if (\n+                                        self.has_attentions\n+                                        and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+                                    ):\n+                                        processed_inputs[\"output_attentions\"] = output_attentions\n+\n+                                    if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n+                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n+                                        mask_length = self.model_tester.seq_length - 1 - dummy_mask.size(0)\n+                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n+                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n+                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n+\n+                                    with torch.no_grad():\n+                                        with sdpa_kernel(\n+                                            enable_flash=enable_kernels,\n+                                            enable_math=True,\n+                                            enable_mem_efficient=enable_kernels,\n+                                        ):\n+                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+                                            outputs_eager = model_eager(**prepared_inputs)\n+                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n+\n+                                    logits_eager = outputs_eager.hidden_states[-1]\n+                                    logits_sdpa = outputs_sdpa.hidden_states[-1]\n+                                    if torch_device in [\"cpu\", \"cuda\"]:\n+                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n+                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                    elif torch_device == \"xpu\":\n+                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                        # which is implemented on PyTorch level using aten operators and is\n+                                        # device agnostic with respect to implementation of each aten operator.\n+                                        atol = atols[\"cuda\", False, torch_dtype]\n+                                        rtol = rtols[\"cuda\", False, torch_dtype]\n+                                    else:\n+                                        atol = 1e-7\n+                                        rtol = 1e-4\n+\n+                                    # Masked tokens output slightly deviates - we don't mind that.\n+                                    if use_mask:\n+                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                        _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                        _logits_eager[:-1] = logits_eager[:-1]\n+\n+                                        if padding_side == \"left\":\n+                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n+                                        elif padding_side == \"right\":\n+                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n+\n+                                        logits_sdpa = _logits_sdpa\n+                                        logits_eager = _logits_eager\n+\n+                                    results = [\n+                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                    ]\n+                                    # If 80% batch elements have matched results, it's fine\n+                                    if np.mean(results) < 0.8:\n+                                        fail_cases.append(\n+                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                        )\n+\n+                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "02276d905fa4023afc4d5dffed93e9437b4f8c28",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 214,
            "deletions": 3,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/747f361da19eb4d06042b593291dfae6e5a05e05/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/747f361da19eb4d06042b593291dfae6e5a05e05/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=747f361da19eb4d06042b593291dfae6e5a05e05",
            "patch": "@@ -14,14 +14,32 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Data2VecVision model.\"\"\"\n \n+import inspect\n+import tempfile\n import unittest\n \n+import numpy as np\n+from parameterized import parameterized\n+\n from transformers import Data2VecVisionConfig\n-from transformers.testing_utils import require_torch, require_torch_multi_gpu, require_vision, slow, torch_device\n-from transformers.utils import cached_property, is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_multi_gpu,\n+    require_torch_sdpa,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    cached_property,\n+    is_torch_available,\n+    is_torch_bf16_available_on_device,\n+    is_torch_fp16_available_on_device,\n+    is_vision_available,\n+)\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor, sdpa_kernel\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -66,6 +84,8 @@ def __init__(\n         num_labels=3,\n         scope=None,\n         out_indices=[0, 1, 2, 3],\n+        attn_implementation=\"eager\",\n+        mask_ratio=0.5,\n     ):\n         self.parent = parent\n         self.vocab_size = 100\n@@ -91,6 +111,8 @@ def __init__(\n         # in BeiT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n+        self.num_masks = int(mask_ratio * self.seq_length)\n+        self.attn_implementation = attn_implementation\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n@@ -121,6 +143,7 @@ def get_config(self):\n             is_decoder=False,\n             initializer_range=self.initializer_range,\n             out_indices=self.out_indices,\n+            attn_implementation=self.attn_implementation,\n         )\n \n     def create_and_check_model(self, config, pixel_values, labels, pixel_labels):\n@@ -300,6 +323,194 @@ def test_model_from_pretrained(self):\n         model = Data2VecVisionModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    # Copied from tests.models.beit.test_modeling_beit.BeitModelTest.test_eager_matches_sdpa_inference with Beit->Data2VecVision\n+    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        # The common test modifies the num_hidden_layers to be 1. However, for Data2VecVision we want to\n+        # avoid that because the num_hidden_layers is generally assumed to be 4. Also, the code\n+        # related to attention masks in the original common tests is not required as the Data2VecVision\n+        # model does not handle attention masks. Furthermore, some extra code like modifying\n+        # the norm layers eps values for specialized configs and checking for the 'noise'\n+        # has been omitted to simply the test.\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self.all_model_classes[0]._supports_sdpa:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n+            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n+            self.skipTest(\n+                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n+            )\n+\n+        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n+        if torch_dtype == \"float16\":\n+            torch_dtype = torch.float16\n+        elif torch_dtype == \"bfloat16\":\n+            torch_dtype = torch.bfloat16\n+        elif torch_dtype == \"float32\":\n+            torch_dtype = torch.float32\n+\n+        atols = {\n+            (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-6,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-6,\n+            (\"cuda\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+        rtols = {\n+            (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-4,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-4,\n+            (\"cuda\", True, torch.bfloat16): 3e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+\n+        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n+            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            config.rms_norm_eps = 1.0\n+            config.layer_norm_eps = 1.0\n+            config.norm_eps = 1.0\n+            config.norm_epsilon = 1.0\n+            config.layer_norm_epsilon = 1.0\n+\n+            model = model_class(config)\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype, use_mask_token=True)\n+                model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n+\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch_dtype,\n+                    attn_implementation=\"eager\",\n+                    use_mask_token=True,\n+                )\n+                model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n+\n+                # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n+                for x in model_eager.modules():\n+                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n+                        x.eps = 1.0\n+                for x in model_sdpa.modules():\n+                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n+                        x.eps = 1.0\n+\n+                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n+                # but it would be nicer to have an efficient way to use parameterized.expand\n+                fail_cases = []\n+                for padding_side in [\"left\", \"right\"]:\n+                    for use_mask in [False, True]:\n+                        for output_attentions in [True, False]:\n+                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+                            if not (self.has_attentions and can_output_attn) and output_attentions:\n+                                continue\n+                            # TODO: if we can also check with `batch_size=1` without being flaky?\n+                            for batch_size in [7]:\n+                                dummy_input = inputs_dict[model.main_input_name]\n+\n+                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n+                                    dummy_input = dummy_input.to(torch_dtype)\n+\n+                                dummy_input = dummy_input[:batch_size]\n+                                for enable_kernels in [False, True]:\n+                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, enable_kernels={enable_kernels}\"\n+                                    processed_inputs = {\n+                                        model.main_input_name: dummy_input,\n+                                        \"output_hidden_states\": True,\n+                                    }\n+\n+                                    if (\n+                                        self.has_attentions\n+                                        and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+                                    ):\n+                                        processed_inputs[\"output_attentions\"] = output_attentions\n+\n+                                    if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n+                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n+                                        mask_length = self.model_tester.seq_length - 1 - dummy_mask.size(0)\n+                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n+                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n+                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n+\n+                                    with torch.no_grad():\n+                                        with sdpa_kernel(\n+                                            enable_flash=enable_kernels,\n+                                            enable_math=True,\n+                                            enable_mem_efficient=enable_kernels,\n+                                        ):\n+                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+                                            outputs_eager = model_eager(**prepared_inputs)\n+                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n+\n+                                    logits_eager = outputs_eager.hidden_states[-1]\n+                                    logits_sdpa = outputs_sdpa.hidden_states[-1]\n+                                    if torch_device in [\"cpu\", \"cuda\"]:\n+                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n+                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                    elif torch_device == \"xpu\":\n+                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                        # which is implemented on PyTorch level using aten operators and is\n+                                        # device agnostic with respect to implementation of each aten operator.\n+                                        atol = atols[\"cuda\", False, torch_dtype]\n+                                        rtol = rtols[\"cuda\", False, torch_dtype]\n+                                    else:\n+                                        atol = 1e-7\n+                                        rtol = 1e-4\n+\n+                                    # Masked tokens output slightly deviates - we don't mind that.\n+                                    if use_mask:\n+                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                        _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                        _logits_eager[:-1] = logits_eager[:-1]\n+\n+                                        if padding_side == \"left\":\n+                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n+                                        elif padding_side == \"right\":\n+                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n+\n+                                        logits_sdpa = _logits_sdpa\n+                                        logits_eager = _logits_eager\n+\n+                                    results = [\n+                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                    ]\n+                                    # If 80% batch elements have matched results, it's fine\n+                                    if np.mean(results) < 0.8:\n+                                        fail_cases.append(\n+                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                        )\n+\n+                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        }
    ],
    "stats": {
        "total": 658,
        "additions": 649,
        "deletions": 9
    }
}