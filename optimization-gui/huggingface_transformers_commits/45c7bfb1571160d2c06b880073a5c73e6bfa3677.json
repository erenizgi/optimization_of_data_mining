{
    "author": "zhoubay",
    "message": "Add evolla rebase main (#36232)\n\n* add evolla\n\n* adding protein encoder part\n\n* add initial processing test\n\n* save processor\n\n* add docstring\n\n* add evolla processor\n\n* add two test\n\n* change vision to protein\n\n* change resampler to sequence_compressor\n\n* change vision to protein\n\n* initial update for llama\n\n* add initial update for llamaForCausalLM\n\n* add `test_processor`, `test_saprot_output`, `test_protein_encoder_output`\n\n* change evolla, but still working on it\n\n* add test_single_forward\n\n* pass test_attention_outputs\n\n* pass test_hidden_states_output\n\n* pass test_save_load and test_from_pretrained_no_checkpoint\n\n* pass test_cpu_offload\n\n* skip some tests\n\n* update new progress\n\n* skip test_model_is_small\n\n* pass test_model_weights_reload_no_missing_tied_weights\n\n* pass test_model_get_set_embeddings\n\n* pass test_cpu_offload\n\n* skip test_resize_embeddings\n\n* add pipeline_model_mapping\n\n* remote old setUp\n\n* pass processor save_pretrained and load_pretrained\n\n* remove pooling layer\n\n* pass test_inputs_embeds_matches_input_ids\n\n* pass test_model_is_small\n\n* pass test_attention_outputs\n\n* pass test_initialization\n\n* pass test_model_get_set_embeddings\n\n* pass test_single_forward\n\n* skip test_disk_offload_bin and test_disk_offload_safetensors\n\n* fix most tests\n\n* pass test_protein_encoder_output\n\n* remove useless code\n\n* add EvollaForProteinText2Text\n\n* pass test_saprot_output\n\n* pass all EvollaModelTest test and remove processor test\n\n* add processor test to its own file\n\n* skip is_training since esm skipped it and the saprot code causes error when setting is_training True\n\n* pass processor tests\n\n* solve all except config\n\n* pass most cases\n\n* change init\n\n* add doc to `configuration_evolla.py`\n\n* remove image_processing test\n\n* remove extra processor test\n\n* remove extra modules\n\n* remove extra modules\n\n* change all configs into one config\n\n* pass all evolla test\n\n* pass `make fixup`\n\n* update short summary\n\n* update Evolla-10B-hf\n\n* pass check_dummies.py and check_code_quality\n\n* fix  `tests/models/auto/test_tokenization_auto.py::AutoTokenizerTest::test_model_name_edge_cases_in_mappings`\n\n* remove dummy codes\n\n* change format\n\n* fix llava issue\n\n* update format\n\n* update to solve llama3 access issue\n\n* update to make forward right\n\n* solve processor save load problem from instructblip solution\n\n* remove unexpected file\n\n* skip `test_generation_tester_mixin_inheritance`\n\n* add `test_single_forward_correct` and `test_inference_natural_language_protein_reasoning`\n\n* add `modular_evolla.py`\n\n* solved issue #36362\n\n* run `make fixup`\n\n* update modular\n\n* solve float32 training\n\n* add fix\n\n* solve `utils/check_docstrings.py`\n\n* update\n\n* update\n\n* update\n\n* remove other files and replace sequential and einsum\n\n* add use case in document\n\n* update the models\n\n* update model\n\n* change some wrong code\n\n* Update src/transformers/models/evolla/modular_evolla.py\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* Update src/transformers/models/evolla/modular_evolla.py\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* Update src/transformers/models/evolla/modular_evolla.py\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* Update src/transformers/models/evolla/modular_evolla.py\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* fix issues mentioned in PR\n\n* update style and rearrange the placement\n\n* fix return_dict argument issue\n\n* solve SaProtConfig issue\n\n* Solve EvollaSaProtRotaryEmbedding issue\n\n* solve attention_mask issue\n\n* solve almosst all issues\n\n* make style\n\n* update config\n\n* remove unrelated pickle file\n\n* delete pickle files\n\n* fix config\n\n* simplify a lot\n\n* remove past k-v from encoder\n\n* continue work\n\n* style\n\n* skip it from init\n\n* fix init\n\n* fix init\n\n* simplify more\n\n* fill in docstrings\n\n* change test for generation\n\n* skip test\n\n* fix style\n\n---------\n\nCo-authored-by: Chenchen Han <13980209828@163.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "45c7bfb1571160d2c06b880073a5c73e6bfa3677",
    "files": [
        {
            "sha": "e317998a3612d48cef8b754df11947bcfb1d4df5",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -975,6 +975,8 @@\n         title: Donut\n       - local: model_doc/emu3\n         title: Emu3\n+      - local: model_doc/evolla\n+        title: Evolla\n       - local: model_doc/flava\n         title: FLAVA\n       - local: model_doc/gemma3"
        },
        {
            "sha": "79c3b120cb3709a80658fc729656479607c8b2c6",
            "filename": "docs/source/en/model_doc/evolla.md",
            "status": "added",
            "additions": 95,
            "deletions": 0,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,95 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Evolla\n+\n+## Overview\n+\n+The Evolla model was proposed in [Decoding the Molecular Language of Proteins with Evolla](https://doi.org/10.1101/2025.01.05.630192) by [Zhou et al.](https://doi.org/10.1101/2025.01.05.630192).\n+\n+Evolla is an advanced 80-billion-parameter protein-language generative model designed to decode the molecular language of proteins. It integrates information from protein sequences, structures, and user queries to generate precise and contextually nuanced insights into protein function. Trained on an unprecedented AI-generated dataset of 546 million protein question-answer pairs and 150 billion word tokens, Evolla significantly advances research in proteomics and functional genomics, providing expert-level insights and shedding light on the molecular logic encoded in proteins.\n+\n+The abstract from the paper is the following:\n+\n+*Proteins, natureâ€™s intricate molecular machines, are the products of billions of years of evolution and play fundamental roles in sustaining life. Yet, deciphering their molecular language - that is, understanding how protein sequences and structures encode and determine biological functions - remains a corner-stone challenge in modern biology. Here, we introduce Evolla, an 80 billion frontier protein-language generative model designed to decode the molecular language of proteins. By integrating information from protein sequences, structures, and user queries, Evolla generates precise and contextually nuanced insights into protein function. A key innovation of Evolla lies in its training on an unprecedented AI-generated dataset: 546 million protein question-answer pairs and 150 billion word tokens, designed to reflect the immense complexity and functional diversity of proteins. Post-pretraining, Evolla integrates Direct Preference Optimization (DPO) to refine the model based on preference signals and Retrieval-Augmented Generation (RAG) for external knowledge incorporation, improving response quality and relevance. To evaluate its performance, we propose a novel framework, Instructional Response Space (IRS), demonstrating that Evolla delivers expert-level insights, advancing research in proteomics and functional genomics while shedding light on the molecular logic encoded in proteins. The online demo is available at http://www.chat-protein.com/.*\n+\n+Examples:\n+\n+```python\n+processor = EvollaProcessor.from_pretrained(\"westlake-repl/Evolla-10B-DPO-hf\")\n+model = EvollaForProteinText2Text.from_pretrained(\"westlake-repl/Evolla-10B-DPO-hf\")\n+# aa_seq should have same length as foldseek\n+protein_inputs = [\n+    {\n+        \n+        \"aa_seq\": \"MATGGRRG...\",\n+        \"foldseek\": \"###lqpfd...\", # hashtag means the low-confidence foldseek tokens\n+    },\n+    {\n+        \"aa_seq\": \"MLPGLALL...\",\n+        \"foldseek\": \"dfwwkwad...\",\n+    }\n+]\n+message_list = [\n+    [\n+        {\n+            \"role\": \"system\",\n+            \"content\": \"You are an AI expert that can answer any questions about protein.\",\n+        },\n+        {\"role\": \"user\", \"content\": \"What is the function of this protein?\"},\n+    ],\n+    [\n+        {\n+            \"role\": \"system\",\n+            \"content\": \"You are an AI expert that can answer any questions about protein.\",\n+        },\n+        {\"role\": \"user\", \"content\": \"What is the function of this protein?\"},\n+    ]\n+]\n+input_dict = processor(\n+    protein_informations, messages_list, return_tensors=\"pt\", text_max_length=512, protein_max_length=1024\n+)\n+with torch.no_grad():\n+    generated_ids = hf_model.generate(**input_dict)\n+generated_texts = processor.batch_decode(\n+    generated_ids, skip_special_tokens=True\n+)\n+```\n+\n+Tips:\n+\n+- This model was contributed by [Xibin Bayes Zhou](https://huggingface.co/XibinBayesZhou).\n+- The original code can be found [here](https://github.com/westlake-repl/Evolla).\n+\n+\n+## EvollaConfig\n+\n+[[autodoc]] EvollaConfig\n+\n+## EvollaModel\n+\n+[[autodoc]] EvollaModel\n+    - forward\n+\n+## EvollaForProteinText2Text\n+\n+[[autodoc]] EvollaForProteinText2Text\n+    - forward\n+\n+## EvollaProcessor\n+\n+[[autodoc]] EvollaProcessor\n+    - __call__"
        },
        {
            "sha": "7b59f958f059e3f5bd37e243e164fd93efc12fe9",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -110,6 +110,7 @@\n     from .encoder_decoder import *\n     from .ernie import *\n     from .esm import *\n+    from .evolla import *\n     from .falcon import *\n     from .falcon_h1 import *\n     from .falcon_mamba import *"
        },
        {
            "sha": "4d22bd00ef2c8dac8be2980afa61fafcabb7cd95",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -133,6 +133,7 @@\n         (\"ernie4_5_moe\", \"Ernie4_5_MoeConfig\"),\n         (\"ernie_m\", \"ErnieMConfig\"),\n         (\"esm\", \"EsmConfig\"),\n+        (\"evolla\", \"EvollaConfig\"),\n         (\"falcon\", \"FalconConfig\"),\n         (\"falcon_h1\", \"FalconH1Config\"),\n         (\"falcon_mamba\", \"FalconMambaConfig\"),\n@@ -528,6 +529,7 @@\n         (\"ernie4_5_moe\", \"Ernie4_5_MoE\"),\n         (\"ernie_m\", \"ErnieM\"),\n         (\"esm\", \"ESM\"),\n+        (\"evolla\", \"Evolla\"),\n         (\"falcon\", \"Falcon\"),\n         (\"falcon3\", \"Falcon3\"),\n         (\"falcon_h1\", \"FalconH1\"),"
        },
        {
            "sha": "9d6622f3895b000de392439b92419fb03ff84a91",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -124,6 +124,7 @@\n         (\"ernie4_5_moe\", \"Ernie4_5_MoeModel\"),\n         (\"ernie_m\", \"ErnieMModel\"),\n         (\"esm\", \"EsmModel\"),\n+        (\"evolla\", \"EvollaModel\"),\n         (\"falcon\", \"FalconModel\"),\n         (\"falcon_h1\", \"FalconH1Model\"),\n         (\"falcon_mamba\", \"FalconMambaModel\"),\n@@ -402,6 +403,7 @@\n         (\"distilbert\", \"DistilBertForMaskedLM\"),\n         (\"electra\", \"ElectraForPreTraining\"),\n         (\"ernie\", \"ErnieForPreTraining\"),\n+        (\"evolla\", \"EvollaForProteinText2Text\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n         (\"flaubert\", \"FlaubertWithLMHeadModel\"),\n         (\"flava\", \"FlavaForPreTraining\"),\n@@ -934,6 +936,7 @@\n         (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n         (\"chameleon\", \"ChameleonForConditionalGeneration\"),\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),\n+        (\"evolla\", \"EvollaForProteinText2Text\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n         (\"gemma3n\", \"Gemma3nForConditionalGeneration\"),"
        },
        {
            "sha": "31b798c8059795f23fac527741180adcebc6d210",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -64,6 +64,7 @@\n         (\"colqwen2\", \"ColQwen2Processor\"),\n         (\"dia\", \"DiaProcessor\"),\n         (\"emu3\", \"Emu3Processor\"),\n+        (\"evolla\", \"EvollaProcessor\"),\n         (\"flava\", \"FlavaProcessor\"),\n         (\"fuyu\", \"FuyuProcessor\"),\n         (\"gemma3\", \"Gemma3Processor\"),"
        },
        {
            "sha": "09be74f03397074c49bf0df9a34b67f695856131",
            "filename": "src/transformers/models/evolla/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2F__init__.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_evolla import *\n+    from .modeling_evolla import *\n+    from .processing_evolla import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "18e12150f1826f6eccb684a508f055a1d3dd6a06",
            "filename": "src/transformers/models/evolla/configuration_evolla.py",
            "status": "added",
            "additions": 279,
            "deletions": 0,
            "changes": 279,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,279 @@\n+# coding=utf-8\n+# Copyright 2025 Westlake Representational Learning Lab (Fajie Yuan Lab) team and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Evolla model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SaProtConfig(PretrainedConfig):\n+    r\"\"\"This is the configuration class to store the configuration of a [`EvollaSaProtProteinEncoder`]. It is used to instantiate a\n+    SaProt model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 446):\n+            Vocabulary size of the protein sequence model. Defines the number of different tokens that can be represented\n+            by the `inputs_ids` passed when calling [`EvollaModel`].\n+        mask_token_id (`int`, *optional*, defaults to 4):\n+            The id of the *mask* token in the protein sequence model.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            The id of the *padding* token in the protein sequence model.\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Dimensionality of the protein sequence model layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 33):\n+            Number of hidden layers in the protein sequence model.\n+        num_attention_heads (`int`, *optional*, defaults to 20):\n+            Number of attention heads for each attention layer in the protein sequence model.\n+        intermediate_size (`int`, *optional*, defaults to 5120):\n+            Dimensionality of the intermediate layers in the protein sequence model.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for the hidden layers in the protein sequence model.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for the attention probabilities in the protein sequence model.\n+        max_position_embeddings (`int`, *optional*, defaults to 1026):\n+            The maximum sequence length that the protein sequence model might ever be used with. Typically set this to\n+            something large just in case (e.g., 512 or 1024 or 2048).\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value for the layer normalization layer in the protein sequence model.\n+        position_embedding_type (`str`, *optional*, defaults to `\"rotary\"`):\n+            The type of position embedding to use in the protein sequence model. Currently only `\"rotary\"` is supported.\n+        emb_layer_norm_before (`bool`, *optional*, defaults to `False`):\n+            Whether to apply layer normalization before the position embedding in the protein sequence model.\n+        token_dropout (`bool`, *optional*, defaults to `True`):\n+            Whether to apply dropout to the tokens in the protein sequence model.\"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=446,\n+        mask_token_id=4,\n+        pad_token_id=1,\n+        hidden_size=1280,\n+        num_hidden_layers=33,\n+        num_attention_heads=20,\n+        intermediate_size=5120,\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        max_position_embeddings=1026,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-05,\n+        position_embedding_type=\"rotary\",\n+        use_cache=True,\n+        emb_layer_norm_before=False,\n+        token_dropout=True,\n+        **kwargs,\n+    ):\n+        super().__init__(pad_token_id=pad_token_id, mask_token_id=mask_token_id, **kwargs)\n+\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.position_embedding_type = position_embedding_type\n+        self.use_cache = use_cache\n+        self.emb_layer_norm_before = emb_layer_norm_before\n+        self.token_dropout = token_dropout\n+\n+\n+class EvollaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`EvollaModel`]. It is used to instantiate an\n+    Evolla model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Evolla-10B.\n+\n+    e.g. [westlake-repl/Evolla-10B-hf](https://huggingface.co/westlake-repl/Evolla-10B-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        protein_encoder_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`SaProtConfig`].\n+        vocab_size (`int`, *optional*, defaults to 128256):\n+            Vocabulary size of the Evolla llama model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`EvollaModel`].\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimensionality of the llama layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 14336):\n+            Dimensionality of the intermediate layers in the llama model.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the llama model.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the llama model.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            Number of key-value pairs for each attention layer in the llama model.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the llama model. If string, `\"gelu\"`, `\"relu\"`,\n+            `\"selu\"` and `\"silu\"` are supported.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value for the RMS-norm layer in the llama model.\n+        rope_theta (`float`, *optional*, defaults to 500000.0):\n+            The threshold value for the RoPE layer in the llama model.\n+        rope_scaling (`float`, *optional*):\n+            The scaling factor for the RoPE layer in the llama model.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the attention layer.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention layer.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the MLP layer.\n+        aligner_ffn_mult (`int`, *optional*, defaults to 4):\n+            The FFN multiplier for the aligner layer.\n+        aligner_enable_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the aligner layer.\n+        aligner_attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for the attention probabilities in the aligner layer.\n+        aligner_num_add_layers (`int`, *optional*, defaults to 8):\n+            The number of additional layers for the aligner layer.\n+        resampler_depth (`int`, *optional*, defaults to 6):\n+            The depth of the resampler layer in the llama model.\n+        resampler_dim_head (`int`, *optional*, defaults to 64):\n+            The dimension of the heads in the resampler layer in the llama model.\n+        resampler_heads (`int`, *optional*, defaults to 8):\n+            The number of heads in the resampler layer in the llama model.\n+        resampler_num_latents (`int`, *optional*, defaults to 64):\n+            The number of latents in the resampler layer in the llama model.\n+        resampler_ff_mult (`int`, *optional*, defaults to 4):\n+            The FFN multiplier for the resampler layer.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        pad_token_id (`int`, *optional*):\n+            The id of the *padding* token.\n+        bos_token_id (`int`, *optional*, defaults to 128000):\n+            The id of the *beginning-of-sequence* token.\n+        eos_token_id (`int`, *optional*, defaults to 128009):\n+            The id of the *end-of-sequence* token.\n+        use_cache (`bool`, *optional*, defaults to `False`):\n+            Whether or not the model should return the last key/values attentions (not used by all models).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether or not to tie the input and output word embeddings.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import EvollaModel, EvollaConfig\n+\n+    >>> # Initializing a Evolla evolla-10b style configuration\n+    >>> configuration = EvollaConfig()\n+\n+    >>> # Initializing a model from the evolla-10b style configuration\n+    >>> model = EvollaModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"EvollaModel\"\n+    sub_configs = {\"protein_encoder_config\": SaProtConfig}\n+\n+    def __init__(\n+        self,\n+        protein_encoder_config=None,\n+        vocab_size=128256,  # llama vocab size\n+        hidden_size=4096,  # llama hidden size\n+        intermediate_size=14336,  # llama intermediate size\n+        num_hidden_layers=32,  # llama num layers\n+        num_attention_heads=32,  # llama num heads\n+        num_key_value_heads=8,  # llama num key-value heads\n+        hidden_act=\"silu\",  # llama activation function\n+        max_position_embeddings=8192,  # llama rope max length\n+        rms_norm_eps=1e-05,\n+        rope_theta=500000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        aligner_ffn_mult=4,\n+        aligner_enable_bias=True,\n+        aligner_attention_probs_dropout_prob=0.1,\n+        aligner_num_add_layers=8,\n+        resampler_depth=6,\n+        resampler_dim_head=64,\n+        resampler_heads=8,\n+        resampler_num_latents=64,\n+        resampler_ff_mult=4,\n+        initializer_range=0.02,\n+        pad_token_id=None,\n+        bos_token_id=128000,\n+        eos_token_id=128009,\n+        use_cache=False,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.rms_norm_eps = rms_norm_eps\n+        self.tie_word_embeddings = tie_word_embeddings\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.aligner_ffn_mult = aligner_ffn_mult\n+        self.aligner_enable_bias = aligner_enable_bias\n+        self.aligner_attention_probs_dropout_prob = aligner_attention_probs_dropout_prob\n+        self.aligner_num_add_layers = aligner_num_add_layers\n+        self.use_cache = use_cache\n+        self.initializer_range = initializer_range\n+\n+        self.resampler_depth = resampler_depth\n+        self.resampler_dim_head = resampler_dim_head\n+        self.resampler_heads = resampler_heads\n+        self.resampler_num_latents = resampler_num_latents\n+        self.resampler_ff_mult = resampler_ff_mult\n+\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        # Subconfig\n+        if protein_encoder_config is None:\n+            protein_encoder_config = {}\n+            logger.info(\"`protein_encoder_config` is `None`. Initializing the `SaProtConfig` with default values.\")\n+        self.protein_encoder_config = SaProtConfig(**protein_encoder_config)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"EvollaConfig\"]"
        },
        {
            "sha": "f51f27d6d342b4c31d955b63cdb00160bb57ec6d",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "added",
            "additions": 1761,
            "deletions": 0,
            "changes": 1761,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,1761 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/evolla/modular_evolla.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_evolla.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Westlake Representational Learning Lab (Fajie Yuan Lab) team and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+import warnings\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor, nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutputWithCrossAttentions,\n+    BaseModelOutputWithPast,\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n+    CausalLMOutputWithPast,\n+    ModelOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import (\n+    ALL_ATTENTION_FUNCTIONS,\n+    ModuleUtilsMixin,\n+    PreTrainedModel,\n+    find_pruneable_heads_and_indices,\n+    get_parameter_dtype,\n+    prune_linear_layer,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_evolla import EvollaConfig, SaProtConfig\n+\n+\n+if is_flash_attn_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def create_position_ids_from_input_ids(input_ids, padding_idx):\n+    \"\"\"\n+    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+    are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+    Args:\n+        x: torch.Tensor x:\n+\n+    Returns: torch.Tensor\n+    \"\"\"\n+    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+    mask = input_ids.ne(padding_idx).int()\n+    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n+    return incremental_indices.long() + padding_idx\n+\n+\n+class EvollaSaProtEmbeddings(nn.Module):\n+    \"\"\"\n+    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+\n+        if config.emb_layer_norm_before:\n+            self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        else:\n+            self.layer_norm = None\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+        self.padding_idx = config.pad_token_id\n+        if self.position_embedding_type == \"absolute\":\n+            self.position_embeddings = nn.Embedding(\n+                config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n+            )\n+        self.token_dropout = config.token_dropout\n+        self.mask_token_id = config.mask_token_id\n+        # remove the position_ids in EsmEmbeddings\n+        self.position_ids = None\n+\n+    def forward(\n+        self,\n+        input_ids=None,\n+        attention_mask=None,\n+        position_ids=None,\n+        inputs_embeds=None,\n+    ):\n+        if position_ids is None:\n+            if input_ids is not None:\n+                # Create the position ids from the input token ids. Any padded tokens remain padded.\n+                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx)\n+            else:\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+\n+        # Note that if we want to support EVOLLA_SA_PROT-1 (not 1b!) in future then we need to support an\n+        # embedding_scale factor here.\n+        embeddings = inputs_embeds\n+\n+        # Matt: EVOLLA_SA_PROT has the option to handle masking in MLM in a slightly unusual way. If the token_dropout\n+        # flag is False then it is handled in the same was as BERT/RoBERTa. If it is set to True, however,\n+        # masked tokens are treated as if they were selected for input dropout and zeroed out.\n+        # This \"mask-dropout\" is compensated for when masked tokens are not present, by scaling embeddings by\n+        # a factor of (fraction of unmasked tokens during training) / (fraction of unmasked tokens in sample).\n+        # This is analogous to the way that dropout layers scale down outputs during evaluation when not\n+        # actually dropping out values (or, equivalently, scale up their un-dropped outputs in training).\n+        if self.token_dropout:\n+            embeddings = embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n+            mask_ratio_train = 0.15 * 0.8  # Hardcoded as the ratio used in all EVOLLA_SA_PROT model training runs\n+            src_lengths = attention_mask.sum(-1)\n+            mask_ratio_observed = (input_ids == self.mask_token_id).sum(-1).float() / src_lengths\n+            embeddings = (embeddings * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None]).to(\n+                embeddings.dtype\n+            )\n+\n+        if self.position_embedding_type == \"absolute\":\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings = embeddings + position_embeddings\n+\n+        if self.layer_norm is not None:\n+            embeddings = self.layer_norm(embeddings)\n+        if attention_mask is not None:\n+            embeddings = (embeddings * attention_mask.unsqueeze(-1)).to(embeddings.dtype)\n+        # Matt: I think this line was copied incorrectly from BERT, disabling it for now.\n+        # embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+        \"\"\"\n+        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n+\n+        Args:\n+            inputs_embeds: torch.Tensor\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        input_shape = inputs_embeds.size()[:-1]\n+        sequence_length = input_shape[1]\n+\n+        position_ids = torch.arange(\n+            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+        )\n+        return position_ids.unsqueeze(0).expand(input_shape)\n+\n+\n+def rotate_half_esm(x):\n+    x1, x2 = x.chunk(2, dim=-1)\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb_esm(x, cos, sin):\n+    cos = cos[:, :, : x.shape[-2], :]\n+    sin = sin[:, :, : x.shape[-2], :]\n+\n+    return (x * cos) + (rotate_half_esm(x) * sin)\n+\n+\n+class EvollaSaProtRotaryEmbedding(nn.Module):\n+    \"\"\"\n+    Rotary position embeddings based on those in\n+    [RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer). Query and keys are transformed by rotation\n+    matrices which depend on their relative positions.\n+    \"\"\"\n+\n+    def __init__(self, dim: int):\n+        super().__init__()\n+        # Generate and save the inverse frequency buffer (non trainable)\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n+        inv_freq = inv_freq\n+        self.register_buffer(\"inv_freq\", inv_freq)\n+\n+        self._seq_len_cached = None\n+        self._cos_cached = None\n+        self._sin_cached = None\n+\n+    def _update_cos_sin_tables(self, x, seq_dimension=2):\n+        seq_len = x.shape[seq_dimension]\n+\n+        # Reset the tables if the sequence length has changed,\n+        # or if we're on a new device (possibly due to tracing for instance)\n+        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n+            self._seq_len_cached = seq_len\n+            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq)\n+            freqs = torch.outer(t, self.inv_freq)\n+            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n+\n+            self._cos_cached = emb.cos()[None, None, :, :]\n+            self._sin_cached = emb.sin()[None, None, :, :]\n+\n+        return self._cos_cached, self._sin_cached\n+\n+    def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n+\n+        return (\n+            apply_rotary_pos_emb_esm(q, self._cos_cached, self._sin_cached),\n+            apply_rotary_pos_emb_esm(k, self._cos_cached, self._sin_cached),\n+        )\n+\n+\n+class EvollaSaProtSelfAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__()\n+        self.config = config\n+\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        self.rotary_embeddings = None\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n+        elif self.position_embedding_type == \"rotary\":\n+            self.rotary_embeddings = EvollaSaProtRotaryEmbedding(dim=self.attention_head_size)\n+\n+        self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n+\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor]:\n+        hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n+\n+        query_layer = self.query(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        # If this is instantiated as a cross-attention module, the keys\n+        # and values come from an encoder; the attention mask needs to be\n+        # such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        if is_cross_attention:\n+            key_layer = self.key(encoder_hidden_states).view(hidden_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(hidden_shape).transpose(1, 2)\n+            attention_mask = encoder_attention_mask\n+        else:\n+            key_layer = self.key(hidden_states).view(hidden_shape).transpose(1, 2)\n+            value_layer = self.value(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n+        # EVOLLA_SA_PROT scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n+        # but not when rotary embeddings get involved. Therefore, we scale the query here to match the original\n+        # EVOLLA_SA_PROT code and fix rotary embeddings.\n+        query_layer = query_layer * self.attention_head_size**-0.5\n+\n+        if self.position_embedding_type == \"rotary\":\n+            query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            seq_length = hidden_states.size()[1]\n+            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n+            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n+            distance = position_ids_l - position_ids_r\n+            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n+            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n+\n+            if self.position_embedding_type == \"relative_key\":\n+                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores\n+            elif self.position_embedding_type == \"relative_key_query\":\n+                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n+\n+        if attention_mask is not None:\n+            # Apply the attention mask is (precomputed for all layers in EvollaSaProtModel forward() function)\n+            attention_scores = attention_scores + attention_mask\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs.to(value_layer.dtype), value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        if self.is_decoder:\n+            outputs = outputs + (None,)\n+        return outputs\n+\n+\n+class EvollaSaProtSelfOutput(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states, input_tensor):\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = hidden_states + input_tensor\n+        return hidden_states\n+\n+\n+class EvollaSaProtFlashAttention2(EvollaSaProtSelfAttention):\n+    \"\"\"\n+    EVOLLA_SA_PROT flash attention module. This module inherits from `EvollaSaProtSelfAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor]:\n+        # Flash attention doesn't support output_attentions or cross attention\n+        if output_attentions or head_mask is not None or encoder_hidden_states is not None:\n+            logger.warning_once(\n+                \"EvollaSaProtFlashAttention2 does not support output_attentions, head_mask, or cross_attention. \"\n+                \"Falling back to the manual attention implementation. This warning can be removed using \"\n+                'the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                output_attentions,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32.\n+        input_dtype = query_layer.dtype\n+        device_type = query_layer.device.type if query_layer.device.type != \"mps\" else \"cpu\"\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.query.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_layer = query_layer.to(target_dtype)\n+            key_layer = key_layer.to(target_dtype)\n+            value_layer = value_layer.to(target_dtype)\n+\n+        # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n+        # EVOLLA_SA_PROT scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n+        # but not when rotary embeddings get involved. Therefore, we scale the query here to match the original\n+        # EVOLLA_SA_PROT code and fix rotary embeddings.\n+        query_layer = query_layer * self.attention_head_size**-0.5\n+\n+        if self.position_embedding_type == \"rotary\":\n+            query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n+        elif self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            raise ValueError(f\"ESM flash attention does not support {self.position_embedding_type} embeddings\")\n+\n+        # It would likely be faster to change self.transpose_for_scores to output the correct\n+        # dimensions for flash_attention_2, but that would also mean changing the rotary embedding\n+        # functions. Here we just permute the dimensions to match the expected input.\n+        attn_output = _flash_attention_forward(\n+            query_layer.permute(0, 2, 1, 3),\n+            key_layer.permute(0, 2, 1, 3),\n+            value_layer.permute(0, 2, 1, 3),\n+            attention_mask,\n+            query_length=q_len,\n+            is_causal=self.is_decoder,\n+            softmax_scale=1.0,\n+            dropout=self.dropout_prob if self.training else 0.0,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n+\n+        outputs = (attn_output, None)\n+        if self.is_decoder:\n+            outputs = outputs + (None,)\n+\n+        return outputs\n+\n+\n+EVOLLA_SA_PROT_ATTENTION_CLASSES = {\n+    \"eager\": EvollaSaProtSelfAttention,\n+    \"flash_attention_2\": EvollaSaProtFlashAttention2,\n+}\n+\n+\n+class EvollaSaProtAttention(nn.Module):\n+    def __init__(self, config, layer_idx=None):\n+        super().__init__()\n+        self.self = EVOLLA_SA_PROT_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.output = EvollaSaProtSelfOutput(config)\n+        self.pruned_heads = set()\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def prune_heads(self, heads):\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.self.query = prune_linear_layer(self.self.query, index)\n+        self.self.key = prune_linear_layer(self.self.key, index)\n+        self.self.value = prune_linear_layer(self.self.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n+        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        head_mask=None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        past_key_value=None,\n+        output_attentions=False,\n+        cache_position=None,\n+    ):\n+        hidden_states_ln = self.LayerNorm(hidden_states)\n+        self_outputs = self.self(\n+            hidden_states_ln,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+def gelu(x):\n+    \"\"\"\n+    This is the gelu implementation from the original EVOLLA_SA_PROT repo. Using F.gelu yields subtly wrong results.\n+    \"\"\"\n+    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n+\n+\n+class EvollaSaProtIntermediate(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = gelu(hidden_states)\n+        return hidden_states\n+\n+\n+class EvollaSaProtOutput(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states, input_tensor):\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = hidden_states + input_tensor\n+        return hidden_states\n+\n+\n+class EvollaSaProtLayer(GradientCheckpointingLayer):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n+        self.seq_len_dim = 1\n+        self.attention = EvollaSaProtAttention(config)\n+        self.is_decoder = config.is_decoder\n+        self.add_cross_attention = config.add_cross_attention\n+        if self.add_cross_attention:\n+            if not self.is_decoder:\n+                raise RuntimeError(f\"{self} should be used as a decoder model if cross attention is added\")\n+            self.crossattention = EvollaSaProtAttention(config)\n+        self.intermediate = EvollaSaProtIntermediate(config)\n+        self.output = EvollaSaProtOutput(config)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        head_mask=None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        past_key_value=None,\n+        output_attentions=False,\n+        cache_position=None,\n+    ):\n+        self_attention_outputs = self.attention(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self_attention_outputs[0]\n+\n+        # if decoder, the last output is tuple of self-attn cache\n+        if self.is_decoder:\n+            outputs = self_attention_outputs[1:-1]\n+        else:\n+            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        if self.is_decoder and encoder_hidden_states is not None:\n+            if not hasattr(self, \"crossattention\"):\n+                raise AttributeError(\n+                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated\"\n+                    \" with cross-attention layers by setting `config.add_cross_attention=True`\"\n+                )\n+\n+            cross_attention_outputs = self.crossattention(\n+                attention_output,\n+                attention_mask=attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+            attention_output = cross_attention_outputs[0]\n+            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n+\n+        layer_output = self.feed_forward_chunk(attention_output)\n+\n+        outputs = (layer_output,) + outputs\n+\n+        # if decoder, return the attn key/values as the last output\n+        if self.is_decoder:\n+            outputs = outputs + (None,)\n+        return outputs\n+\n+    def feed_forward_chunk(self, attention_output):\n+        attention_output_ln = self.LayerNorm(attention_output)\n+        intermediate_output = self.intermediate(attention_output_ln)\n+        layer_output = self.output(intermediate_output, attention_output)\n+        return layer_output\n+\n+\n+class EvollaSaProtEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([EvollaSaProtLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.emb_layer_norm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        head_mask=None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        past_key_values=None,\n+        use_cache=None,\n+        output_attentions=False,\n+        output_hidden_states=False,\n+        return_dict=True,\n+        cache_position=None,\n+    ):\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+\n+            layer_outputs = layer_module(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                head_mask=layer_head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+                if self.config.add_cross_attention:\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n+\n+        if self.emb_layer_norm_after:\n+            hidden_states = self.emb_layer_norm_after(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        return BaseModelOutputWithCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+class EvollaSaProtPooler(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.activation = nn.Tanh()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        # We \"pool\" the model by simply taking the hidden state corresponding\n+        # to the first token.\n+        first_token_tensor = hidden_states[:, 0]\n+        pooled_output = self.dense(first_token_tensor)\n+        pooled_output = self.activation(pooled_output)\n+        return pooled_output\n+\n+\n+@auto_docstring\n+class EvollaSaProtPreTrainedModel(PreTrainedModel):\n+    config: SaProtConfig\n+    _no_split_modules = [\"EvollaSaProtLayer\"]\n+    _supports_flash_attn = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n+    def __init__(self, config: SaProtConfig):\n+        super().__init__(config)\n+        self.embeddings = EvollaSaProtEmbeddings(config)\n+        self.encoder = EvollaSaProtEncoder(config)\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.word_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.word_embeddings = value\n+\n+    def _prune_heads(self, heads_to_prune):\n+        \"\"\"\n+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n+        class PreTrainedModel\n+        \"\"\"\n+        for layer, heads in heads_to_prune.items():\n+            self.encoder.layer[layer].attention.prune_heads(heads)\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+        input_shape = input_ids.size()\n+        batch_size, seq_length = input_shape\n+\n+        device = input_ids.device\n+        if attention_mask is None:\n+            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n+\n+        inputs_embeds = self.embeddings(input_ids=input_ids, attention_mask=attention_mask)\n+        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        encoder_outputs = self.encoder(inputs_embeds, attention_mask=extended_attention_mask)\n+        sequence_output = encoder_outputs[0]\n+\n+        return BaseModelOutputWithPoolingAndCrossAttentions(\n+            last_hidden_state=sequence_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+            cross_attentions=encoder_outputs.cross_attentions,\n+        )\n+\n+    def get_extended_attention_mask(\n+        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n+    ) -> Tensor:\n+        \"\"\"\n+        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n+\n+        Arguments:\n+            attention_mask (`torch.Tensor`):\n+                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n+            input_shape (`Tuple[int]`):\n+                The shape of the input to the model.\n+\n+        Returns:\n+            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n+        \"\"\"\n+        if dtype is None:\n+            dtype = get_parameter_dtype(self)\n+\n+        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n+            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n+            if device is not None:\n+                warnings.warn(\n+                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n+                )\n+        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+        # ourselves in which case we just need to make it broadcastable to all heads.\n+        if attention_mask.dim() == 3:\n+            extended_attention_mask = attention_mask[:, None, :, :]\n+        elif attention_mask.dim() == 2:\n+            # Provided a padding mask of dimensions [batch_size, seq_length]\n+            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n+            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+            if self.config.is_decoder:\n+                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n+                    input_shape, attention_mask, device\n+                )\n+            else:\n+                extended_attention_mask = attention_mask[:, None, None, :]\n+        else:\n+            raise ValueError(\n+                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n+            )\n+\n+        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n+        # masked positions, this operation will create a tensor which is 0.0 for\n+        # positions we want to attend and the dtype's smallest value for masked positions.\n+        # Since we are adding it to the raw scores before the softmax, this is\n+        # effectively the same as removing these entirely.\n+        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n+        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n+        return extended_attention_mask\n+\n+\n+class EvollaSequenceCompressorAttention(nn.Module):\n+    def __init__(self, dim, dim_head=64, heads=8):\n+        super().__init__()\n+        self.scale = dim_head**-0.5\n+        self.heads = heads\n+        inner_dim = dim_head * heads\n+\n+        self.norm_media = nn.LayerNorm(dim)\n+        self.norm_latents = nn.LayerNorm(dim)\n+\n+        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n+        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n+        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n+\n+    def forward(self, x, latents, mask):\n+        \"\"\"\n+        Args:\n+            x (torch.Tensor): image features\n+                shape (b, n1, D)\n+            latent (torch.Tensor): latent features\n+                shape (b, n2, D);  n2: num of latent tokens\n+        \"\"\"\n+        x = self.norm_media(x)\n+        latents = self.norm_latents(latents)\n+\n+        h = self.heads\n+\n+        q = self.to_q(latents)\n+        kv_input = torch.cat((x, latents), dim=-2)\n+        k, v = self.to_kv(kv_input).chunk(\n+            2, dim=-1\n+        )  # each: batch_size, max_protein_length+num_latents, dim_head*num_heads\n+\n+        q = q.view(q.size(0), q.size(1), h, -1).permute(0, 2, 1, 3)\n+        k = k.view(k.size(0), k.size(1), h, -1).permute(0, 2, 1, 3)\n+        v = v.view(v.size(0), v.size(1), h, -1).permute(0, 2, 1, 3)\n+        q = q * self.scale  # batch_size, num_heads, num_latents, dim_head\n+\n+        # attention\n+        sim = torch.matmul(q, k.transpose(-1, -2))\n+        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n+        bs, nh, skd, okd = sim.shape\n+        ones = torch.ones(nh, skd).to(mask.device)  # Create a tensor of ones with shape (nh, skd)\n+        mask_exp = mask[:, None, None, :]\n+        ones_exp = ones[None, :, :, None]\n+        mask = mask_exp * ones_exp\n+\n+        sim = sim.masked_fill((1 - mask).bool(), -1e4)\n+        attn = sim.softmax(dim=-1)\n+        out = torch.matmul(attn, v)\n+        out = out.permute(0, 2, 1, 3)\n+\n+        # [batch, seq, head, features] -> [batch, seq, head*features]\n+        out = out.reshape(out.size(0), out.size(1), -1)\n+\n+        return self.to_out(out)\n+\n+\n+class EvollaFeedForward(nn.Module):\n+    def __init__(self, dim, mult=4):\n+        super().__init__()\n+        inner_dim = int(dim * mult)\n+\n+        self.norm = nn.LayerNorm(dim)\n+        self.fc1 = nn.Linear(dim, inner_dim, bias=False)\n+        self.activation = nn.GELU()\n+        self.fc2 = nn.Linear(inner_dim, dim, bias=False)\n+\n+    def forward(self, x):\n+        return self.fc2(self.activation(self.fc1(self.norm(x))))\n+\n+\n+class EvollaSequenceCompressorResampler(nn.Module):\n+    def __init__(self, config: EvollaConfig):\n+        super().__init__()\n+        protein_repr_dim = config.protein_encoder_config.hidden_size\n+        self.num_latents = config.resampler_num_latents\n+        self.latents = nn.Parameter(torch.randn(self.num_latents, protein_repr_dim), requires_grad=True)\n+        self.layers = nn.ModuleList([])\n+        for _ in range(config.resampler_depth):\n+            self.layers.append(\n+                nn.ModuleList(\n+                    [\n+                        EvollaSequenceCompressorAttention(\n+                            dim=protein_repr_dim, dim_head=config.resampler_dim_head, heads=config.resampler_heads\n+                        ),\n+                        EvollaFeedForward(dim=protein_repr_dim, mult=config.resampler_ff_mult),\n+                    ]\n+                )\n+            )\n+\n+        self.norm = nn.LayerNorm(config.hidden_size)\n+        self.protein_projector = nn.Linear(protein_repr_dim, config.hidden_size)\n+\n+    def forward(self, embeds, mask):\n+        b = embeds.shape[0]\n+\n+        bs, _ = mask.shape  # bs, max_protein_length\n+        latent_mask = torch.ones(bs, self.num_latents).to(mask.device)\n+        mask = torch.cat((mask, latent_mask), dim=1)  # bs, max_protein_length + num_latents\n+\n+        # blocks\n+        ones = torch.ones(b).to(self.latents.device)\n+        latents = self.latents[None] * ones.view(-1, 1, 1)  # [b,n,d]\n+        latents = latents.to(embeds.dtype)\n+        for attn, ff in self.layers:\n+            latents = attn(embeds, latents, mask) + latents\n+            latents = ff(latents) + latents\n+\n+        transformed_feature = self.protein_projector(latents)\n+\n+        return self.norm(transformed_feature)\n+\n+\n+@dataclass\n+@auto_docstring\n+class EvollaProteinEncoderModelOutput(ModelOutput):\n+    sequence_compressor_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class EvollaProteinEncoder(nn.Module):\n+    def __init__(self, config: EvollaConfig):\n+        super().__init__()\n+        self.model = EvollaSaProtProteinEncoder(config=config.protein_encoder_config)\n+        self.sequence_compressor_resampler = EvollaSequenceCompressorResampler(config=config)\n+\n+    @can_return_tuple\n+    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.FloatTensor, **kwargs):\n+        protein_output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n+        protein_embeds = protein_output.last_hidden_state\n+        sequence_repr = self.sequence_compressor_resampler(protein_embeds, attention_mask)\n+\n+        return EvollaProteinEncoderModelOutput(\n+            sequence_compressor_output=sequence_repr,\n+            last_hidden_state=protein_output.last_hidden_state,\n+        )\n+\n+\n+class EvollaSequenceAlignerCrossAttention(nn.Module):\n+    def __init__(\n+        self,\n+        config,\n+        protein_encoder_dim: Optional[int] = None,\n+        structure_encoder_dim: Optional[int] = None,\n+        msa_encoder_dim: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        self.hidden_size = config.hidden_size\n+        self.num_attention_heads = config.num_attention_heads\n+        self.scale = self.num_attention_heads**-0.5\n+        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        attention_probs_dropout_prob = config.aligner_attention_probs_dropout_prob\n+        enable_bias = config.aligner_enable_bias\n+        ffn_mult = config.aligner_ffn_mult\n+\n+        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n+        if protein_encoder_dim is not None:\n+            self.key_protein = nn.Linear(protein_encoder_dim, self.all_head_size)\n+            self.value_protein = nn.Linear(protein_encoder_dim, self.all_head_size)\n+        else:\n+            self.key_protein = None\n+            self.value_protein = None\n+\n+        if structure_encoder_dim is not None:\n+            self.key_structure = nn.Linear(structure_encoder_dim, self.all_head_size)\n+            self.value_structure = nn.Linear(structure_encoder_dim, self.all_head_size)\n+        else:\n+            self.key_structure = None\n+            self.value_structure = None\n+\n+        if msa_encoder_dim is not None:\n+            self.key_msa = nn.Linear(msa_encoder_dim, self.all_head_size)\n+            self.value_msa = nn.Linear(msa_encoder_dim, self.all_head_size)\n+        else:\n+            self.key_msa = None\n+            self.value_msa = None\n+\n+        self.attention_norm = EvollaRMSNorm(self.hidden_size)\n+\n+        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n+\n+        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=enable_bias)\n+\n+        self.ff = EvollaFeedForward(self.hidden_size, ffn_mult)\n+        self.gate_attention = nn.Parameter(torch.tensor([0.0]))\n+        self.gate_ffw = nn.Parameter(torch.tensor([0.0]))\n+\n+    def cross_attention(\n+        self,\n+        query_states,\n+        protein_key_value_states,\n+        structure_key_value_states,\n+        msa_key_value_states,\n+        query_attn_mask,\n+        protein_kv_attn_mask,\n+        structure_kv_attn_mask,\n+        msa_kv_attn_mask,\n+    ):\n+        \"\"\"\n+        query_states: text\n+        key_value_states: protein\n+        query_states: [bs, query_seq_len, dim]\n+        key_value_states: [bs, kv_seq_len, dim]\n+        query_attn_mask: [bs, query_seq_len]\n+        kv_attn_mask: [bs, kv_seq_len]\n+        \"\"\"\n+\n+        # Concatenate protein and structure\n+        kv_attn_mask = [protein_kv_attn_mask, structure_kv_attn_mask, msa_kv_attn_mask]\n+        kv_attn_mask = [_ for _ in kv_attn_mask if _ is not None]\n+        if not kv_attn_mask:\n+            raise ValueError(\"At least one modality should be provided for cross attention.\")\n+        kv_attn_mask = torch.cat(kv_attn_mask, dim=1)\n+\n+        query_layer = self.attention_norm(query_states)\n+\n+        # Warning: This place might cause issues, refers to\n+        # https://discuss.pytorch.org/t/cuda-error-cublas-status-not-supported-when-calling-cublasltmatmul-from-torch-nn-functional-linear/170214/13\n+        # Solution: add `DISABLE_ADDMM_CUDA_LT=1` as environment variable\n+        # Apply linear transformation to input_query, input_key, and input_value\n+        query_layer = self.query(query_layer)  # [bs, querylength, dim]\n+\n+        if self.key_protein is not None and self.value_protein is not None:\n+            protein_key_value_states = protein_key_value_states.to(query_states)\n+            key_layer_protein = self.key_protein(protein_key_value_states)  # [bs, keylength, dim]\n+            value_layer_protein = self.value_protein(protein_key_value_states)  # [bs, keylength, dim]\n+        else:\n+            key_layer_protein = None\n+            value_layer_protein = None\n+\n+        if self.key_structure is not None and self.value_structure is not None:\n+            structure_key_value_states = structure_key_value_states.to(query_states)\n+            key_layer_structure = self.key_structure(structure_key_value_states)  # [bs, keylength, dim]\n+            value_layer_structure = self.value_structure(structure_key_value_states)  # [bs, keylength, dim]\n+        else:\n+            key_layer_structure = None\n+            value_layer_structure = None\n+\n+        if self.key_msa is not None and self.value_msa is not None:\n+            msa_key_value_states = msa_key_value_states.to(query_states)\n+            key_layer_msa = self.key_msa(msa_key_value_states)  # [bs, keylength, dim]\n+            value_layer_msa = self.value_msa(msa_key_value_states)  # [bs, keylength, dim]\n+        else:\n+            key_layer_msa = None\n+            value_layer_msa = None\n+\n+        key_layer = [key_layer_protein, key_layer_structure, key_layer_msa]\n+        key_layer = [_ for _ in key_layer if _ is not None]\n+        key_layer = torch.cat(key_layer, dim=1)\n+\n+        value_layer = [value_layer_protein, value_layer_structure, value_layer_msa]\n+        value_layer = [_ for _ in value_layer if _ is not None]\n+        value_layer = torch.cat(value_layer, dim=1)\n+\n+        new_query_layer_shape = query_layer.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        query_layer = query_layer.view(*new_query_layer_shape).permute(0, 2, 1, 3)\n+\n+        new_key_layer_shape = key_layer.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        key_layer = key_layer.view(*new_key_layer_shape).permute(0, 2, 1, 3)\n+\n+        new_value_layer_shape = value_layer.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        value_layer = value_layer.view(*new_value_layer_shape).permute(0, 2, 1, 3)\n+\n+        query_layer = query_layer * self.scale\n+\n+        # attention_mask: [bs, 1, querylength, keylength]\n+        if query_attn_mask is None:\n+            query_attn_mask = torch.ones(query_states.size(0), query_states.size(1)).to(query_states.device)\n+        attention_mask = query_attn_mask[:, None, :, None] * kv_attn_mask[:, None, None, :]\n+        # Compute the scaled dot-product attention scores\n+        attn_weights = torch.matmul(query_layer, key_layer.transpose(-1, -2))  # [bs, numheads, querylength, keylength]\n+        attn_weights = attn_weights - attn_weights.amax(dim=-1, keepdim=True).detach()  # To stablize score\n+        attention_scores = attn_weights.masked_fill(\n+            (1 - attention_mask).bool(), torch.finfo(attn_weights.dtype).min\n+        )  # [bs, numheads, querylength, keylength]\n+\n+        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n+\n+        # attention_probs_dropped = self.dropout(attention_probs)\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)  # [bs, numheads, querylength, dim/numheads]\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(*new_context_layer_shape)\n+\n+        context_layer = self.out_proj(context_layer)\n+\n+        return context_layer\n+\n+    def forward(\n+        self,\n+        query_states,\n+        protein_kv_states,\n+        structure_kv_states,\n+        msa_kv_states,\n+        query_attn_mask,\n+        protein_kv_attn_mask=None,\n+        structure_kv_attn_mask=None,\n+        msa_kv_attn_mask=None,\n+        protein_batch_mask=None,\n+        structure_batch_mask=None,\n+        msa_batch_mask=None,\n+        past_key_value=None,\n+    ):\n+        if protein_kv_states is not None:\n+            bs, protein_kv_seq_len, dim = protein_kv_states.shape\n+            if protein_kv_attn_mask is None:\n+                protein_kv_attn_mask = (\n+                    torch.ones(bs, protein_kv_seq_len).to(protein_batch_mask.device)\n+                    * protein_batch_mask.expand(size=(protein_kv_seq_len, bs)).T\n+                ).to(protein_kv_states.device)\n+        else:\n+            protein_kv_attn_mask = None\n+\n+        if structure_kv_states is not None:\n+            bs, structure_kv_seq_len, dim = structure_kv_states.shape\n+            if structure_kv_attn_mask is None:\n+                structure_kv_attn_mask = (\n+                    torch.ones(bs, structure_kv_seq_len).to(protein_batch_mask.device)\n+                    * structure_batch_mask.expand(size=(structure_kv_seq_len, bs)).T\n+                ).to(structure_kv_states.device)\n+        else:\n+            structure_kv_attn_mask = None\n+\n+        if msa_kv_states is not None:\n+            bs, msa_kv_seq_len, dim = msa_kv_states.shape\n+            if msa_kv_attn_mask is None:\n+                msa_kv_attn_mask = (\n+                    torch.ones(bs, msa_kv_seq_len).to(protein_batch_mask.device)\n+                    * msa_batch_mask.expand(size=(msa_kv_seq_len, bs)).T\n+                ).to(msa_kv_states.device)\n+        else:\n+            msa_kv_attn_mask = None\n+        hidden_states = query_states\n+        # only when there's at least one valid modality, crossattention will be performed\n+        if (\n+            (protein_kv_states is not None and protein_kv_attn_mask.any())\n+            or (structure_kv_states is not None and structure_kv_attn_mask.any())\n+            or (msa_kv_states is not None and msa_kv_attn_mask.any())\n+        ):\n+            residual = hidden_states\n+            hidden_states = self.cross_attention(\n+                query_states=hidden_states,\n+                protein_key_value_states=protein_kv_states,\n+                structure_key_value_states=structure_kv_states,\n+                msa_key_value_states=msa_kv_states,\n+                query_attn_mask=query_attn_mask,\n+                protein_kv_attn_mask=protein_kv_attn_mask,\n+                structure_kv_attn_mask=structure_kv_attn_mask,\n+                msa_kv_attn_mask=msa_kv_attn_mask,\n+            )  # [bs, query_seq_len, dim]\n+            # tanh gate\n+            hidden_states = torch.tanh(self.gate_attention) * hidden_states\n+\n+            hidden_states = residual + hidden_states  # input_query\n+\n+            residual = hidden_states\n+            hidden_states = self.ff(hidden_states) * torch.tanh(self.gate_ffw)\n+            hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class EvollaRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        EvollaRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class EvollaRotaryEmbedding(nn.Module):\n+    def __init__(self, config: EvollaConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class EvollaMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class EvollaAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: EvollaConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class EvollaDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: EvollaConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = EvollaAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = EvollaMLP(config)\n+        self.input_layernorm = EvollaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = EvollaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        if (layer_idx + 1) % max(config.num_hidden_layers // config.aligner_num_add_layers, 1) == 0:\n+            self.adapter = EvollaSequenceAlignerCrossAttention(\n+                config,\n+                protein_encoder_dim=config.hidden_size,\n+            )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        protein_kv_states: Optional[torch.Tensor] = None,\n+        structure_kv_states: Optional[torch.Tensor] = None,\n+        msa_kv_states: Optional[torch.Tensor] = None,\n+        protein_batch_mask: Optional[torch.Tensor] = None,\n+        structure_batch_mask: Optional[torch.Tensor] = None,\n+        msa_batch_mask: Optional[torch.Tensor] = None,\n+        query_attn_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        if hasattr(self, \"adapter\"):\n+            hidden_states = self.adapter(\n+                query_states=hidden_states,\n+                protein_kv_states=protein_kv_states,\n+                structure_kv_states=structure_kv_states,\n+                msa_kv_states=msa_kv_states,\n+                query_attn_mask=query_attn_mask,\n+                protein_batch_mask=protein_batch_mask,\n+                structure_batch_mask=structure_batch_mask,\n+                msa_batch_mask=msa_batch_mask,\n+            )\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class EvollaPreTrainedModel(PreTrainedModel):\n+    config: EvollaConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"EvollaDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _supports_static_cache = True\n+    _supports_attention_backend = False\n+    _can_record_outputs = {\n+        \"hidden_states\": EvollaDecoderLayer,\n+        \"attentions\": EvollaAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, EvollaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, EvollaSequenceAlignerCrossAttention):\n+            module.gate_attention.zero_()\n+            module.gate_ffw.zero_()\n+            module.attention_norm.weight.data.fill_(1.0)\n+        elif isinstance(module, EvollaSequenceCompressorResampler):\n+            module.latents.data.normal_(mean=0.0, std=std)\n+\n+\n+class EvollaModel(EvollaPreTrainedModel):\n+    def __init__(self, config: EvollaConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+        self.embed_tokens = nn.Embedding(self.vocab_size, config.hidden_size, self.padding_idx)\n+        self.protein_encoder = EvollaProteinEncoder(config=config)\n+        self.layers = nn.ModuleList(\n+            [\n+                EvollaDecoderLayer(\n+                    config=config,\n+                    layer_idx=layer_idx,\n+                )\n+                for layer_idx in range(config.num_hidden_layers)\n+            ]\n+        )\n+\n+        self.norm = EvollaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = EvollaRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = getattr(config, \"gradient_checkpointing\", False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @auto_docstring\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        protein_input_ids: Optional[torch.LongTensor] = None,\n+        protein_attention_mask: Optional[torch.Tensor] = None,\n+        structure_feats: Optional[torch.FloatTensor] = None,\n+        msa_feats: Optional[torch.FloatTensor] = None,\n+        structure_batch_mask: Optional[torch.Tensor] = None,\n+        msa_batch_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        protein_input_ids (torch.LongTensor):\n+            The input IDs for the protein sequence in structure-aware tokens. Should be of shape `(batch_size, protein_seq_length)` and type `torch.LongTensor`.\n+        protein_attention_mask (torch.Tensor):\n+            The attention mask for the protein sequence. Should be of shape `(batch_size, protein_seq_length)` and type `torch.Tensor`.\n+        structure_feats (torch.FloatTensor):\n+            The input IDs for purely structure-based features. Should be of shape `(batch_size, structure_seq_length, structure_feat_dim)` and type `torch.FloatTensor`. Dummy input for now.\n+        msa_feats (torch.FloatTensor):\n+            The input IDs for purely MSA-based features. Should be of shape `(batch_size, msa_seq_length, msa_feat_dim)` and type `torch.FloatTensor`. Dummy input for now.\n+        structure_batch_mask (torch.Tensor):\n+            The batch mask to decide which protein sequences are purely structure-based. Should be of shape `(batch_size)` and type `torch.Tensor`. Should be paired with `structure_feats`. Dummpy input for now.\n+        msa_batch_mask (torch.Tensor):\n+            The batch mask to decide which protein sequences are purely MSA-based. Should be of shape `(batch_size)` and type `torch.Tensor`. Should be paired with `msa_feats`. Dummpy input for now.\n+        \"\"\"\n+        # If not provided `protein_feats`, use the `protein_encoder` to get the protein features\n+        if protein_input_ids is not None and protein_attention_mask is not None:\n+            protein_outputs = self.protein_encoder(\n+                input_ids=protein_input_ids,\n+                attention_mask=protein_attention_mask,\n+            )\n+            protein_feats = protein_outputs.sequence_compressor_output\n+            protein_batch_mask = torch.tensor([True] * protein_input_ids.shape[0], device=protein_input_ids.device)\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                protein_kv_states=protein_feats,\n+                structure_kv_states=structure_feats,\n+                msa_kv_states=msa_feats,\n+                protein_batch_mask=protein_batch_mask,\n+                structure_batch_mask=structure_batch_mask,\n+                msa_batch_mask=msa_batch_mask,\n+                query_attn_mask=attention_mask,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+        return output\n+\n+\n+class EvollaForProteinText2Text(EvollaPreTrainedModel, GenerationMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = EvollaModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, self.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        return self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,  # text input ids\n+        attention_mask: Optional[torch.Tensor] = None,  # text attention mask\n+        inputs_embeds: Optional[torch.FloatTensor] = None,  # text input embeddings\n+        labels: Optional[torch.LongTensor] = None,\n+        protein_input_ids: torch.LongTensor = None,\n+        protein_attention_mask: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        protein_input_ids (torch.LongTensor):\n+            The input IDs for the protein sequence. Should be of shape `(batch_size, protein_seq_length)` and type `torch.LongTensor`.\n+        protein_attention_mask (torch.Tensor):\n+            The attention mask for the protein sequence. Should be of shape `(batch_size, protein_seq_length)` and type `torch.Tensor`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import EvollaProcessor, EvollaForProteinText2Text\n+        >>> model = EvollaForProteinText2Text.from_pretrained(\"westlake/Evolla-10B-hf\")\n+        >>> processor = EvollaProcessor.from_pretrained(\"westlake/Evolla-10B-hf\")\n+\n+        >>> protein_information = {\n+            \"aa_seq\": \"your amino acid sequence\",\n+            \"foldseek\": \"your foldseek sequence\",\n+        }\n+        >>> question = \"What is the function of this protein?\"\n+        >>> message = [\n+            {\"role\": \"system\", \"content\": \"You are an AI expert that can answer any questions about protein.\"},\n+            {\"role\": \"user\", \"content\": question},\n+        ]\n+\n+        >>> inputs = processor(proteins=[protein_information], messages_list=[message], return_tensors=\"pt\", padding=\"longest\")\n+        >>> outputs = model.generate(**inputs)\n+\n+        >>> print(processor.batch_decode(outputs, skip_special_tokens=True))\n+        ```\"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            protein_input_ids=protein_input_ids,\n+            protein_attention_mask=protein_attention_mask,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        hidden_states = outputs[0]\n+        logits = self.lm_head(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.vocab_size, **kwargs)\n+\n+        lm_outputs = CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+        return lm_outputs\n+\n+\n+__all__ = [\"EvollaForProteinText2Text\", \"EvollaModel\", \"EvollaPreTrainedModel\"]"
        },
        {
            "sha": "30cf93b5c916325f0fada9c6e97528e48f5bb59f",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "added",
            "additions": 1008,
            "deletions": 0,
            "changes": 1008,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,1008 @@\n+# coding=utf-8\n+# Copyright 2025 Westlake Representational Learning Lab (Fajie Yuan Lab) team and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import warnings\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import Tensor, nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n+    CausalLMOutputWithPast,\n+    ModelOutput,\n+)\n+from ...modeling_utils import ModuleUtilsMixin, PreTrainedModel, get_parameter_dtype\n+from ...utils import (\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n+from ...utils.generic import check_model_inputs\n+from ..esm.modeling_esm import (\n+    EsmAttention,\n+    EsmEmbeddings,\n+    EsmEncoder,\n+    EsmIntermediate,\n+    EsmLayer,\n+    EsmOutput,\n+    EsmPooler,\n+    EsmSelfAttention,\n+    EsmSelfOutput,\n+)\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaMLP,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+)\n+from .configuration_evolla import EvollaConfig, SaProtConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class EvollaSaProtEmbeddings(EsmEmbeddings):\n+    def __init__(self, config):\n+        super().__init__()\n+        # remove the position_ids in EsmEmbeddings\n+        self.position_ids = None\n+\n+\n+def rotate_half_esm(x):\n+    x1, x2 = x.chunk(2, dim=-1)\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb_esm(x, cos, sin):\n+    cos = cos[:, :, : x.shape[-2], :]\n+    sin = sin[:, :, : x.shape[-2], :]\n+\n+    return (x * cos) + (rotate_half_esm(x) * sin)\n+\n+\n+class EvollaSaProtRotaryEmbedding(nn.Module):\n+    \"\"\"\n+    Rotary position embeddings based on those in\n+    [RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer). Query and keys are transformed by rotation\n+    matrices which depend on their relative positions.\n+    \"\"\"\n+\n+    def __init__(self, dim: int):\n+        super().__init__()\n+        # Generate and save the inverse frequency buffer (non trainable)\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n+        inv_freq = inv_freq\n+        self.register_buffer(\"inv_freq\", inv_freq)\n+\n+        self._seq_len_cached = None\n+        self._cos_cached = None\n+        self._sin_cached = None\n+\n+    def _update_cos_sin_tables(self, x, seq_dimension=2):\n+        seq_len = x.shape[seq_dimension]\n+\n+        # Reset the tables if the sequence length has changed,\n+        # or if we're on a new device (possibly due to tracing for instance)\n+        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n+            self._seq_len_cached = seq_len\n+            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq)\n+            freqs = torch.outer(t, self.inv_freq)\n+            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n+\n+            self._cos_cached = emb.cos()[None, None, :, :]\n+            self._sin_cached = emb.sin()[None, None, :, :]\n+\n+        return self._cos_cached, self._sin_cached\n+\n+    def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n+\n+        return (\n+            apply_rotary_pos_emb_esm(q, self._cos_cached, self._sin_cached),\n+            apply_rotary_pos_emb_esm(k, self._cos_cached, self._sin_cached),\n+        )\n+\n+\n+class EvollaSaProtSelfAttention(EsmSelfAttention, nn.Module):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        nn.Module.__init__()\n+        self.config = config\n+\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        self.rotary_embeddings = None\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n+        elif self.position_embedding_type == \"rotary\":\n+            self.rotary_embeddings = EvollaSaProtRotaryEmbedding(dim=self.attention_head_size)\n+\n+        self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n+\n+\n+class EvollaSaProtSelfOutput(EsmSelfOutput):\n+    pass\n+\n+\n+class EvollaSaProtAttention(EsmAttention):\n+    pass\n+\n+\n+class EvollaSaProtIntermediate(EsmIntermediate):\n+    pass\n+\n+\n+class EvollaSaProtOutput(EsmOutput):\n+    pass\n+\n+\n+class EvollaSaProtLayer(EsmLayer):\n+    pass\n+\n+\n+class EvollaSaProtEncoder(EsmEncoder):\n+    pass\n+\n+\n+class EvollaSaProtPooler(EsmPooler):\n+    pass\n+\n+\n+@auto_docstring\n+class EvollaSaProtPreTrainedModel(PreTrainedModel):\n+    config: SaProtConfig\n+    _no_split_modules = [\"EvollaSaProtLayer\"]\n+    _supports_flash_attn = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n+    def __init__(self, config: SaProtConfig):\n+        super().__init__(config)\n+        self.embeddings = EvollaSaProtEmbeddings(config)\n+        self.encoder = EvollaSaProtEncoder(config)\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.word_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.word_embeddings = value\n+\n+    def _prune_heads(self, heads_to_prune):\n+        \"\"\"\n+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n+        class PreTrainedModel\n+        \"\"\"\n+        for layer, heads in heads_to_prune.items():\n+            self.encoder.layer[layer].attention.prune_heads(heads)\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+        input_shape = input_ids.size()\n+        batch_size, seq_length = input_shape\n+\n+        device = input_ids.device\n+        if attention_mask is None:\n+            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n+\n+        inputs_embeds = self.embeddings(input_ids=input_ids, attention_mask=attention_mask)\n+        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        encoder_outputs = self.encoder(inputs_embeds, attention_mask=extended_attention_mask)\n+        sequence_output = encoder_outputs[0]\n+\n+        return BaseModelOutputWithPoolingAndCrossAttentions(\n+            last_hidden_state=sequence_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+            cross_attentions=encoder_outputs.cross_attentions,\n+        )\n+\n+    def get_extended_attention_mask(\n+        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n+    ) -> Tensor:\n+        \"\"\"\n+        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n+\n+        Arguments:\n+            attention_mask (`torch.Tensor`):\n+                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n+            input_shape (`Tuple[int]`):\n+                The shape of the input to the model.\n+\n+        Returns:\n+            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n+        \"\"\"\n+        if dtype is None:\n+            dtype = get_parameter_dtype(self)\n+\n+        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n+            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n+            if device is not None:\n+                warnings.warn(\n+                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n+                )\n+        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+        # ourselves in which case we just need to make it broadcastable to all heads.\n+        if attention_mask.dim() == 3:\n+            extended_attention_mask = attention_mask[:, None, :, :]\n+        elif attention_mask.dim() == 2:\n+            # Provided a padding mask of dimensions [batch_size, seq_length]\n+            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n+            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+            if self.config.is_decoder:\n+                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n+                    input_shape, attention_mask, device\n+                )\n+            else:\n+                extended_attention_mask = attention_mask[:, None, None, :]\n+        else:\n+            raise ValueError(\n+                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n+            )\n+\n+        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n+        # masked positions, this operation will create a tensor which is 0.0 for\n+        # positions we want to attend and the dtype's smallest value for masked positions.\n+        # Since we are adding it to the raw scores before the softmax, this is\n+        # effectively the same as removing these entirely.\n+        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n+        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n+        return extended_attention_mask\n+\n+\n+class EvollaSequenceCompressorAttention(nn.Module):\n+    def __init__(self, dim, dim_head=64, heads=8):\n+        super().__init__()\n+        self.scale = dim_head**-0.5\n+        self.heads = heads\n+        inner_dim = dim_head * heads\n+\n+        self.norm_media = nn.LayerNorm(dim)\n+        self.norm_latents = nn.LayerNorm(dim)\n+\n+        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n+        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n+        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n+\n+    def forward(self, x, latents, mask):\n+        \"\"\"\n+        Args:\n+            x (torch.Tensor): image features\n+                shape (b, n1, D)\n+            latent (torch.Tensor): latent features\n+                shape (b, n2, D);  n2: num of latent tokens\n+        \"\"\"\n+        x = self.norm_media(x)\n+        latents = self.norm_latents(latents)\n+\n+        h = self.heads\n+\n+        q = self.to_q(latents)\n+        kv_input = torch.cat((x, latents), dim=-2)\n+        k, v = self.to_kv(kv_input).chunk(\n+            2, dim=-1\n+        )  # each: batch_size, max_protein_length+num_latents, dim_head*num_heads\n+\n+        q = q.view(q.size(0), q.size(1), h, -1).permute(0, 2, 1, 3)\n+        k = k.view(k.size(0), k.size(1), h, -1).permute(0, 2, 1, 3)\n+        v = v.view(v.size(0), v.size(1), h, -1).permute(0, 2, 1, 3)\n+        q = q * self.scale  # batch_size, num_heads, num_latents, dim_head\n+\n+        # attention\n+        sim = torch.matmul(q, k.transpose(-1, -2))\n+        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n+        bs, nh, skd, okd = sim.shape\n+        ones = torch.ones(nh, skd).to(mask.device)  # Create a tensor of ones with shape (nh, skd)\n+        mask_exp = mask[:, None, None, :]\n+        ones_exp = ones[None, :, :, None]\n+        mask = mask_exp * ones_exp\n+\n+        sim = sim.masked_fill((1 - mask).bool(), -1e4)\n+        attn = sim.softmax(dim=-1)\n+        out = torch.matmul(attn, v)\n+        out = out.permute(0, 2, 1, 3)\n+\n+        # [batch, seq, head, features] -> [batch, seq, head*features]\n+        out = out.reshape(out.size(0), out.size(1), -1)\n+\n+        return self.to_out(out)\n+\n+\n+class EvollaFeedForward(nn.Module):\n+    def __init__(self, dim, mult=4):\n+        super().__init__()\n+        inner_dim = int(dim * mult)\n+\n+        self.norm = nn.LayerNorm(dim)\n+        self.fc1 = nn.Linear(dim, inner_dim, bias=False)\n+        self.activation = nn.GELU()\n+        self.fc2 = nn.Linear(inner_dim, dim, bias=False)\n+\n+    def forward(self, x):\n+        return self.fc2(self.activation(self.fc1(self.norm(x))))\n+\n+\n+class EvollaSequenceCompressorResampler(nn.Module):\n+    def __init__(self, config: EvollaConfig):\n+        super().__init__()\n+        protein_repr_dim = config.protein_encoder_config.hidden_size\n+        self.num_latents = config.resampler_num_latents\n+        self.latents = nn.Parameter(torch.randn(self.num_latents, protein_repr_dim), requires_grad=True)\n+        self.layers = nn.ModuleList([])\n+        for _ in range(config.resampler_depth):\n+            self.layers.append(\n+                nn.ModuleList(\n+                    [\n+                        EvollaSequenceCompressorAttention(\n+                            dim=protein_repr_dim, dim_head=config.resampler_dim_head, heads=config.resampler_heads\n+                        ),\n+                        EvollaFeedForward(dim=protein_repr_dim, mult=config.resampler_ff_mult),\n+                    ]\n+                )\n+            )\n+\n+        self.norm = nn.LayerNorm(config.hidden_size)\n+        self.protein_projector = nn.Linear(protein_repr_dim, config.hidden_size)\n+\n+    def forward(self, embeds, mask):\n+        b = embeds.shape[0]\n+\n+        bs, _ = mask.shape  # bs, max_protein_length\n+        latent_mask = torch.ones(bs, self.num_latents).to(mask.device)\n+        mask = torch.cat((mask, latent_mask), dim=1)  # bs, max_protein_length + num_latents\n+\n+        # blocks\n+        ones = torch.ones(b).to(self.latents.device)\n+        latents = self.latents[None] * ones.view(-1, 1, 1)  # [b,n,d]\n+        latents = latents.to(embeds.dtype)\n+        for attn, ff in self.layers:\n+            latents = attn(embeds, latents, mask) + latents\n+            latents = ff(latents) + latents\n+\n+        transformed_feature = self.protein_projector(latents)\n+\n+        return self.norm(transformed_feature)\n+\n+\n+@dataclass\n+@auto_docstring\n+class EvollaProteinEncoderModelOutput(ModelOutput):\n+    sequence_compressor_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class EvollaProteinEncoder(nn.Module):\n+    def __init__(self, config: EvollaConfig):\n+        super().__init__()\n+        self.model = EvollaSaProtProteinEncoder(config=config.protein_encoder_config)\n+        self.sequence_compressor_resampler = EvollaSequenceCompressorResampler(config=config)\n+\n+    @can_return_tuple\n+    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.FloatTensor, **kwargs):\n+        protein_output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n+        protein_embeds = protein_output.last_hidden_state\n+        sequence_repr = self.sequence_compressor_resampler(protein_embeds, attention_mask)\n+\n+        return EvollaProteinEncoderModelOutput(\n+            sequence_compressor_output=sequence_repr,\n+            last_hidden_state=protein_output.last_hidden_state,\n+        )\n+\n+\n+class EvollaSequenceAlignerCrossAttention(nn.Module):\n+    def __init__(\n+        self,\n+        config,\n+        protein_encoder_dim: Optional[int] = None,\n+        structure_encoder_dim: Optional[int] = None,\n+        msa_encoder_dim: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        self.hidden_size = config.hidden_size\n+        self.num_attention_heads = config.num_attention_heads\n+        self.scale = self.num_attention_heads**-0.5\n+        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        attention_probs_dropout_prob = config.aligner_attention_probs_dropout_prob\n+        enable_bias = config.aligner_enable_bias\n+        ffn_mult = config.aligner_ffn_mult\n+\n+        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n+        if protein_encoder_dim is not None:\n+            self.key_protein = nn.Linear(protein_encoder_dim, self.all_head_size)\n+            self.value_protein = nn.Linear(protein_encoder_dim, self.all_head_size)\n+        else:\n+            self.key_protein = None\n+            self.value_protein = None\n+\n+        if structure_encoder_dim is not None:\n+            self.key_structure = nn.Linear(structure_encoder_dim, self.all_head_size)\n+            self.value_structure = nn.Linear(structure_encoder_dim, self.all_head_size)\n+        else:\n+            self.key_structure = None\n+            self.value_structure = None\n+\n+        if msa_encoder_dim is not None:\n+            self.key_msa = nn.Linear(msa_encoder_dim, self.all_head_size)\n+            self.value_msa = nn.Linear(msa_encoder_dim, self.all_head_size)\n+        else:\n+            self.key_msa = None\n+            self.value_msa = None\n+\n+        self.attention_norm = EvollaRMSNorm(self.hidden_size)\n+\n+        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n+\n+        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=enable_bias)\n+\n+        self.ff = EvollaFeedForward(self.hidden_size, ffn_mult)\n+        self.gate_attention = nn.Parameter(torch.tensor([0.0]))\n+        self.gate_ffw = nn.Parameter(torch.tensor([0.0]))\n+\n+    def cross_attention(\n+        self,\n+        query_states,\n+        protein_key_value_states,\n+        structure_key_value_states,\n+        msa_key_value_states,\n+        query_attn_mask,\n+        protein_kv_attn_mask,\n+        structure_kv_attn_mask,\n+        msa_kv_attn_mask,\n+    ):\n+        \"\"\"\n+        query_states: text\n+        key_value_states: protein\n+        query_states: [bs, query_seq_len, dim]\n+        key_value_states: [bs, kv_seq_len, dim]\n+        query_attn_mask: [bs, query_seq_len]\n+        kv_attn_mask: [bs, kv_seq_len]\n+        \"\"\"\n+\n+        # Concatenate protein and structure\n+        kv_attn_mask = [protein_kv_attn_mask, structure_kv_attn_mask, msa_kv_attn_mask]\n+        kv_attn_mask = [_ for _ in kv_attn_mask if _ is not None]\n+        if not kv_attn_mask:\n+            raise ValueError(\"At least one modality should be provided for cross attention.\")\n+        kv_attn_mask = torch.cat(kv_attn_mask, dim=1)\n+\n+        query_layer = self.attention_norm(query_states)\n+\n+        # Warning: This place might cause issues, refers to\n+        # https://discuss.pytorch.org/t/cuda-error-cublas-status-not-supported-when-calling-cublasltmatmul-from-torch-nn-functional-linear/170214/13\n+        # Solution: add `DISABLE_ADDMM_CUDA_LT=1` as environment variable\n+        # Apply linear transformation to input_query, input_key, and input_value\n+        query_layer = self.query(query_layer)  # [bs, querylength, dim]\n+\n+        if self.key_protein is not None and self.value_protein is not None:\n+            protein_key_value_states = protein_key_value_states.to(query_states)\n+            key_layer_protein = self.key_protein(protein_key_value_states)  # [bs, keylength, dim]\n+            value_layer_protein = self.value_protein(protein_key_value_states)  # [bs, keylength, dim]\n+        else:\n+            key_layer_protein = None\n+            value_layer_protein = None\n+\n+        if self.key_structure is not None and self.value_structure is not None:\n+            structure_key_value_states = structure_key_value_states.to(query_states)\n+            key_layer_structure = self.key_structure(structure_key_value_states)  # [bs, keylength, dim]\n+            value_layer_structure = self.value_structure(structure_key_value_states)  # [bs, keylength, dim]\n+        else:\n+            key_layer_structure = None\n+            value_layer_structure = None\n+\n+        if self.key_msa is not None and self.value_msa is not None:\n+            msa_key_value_states = msa_key_value_states.to(query_states)\n+            key_layer_msa = self.key_msa(msa_key_value_states)  # [bs, keylength, dim]\n+            value_layer_msa = self.value_msa(msa_key_value_states)  # [bs, keylength, dim]\n+        else:\n+            key_layer_msa = None\n+            value_layer_msa = None\n+\n+        key_layer = [key_layer_protein, key_layer_structure, key_layer_msa]\n+        key_layer = [_ for _ in key_layer if _ is not None]\n+        key_layer = torch.cat(key_layer, dim=1)\n+\n+        value_layer = [value_layer_protein, value_layer_structure, value_layer_msa]\n+        value_layer = [_ for _ in value_layer if _ is not None]\n+        value_layer = torch.cat(value_layer, dim=1)\n+\n+        new_query_layer_shape = query_layer.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        query_layer = query_layer.view(*new_query_layer_shape).permute(0, 2, 1, 3)\n+\n+        new_key_layer_shape = key_layer.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        key_layer = key_layer.view(*new_key_layer_shape).permute(0, 2, 1, 3)\n+\n+        new_value_layer_shape = value_layer.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        value_layer = value_layer.view(*new_value_layer_shape).permute(0, 2, 1, 3)\n+\n+        query_layer = query_layer * self.scale\n+\n+        # attention_mask: [bs, 1, querylength, keylength]\n+        if query_attn_mask is None:\n+            query_attn_mask = torch.ones(query_states.size(0), query_states.size(1)).to(query_states.device)\n+        attention_mask = query_attn_mask[:, None, :, None] * kv_attn_mask[:, None, None, :]\n+        # Compute the scaled dot-product attention scores\n+        attn_weights = torch.matmul(query_layer, key_layer.transpose(-1, -2))  # [bs, numheads, querylength, keylength]\n+        attn_weights = attn_weights - attn_weights.amax(dim=-1, keepdim=True).detach()  # To stablize score\n+        attention_scores = attn_weights.masked_fill(\n+            (1 - attention_mask).bool(), torch.finfo(attn_weights.dtype).min\n+        )  # [bs, numheads, querylength, keylength]\n+\n+        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n+\n+        # attention_probs_dropped = self.dropout(attention_probs)\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)  # [bs, numheads, querylength, dim/numheads]\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(*new_context_layer_shape)\n+\n+        context_layer = self.out_proj(context_layer)\n+\n+        return context_layer\n+\n+    def forward(\n+        self,\n+        query_states,\n+        protein_kv_states,\n+        structure_kv_states,\n+        msa_kv_states,\n+        query_attn_mask,\n+        protein_kv_attn_mask=None,\n+        structure_kv_attn_mask=None,\n+        msa_kv_attn_mask=None,\n+        protein_batch_mask=None,\n+        structure_batch_mask=None,\n+        msa_batch_mask=None,\n+        past_key_value=None,\n+    ):\n+        if protein_kv_states is not None:\n+            bs, protein_kv_seq_len, dim = protein_kv_states.shape\n+            if protein_kv_attn_mask is None:\n+                protein_kv_attn_mask = (\n+                    torch.ones(bs, protein_kv_seq_len).to(protein_batch_mask.device)\n+                    * protein_batch_mask.expand(size=(protein_kv_seq_len, bs)).T\n+                ).to(protein_kv_states.device)\n+        else:\n+            protein_kv_attn_mask = None\n+\n+        if structure_kv_states is not None:\n+            bs, structure_kv_seq_len, dim = structure_kv_states.shape\n+            if structure_kv_attn_mask is None:\n+                structure_kv_attn_mask = (\n+                    torch.ones(bs, structure_kv_seq_len).to(protein_batch_mask.device)\n+                    * structure_batch_mask.expand(size=(structure_kv_seq_len, bs)).T\n+                ).to(structure_kv_states.device)\n+        else:\n+            structure_kv_attn_mask = None\n+\n+        if msa_kv_states is not None:\n+            bs, msa_kv_seq_len, dim = msa_kv_states.shape\n+            if msa_kv_attn_mask is None:\n+                msa_kv_attn_mask = (\n+                    torch.ones(bs, msa_kv_seq_len).to(protein_batch_mask.device)\n+                    * msa_batch_mask.expand(size=(msa_kv_seq_len, bs)).T\n+                ).to(msa_kv_states.device)\n+        else:\n+            msa_kv_attn_mask = None\n+        hidden_states = query_states\n+        # only when there's at least one valid modality, crossattention will be performed\n+        if (\n+            (protein_kv_states is not None and protein_kv_attn_mask.any())\n+            or (structure_kv_states is not None and structure_kv_attn_mask.any())\n+            or (msa_kv_states is not None and msa_kv_attn_mask.any())\n+        ):\n+            residual = hidden_states\n+            hidden_states = self.cross_attention(\n+                query_states=hidden_states,\n+                protein_key_value_states=protein_kv_states,\n+                structure_key_value_states=structure_kv_states,\n+                msa_key_value_states=msa_kv_states,\n+                query_attn_mask=query_attn_mask,\n+                protein_kv_attn_mask=protein_kv_attn_mask,\n+                structure_kv_attn_mask=structure_kv_attn_mask,\n+                msa_kv_attn_mask=msa_kv_attn_mask,\n+            )  # [bs, query_seq_len, dim]\n+            # tanh gate\n+            hidden_states = torch.tanh(self.gate_attention) * hidden_states\n+\n+            hidden_states = residual + hidden_states  # input_query\n+\n+            residual = hidden_states\n+            hidden_states = self.ff(hidden_states) * torch.tanh(self.gate_ffw)\n+            hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+class EvollaRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class EvollaRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class EvollaMLP(LlamaMLP):\n+    pass\n+\n+\n+class EvollaAttention(LlamaAttention):\n+    pass\n+\n+\n+class EvollaDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: EvollaConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        if (layer_idx + 1) % max(config.num_hidden_layers // config.aligner_num_add_layers, 1) == 0:\n+            self.adapter = EvollaSequenceAlignerCrossAttention(\n+                config,\n+                protein_encoder_dim=config.hidden_size,\n+            )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        protein_kv_states: Optional[torch.Tensor] = None,\n+        structure_kv_states: Optional[torch.Tensor] = None,\n+        msa_kv_states: Optional[torch.Tensor] = None,\n+        protein_batch_mask: Optional[torch.Tensor] = None,\n+        structure_batch_mask: Optional[torch.Tensor] = None,\n+        msa_batch_mask: Optional[torch.Tensor] = None,\n+        query_attn_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ):\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        if hasattr(self, \"adapter\"):\n+            hidden_states = self.adapter(\n+                query_states=hidden_states,\n+                protein_kv_states=protein_kv_states,\n+                structure_kv_states=structure_kv_states,\n+                msa_kv_states=msa_kv_states,\n+                query_attn_mask=query_attn_mask,\n+                protein_batch_mask=protein_batch_mask,\n+                structure_batch_mask=structure_batch_mask,\n+                msa_batch_mask=msa_batch_mask,\n+            )\n+\n+        return hidden_states\n+\n+\n+class EvollaPreTrainedModel(LlamaPreTrainedModel):\n+    _supports_attention_backend = False\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, EvollaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, EvollaSequenceAlignerCrossAttention):\n+            module.gate_attention.zero_()\n+            module.gate_ffw.zero_()\n+            module.attention_norm.weight.data.fill_(1.0)\n+        elif isinstance(module, EvollaSequenceCompressorResampler):\n+            module.latents.data.normal_(mean=0.0, std=std)\n+\n+\n+class EvollaModel(EvollaPreTrainedModel):\n+    def __init__(self, config: EvollaConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+        self.embed_tokens = nn.Embedding(self.vocab_size, config.hidden_size, self.padding_idx)\n+        self.protein_encoder = EvollaProteinEncoder(config=config)\n+        self.layers = nn.ModuleList(\n+            [\n+                EvollaDecoderLayer(\n+                    config=config,\n+                    layer_idx=layer_idx,\n+                )\n+                for layer_idx in range(config.num_hidden_layers)\n+            ]\n+        )\n+\n+        self.norm = EvollaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = EvollaRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = getattr(config, \"gradient_checkpointing\", False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @auto_docstring\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        protein_input_ids: Optional[torch.LongTensor] = None,\n+        protein_attention_mask: Optional[torch.Tensor] = None,\n+        structure_feats: Optional[torch.FloatTensor] = None,\n+        msa_feats: Optional[torch.FloatTensor] = None,\n+        structure_batch_mask: Optional[torch.Tensor] = None,\n+        msa_batch_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        protein_input_ids (torch.LongTensor):\n+            The input IDs for the protein sequence in structure-aware tokens. Should be of shape `(batch_size, protein_seq_length)` and type `torch.LongTensor`.\n+        protein_attention_mask (torch.Tensor):\n+            The attention mask for the protein sequence. Should be of shape `(batch_size, protein_seq_length)` and type `torch.Tensor`.\n+        structure_feats (torch.FloatTensor):\n+            The input IDs for purely structure-based features. Should be of shape `(batch_size, structure_seq_length, structure_feat_dim)` and type `torch.FloatTensor`. Dummy input for now.\n+        msa_feats (torch.FloatTensor):\n+            The input IDs for purely MSA-based features. Should be of shape `(batch_size, msa_seq_length, msa_feat_dim)` and type `torch.FloatTensor`. Dummy input for now.\n+        structure_batch_mask (torch.Tensor):\n+            The batch mask to decide which protein sequences are purely structure-based. Should be of shape `(batch_size)` and type `torch.Tensor`. Should be paired with `structure_feats`. Dummpy input for now.\n+        msa_batch_mask (torch.Tensor):\n+            The batch mask to decide which protein sequences are purely MSA-based. Should be of shape `(batch_size)` and type `torch.Tensor`. Should be paired with `msa_feats`. Dummpy input for now.\n+        \"\"\"\n+        # If not provided `protein_feats`, use the `protein_encoder` to get the protein features\n+        if protein_input_ids is not None and protein_attention_mask is not None:\n+            protein_outputs = self.protein_encoder(\n+                input_ids=protein_input_ids,\n+                attention_mask=protein_attention_mask,\n+            )\n+            protein_feats = protein_outputs.sequence_compressor_output\n+            protein_batch_mask = torch.tensor([True] * protein_input_ids.shape[0], device=protein_input_ids.device)\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                protein_kv_states=protein_feats,\n+                structure_kv_states=structure_feats,\n+                msa_kv_states=msa_feats,\n+                protein_batch_mask=protein_batch_mask,\n+                structure_batch_mask=structure_batch_mask,\n+                msa_batch_mask=msa_batch_mask,\n+                query_attn_mask=attention_mask,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+        return output\n+\n+\n+class EvollaForProteinText2Text(EvollaPreTrainedModel, GenerationMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = EvollaModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, self.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        return self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,  # text input ids\n+        attention_mask: Optional[torch.Tensor] = None,  # text attention mask\n+        inputs_embeds: Optional[torch.FloatTensor] = None,  # text input embeddings\n+        labels: Optional[torch.LongTensor] = None,\n+        protein_input_ids: torch.LongTensor = None,\n+        protein_attention_mask: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        protein_input_ids (torch.LongTensor):\n+            The input IDs for the protein sequence. Should be of shape `(batch_size, protein_seq_length)` and type `torch.LongTensor`.\n+        protein_attention_mask (torch.Tensor):\n+            The attention mask for the protein sequence. Should be of shape `(batch_size, protein_seq_length)` and type `torch.Tensor`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import EvollaProcessor, EvollaForProteinText2Text\n+        >>> model = EvollaForProteinText2Text.from_pretrained(\"westlake/Evolla-10B-hf\")\n+        >>> processor = EvollaProcessor.from_pretrained(\"westlake/Evolla-10B-hf\")\n+\n+        >>> protein_information = {\n+            \"aa_seq\": \"your amino acid sequence\",\n+            \"foldseek\": \"your foldseek sequence\",\n+        }\n+        >>> question = \"What is the function of this protein?\"\n+        >>> message = [\n+            {\"role\": \"system\", \"content\": \"You are an AI expert that can answer any questions about protein.\"},\n+            {\"role\": \"user\", \"content\": question},\n+        ]\n+\n+        >>> inputs = processor(proteins=[protein_information], messages_list=[message], return_tensors=\"pt\", padding=\"longest\")\n+        >>> outputs = model.generate(**inputs)\n+\n+        >>> print(processor.batch_decode(outputs, skip_special_tokens=True))\n+        ```\"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            protein_input_ids=protein_input_ids,\n+            protein_attention_mask=protein_attention_mask,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        hidden_states = outputs[0]\n+        logits = self.lm_head(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.vocab_size, **kwargs)\n+\n+        lm_outputs = CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+        return lm_outputs\n+\n+\n+__all__ = [\"EvollaForProteinText2Text\", \"EvollaModel\", \"EvollaPreTrainedModel\"]"
        },
        {
            "sha": "d44981bff51fab846edcf94a5c3bc8f57db31790",
            "filename": "src/transformers/models/evolla/processing_evolla.py",
            "status": "added",
            "additions": 247,
            "deletions": 0,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,247 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for EVOLLA.\n+\"\"\"\n+\n+import os\n+from typing import Optional, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import (\n+    ProcessorMixin,\n+)\n+from ..auto import AutoTokenizer\n+\n+\n+PROTEIN_VALID_KEYS = [\"aa_seq\", \"foldseek\", \"msa\"]\n+\n+\n+class EvollaProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a EVOLLA processor which wraps a LLama tokenizer and SaProt tokenizer (EsmTokenizer) into a single processor.\n+\n+    [`EvollaProcessor`] offers all the functionalities of [`EsmTokenizer`] and [`LlamaTokenizerFast`]. See the\n+    docstring of [`~EvollaProcessor.__call__`] and [`~EvollaProcessor.decode`] for more information.\n+\n+    Args:\n+        protein_tokenizer (`EsmTokenizer`):\n+            An instance of [`EsmTokenizer`]. The protein tokenizer is a required input.\n+        tokenizer (`LlamaTokenizerFast`, *optional*):\n+            An instance of [`LlamaTokenizerFast`]. The tokenizer is a required input.\n+        protein_max_length (`int`, *optional*, defaults to 1024):\n+            The maximum length of the sequence to be generated.\n+        text_max_length (`int`, *optional*, defaults to 512):\n+            The maximum length of the text to be generated.\n+    \"\"\"\n+\n+    attributes = [\"protein_tokenizer\", \"tokenizer\"]\n+    valid_kwargs = [\"sequence_max_length\"]\n+    # protein_tokenizer_class = \"EsmTokenizer\"\n+    # tokenizer_class = \"LlamaTokenizerFast\"\n+    protein_tokenizer_class = \"AutoTokenizer\"\n+    tokenizer_class = \"AutoTokenizer\"\n+    protein_tokenizer_dir_name = \"protein_tokenizer\"\n+    # tokenizer_dir_name = \"text_tokenizer\"\n+\n+    def __init__(self, protein_tokenizer, tokenizer=None, protein_max_length=1024, text_max_length=512, **kwargs):\n+        if protein_tokenizer is None:\n+            raise ValueError(\"You need to specify an `protein_tokenizer`.\")\n+        if tokenizer is None:\n+            raise ValueError(\"You need to specify a `tokenizer`.\")\n+\n+        super().__init__(protein_tokenizer, tokenizer)\n+\n+        self.tokenizer.pad_token = \"<|reserved_special_token_0|>\"\n+        self.protein_max_length = protein_max_length\n+        self.text_max_length = text_max_length\n+\n+    def process_proteins(self, proteins, protein_max_length=1024):\n+        sa_sequences = []\n+        for protein in proteins:\n+            aa_seq = protein.get(\"aa_seq\")\n+            foldseek = protein.get(\"foldseek\")\n+            sa_sequence = \"\".join([s.upper() + f.lower() for s, f in zip(aa_seq, foldseek)])\n+            sa_sequences.append(sa_sequence)\n+\n+        sa_tokens = self.protein_tokenizer.batch_encode_plus(\n+            sa_sequences, return_tensors=\"pt\", truncation=True, max_length=protein_max_length, padding=True\n+        )\n+        return sa_tokens\n+\n+    def process_text(\n+        self,\n+        texts,\n+        text_max_length: int = 512,\n+    ):\n+        prompts = []\n+        for messages in texts:\n+            prompt = self.tokenizer.apply_chat_template(\n+                messages,\n+                tokenize=False,\n+                add_generation_prompt=True,\n+            )\n+            prompts.append(prompt)\n+\n+        prompt_inputs = self.tokenizer(\n+            prompts,\n+            add_special_tokens=False,\n+            return_tensors=\"pt\",\n+            padding=\"longest\",\n+            truncation=True,\n+            max_length=text_max_length,\n+        )\n+        return prompt_inputs\n+\n+    def __call__(\n+        self,\n+        proteins: Optional[Union[list[dict], dict]] = None,\n+        messages_list: Optional[Union[list[list[dict]], list[dict]]] = None,\n+        protein_max_length: Optional[int] = None,\n+        text_max_length: Optional[int] = None,\n+        **kwargs,\n+    ):\n+        r\"\"\"This method takes batched or non-batched proteins and messages_list and converts them into format that can be used by\n+        the model.\n+\n+        Args:\n+            proteins (`Union[List[dict], dict]`):\n+                A list of dictionaries or a single dictionary containing the following keys:\n+                    - `\"aa_seq\"` (`str`) -- The amino acid sequence of the protein.\n+                    - `\"foldseek\"` (`str`) -- The foldseek string of the protein.\n+            messages_list (`Union[List[List[dict]], List[dict]]`):\n+                A list of lists of dictionaries or a list of dictionaries containing the following keys:\n+                    - `\"role\"` (`str`) -- The role of the message.\n+                    - `\"content\"` (`str`) -- The content of the message.\n+            protein_max_length (`int`, *optional*, defaults to 1024):\n+                The maximum length of the sequence to be generated.\n+            text_max_length (`int`, *optional*, defaults to 512):\n+                The maximum length of the text.\n+\n+        Return:\n+            a dict with following keys:\n+                - `protein_input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) -- The input IDs for the protein sequence.\n+                - `protein_attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`) -- The attention mask for the protein sequence.\n+                - `text_input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) -- The input IDs for the text sequence.\n+                - `text_attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`) -- The attention mask for the text sequence.\n+        \"\"\"\n+        # proteins and messages_list should be provided\n+        if proteins is None or messages_list is None:\n+            raise ValueError(\"You need to specify `messages_list` and `proteins`.\")\n+\n+        protein_max_length = protein_max_length if protein_max_length is not None else self.protein_max_length\n+        text_max_length = text_max_length if text_max_length is not None else self.text_max_length\n+\n+        # proteins should be List[dict]\n+        if isinstance(proteins, dict):\n+            proteins = [proteins]\n+        # messages_list should be List[List[dict]]\n+        if isinstance(messages_list, (list, tuple)) and not isinstance(messages_list[0], (list, tuple)):\n+            messages_list = [messages_list]\n+        # Check if batched proteins are in the correct format\n+        if isinstance(proteins, (list, tuple)) and not all(isinstance(p, dict) for p in proteins):\n+            raise ValueError(\"The proteins should be a list of dictionaries, but not all elements are dictionaries.\")\n+        if isinstance(proteins, (list, tuple)) and not all(\n+            all(k in PROTEIN_VALID_KEYS for k in p.keys()) for p in proteins\n+        ):\n+            raise ValueError(\n+                \"There should be a list of dictionaries with keys: \"\n+                f\"{', '.join(PROTEIN_VALID_KEYS)} for each protein.\"\n+                f\"But got: {proteins}\"\n+            )\n+        # Check if batched messages_list is in the correct format\n+        if isinstance(messages_list, (list, tuple)):\n+            for messages in messages_list:\n+                if not isinstance(messages, (list, tuple)):\n+                    raise ValueError(f\"Each messages in messages_list should be a list instead of {type(messages)}.\")\n+                if not all(isinstance(m, dict) for m in messages):\n+                    raise ValueError(\n+                        \"Each message in messages_list should be a list of dictionaries, but not all elements are dictionaries.\"\n+                    )\n+                if any(len(m.keys()) != 2 for m in messages) or any(\n+                    set(m.keys()) != {\"role\", \"content\"} for m in messages\n+                ):\n+                    raise ValueError(\n+                        \"Each message in messages_list should be a list of dictionaries with two keys: 'role' and 'content'.\"\n+                        f\"But got: {messages}\"\n+                    )\n+        else:\n+            raise ValueError(\n+                f\"The messages_list should be a list of lists of dictionaries, but it's {type(messages_list)}.\"\n+            )\n+        sa_tokens = self.process_proteins(proteins, protein_max_length)\n+\n+        text_tokens = self.process_text(messages_list, text_max_length)\n+\n+        return BatchFeature(\n+            data={\n+                \"protein_input_ids\": sa_tokens[\"input_ids\"],\n+                \"protein_attention_mask\": sa_tokens[\"attention_mask\"],\n+                \"input_ids\": text_tokens[\"input_ids\"],\n+                \"attention_mask\": text_tokens[\"attention_mask\"],\n+            }\n+        )\n+\n+    def batch_decode(self, *args, **kwargs):\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    def protein_batch_decode(self, *args, **kwargs):\n+        return self.protein_tokenizer.batch_decode(*args, **kwargs)\n+\n+    def protein_decode(self, *args, **kwargs):\n+        return self.protein_tokenizer.decode(*args, **kwargs)\n+\n+    # overwrite to save the protein tokenizer in a separate folder\n+    # Adapted from instructblip.processing_instructblip.py (https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src/transformers/models/instructblip/processing_instructblip.py#L191-L221)\n+    def save_pretrained(self, save_directory, **kwargs):\n+        # only save the protein tokenizer in sub_dir\n+        self.protein_tokenizer.save_pretrained(os.path.join(save_directory, self.protein_tokenizer_dir_name))\n+\n+        # we modify the attributes so that only the text tokenizer are saved in the main folder\n+        protein_tokenizer_present = \"protein_tokenizer\" in self.attributes\n+        # find the correct position of it in the attributes list\n+        protein_tokenizer_index = self.attributes.index(\"protein_tokenizer\") if protein_tokenizer_present else None\n+        if protein_tokenizer_present and protein_tokenizer_index is not None:\n+            self.attributes.remove(\"protein_tokenizer\")\n+\n+        outputs = super().save_pretrained(save_directory, **kwargs)\n+\n+        if protein_tokenizer_present and protein_tokenizer_index is not None:\n+            self.attributes.insert(protein_tokenizer_index, \"protein_tokenizer\")\n+\n+        return outputs\n+\n+    # overwirte to load the protein tokenizer from a separate folder\n+    # Adapted from instructblip.processing_instructblip.py (https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src/transformers/models/instructblip/processing_instructblip.py#L191-L221)\n+    @classmethod\n+    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n+        processor = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n+\n+        # if return_unused_kwargs a tuple is returned where the second element is 'unused_kwargs'\n+        if isinstance(processor, tuple):\n+            processor = processor[0]\n+        protein_tokenizer = AutoTokenizer.from_pretrained(\n+            pretrained_model_name_or_path, subfolder=cls.protein_tokenizer_dir_name\n+        )\n+\n+        processor.protein_tokenizer = protein_tokenizer\n+\n+        return processor\n+\n+\n+__all__ = [\"EvollaProcessor\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/evolla/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/tests%2Fmodels%2Fevolla%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/tests%2Fmodels%2Fevolla%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2F__init__.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677"
        },
        {
            "sha": "28643610773a266a1191ef08fe6caf4442ab3a3e",
            "filename": "tests/models/evolla/test_modeling_evolla.py",
            "status": "added",
            "additions": 397,
            "deletions": 0,
            "changes": 397,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,397 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Evolla model.\"\"\"\n+\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers import BitsAndBytesConfig, EvollaConfig, is_torch_available\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    require_bitsandbytes,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    cached_property,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    ids_tensor,\n+    random_attention_mask,\n+)\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import EvollaForProteinText2Text, EvollaModel, EvollaProcessor\n+\n+\n+class EvollaModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=1,\n+        is_training=False,\n+        text_seq_length=20,\n+        text_vocab_size=100,\n+        protein_seq_length=10,\n+        protein_vocab_size=20,\n+        hidden_size=4,  # llama hidden size\n+        intermediate_size=7,  # llama intermediate size\n+        num_hidden_layers=1,  # llama hidden layers\n+        num_attention_heads=2,  # llama attention heads\n+        num_key_value_heads=2,  # llama key value heads\n+        protein_hidden_size=8,  # protein encoder hidden size\n+        protein_num_hidden_layers=1,  # protein encoder hidden layers\n+        protein_num_attention_heads=4,  # protein encoder attention heads\n+        protein_intermediate_size=11,  # protein encoder intermediate size\n+        resampler_num_latents=7,  # sequence compressor num latents\n+        resampler_ff_mult=1,  # sequence compressor ff mult\n+        resampler_depth=2,  # sequence compressor depth\n+        resampler_dim_head=4,  # sequence compressor dim head\n+        resampler_heads=2,  # sequence compressor heads\n+        aligner_num_add_layers=1,  # sequence aligner num add layers\n+        aligner_ffn_mult=1,  # sequence aligner ffn mult\n+        use_input_mask=True,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.protein_seq_length = protein_seq_length\n+        self.protein_vocab_size = protein_vocab_size\n+        self.text_seq_length = text_seq_length\n+        self.text_vocab_size = text_vocab_size\n+        self.seq_length = text_seq_length\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.protein_hidden_size = protein_hidden_size\n+        self.protein_num_hidden_layers = protein_num_hidden_layers\n+        self.protein_num_attention_heads = protein_num_attention_heads\n+        self.protein_intermediate_size = protein_intermediate_size\n+\n+        self.resampler_num_latents = resampler_num_latents\n+        self.resampler_ff_mult = resampler_ff_mult\n+        self.resampler_depth = resampler_depth\n+        self.resampler_dim_head = resampler_dim_head\n+        self.resampler_heads = resampler_heads\n+\n+        self.aligner_num_add_layers = aligner_num_add_layers\n+        self.aligner_ffn_mult = aligner_ffn_mult\n+\n+        self.use_input_mask = use_input_mask\n+        self.is_training = is_training\n+\n+    @property\n+    def is_encoder_decoder(self):\n+        return False\n+\n+    def prepare_config_and_inputs(self, num_proteins=None):\n+        batch_size = num_proteins if num_proteins is not None else self.batch_size\n+        text_input_ids = ids_tensor([batch_size, self.text_seq_length], self.text_vocab_size)\n+\n+        protein_input_ids = ids_tensor([batch_size, self.protein_seq_length], self.protein_vocab_size)\n+\n+        if self.use_input_mask:\n+            text_input_mask = random_attention_mask([batch_size, self.text_seq_length])\n+            protein_input_mask = random_attention_mask([batch_size, self.protein_seq_length])\n+\n+        config = self.get_config()\n+        return (config, text_input_ids, text_input_mask, protein_input_ids, protein_input_mask)\n+\n+    def get_config(self):\n+        return EvollaConfig(\n+            protein_encoder_config={\n+                \"vocab_size\": self.protein_vocab_size,\n+                \"hidden_size\": self.protein_hidden_size,\n+                \"num_hidden_layers\": self.protein_num_hidden_layers,\n+                \"num_attention_heads\": self.protein_num_attention_heads,\n+                \"intermediate_size\": self.protein_intermediate_size,\n+            },\n+            vocab_size=self.text_vocab_size,\n+            hidden_size=self.hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            aligner_ffn_mult=self.aligner_ffn_mult,\n+            aligner_num_add_layers=self.aligner_num_add_layers,\n+            resampler_depth=self.resampler_depth,\n+            resampler_dim_head=self.resampler_dim_head,\n+            resampler_heads=self.resampler_heads,\n+            resampler_num_latents=self.resampler_num_latents,\n+            resampler_ff_mult=self.resampler_ff_mult,\n+        )\n+\n+    def create_and_check_model(\n+        self,\n+        config,\n+        input_ids,\n+        input_mask,\n+        protein_input_ids,\n+        protein_input_mask,\n+        batch_size=None,\n+    ):\n+        batch_size = batch_size if batch_size is not None else self.batch_size\n+        model = EvollaModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            protein_input_ids=protein_input_ids,\n+            protein_attention_mask=protein_input_mask,\n+        )\n+        self.parent.assertEqual(result.last_hidden_state.shape, (batch_size, input_ids.shape[1], self.hidden_size))\n+\n+    def create_and_check_model_gen(\n+        self,\n+        config,\n+        input_ids,\n+        input_mask,\n+        protein_input_ids,\n+        protein_input_mask,\n+    ):\n+        model = EvollaForProteinText2Text(config)\n+        model.to(torch_device)\n+        model.eval()\n+        model.generate(\n+            input_ids,\n+            attention_mask=input_mask,\n+            protein_input_ids=protein_input_ids,\n+            protein_attention_mask=protein_input_mask,\n+            max_length=self.seq_length + 2,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (config, text_input_ids, text_input_mask, protein_input_ids, protein_input_mask) = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": text_input_ids,\n+            \"attention_mask\": text_input_mask,\n+            \"protein_input_ids\": protein_input_ids,\n+            \"protein_attention_mask\": protein_input_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class EvollaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (EvollaModel, EvollaForProteinText2Text) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"feature-extraction\": EvollaModel} if is_torch_available() else {}\n+    test_pruning = False\n+    test_headmasking = False\n+    test_torchscript = False\n+    test_resize_embeddings = False\n+    maxDiff = None\n+\n+    def setUp(self):\n+        self.model_tester = EvollaModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=EvollaConfig, hidden_size=37)\n+\n+    @property\n+    def is_encoder_decoder(self):\n+        return self.model_tester.is_encoder_decoder\n+\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+        # XXX: EvollaForProteinText2Text has no MODEL_FOR group yet, but it should be the same\n+        # as MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, so for now manually changing to do the right thing\n+        # as super won't do it\n+        if return_labels:\n+            inputs_dict[\"labels\"] = torch.zeros(\n+                (self.model_tester.batch_size, self.model_tester.seq_length), dtype=torch.long, device=torch_device\n+            )\n+\n+        return inputs_dict\n+\n+    def test_model_outputs_equivalence(self):\n+        try:\n+            orig = self.all_model_classes\n+            # EvollaModel.forward doesn't have labels input arg - only EvollaForProteinText2Text does\n+            self.all_model_classes = (EvollaForProteinText2Text,) if is_torch_available() else ()\n+            super().test_model_outputs_equivalence()\n+        finally:\n+            self.all_model_classes = orig\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model_single_protein(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(num_proteins=1)\n+        self.model_tester.create_and_check_model(*config_and_inputs, batch_size=1)\n+\n+    def test_model_multiple_proteins(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(num_proteins=2)\n+        self.model_tester.create_and_check_model(*config_and_inputs, batch_size=2)\n+\n+    def test_generate_single_protein(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(num_proteins=1)\n+        self.model_tester.create_and_check_model_gen(*config_and_inputs)\n+\n+    def test_generate_multiple_proteins(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(num_proteins=2)\n+        self.model_tester.create_and_check_model_gen(*config_and_inputs)\n+\n+    def test_saprot_output(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        protein_informations = {\n+            \"input_ids\": inputs_dict[\"protein_input_ids\"],\n+            \"attention_mask\": inputs_dict[\"protein_attention_mask\"],\n+        }\n+        for model_class in self.all_model_classes:\n+            if model_class is not EvollaModel:\n+                continue\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            protein_encoder_outputs = model.protein_encoder.model(**protein_informations, return_dict=True)\n+            print(model_class, protein_encoder_outputs)\n+\n+    def test_protein_encoder_output(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        protein_informations = {\n+            \"input_ids\": inputs_dict[\"protein_input_ids\"],\n+            \"attention_mask\": inputs_dict[\"protein_attention_mask\"],\n+        }\n+        for model_class in self.all_model_classes:\n+            if model_class is not EvollaModel:\n+                continue\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            protein_encoder_outputs = model.protein_encoder(**protein_informations, return_dict=True)\n+            print(model_class, protein_encoder_outputs)\n+\n+    def test_single_forward(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            print(outputs)\n+\n+    def test_initialization(self):\n+        # we skip the latents initialization test\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    # skip latents\n+                    if name.endswith(\"latents\"):\n+                        print(f\"Skipping latents {name}\")\n+                        continue\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @unittest.skip(\"Evolla requires both text and protein inputs which is currently not done in this test.\")\n+    def test_eager_matches_sdpa_inference(self):\n+        pass\n+\n+    @unittest.skip(\"Evolla does not support eager attention implementation.\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Evolla has a separate test runner for generation tests with complex inheritance, causing this check to fail.\"\n+    )\n+    def test_generation_tester_mixin_inheritance(self):\n+        pass\n+\n+    @unittest.skip(\"Evolla requires both text and protein inputs which is currently not done in this test.\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n+\n+@require_torch\n+class EvollaModelIntegrationTest(TestCasePlus):\n+    def _prepare_for_inputs(self):\n+        aa_seq = \"MLLEETLKSCPIVKRGKYHYFIHPISDGVPLVEPKLLREVATRIIKIGNFEGVNKIVTAEAMGIPLVTTLSLYTDIPYVIMRKREYKLPGEVPVFQSTGYSKGQLYLNGIEKGDKVIIIDDVISTGGTMIAIINALERAGAEIKDIICVIERGDGKKIVEEKTGYKIKTLVKIDVVDGEVVIL\"\n+        foldseek = \"dvvvvqqqpfawdddppdtdgcgclapvpdpddpvvlvvllvlcvvpadpvqaqeeeeeddscpsnvvsncvvpvhyydywylddppdppkdwqwf######gitidpdqaaaheyeyeeaeqdqlrvvlsvvvrcvvrnyhhrayeyaeyhycnqvvccvvpvghyhynwywdqdpsgidtd\"\n+        question = \"What is the function of this protein?\"\n+\n+        protein_information = {\n+            \"aa_seq\": aa_seq,\n+            \"foldseek\": foldseek,\n+        }\n+        messages = [\n+            {\"role\": \"system\", \"content\": \"You are an AI expert that can answer any questions about protein.\"},\n+            {\"role\": \"user\", \"content\": question},\n+        ]\n+        return protein_information, messages\n+\n+    @cached_property\n+    def default_processor(self):\n+        return EvollaProcessor.from_pretrained(\"westlake-repl/Evolla-10B-hf\", revision=\"refs/pr/11\")\n+\n+    @require_bitsandbytes\n+    @slow\n+    def test_inference_natural_language_protein_reasoning(self):\n+        protein_information, messages = self._prepare_for_inputs()\n+        processor = self.default_processor\n+        inputs = processor(\n+            messages_list=[messages], proteins=[protein_information], return_tensors=\"pt\", padding=\"longest\"\n+        ).to(torch_device)\n+\n+        # the CI gpu is small so using quantization to fit\n+        quantization_config = BitsAndBytesConfig(\n+            load_in_4bit=True,\n+            bnb_4bit_compute_dtype=\"float16\",\n+        )\n+        model = EvollaForProteinText2Text.from_pretrained(\n+            \"westlake-repl/Evolla-10B-hf\",\n+            quantization_config=quantization_config,\n+            device_map=\"auto\",\n+        )\n+        generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        # keep for debugging\n+        for i, t in enumerate(generated_text):\n+            t = bytes(t, \"utf-8\").decode(\"unicode_escape\")\n+            print(f\"{i}:\\n{t}\\n\")\n+\n+        self.assertIn(\"This protein\", generated_text[0])\n+\n+        self.assertIn(\"purine\", generated_text[0])"
        },
        {
            "sha": "0a1f1f3cd2d507043f36f05822a598cb0693b79e",
            "filename": "tests/models/evolla/test_processor_evolla.py",
            "status": "added",
            "additions": 295,
            "deletions": 0,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/tests%2Fmodels%2Fevolla%2Ftest_processor_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/tests%2Fmodels%2Fevolla%2Ftest_processor_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_processor_evolla.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -0,0 +1,295 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import random\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    EvollaProcessor,\n+)\n+from transformers.testing_utils import require_torch\n+from transformers.utils import is_torch_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+EVOLLA_VALID_AA = list(\"ACDEFGHIKLMNPQRSTVWY#\")\n+EVOLLA_VALID_FS = list(\"pynwrqhgdlvtmfsaeikc#\")\n+\n+\n+@require_torch\n+class EvollaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = EvollaProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        processor = EvollaProcessor.from_pretrained(\"westlake-repl/Evolla-10B-hf\")\n+\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        self.input_keys = [\"protein_input_ids\", \"protein_attention_mask\", \"input_ids\", \"attention_mask\"]\n+\n+    def prepare_input_and_expected_output(self):\n+        amino_acid_sequence = \"AAAA\"\n+        foldseek_sequence = \"dddd\"\n+        question = \"What is the function of this protein?\"\n+\n+        expected_output = {\n+            \"protein_input_ids\": torch.tensor([[0, 13, 13, 13, 13, 2]]),\n+            \"protein_attention_mask\": torch.tensor([[1, 1, 1, 1, 1, 1]]),\n+            \"input_ids\": torch.tensor(\n+                [\n+                    [\n+                        128000,\n+                        128006,\n+                        9125,\n+                        128007,\n+                        271,\n+                        2675,\n+                        527,\n+                        459,\n+                        15592,\n+                        6335,\n+                        430,\n+                        649,\n+                        4320,\n+                        904,\n+                        4860,\n+                        922,\n+                        13128,\n+                        13,\n+                        128009,\n+                        128006,\n+                        882,\n+                        128007,\n+                        271,\n+                        3923,\n+                        374,\n+                        279,\n+                        734,\n+                        315,\n+                        420,\n+                        13128,\n+                        30,\n+                        128009,\n+                        128006,\n+                        78191,\n+                        128007,\n+                        271,\n+                    ]\n+                ]\n+            ),\n+            \"attention_mask\": torch.tensor(\n+                [\n+                    [\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                        1,\n+                    ]\n+                ]\n+            ),\n+        }\n+        protein_dict = {\"aa_seq\": amino_acid_sequence, \"foldseek\": foldseek_sequence}\n+        message = [\n+            {\"role\": \"system\", \"content\": \"You are an AI expert that can answer any questions about protein.\"},\n+            {\"role\": \"user\", \"content\": question},\n+        ]\n+        return protein_dict, message, expected_output\n+\n+    def test_processor(self):\n+        protein_tokenizer = self.get_protein_tokenizer()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = EvollaProcessor(protein_tokenizer, tokenizer)\n+\n+        protein_dict, message, expected_output = self.prepare_input_and_expected_output()\n+        inputs = processor(proteins=[protein_dict], messages_list=[message])\n+\n+        # check if the input is correct\n+        for key, value in expected_output.items():\n+            self.assertTrue(\n+                torch.equal(inputs[key], value),\n+                f\"inputs[key] is {inputs[key]} and expected_output[key] is {expected_output[key]}\",\n+            )\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_protein_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).protein_tokenizer\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_inputs_single(self):\n+        proteins = {\n+            \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=100)),\n+            \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=100)),\n+        }\n+        return proteins\n+\n+    def prepare_inputs_pair(self):\n+        proteins = [\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=100)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=100)),\n+            },\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=100)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=100)),\n+            },\n+        ]\n+        return proteins\n+\n+    def prepare_inputs_long(self):\n+        proteins = [\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=100)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=100)),\n+            },\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=2000)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=2000)),\n+            },\n+        ]\n+        return proteins\n+\n+    def prepare_inputs_short(self):\n+        proteins = [\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=1)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=1)),\n+            },\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=100)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=100)),\n+            },\n+        ]\n+        return proteins\n+\n+    def prepare_inputs_empty(self):\n+        proteins = [\n+            {\n+                \"aa_seq\": \"\",\n+                \"foldseek\": \"\",\n+            },\n+            {\n+                \"aa_seq\": \"\".join(random.choices(EVOLLA_VALID_AA, k=100)),\n+                \"foldseek\": \"\".join(random.choices(EVOLLA_VALID_FS, k=100)),\n+            },\n+        ]\n+        return proteins\n+\n+    def prepare_inputs(self, protein_types=\"pair\"):\n+        r\"\"\"\n+        Prepare inputs for the test.\n+\n+        Args:\n+            protein_types (`str`): the types of proteins to prepare.\n+                - \"single\": a single correct protein.\n+                - \"pair\": a pair of correct proteins.\n+                - \"long\": a long sequence of correct proteins and a correct protein.\n+                - \"short\": a short sequence of correct proteins (only have 1 aa) and a correct protein.\n+                - \"empty\": an empty sequence of proteins and a correct protein.\n+        \"\"\"\n+        if protein_types == \"single\":\n+            proteins = self.prepare_inputs_single()\n+        elif protein_types == \"pair\":\n+            proteins = self.prepare_inputs_pair()\n+        elif protein_types == \"long\":\n+            proteins = self.prepare_inputs_long()\n+        elif protein_types == \"short\":\n+            proteins = self.prepare_inputs_short()\n+        elif protein_types == \"empty\":\n+            proteins = self.prepare_inputs_empty()\n+        else:\n+            raise ValueError(\n+                f\"protein_types should be one of 'single', 'pair', 'long','short', 'empty', but got {protein_types}\"\n+            )\n+\n+        questions = [\"What is the function of the protein?\"] * len(proteins)\n+        messages_list = []\n+        for question in questions:\n+            messages = [\n+                {\"role\": \"system\", \"content\": \"You are an AI expert that can answer any questions about protein.\"},\n+                {\"role\": \"user\", \"content\": question},\n+            ]\n+            messages_list.append(messages)\n+        return proteins, messages_list\n+\n+    def test_tokenizer_decode(self):\n+        protein_tokenizer = self.get_protein_tokenizer()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = EvollaProcessor(tokenizer=tokenizer, protein_tokenizer=protein_tokenizer, return_tensors=\"pt\")\n+\n+        predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n+\n+        decoded_processor = processor.batch_decode(predicted_ids)\n+        decoded_tok = tokenizer.batch_decode(predicted_ids)\n+\n+        self.assertListEqual(decoded_tok, decoded_processor)\n+\n+    def test_model_input_names(self):\n+        protein_tokenizer = self.get_protein_tokenizer()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = EvollaProcessor(tokenizer=tokenizer, protein_tokenizer=protein_tokenizer)\n+        proteins, messages_list = self.prepare_inputs()\n+\n+        inputs = processor(messages_list=messages_list, proteins=proteins, padding=\"longest\", return_tensors=\"pt\")\n+\n+        # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n+        self.assertSetEqual(set(inputs.keys()), set(self.input_keys))"
        },
        {
            "sha": "01ed84939f5b69afe09eb74043e881147b0924e3",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/45c7bfb1571160d2c06b880073a5c73e6bfa3677/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45c7bfb1571160d2c06b880073a5c73e6bfa3677/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=45c7bfb1571160d2c06b880073a5c73e6bfa3677",
            "patch": "@@ -92,6 +92,7 @@\n     \"Phi4MultimodalAudioModel\",\n     \"Phi4MultimodalVisionModel\",\n     \"Glm4vVisionModel\",\n+    \"EvollaSaProtPreTrainedModel\",\n ]\n \n # Update this list for models that are not tested with a comment explaining the reason it should not be."
        }
    ],
    "stats": {
        "total": 4120,
        "additions": 4120,
        "deletions": 0
    }
}