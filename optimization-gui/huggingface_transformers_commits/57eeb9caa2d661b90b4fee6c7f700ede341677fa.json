{
    "author": "zucchini-nlp",
    "message": "Fix Qwen-VL family with prompt tuning (#42508)\n\n* fix all qwen models with promp tuning\n\n* forgot to rename\n\n* fix style\n\n* fallback better when no cache position\n\n* just why?",
    "sha": "57eeb9caa2d661b90b4fee6c7f700ede341677fa",
    "files": [
        {
            "sha": "f53dd900dbfe484b715b1ba244dda85909a0d735",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -1958,11 +1958,8 @@ def forward(\n             audio_feature_lengths = None\n \n         if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n@@ -1977,7 +1974,7 @@ def forward(\n                 self.rope_deltas = rope_deltas\n             else:\n                 batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)\n@@ -2366,11 +2363,8 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_text_ids,\n                     image_grid_thw,\n@@ -2390,8 +2384,8 @@ def forward(\n                 self.rope_deltas = rope_deltas\n \n             else:\n-                batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                batch_size, seq_length, _ = inputs_embeds.shape\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)"
        },
        {
            "sha": "b1198be510cc886d7029b07d548f21b0376863d0",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -2306,11 +2306,8 @@ def forward(\n             audio_feature_lengths = None\n \n         if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n@@ -2325,7 +2322,7 @@ def forward(\n                 self.rope_deltas = rope_deltas\n             else:\n                 batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)\n@@ -2567,11 +2564,8 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_text_ids,\n                     image_grid_thw,\n@@ -2591,8 +2585,8 @@ def forward(\n                 self.rope_deltas = rope_deltas\n \n             else:\n-                batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                batch_size, seq_length, _ = inputs_embeds.shape\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)"
        },
        {
            "sha": "ab377b745f5e0c282d04932442957ef672ee2e73",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -1290,7 +1290,8 @@ def forward(\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            if self.rope_deltas is None or cache_position is None or cache_position[0] == 0:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if self.rope_deltas is None or past_key_values_length == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n@@ -1303,10 +1304,7 @@ def forward(\n                 batch_size, seq_length, _ = inputs_embeds.shape\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n                 position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n-                if cache_position is not None:\n-                    delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                else:\n-                    delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)\n+                delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)\n                 delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=1)\n                 position_ids = position_ids + delta.to(position_ids.device)\n "
        },
        {
            "sha": "74458be7e04ec8530f827ef768454f45af6dc01b",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -595,7 +595,8 @@ def forward(\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            if self.rope_deltas is None or cache_position is None or cache_position[0] == 0:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if self.rope_deltas is None or past_key_values_length == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n@@ -608,10 +609,7 @@ def forward(\n                 batch_size, seq_length, _ = inputs_embeds.shape\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n                 position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n-                if cache_position is not None:\n-                    delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                else:\n-                    delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)\n+                delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)\n                 delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=1)\n                 position_ids = position_ids + delta.to(position_ids.device)\n "
        },
        {
            "sha": "335439a1d3a1c9f448657db8d2a2f448ac85e74f",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -42,7 +42,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from ..qwen2.modeling_qwen2 import (\n@@ -1222,7 +1221,8 @@ def forward(\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            if self.rope_deltas is None or cache_position is None or cache_position[0] == 0:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if self.rope_deltas is None or past_key_values_length == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids, image_grid_thw, video_grid_thw, attention_mask\n                 )\n@@ -1232,10 +1232,7 @@ def forward(\n                 batch_size, seq_length, _ = inputs_embeds.shape\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n                 position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n-                if cache_position is not None:\n-                    delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                else:\n-                    delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)\n+                delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)\n                 delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n                 position_ids = position_ids + delta.to(position_ids.device)\n \n@@ -1443,15 +1440,7 @@ def prepare_inputs_for_generation(\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n             # models currently cannot do asssisted decoding\n-            prefill_compiled_stage = is_torchdynamo_compiling() and (\n-                (input_ids is not None and input_ids.shape[1] != 1)\n-                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n-            )\n-            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n-                (cache_position is not None and cache_position[0] == 0)\n-                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            )\n-            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.model.rope_deltas is None:\n+            if model_inputs[\"cache_position\"][0] == 0 or self.model.rope_deltas is None:\n                 vision_positions, rope_deltas = self.model.get_rope_index(\n                     model_inputs.get(\"input_ids\", None),\n                     image_grid_thw=image_grid_thw,"
        },
        {
            "sha": "35cc4215c3aeba71991658490e9017e201e308a0",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -2165,11 +2165,8 @@ def forward(\n             audio_feature_lengths = None\n \n         if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n@@ -2184,7 +2181,7 @@ def forward(\n                 self.rope_deltas = rope_deltas\n             else:\n                 batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)\n@@ -3103,12 +3100,9 @@ def forward(\n         if inputs_embeds is not None and inputs_embeds.shape[1] > 1:\n             generation_step = -1\n             residual_codes = None\n-        if attention_mask is not None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+        if position_ids is None:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n                 position_ids, rope_deltas = self.get_rope_index(\n                     talker_input_ids,\n@@ -3123,7 +3117,7 @@ def forward(\n                 self.rope_deltas = rope_deltas\n             else:\n                 batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)\n@@ -3224,7 +3218,10 @@ def prepare_inputs_for_generation(\n         inputs = super().prepare_inputs_for_generation(\n             input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs\n         )\n-        # Decode stage\n+\n+        # Qwen3-Omni will prepare position ids in forward with deltas\n+        inputs[\"position_ids\"] = None\n+\n         # TODO(raushan, gante): Refactor this part to a utility function\n         if cache_position[0] != 0:\n             input_ids = input_ids[:, -1:]"
        },
        {
            "sha": "0a96f2d2625a50d1a82dc3680f4c8c813df4c1c0",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -1521,11 +1521,8 @@ def forward(\n             audio_feature_lengths = None\n \n         if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n@@ -1540,7 +1537,7 @@ def forward(\n                 self.rope_deltas = rope_deltas\n             else:\n                 batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)\n@@ -1961,12 +1958,9 @@ def forward(\n         if inputs_embeds is not None and inputs_embeds.shape[1] > 1:\n             generation_step = -1\n             residual_codes = None\n-        if attention_mask is not None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n+        if position_ids is None:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if past_key_values_length == 0 or self.rope_deltas is None:\n                 delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n                 position_ids, rope_deltas = self.get_rope_index(\n                     talker_input_ids,\n@@ -1981,7 +1975,7 @@ def forward(\n                 self.rope_deltas = rope_deltas\n             else:\n                 batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                delta = (past_key_values_length + self.rope_deltas).to(input_ids.device)\n                 position_ids = torch.arange(seq_length, device=input_ids.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 position_ids = position_ids.add(delta)\n@@ -2044,7 +2038,10 @@ def prepare_inputs_for_generation(\n         inputs = super().prepare_inputs_for_generation(\n             input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs\n         )\n-        # Decode stage\n+\n+        # Qwen3-Omni will prepare position ids in forward with deltas\n+        inputs[\"position_ids\"] = None\n+\n         # TODO(raushan, gante): Refactor this part to a utility function\n         if cache_position[0] != 0:\n             input_ids = input_ids[:, -1:]"
        },
        {
            "sha": "5fc451e47d3ac258bd9aae9ffdd89e4a8bd669c5",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling\n+from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import check_model_inputs\n from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig\n \n@@ -1201,44 +1201,19 @@ def forward(\n             deepstack_visual_embeds = deepstack_video_embeds\n \n         if position_ids is None:\n-            attention_mask_tensor = (\n-                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n-            )\n-            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n-                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                # Only apply conversion for floating point tensors (inverted masks)\n-                if attention_mask_tensor.dtype.is_floating_point:\n-                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n-\n-            # Calculate RoPE index once per generation in the pre-fill stage only.\n-            # When compiling, we can't check tensor values thus we check only input length\n-            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n-            # models currently cannot do asssisted decoding\n-            prefill_compiled_stage = is_torchdynamo_compiling() and (\n-                (input_ids is not None and input_ids.shape[1] != 1)\n-                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n-            )\n-            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n-                (cache_position is not None and cache_position[0] == 0)\n-                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            )\n-            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if self.rope_deltas is None or past_key_values_length == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n                     video_grid_thw,\n-                    attention_mask=attention_mask_tensor,\n+                    attention_mask=attention_mask,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n             else:\n                 batch_size, seq_length, _ = inputs_embeds.shape\n-                delta = (\n-                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                    if cache_position is not None\n-                    else 0\n-                )\n+                delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 if cache_position is not None:  # otherwise `deltas` is an int `0`\n@@ -1449,8 +1424,33 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # Qwen3VL position_ids are prepareed with rope_deltas in forward\n-        model_inputs[\"position_ids\"] = None\n+        # Qwen3VL position_ids are prepared with rope_deltas\n+        if position_ids is None:\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            if model_inputs[\"cache_position\"][0] == 0 or self.model.rope_deltas is None:\n+                vision_positions, rope_deltas = self.model.get_rope_index(\n+                    model_inputs.get(\"input_ids\", None),\n+                    image_grid_thw=image_grid_thw,\n+                    video_grid_thw=video_grid_thw,\n+                    attention_mask=attention_mask,\n+                )\n+                self.model.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            elif \"position_ids\" in model_inputs:\n+                batch_size, seq_length = model_inputs[\"position_ids\"].shape\n+                device = model_inputs[\"position_ids\"].device\n+                position_ids = torch.arange(seq_length, device=device)\n+                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n+                delta = cache_position[0] + self.model.rope_deltas\n+                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                vision_positions = position_ids + delta.expand_as(position_ids)\n+\n+            # Concatenate \"text + vision\" positions into [4, bs, seq-len]\n+            text_positions = model_inputs[\"position_ids\"][None, ...]\n+            model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n         if cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None"
        },
        {
            "sha": "bde442d39cfc8d606abb843c6609252cb5f42909",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -34,7 +34,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, logging\n from ...utils.generic import check_model_inputs\n from ...video_utils import VideoInput\n from ..llama.modeling_llama import LlamaRotaryEmbedding\n@@ -1033,44 +1033,19 @@ def forward(\n             deepstack_visual_embeds = deepstack_video_embeds\n \n         if position_ids is None:\n-            attention_mask_tensor = (\n-                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n-            )\n-            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n-                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                # Only apply conversion for floating point tensors (inverted masks)\n-                if attention_mask_tensor.dtype.is_floating_point:\n-                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n-\n-            # Calculate RoPE index once per generation in the pre-fill stage only.\n-            # When compiling, we can't check tensor values thus we check only input length\n-            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n-            # models currently cannot do asssisted decoding\n-            prefill_compiled_stage = is_torchdynamo_compiling() and (\n-                (input_ids is not None and input_ids.shape[1] != 1)\n-                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n-            )\n-            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n-                (cache_position is not None and cache_position[0] == 0)\n-                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            )\n-            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if self.rope_deltas is None or past_key_values_length == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n                     video_grid_thw,\n-                    attention_mask=attention_mask_tensor,\n+                    attention_mask=attention_mask,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n             else:\n                 batch_size, seq_length, _ = inputs_embeds.shape\n-                delta = (\n-                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                    if cache_position is not None\n-                    else 0\n-                )\n+                delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 if cache_position is not None:  # otherwise `deltas` is an int `0`\n@@ -1232,8 +1207,33 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # Qwen3VL position_ids are prepareed with rope_deltas in forward\n-        model_inputs[\"position_ids\"] = None\n+        # Qwen3VL position_ids are prepared with rope_deltas\n+        if position_ids is None:\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            if model_inputs[\"cache_position\"][0] == 0 or self.model.rope_deltas is None:\n+                vision_positions, rope_deltas = self.model.get_rope_index(\n+                    model_inputs.get(\"input_ids\", None),\n+                    image_grid_thw=image_grid_thw,\n+                    video_grid_thw=video_grid_thw,\n+                    attention_mask=attention_mask,\n+                )\n+                self.model.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            elif \"position_ids\" in model_inputs:\n+                batch_size, seq_length = model_inputs[\"position_ids\"].shape\n+                device = model_inputs[\"position_ids\"].device\n+                position_ids = torch.arange(seq_length, device=device)\n+                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n+                delta = cache_position[0] + self.model.rope_deltas\n+                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                vision_positions = position_ids + delta.expand_as(position_ids)\n+\n+            # Concatenate \"text + vision\" positions into [4, bs, seq-len]\n+            text_positions = model_inputs[\"position_ids\"][None, ...]\n+            model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n         if cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None"
        },
        {
            "sha": "d10dac468dd1086f19dce02ff64703ed27e74c0e",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57eeb9caa2d661b90b4fee6c7f700ede341677fa/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=57eeb9caa2d661b90b4fee6c7f700ede341677fa",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling\n+from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_vl_moe import Qwen3VLMoeConfig, Qwen3VLMoeTextConfig, Qwen3VLMoeVisionConfig\n \n@@ -1358,44 +1358,19 @@ def forward(\n             deepstack_visual_embeds = deepstack_video_embeds\n \n         if position_ids is None:\n-            attention_mask_tensor = (\n-                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n-            )\n-            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n-                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                # Only apply conversion for floating point tensors (inverted masks)\n-                if attention_mask_tensor.dtype.is_floating_point:\n-                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n-\n-            # Calculate RoPE index once per generation in the pre-fill stage only.\n-            # When compiling, we can't check tensor values thus we check only input length\n-            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n-            # models currently cannot do asssisted decoding\n-            prefill_compiled_stage = is_torchdynamo_compiling() and (\n-                (input_ids is not None and input_ids.shape[1] != 1)\n-                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n-            )\n-            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n-                (cache_position is not None and cache_position[0] == 0)\n-                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            )\n-            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+            past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n+            if self.rope_deltas is None or past_key_values_length == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n                     video_grid_thw,\n-                    attention_mask=attention_mask_tensor,\n+                    attention_mask=attention_mask,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n             else:\n                 batch_size, seq_length, _ = inputs_embeds.shape\n-                delta = (\n-                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                    if cache_position is not None\n-                    else 0\n-                )\n+                delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 if cache_position is not None:  # otherwise `deltas` is an int `0`\n@@ -1677,8 +1652,33 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # Qwen3VLMoe position_ids are prepareed with rope_deltas in forward\n-        model_inputs[\"position_ids\"] = None\n+        # Qwen3VLMoe position_ids are prepared with rope_deltas\n+        if position_ids is None:\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            if model_inputs[\"cache_position\"][0] == 0 or self.model.rope_deltas is None:\n+                vision_positions, rope_deltas = self.model.get_rope_index(\n+                    model_inputs.get(\"input_ids\", None),\n+                    image_grid_thw=image_grid_thw,\n+                    video_grid_thw=video_grid_thw,\n+                    attention_mask=attention_mask,\n+                )\n+                self.model.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            elif \"position_ids\" in model_inputs:\n+                batch_size, seq_length = model_inputs[\"position_ids\"].shape\n+                device = model_inputs[\"position_ids\"].device\n+                position_ids = torch.arange(seq_length, device=device)\n+                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n+                delta = cache_position[0] + self.model.rope_deltas\n+                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                vision_positions = position_ids + delta.expand_as(position_ids)\n+\n+            # Concatenate \"text + vision\" positions into [4, bs, seq-len]\n+            text_positions = model_inputs[\"position_ids\"][None, ...]\n+            model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n         if cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None"
        }
    ],
    "stats": {
        "total": 317,
        "additions": 142,
        "deletions": 175
    }
}