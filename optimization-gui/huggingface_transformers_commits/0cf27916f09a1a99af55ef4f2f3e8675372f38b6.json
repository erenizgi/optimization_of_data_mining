{
    "author": "Cyrilvallez",
    "message": "Add packed tensor format support for flex/sdpa/eager through the mask! (#39194)\n\n* Add the necesary logic to mask_utils\n\n* add it everywhere\n\n* Update masking_utils.py\n\n* style\n\n* Update masking_utils.py\n\n* Update modeling_mimi.py\n\n* Update masking_utils.py\n\n* add support for more than batch size 1\n\n* Update masking_utils.py\n\n* add test\n\n* style\n\n* Update test_masking_utils.py\n\n* Update masking_utils.py\n\n* add require_token\n\n* fix tests\n\n* fix",
    "sha": "0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
    "files": [
        {
            "sha": "e36417269d66661d30bd3c1f09b53533f50a8845",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -656,6 +656,7 @@ def prepare_inputs_for_generation(\n             # If it's not defined, it means the model uses the new general mask API\n             if causal_mask_creation_function is None:  # can't be found\n                 token_type_ids = getattr(model_input, \"token_type_ids\", None)\n+                position_ids = getattr(model_input, position_ids_key, None)\n                 # Some models may overwrite the general one\n                 causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n                 attention_mask = causal_mask_creation_function(\n@@ -665,6 +666,7 @@ def prepare_inputs_for_generation(\n                     attention_mask=attention_mask,\n                     cache_position=cache_position,\n                     past_key_values=past_key_values,\n+                    position_ids=position_ids,\n                     token_type_ids=token_type_ids,\n                 )\n             else:"
        },
        {
            "sha": "8d5aab9f1342a4654b228832c48470aa91561b7e",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 95,
            "deletions": 9,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -112,6 +112,10 @@ def chunked_causal_mask_function(chunk_size: int) -> Callable:\n \n \n def padding_mask_function(padding_mask: torch.Tensor) -> Callable:\n+    \"\"\"\n+    This return the mask_function function corresponding to a 2D padding mask.\n+    \"\"\"\n+\n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         # Note that here the mask should ALWAYS be at least of the max `kv_index` size in the dimension 1. This is because\n         # we cannot pad it here in the mask_function as we don't know the final size, and we cannot try/except, as it is not\n@@ -121,6 +125,17 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     return inner_mask\n \n \n+def packed_sequence_mask_function(packed_sequence_mask: torch.Tensor) -> Callable:\n+    \"\"\"\n+    This return the mask_function function corresponding to a 2D packed sequence mask.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return packed_sequence_mask[batch_idx, q_idx] == packed_sequence_mask[batch_idx, kv_idx]\n+\n+    return inner_mask\n+\n+\n def add_offsets_to_mask_function(mask_function: Callable, q_offset: int, kv_offset: int) -> Callable:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n@@ -592,12 +607,40 @@ class AttentionMaskInterface(GeneralInterface):\n ALL_MASK_ATTENTION_FUNCTIONS: AttentionMaskInterface = AttentionMaskInterface()\n \n \n+def find_packed_sequence_indices(position_ids: torch.Tensor) -> Optional[torch.Tensor]:\n+    \"\"\"\n+    Find the indices of the sequence to which each new query token in the sequence belongs when using packed\n+    tensor format (i.e. several sequences packed in the same batch dimension).\n+\n+    Args:\n+        position_ids (`torch.Tensor`)\n+            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n+\n+    Returns:\n+        A 2D tensor where each similar integer indicates that the tokens belong to the same sequence. For example, if we\n+        pack 3 sequences of 2, 3 and 1 tokens respectively along a single batch dim, this will return [[0, 0, 1, 1, 1, 2]].\n+    \"\"\"\n+    # What separate different sequences is when 2 consecutive positions_ids are separated by more than 1. So\n+    # taking the diff (by prepending the first value - 1 to keep correct indexing) and applying cumsum to the result\n+    # gives exactly the sequence indices\n+    # Note that we assume that a single sequence cannot span several batch dimensions, i.e. 1 single sequence\n+    # cannot be part of the end of the first batch dim and the start of the 2nd one for example\n+    first_dummy_value = position_ids[:, :1] - 1  # We just need the diff on this first value to be 1\n+    position_diff = torch.diff(position_ids, prepend=first_dummy_value, dim=-1)\n+    packed_sequence_mask = (position_diff != 1).cumsum(-1)\n+\n+    # Here it would be nice to return None if we did not detect packed sequence format, i.e. if `packed_sequence_mask[:, -1] == 0`\n+    # but it causes issues with export\n+    return packed_sequence_mask\n+\n+\n def _preprocess_mask_arguments(\n     config: PretrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[Union[torch.Tensor, BlockMask]],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n     layer_idx: Optional[int],\n ) -> tuple[bool, Optional[Union[torch.Tensor, BlockMask]], int, int]:\n     \"\"\"\n@@ -617,6 +660,8 @@ def _preprocess_mask_arguments(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n+        position_ids (`torch.Tensor`, optional)\n+            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n         layer_idx (`int`, optional):\n             If `past_key_values` is not None, this is the layer index of the cache from which to get the key-value\n             length and offset. Indeed, for hybrid caches, different layers may return different lengths.\n@@ -626,22 +671,25 @@ def _preprocess_mask_arguments(\n             Whether we should early exit mask creation, and return the mask as-is.\n         attention_mask (`torch.Tensor` or `BlockMask` or `None`):\n             The attention mask to either return immediately, or to use in downstream mask creation.\n+        packed_sequence_mask (`torch.Tensor`, optional):\n+            In case we detected packed sequence format, this is a tensor where each similar integer indicates that\n+            the tokens belong to the same sequence.\n         kv_length (`int`):\n             The size that the key and value states will have during the attention computation.\n         kv_offset (`int`):\n             An offset to indicate at which first position the key and values states will refer to.\n     \"\"\"\n     # If the mask is already 4D, simply return as-is (it was already prepared, or it is custom)\n     if isinstance(attention_mask, (torch.Tensor, BlockMask)) and len(attention_mask.shape) == 4:\n-        return True, attention_mask, None, None\n+        return True, attention_mask, None, None, None\n \n     # For TGI/vLLM backends, or other custom attention without equivalent mask creation: we don't need a mask!\n     # Note: it's not ideal to check the `_global_mapping` attribute instead of the object itself, however otherwise\n     # full graph dynamo tracing (i.e. torch.export or compile with `fullgraph=True`) will fail on Python<3.11\n     # with `torch._dynamo.exc.Unsupported: 'inline in skipfiles:Mapping.__contains__ | __contains__, skipped\n     # according trace_rules.lookup SKIP_DIRS'` -- can be removed when we require Python>=3.11\n     if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS._global_mapping:\n-        return True, None, None, None\n+        return True, None, None, None, None\n \n     # Move the mask to correct device, and potentially switch dtype for efficiency\n     if attention_mask is not None and attention_mask.ndim == 2:\n@@ -654,7 +702,17 @@ def _preprocess_mask_arguments(\n     else:\n         kv_length, kv_offset = input_embeds.shape[1], 0\n \n-    return False, attention_mask, kv_length, kv_offset\n+    # We check the position_ids for potential packed sequence format (only if the 2D attention mask is explicitly None,\n+    # and we don't have past_key_values, i.e. generally a training setup)\n+    packed_sequence_mask = None\n+    if position_ids is not None and attention_mask is None and past_key_values is None:\n+        batch_size = input_embeds.shape[0]\n+        # The position ids are sometimes just unsqueezed, without being expanded\n+        if batch_size != position_ids.shape[0]:\n+            position_ids = position_ids.expand(batch_size, -1)\n+        packed_sequence_mask = find_packed_sequence_indices(position_ids)\n+\n+    return False, attention_mask, packed_sequence_mask, kv_length, kv_offset\n \n \n def create_causal_mask(\n@@ -663,6 +721,7 @@ def create_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n@@ -684,6 +743,8 @@ def create_causal_mask(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n+        position_ids (`torch.Tensor`, optional)\n+            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the causal mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n@@ -697,8 +758,8 @@ def create_causal_mask(\n     else:\n         layer_idx = 0\n \n-    early_exit, attention_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n-        config, input_embeds, attention_mask, cache_position, past_key_values, layer_idx\n+    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n     )\n     if early_exit:\n         return attention_mask\n@@ -711,6 +772,11 @@ def create_causal_mask(\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n     allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n \n+    # If we detected packing format\n+    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n+        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n+        allow_is_causal_skip = False\n+\n     # Allow slight deviations from causal mask\n     if or_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n@@ -744,6 +810,7 @@ def create_sliding_window_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n@@ -766,6 +833,8 @@ def create_sliding_window_causal_mask(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n+        position_ids (`torch.Tensor`, optional)\n+            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the sliding causal mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n@@ -779,8 +848,8 @@ def create_sliding_window_causal_mask(\n     else:\n         layer_idx = 0\n \n-    early_exit, attention_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n-        config, input_embeds, attention_mask, cache_position, past_key_values, layer_idx\n+    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n     )\n     if early_exit:\n         return attention_mask\n@@ -797,6 +866,11 @@ def create_sliding_window_causal_mask(\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n     allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n \n+    # If we detected packing format\n+    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n+        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n+        allow_is_causal_skip = False\n+\n     # Allow slight deviations from sliding causal mask\n     if or_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n@@ -831,6 +905,7 @@ def create_chunked_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n@@ -853,6 +928,8 @@ def create_chunked_causal_mask(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n+        position_ids (`torch.Tensor`, optional)\n+            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the chunked causal mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the chunked causal one, for example for image tokens handling.\n@@ -866,8 +943,8 @@ def create_chunked_causal_mask(\n     else:\n         layer_idx = 0\n \n-    early_exit, attention_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n-        config, input_embeds, attention_mask, cache_position, past_key_values, layer_idx\n+    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n     )\n     if early_exit:\n         return attention_mask\n@@ -891,6 +968,11 @@ def create_chunked_causal_mask(\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n     allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n \n+    # If we detected packing format\n+    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n+        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n+        allow_is_causal_skip = False\n+\n     # Allow slight deviations from chunked causal mask\n     if or_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n@@ -932,6 +1014,7 @@ def create_masks_for_generate(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n     **kwargs,\n@@ -953,6 +1036,8 @@ def create_masks_for_generate(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n+        position_ids (`torch.Tensor`, optional)\n+            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the other mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n@@ -969,6 +1054,7 @@ def create_masks_for_generate(\n         \"attention_mask\": attention_mask,\n         \"cache_position\": cache_position,\n         \"past_key_values\": past_key_values,\n+        \"position_ids\": position_ids,\n         \"or_mask_function\": or_mask_function,\n         \"and_mask_function\": and_mask_function,\n     }"
        },
        {
            "sha": "b1b58667a09b5e9574711fd33295abaa0389912b",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -423,6 +423,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "af2b88ca722efba12ce2e62006a7f510a0304a1d",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -806,6 +806,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "48a804b0a76d97aa29c7e357ca1d34b85837f7bc",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -420,6 +420,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "19a140ae8138d84495ed6256224aff941808ba26",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -457,6 +457,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "afcaee5c2fa575eb9971ec5939e4163d85883c91",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -434,6 +434,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "fc4f24b834ab747b9e5e2bd1254ce79d7bcbb590",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -455,6 +455,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "de76c0361677fd91737d3eb858ca5869ba4ad057",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -500,6 +500,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds\n@@ -811,6 +812,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "1f6627bef54df285ad27a62f53f01bf7b679e21f",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -238,6 +238,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "4287e44a7f1b45f205ae25f8b085e137b8f172e5",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -610,6 +610,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "9e317e029cee496afe3e703cf92dbce3008a9bb4",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -629,6 +629,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n         encoder_attention_mask = self._update_cross_attn_mask(\n             encoder_hidden_states,"
        },
        {
            "sha": "2225788f615b6e6c2555c6aa9e7e00fa29ae3f23",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -455,6 +455,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n         encoder_attention_mask = self._update_cross_attn_mask(\n             encoder_hidden_states,"
        },
        {
            "sha": "06ec77b1bff5aead1653b8051c74284a8dc6c419",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -697,6 +697,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e0c2f5ce5132d2655a4b07ef6d874f1b83a0fec8",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -532,6 +532,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "6d3ab0402f18247e44b1a6c3e5705bfc6d2f1a23",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -1264,6 +1264,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "906b29ea0db5281d8ac1ca850ce0d3a17e553b70",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -416,6 +416,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # embed positions"
        },
        {
            "sha": "d2361ab114862adb635b25dabc5920b7911f9fcc",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -418,6 +418,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # embed positions"
        },
        {
            "sha": "15a502b26449ef89a39935eb697c7e02aa25f35a",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -440,6 +440,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "c5f8d639753c3508db98cc234d69343cd794a47d",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -422,6 +422,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "eddea94b91afa267c8a750e666d912b62ce7888d",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -540,6 +540,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -923,6 +924,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n@@ -1182,6 +1184,7 @@ def create_masks_for_generate(\n         attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n         past_key_values: Optional[Cache],\n+        position_ids: Optional[torch.Tensor],\n         token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> dict:\n@@ -1192,6 +1195,7 @@ def create_masks_for_generate(\n             \"attention_mask\": attention_mask,\n             \"cache_position\": cache_position,\n             \"past_key_values\": past_key_values,\n+            \"position_ids\": position_ids,\n         }\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:"
        },
        {
            "sha": "1fa0ee273a453f51b489eafe538b98e3671eb7c0",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -607,6 +607,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -825,6 +826,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n@@ -1034,6 +1036,7 @@ def create_masks_for_generate(\n         attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n         past_key_values: Optional[Cache],\n+        position_ids: Optional[torch.Tensor],\n         token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> dict:\n@@ -1044,6 +1047,7 @@ def create_masks_for_generate(\n             \"attention_mask\": attention_mask,\n             \"cache_position\": cache_position,\n             \"past_key_values\": past_key_values,\n+            \"position_ids\": position_ids,\n         }\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:"
        },
        {
            "sha": "9ba504a5bd19df31b4a1833f19eeefa12870d0c6",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -1649,6 +1649,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "e65a7696ec9e8e28a7a0b64e22b059db69d2f4e0",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -2088,6 +2088,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "47d42d58e472a00bbb63f9ccef767c8a883d085f",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -437,6 +437,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "00c0f9ab59302cbd5acb626c36df4148e3cde7bb",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -445,6 +445,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "1c2be3fdcdeb20d22a1c00a91a07d6ccb41f124c",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -902,6 +902,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "2d296d53a9caa8c892827ee97cd5affb3c832602",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -956,6 +956,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "1493c1cc11a58cc439daa37513194e9744620c24",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -392,6 +392,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # Prepare head mask if needed"
        },
        {
            "sha": "03c8300ed07b4740ecd477d0c3988accec1996bf",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -338,6 +338,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # Prepare head mask if needed"
        },
        {
            "sha": "37ede89bd428ba131e935e9b0d7fdea26f249e83",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -440,6 +440,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "c91cd4b12bae04760b73a7e57fb531ae7927b9a4",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -181,6 +181,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "cb9a4b268e1647d86368647046081b41d7ee0c0f",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -422,6 +422,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "78ceb22ee6065e2629586c3c4668d15f2066cd42",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -422,6 +422,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "0e52700c2fe005bcf12bfbf12c09bf18a14974cb",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -552,6 +552,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "abfb4892d70635d7cdf190bb42abbe3f6e532c4b",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -1120,6 +1120,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # decoder layers"
        },
        {
            "sha": "190fc8e529363516869669b55956ae30bfac2f1f",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -731,6 +731,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "ed88d10bb9d80865c2b10bad789492f9754dd536",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -536,6 +536,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "1bf4ea1f1ea8a0af775df8daa1eaa7c9274f8031",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -399,6 +399,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "2cd2be1eaae6cd14c6272294cd7b11d83d4d67c5",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -160,6 +160,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "10520f6d4c6bae90c8f993bf89a41041d89fd9db",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -520,6 +520,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "cc9bfb52973b29374094e3cf4f61ab83b018177a",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -370,6 +370,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "4f33ee6e2b32dd19f15ff9a4eed162fe2800de40",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -743,6 +743,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "99ebb09c72a2bd498c4c0dc15b3f1857882ad4a0",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -749,6 +749,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "1e2c9f6bbc4060cf1420b6a0a130743ea12bbd73",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -401,6 +401,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "97559927de31b69e118cf470f45917521156f2c1",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -407,6 +407,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "f0401997429ab16ed83ee0b5a744f10357169ea0",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -395,6 +395,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama"
        },
        {
            "sha": "93690075ae038ece7e650318d55464deafbfdc4b",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -245,6 +245,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama"
        },
        {
            "sha": "7113ef56f7bfcfdf1cd6a4176564980a960994d2",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -454,6 +454,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "ce49f8f901823baf6dfff34ec805cbc3149f2126",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -1762,6 +1762,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "de85fc87276b3149767dbe6a062fa83641f71963",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -1577,6 +1577,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "01432c3ca9b0561f63448787a2a6e05a0161709d",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -406,6 +406,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "114f6b7feac3ee587c405f564a95edc501f3ed45",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -167,6 +167,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "aab42610461c8df5694453283c6ef3a4f22b7d60",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -1629,6 +1629,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -2189,6 +2190,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "90f99a49bd754d8712bbb766624797ae3998177b",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -893,6 +893,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "5e77ffd3298da64656a05d061442c9a308fca99b",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -868,6 +868,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "7fbb5a90d0524c3887a3008ba4a14a2d9383117a",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -432,6 +432,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "5ba2bccd11fc536d6dd0f94efaaf59373cccbb1e",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -527,6 +527,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "27f2b1aa41783c72aeaf7b67e6108ccc134ef1f9",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -436,6 +436,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "c6af5bfd94f6775ea05ffab426ad96ce2c3f6789",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -400,6 +400,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "5c633859589969d15d98c3e7288998b07c7b7c86",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -214,6 +214,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "1eacec6a27cfa09a8dca1006a3ca14a0317d6b07",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -731,6 +731,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": None,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             self_attn_mask_mapping = {\n@@ -874,6 +875,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values.self_attention_cache if past_key_values is not None else None,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             self_attn_mask_mapping = {\n@@ -890,6 +892,7 @@ def forward(\n                 \"attention_mask\": encoder_attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": None,\n+                \"position_ids\": None,\n             }\n             cross_attn_mask_mapping = {\n                 \"full_attention\": create_causal_mask("
        },
        {
            "sha": "01d8a401f4c4eac002f5e2c3b58e33c3bbbbf973",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -678,6 +678,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": None,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             self_attn_mask_mapping = {\n@@ -821,6 +822,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values.self_attention_cache if past_key_values is not None else None,\n+                \"position_ids\": position_ids,\n             }\n             # Create the masks\n             self_attn_mask_mapping = {\n@@ -837,6 +839,7 @@ def forward(\n                 \"attention_mask\": encoder_attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": None,\n+                \"position_ids\": None,\n             }\n             cross_attn_mask_mapping = {\n                 \"full_attention\": create_causal_mask("
        },
        {
            "sha": "43f1eccdc0c521cfcc56bf4c1359cdc9d3552162",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -927,6 +927,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         if self.gradient_checkpointing and self.training:"
        },
        {
            "sha": "3b162e0b08ff00a444fd98f5d067fef1ecc30477",
            "filename": "tests/utils/test_masking_utils.py",
            "status": "added",
            "additions": 132,
            "deletions": 0,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/tests%2Futils%2Ftest_masking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cf27916f09a1a99af55ef4f2f3e8675372f38b6/tests%2Futils%2Ftest_masking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_masking_utils.py?ref=0cf27916f09a1a99af55ef4f2f3e8675372f38b6",
            "patch": "@@ -0,0 +1,132 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers.testing_utils import is_torch_available, require_torch\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch.nn.attention.flex_attention import create_block_mask\n+\n+    from transformers import LlamaConfig\n+    from transformers.masking_utils import create_causal_mask\n+\n+\n+# fmt: off\n+EXPECTED_PACKED_MASK = torch.tensor([[[\n+    [ True, False, False, False, False, False, False, False, False, False],\n+    [ True,  True, False, False, False, False, False, False, False, False],\n+    [ True,  True,  True, False, False, False, False, False, False, False],\n+    [ True,  True,  True,  True, False, False, False, False, False, False],\n+    [False, False, False, False,  True, False, False, False, False, False],\n+    [False, False, False, False,  True,  True, False, False, False, False],\n+    [False, False, False, False, False, False,  True, False, False, False],\n+    [False, False, False, False, False, False,  True,  True, False, False],\n+    [False, False, False, False, False, False,  True,  True,  True, False],\n+    [False, False, False, False, False, False,  True,  True,  True,  True]]],\n+\n+\n+  [[[ True, False, False, False, False, False, False, False, False, False],\n+    [ True,  True, False, False, False, False, False, False, False, False],\n+    [ True,  True,  True, False, False, False, False, False, False, False],\n+    [ True,  True,  True,  True, False, False, False, False, False, False],\n+    [ True,  True,  True,  True,  True, False, False, False, False, False],\n+    [ True,  True,  True,  True,  True,  True, False, False, False, False],\n+    [False, False, False, False, False, False,  True, False, False, False],\n+    [False, False, False, False, False, False,  True,  True, False, False],\n+    [False, False, False, False, False, False,  True,  True,  True, False],\n+    [False, False, False, False, False, False,  True,  True,  True,  True]\n+]]], dtype=torch.bool)\n+# fmt: on\n+\n+\n+@require_torch\n+class MaskTest(unittest.TestCase):\n+    def test_packed_sequence_mask_sdpa(self):\n+        config = LlamaConfig()\n+        config._attn_implementation = \"sdpa\"\n+\n+        batch_size = 2\n+        sequence_length = 10\n+        cache_position = torch.arange(sequence_length)\n+\n+        # First batch has 3 packed sequences of 4, 2 and 4 tokens respectively, second has 2 of 6 and 4 tokens\n+        position_ids = torch.tensor([[0, 1, 2, 3, 0, 1, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5, 0, 1, 2, 3]])\n+\n+        causal_mask = create_causal_mask(\n+            config=config,\n+            # we only need batch size, seq_length and dtype here - we don't care about the values of the embeddings\n+            input_embeds=torch.empty((batch_size, sequence_length), dtype=torch.float16),\n+            attention_mask=None,\n+            cache_position=cache_position,\n+            past_key_values=None,\n+            position_ids=position_ids,\n+        )\n+\n+        self.assertTrue((causal_mask == EXPECTED_PACKED_MASK).all())\n+\n+    def test_packed_sequence_mask_eager(self):\n+        config = LlamaConfig()\n+        config._attn_implementation = \"eager\"\n+\n+        batch_size = 2\n+        sequence_length = 10\n+        cache_position = torch.arange(sequence_length)\n+\n+        # First batch has 3 packed sequences of 4, 2 and 4 tokens respectively, second has 2 of 6 and 4 tokens\n+        position_ids = torch.tensor([[0, 1, 2, 3, 0, 1, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5, 0, 1, 2, 3]])\n+\n+        causal_mask = create_causal_mask(\n+            config=config,\n+            # we only need batch size, seq_length and dtype here - we don't care about the values of the embeddings\n+            input_embeds=torch.empty((batch_size, sequence_length), dtype=torch.float16),\n+            attention_mask=None,\n+            cache_position=cache_position,\n+            past_key_values=None,\n+            position_ids=position_ids,\n+        )\n+\n+        min_dtype = torch.finfo(torch.float16).min\n+        self.assertTrue((causal_mask == torch.where(EXPECTED_PACKED_MASK, 0.0, min_dtype)).all())\n+\n+    def test_packed_sequence_mask_flex_attention(self):\n+        config = LlamaConfig()\n+        config._attn_implementation = \"flex_attention\"\n+\n+        batch_size = 2\n+        sequence_length = 10\n+        cache_position = torch.arange(sequence_length)\n+\n+        # First batch has 3 packed sequences of 4, 2 and 4 tokens respectively, second has 2 of 6 and 4 tokens\n+        position_ids = torch.tensor([[0, 1, 2, 3, 0, 1, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5, 0, 1, 2, 3]])\n+\n+        causal_mask = create_causal_mask(\n+            config=config,\n+            # we only need batch size, seq_length and dtype here - we don't care about the values of the embeddings\n+            input_embeds=torch.empty((batch_size, sequence_length), dtype=torch.float16),\n+            attention_mask=None,\n+            cache_position=cache_position,\n+            past_key_values=None,\n+            position_ids=position_ids,\n+        )\n+\n+        def dummy_mask_mod(b, h, q, kv):\n+            return EXPECTED_PACKED_MASK[b, h, q, kv]\n+\n+        EXPECTED_BLOCK_MASK = create_block_mask(dummy_mask_mod, 2, None, 10, 10, device=\"cpu\")\n+\n+        # We compatre the str representations, as the BlockMask objects themselves cannot easily be compared\n+        self.assertEqual(causal_mask.to_string(), EXPECTED_BLOCK_MASK.to_string())"
        }
    ],
    "stats": {
        "total": 312,
        "additions": 303,
        "deletions": 9
    }
}