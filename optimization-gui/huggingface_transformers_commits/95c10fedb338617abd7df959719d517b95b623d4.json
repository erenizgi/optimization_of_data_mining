{
    "author": "viktor-shcherb",
    "message": "Updated documentation and added conversion utility (#34319)\n\n* Updated documentation and added conversion utility\r\n\r\n* Update docs/source/en/tiktoken.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/tiktoken.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Moved util function to integration folder + allow for str\r\n\r\n* Update formatting\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Updated formatting\r\n\r\n* style changes\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "95c10fedb338617abd7df959719d517b95b623d4",
    "files": [
        {
            "sha": "aac81e24fdd7387695c3add6d02ac6da840d4d13",
            "filename": "docs/source/en/tiktoken.md",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/95c10fedb338617abd7df959719d517b95b623d4/docs%2Fsource%2Fen%2Ftiktoken.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/95c10fedb338617abd7df959719d517b95b623d4/docs%2Fsource%2Fen%2Ftiktoken.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftiktoken.md?ref=95c10fedb338617abd7df959719d517b95b623d4",
            "patch": "@@ -36,3 +36,25 @@ from transformers import AutoTokenizer\n model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder=\"original\") \n ```\n+## Create tiktoken tokenizer\n+\n+The `tokenizer.model` file contains no information about additional tokens or pattern strings. If these are important, convert the tokenizer to `tokenizer.json`, the appropriate format for [`PreTrainedTokenizerFast`].\n+\n+Generate the `tokenizer.model` file with [tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63) and then convert it to `tokenizer.json` with [`convert_tiktoken_to_fast`].\n+\n+```py\n+\n+from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n+from tiktoken import get_encoding\n+\n+# You can load your custom encoding or the one provided by OpenAI\n+encoding = get_encoding(\"gpt2\")\n+convert_tiktoken_to_fast(encoding, \"config/save/dir\")\n+```\n+\n+The resulting `tokenizer.json` file is saved to the specified directory and can be loaded with [`PreTrainedTokenizerFast`].\n+\n+```py\n+tokenizer = PreTrainedTokenizerFast.from_pretrained(\"config/save/dir\")\n+```\n+"
        },
        {
            "sha": "60f733928406c2ff5aec94c7680a452c2f9fd30b",
            "filename": "src/transformers/integrations/tiktoken.py",
            "status": "added",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/95c10fedb338617abd7df959719d517b95b623d4/src%2Ftransformers%2Fintegrations%2Ftiktoken.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95c10fedb338617abd7df959719d517b95b623d4/src%2Ftransformers%2Fintegrations%2Ftiktoken.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftiktoken.py?ref=95c10fedb338617abd7df959719d517b95b623d4",
            "patch": "@@ -0,0 +1,45 @@\n+from pathlib import Path\n+from typing import Any\n+\n+from transformers.convert_slow_tokenizer import TikTokenConverter\n+from transformers.tokenization_utils_fast import TIKTOKEN_VOCAB_FILE, TOKENIZER_FILE\n+\n+\n+def convert_tiktoken_to_fast(encoding: Any, output_dir: str):\n+    \"\"\"\n+    Converts given `tiktoken` encoding to `PretrainedTokenizerFast` and saves the configuration of converted tokenizer\n+    on disk.\n+\n+    Args:\n+        encoding (`str` or `tiktoken.Encoding`):\n+            Tokenizer from `tiktoken` library. If `encoding` is `str`, the tokenizer will be loaded with\n+            `tiktoken.get_encoding(encoding)`.\n+        output_dir (`str`):\n+            Save path for converted tokenizer configuration file.\n+    \"\"\"\n+    output_dir = Path(output_dir)\n+    output_dir.mkdir(exist_ok=True)\n+\n+    save_file = output_dir / \"tiktoken\" / TIKTOKEN_VOCAB_FILE\n+    tokenizer_file = output_dir / TOKENIZER_FILE\n+\n+    save_file_absolute = str(save_file.absolute())\n+    output_file_absolute = str(tokenizer_file.absolute())\n+\n+    try:\n+        from tiktoken import get_encoding\n+        from tiktoken.load import dump_tiktoken_bpe\n+\n+        if isinstance(encoding, str):\n+            encoding = get_encoding(encoding)\n+\n+        dump_tiktoken_bpe(encoding._mergeable_ranks, save_file_absolute)\n+    except ImportError:\n+        raise ValueError(\n+            \"`tiktoken` is required to save a `tiktoken` file. Install it with \" \"`pip install tiktoken`.\"\n+        )\n+\n+    tokenizer = TikTokenConverter(\n+        vocab_file=save_file_absolute, pattern=encoding._pat_str, additional_special_tokens=encoding._special_tokens\n+    ).tokenizer()\n+    tokenizer.save(output_file_absolute)"
        }
    ],
    "stats": {
        "total": 67,
        "additions": 67,
        "deletions": 0
    }
}