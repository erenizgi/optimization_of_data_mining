{
    "author": "BowenBao",
    "message": "[Regression] Fix Quark quantized model loading after refactorization (#37407)",
    "sha": "6cef03ba660a0fe35d8cbcf00195410ae4c7557b",
    "files": [
        {
            "sha": "a73aecfb885e1383d08e253c5dbf5d3f43aad497",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 9,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cef03ba660a0fe35d8cbcf00195410ae4c7557b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cef03ba660a0fe35d8cbcf00195410ae4c7557b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6cef03ba660a0fe35d8cbcf00195410ae4c7557b",
            "patch": "@@ -649,7 +649,10 @@ def _infer_parameter_dtype(\n     try:\n         old_param = model.get_parameter_or_buffer(param_name)\n     except Exception as e:\n-        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ:\n+        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method in {\n+            QuantizationMethod.HQQ,\n+            QuantizationMethod.QUARK,\n+        }:\n             return True, None\n         else:\n             raise e\n@@ -708,11 +711,12 @@ def _load_state_dict_into_meta_model(\n         device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n \n     is_quantized = hf_quantizer is not None\n-    is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in [\n+    is_hqq_or_bnb_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n         QuantizationMethod.HQQ,\n         QuantizationMethod.BITS_AND_BYTES,\n-    ]\n-    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb\n+        QuantizationMethod.QUARK,\n+    }\n+    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb_or_quark\n     file_pointer = None\n     if is_meta_state_dict:\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n@@ -4632,11 +4636,15 @@ def _load_pretrained_model(\n     ):\n         # Useful flags\n         is_quantized = hf_quantizer is not None\n-        is_hqq = is_quantized and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ\n-        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in [\n+        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n+            QuantizationMethod.HQQ,\n+            QuantizationMethod.QUARK,\n+        }\n+        is_hqq_or_bnb_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n             QuantizationMethod.HQQ,\n             QuantizationMethod.BITS_AND_BYTES,\n-        ]\n+            QuantizationMethod.QUARK,\n+        }\n \n         # Get all the keys of the state dicts that we have to initialize the model\n         if sharded_metadata is not None:\n@@ -4798,7 +4806,7 @@ def _load_pretrained_model(\n             expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n \n         # Warmup cuda to load the weights much faster on devices\n-        if device_map is not None and not is_hqq:\n+        if device_map is not None and not is_hqq_or_quark:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n             caching_allocator_warmup(model_to_load, expanded_device_map, factor=2 if hf_quantizer is None else 4)\n \n@@ -4812,7 +4820,7 @@ def _load_pretrained_model(\n             map_location = \"cpu\"\n             if (\n                 shard_file.endswith(\".safetensors\")\n-                and not is_hqq_or_bnb\n+                and not is_hqq_or_bnb_or_quark\n                 and not (is_deepspeed_zero3_enabled() and not is_quantized)\n             ):\n                 map_location = \"meta\""
        },
        {
            "sha": "aefe1ebf44fb78b8b61ef975a56e39e02d30b774",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cef03ba660a0fe35d8cbcf00195410ae4c7557b/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cef03ba660a0fe35d8cbcf00195410ae4c7557b/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=6cef03ba660a0fe35d8cbcf00195410ae4c7557b",
            "patch": "@@ -53,6 +53,7 @@ class QuarkTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am not in Paris, France\\nToday I am in Paris, Illinois\")\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying the city of light. I am not just any ordinary Paris\")\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying my day off! The sun is shining, the birds are\")\n+    EXPECTED_OUTPUTS.add(\"Today I am in Paris and I'm here to tell you about it. It's a beautiful day,\")\n \n     EXPECTED_RELATIVE_DIFFERENCE = 1.66\n     device_map = None"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 18,
        "deletions": 9
    }
}