{
    "author": "threewebcode",
    "message": "fix typos in the docs directory (#36639)\n\n* chore: fix typos in the docs directory\n\n* chore: fix typos in the docs directory\n\n* chore: fix typos in the docs directory",
    "sha": "81aa9b2e07b359cd3555c118010fd9f26c601e54",
    "files": [
        {
            "sha": "5c56fb0e04f8ec2282b88e2450c461cf082b767f",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=81aa9b2e07b359cd3555c118010fd9f26c601e54",
            "patch": "@@ -271,7 +271,7 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n ## DoLa\n \n-[Decoding by Contrasting Layers (DoLa)](https://hf.co/papers/2309.03883) is a contrastive decoding strategy for improving factuality and reducing hallucination. This strategy works by contrasting the logit diffferences between the final and early layers. As a result, factual knowledge localized to particular layers are amplified. DoLa is not recommended for smaller models like GPT-2.\n+[Decoding by Contrasting Layers (DoLa)](https://hf.co/papers/2309.03883) is a contrastive decoding strategy for improving factuality and reducing hallucination. This strategy works by contrasting the logit differences between the final and early layers. As a result, factual knowledge localized to particular layers are amplified. DoLa is not recommended for smaller models like GPT-2.\n \n Enable DoLa with the following parameters.\n "
        },
        {
            "sha": "cc229f6b01481f9896f87d6323d5149079f1820d",
            "filename": "docs/source/en/how_to_hack_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fhow_to_hack_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fhow_to_hack_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhow_to_hack_models.md?ref=81aa9b2e07b359cd3555c118010fd9f26c601e54",
            "patch": "@@ -36,7 +36,7 @@ This guide will show you how to customize a models attention mechanism in order\n \n ## Attention class\n \n-[Segment Anything](./model_doc/sam) is an image segmentation model, and it combines the query-key-value (`qkv`) projection in its attention mechanims. To reduce the number of trainable parameters and computational overhead, you can apply LoRA to the `qkv` projection. This requires splitting the `qkv` projection so that you can separately target the `q` and `v` with LoRA.\n+[Segment Anything](./model_doc/sam) is an image segmentation model, and it combines the query-key-value (`qkv`) projection in its attention mechanisms. To reduce the number of trainable parameters and computational overhead, you can apply LoRA to the `qkv` projection. This requires splitting the `qkv` projection so that you can separately target the `q` and `v` with LoRA.\n \n 1. Create a custom attention class, `SamVisionAttentionSplit`, by subclassing the original `SamVisionAttention` class. In the `__init__`, delete the combined `qkv` and create a separate linear layer for `q`, `k` and `v`.\n "
        },
        {
            "sha": "c7c53765a2f8a9fc20dc12c9b1fc9223a1cc1d95",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=81aa9b2e07b359cd3555c118010fd9f26c601e54",
            "patch": "@@ -56,7 +56,7 @@ To give some examples of how much VRAM it roughly takes to load a model in bfloa\n \n As of writing this document, the largest GPU chip on the market is the A100 & H100 offering 80GB of VRAM. Most of the models listed before require more than 80GB just to be loaded and therefore necessarily require [tensor parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism) and/or [pipeline parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).\n \n-ðŸ¤— Transformers now supports tensor parallelism for supported models having `base_tp_plan` in their respecitve config classes. Learn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism). Furthermore, if you're interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at [the text-generation-inference library](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling).\n+ðŸ¤— Transformers now supports tensor parallelism for supported models having `base_tp_plan` in their respective config classes. Learn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism). Furthermore, if you're interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at [the text-generation-inference library](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling).\n \n Naive pipeline parallelism is supported out of the box. For this, simply load the model with `device=\"auto\"` which will automatically place the different layers on the available GPUs as explained [here](https://huggingface.co/docs/accelerate/v0.22.0/en/concept_guides/big_model_inference).\n Note, however that while very effective, this naive pipeline parallelism does not tackle the issues of GPU idling. For this more advanced pipeline parallelism is required as explained [here](https://huggingface.co/docs/transformers/en/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).\n@@ -551,7 +551,7 @@ $$ \\mathbf{\\hat{q}}_i^T \\mathbf{\\hat{x}}_j = \\mathbf{{q}}_i^T \\mathbf{R}_{\\theta\n \n \\\\( \\mathbf{R}_{\\theta, i - j} \\\\) thereby represents a rotational matrix. \\\\( \\theta \\\\) is *not* learned during training, but instead set to a pre-defined value that depends on the maximum input sequence length during training.\n \n-> By doing so, the propability score between \\\\( \\mathbf{q}_i \\\\) and \\\\( \\mathbf{q}_j \\\\) is only affected if \\\\( i \\ne j \\\\) and solely depends on the relative distance \\\\( i - j \\\\) regardless of each vector's specific positions \\\\( i \\\\) and \\\\( j \\\\) .\n+> By doing so, the probability score between \\\\( \\mathbf{q}_i \\\\) and \\\\( \\mathbf{q}_j \\\\) is only affected if \\\\( i \\ne j \\\\) and solely depends on the relative distance \\\\( i - j \\\\) regardless of each vector's specific positions \\\\( i \\\\) and \\\\( j \\\\) .\n \n *RoPE* is used in multiple of today's most important LLMs, such as:\n "
        },
        {
            "sha": "42fd725a9abefb52fdf3af82e966983268af436f",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=81aa9b2e07b359cd3555c118010fd9f26c601e54",
            "patch": "@@ -90,7 +90,7 @@ The `DepthProEncoder` further uses two encoders:\n - `image_encoder`\n    - Input image is also rescaled to `patch_size` and processed by the **`image_encoder`**\n \n-Both these encoders can be configured via `patch_model_config` and `image_model_config` respectively, both of which are seperate `Dinov2Model` by default.\n+Both these encoders can be configured via `patch_model_config` and `image_model_config` respectively, both of which are separate `Dinov2Model` by default.\n \n Outputs from both encoders (`last_hidden_state`) and selected intermediate states (`hidden_states`) from **`patch_encoder`** are fused by a `DPT`-based `FeatureFusionStage` for depth estimation.\n "
        },
        {
            "sha": "e435861cbe26f4c1176f11d2b85d071c5aea1454",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=81aa9b2e07b359cd3555c118010fd9f26c601e54",
            "patch": "@@ -36,7 +36,7 @@ On January 30, 2024, we released LLaVA-NeXT, an open-source Large Multimodal Mod\n \n **In todayâ€™s exploration, we delve into the performance of LLaVA-NeXT within the realm of video understanding tasks. We reveal that LLaVA-NeXT surprisingly has strong performance in understanding video content. The current version of LLaVA-NeXT for videos has several improvements:\n \n-- Zero-shot video representation capabilities with AnyRes: The AnyRes technique naturally represents a high-resolution image into multiple images that a pre-trained VIT is able to digest, and forms them into a concantenated sequence. This technique is naturally generalizable to represent videos (consisting of multiple frames), allowing the image-only-trained LLaVA-Next model to perform surprisingly well on video tasks. Notably, this is the first time that LMMs show strong zero-shot modality transfer ability.\n+- Zero-shot video representation capabilities with AnyRes: The AnyRes technique naturally represents a high-resolution image into multiple images that a pre-trained VIT is able to digest, and forms them into a concatenated sequence. This technique is naturally generalizable to represent videos (consisting of multiple frames), allowing the image-only-trained LLaVA-Next model to perform surprisingly well on video tasks. Notably, this is the first time that LMMs show strong zero-shot modality transfer ability.\n - Inference with length generalization improves on longer videos. The linear scaling technique enables length generalization, allowing LLaVA-NeXT to effectively handle long-video beyond the limitation of the \"max_token_length\" of the LLM.\n - Strong video understanding ability. (1) LLaVA-Next-Image, which combines the above two techniques, yields superior zero-shot performance than open-source LMMs tuned on videos. (2) LLaVA-Next-Video, further supervised fine-tuning (SFT) LLaVA-Next-Image on video data, achieves better video understanding capabilities compared to LLaVA-Next-Image. (3) LLaVA-Next-Video-DPO, which aligns the model response with AI feedback using direct preference optimization (DPO), showing significant performance boost.\n - Efficient deployment and inference with SGLang. It allows 5x faster inference on video tasks, allowing more scalable serving such as million-level video re-captioning. See instructions in our repo.**"
        },
        {
            "sha": "7635b72d9767e810353f5987af0ceca617aca6c3",
            "filename": "docs/source/en/quantization/quanto.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aa9b2e07b359cd3555c118010fd9f26c601e54/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md?ref=81aa9b2e07b359cd3555c118010fd9f26c601e54",
            "patch": "@@ -26,7 +26,7 @@ Install Quanto with the following command.\n pip install optimum-quanto accelerate transformers\n ```\n \n-Quantize a model by creating a [`QuantoConfig`] and specifiying the `weights` parameter to quantize to. This works for any model in any modality as long as it contains [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n+Quantize a model by creating a [`QuantoConfig`] and specifying the `weights` parameter to quantize to. This works for any model in any modality as long as it contains [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n \n > [!TIP]\n > The Transformers integration only supports weight quantization. Use the Quanto library directly if you need activation quantization, calibration, or QAT."
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}