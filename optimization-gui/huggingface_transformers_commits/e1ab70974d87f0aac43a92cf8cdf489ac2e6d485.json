{
    "author": "Wauplin",
    "message": "[cleanup] Remove deprecated load config from file (#42383)\n\n* Remove deprecated load config from file\n\n* code quality\n\n* ruff\n\n* format",
    "sha": "e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
    "files": [
        {
            "sha": "4b69b453359c41424a23d4cb9ec05226c451b617",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -30,9 +30,7 @@\n     PushToHubMixin,\n     cached_file,\n     copy_func,\n-    download_url,\n     extract_commit_hash,\n-    is_remote_url,\n     is_torch_available,\n     logging,\n )\n@@ -659,9 +657,6 @@ def _get_config_dict(\n             # Special case when pretrained_model_name_or_path is a local file\n             resolved_config_file = pretrained_model_name_or_path\n             is_local = True\n-        elif is_remote_url(pretrained_model_name_or_path):\n-            configuration_file = pretrained_model_name_or_path if gguf_file is None else gguf_file\n-            resolved_config_file = download_url(pretrained_model_name_or_path)\n         else:\n             configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n "
        },
        {
            "sha": "7d9dda2736365724186286a26e565f29d689dc70",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -30,10 +30,8 @@\n     PushToHubMixin,\n     TensorType,\n     copy_func,\n-    download_url,\n     is_numpy_array,\n     is_offline_mode,\n-    is_remote_url,\n     is_torch_available,\n     is_torch_device,\n     is_torch_dtype,\n@@ -430,10 +428,6 @@ def get_feature_extractor_dict(\n             resolved_feature_extractor_file = pretrained_model_name_or_path\n             resolved_processor_file = None\n             is_local = True\n-        elif is_remote_url(pretrained_model_name_or_path):\n-            feature_extractor_file = pretrained_model_name_or_path\n-            resolved_processor_file = None\n-            resolved_feature_extractor_file = download_url(pretrained_model_name_or_path)\n         else:\n             feature_extractor_file = FEATURE_EXTRACTOR_NAME\n             try:"
        },
        {
            "sha": "25000e5adc30bd6b090d12fbe2fcc3e34e8b54b2",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -29,9 +29,7 @@\n     ExplicitEnum,\n     PushToHubMixin,\n     cached_file,\n-    download_url,\n     extract_commit_hash,\n-    is_remote_url,\n     is_torch_available,\n     logging,\n )\n@@ -879,9 +877,6 @@ def from_pretrained(\n             # Special case when config_path is a local file\n             resolved_config_file = config_path\n             is_local = True\n-        elif is_remote_url(config_path):\n-            configuration_file = config_path\n-            resolved_config_file = download_url(config_path)\n         else:\n             configuration_file = config_file_name\n             try:"
        },
        {
            "sha": "2c1207fd8ca2564afe355ccde23306ac14868141",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -28,9 +28,7 @@\n     PROCESSOR_NAME,\n     PushToHubMixin,\n     copy_func,\n-    download_url,\n     is_offline_mode,\n-    is_remote_url,\n     logging,\n     safe_load_json_file,\n )\n@@ -283,10 +281,6 @@ def get_image_processor_dict(\n             resolved_image_processor_file = pretrained_model_name_or_path\n             resolved_processor_file = None\n             is_local = True\n-        elif is_remote_url(pretrained_model_name_or_path):\n-            image_processor_file = pretrained_model_name_or_path\n-            resolved_processor_file = None\n-            resolved_image_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n             image_processor_file = image_processor_filename\n             try:"
        },
        {
            "sha": "8c465011be8ba692fe551d99ca3aed689b8b571b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -105,14 +105,12 @@\n     cached_file,\n     check_torch_load_is_safe,\n     copy_func,\n-    download_url,\n     has_file,\n     is_accelerate_available,\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n     is_kernels_available,\n     is_offline_mode,\n-    is_remote_url,\n     is_torch_flex_attn_available,\n     is_torch_greater_or_equal,\n     is_torch_mlu_available,\n@@ -531,9 +529,6 @@ def _get_resolved_checkpoint_files(\n         elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n             archive_file = pretrained_model_name_or_path\n             is_local = True\n-        elif is_remote_url(pretrained_model_name_or_path):\n-            filename = pretrained_model_name_or_path\n-            resolved_archive_file = download_url(pretrained_model_name_or_path)\n         else:\n             # set correct filename\n             if transformers_explicit_filename is not None:"
        },
        {
            "sha": "0c67b3258d0e49e02b5a21a4c589ec0a50f42519",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -53,9 +53,7 @@\n     cached_file,\n     copy_func,\n     direct_transformers_import,\n-    download_url,\n     is_offline_mode,\n-    is_remote_url,\n     is_torch_available,\n     list_repo_templates,\n     logging,\n@@ -940,13 +938,6 @@ def get_processor_dict(\n             resolved_raw_chat_template_file = None\n             resolved_audio_tokenizer_file = None\n             is_local = True\n-        elif is_remote_url(pretrained_model_name_or_path):\n-            processor_file = pretrained_model_name_or_path\n-            resolved_processor_file = download_url(pretrained_model_name_or_path)\n-            # can't load chat-template and audio tokenizer when given a file url as pretrained_model_name_or_path\n-            resolved_chat_template_file = None\n-            resolved_raw_chat_template_file = None\n-            resolved_audio_tokenizer_file = None\n         else:\n             if is_local:\n                 template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)"
        },
        {
            "sha": "5e6b24b16c73930113bb41cd8a70093c2244aa7b",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 68,
            "deletions": 89,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -48,13 +48,11 @@\n     add_end_docstrings,\n     cached_file,\n     copy_func,\n-    download_url,\n     extract_commit_hash,\n     is_mlx_available,\n     is_numpy_array,\n     is_offline_mode,\n     is_protobuf_available,\n-    is_remote_url,\n     is_tokenizers_available,\n     is_torch_available,\n     is_torch_device,\n@@ -2010,94 +2008,77 @@ def from_pretrained(\n \n         is_local = os.path.isdir(pretrained_model_name_or_path)\n         single_file_id = None\n-        if os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n-            if len(cls.vocab_files_names) > 1 and not gguf_file:\n-                raise ValueError(\n-                    f\"Calling {cls.__name__}.from_pretrained() with the path to a single file or url is not \"\n-                    \"supported for this tokenizer. Use a model identifier or the path to a directory instead.\"\n-                )\n-            warnings.warn(\n-                f\"Calling {cls.__name__}.from_pretrained() with the path to a single file or url is deprecated and \"\n-                \"won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\",\n-                FutureWarning,\n-            )\n-            file_id = list(cls.vocab_files_names.keys())[0]\n \n-            vocab_files[file_id] = pretrained_model_name_or_path\n-            single_file_id = file_id\n+        if gguf_file:\n+            vocab_files[\"vocab_file\"] = gguf_file\n         else:\n-            if gguf_file:\n-                vocab_files[\"vocab_file\"] = gguf_file\n-            else:\n-                # At this point pretrained_model_name_or_path is either a directory or a model identifier name\n-                additional_files_names = {\n-                    \"added_tokens_file\": ADDED_TOKENS_FILE,  # kept only for legacy\n-                    \"special_tokens_map_file\": SPECIAL_TOKENS_MAP_FILE,  # kept only for legacy\n-                    \"tokenizer_config_file\": TOKENIZER_CONFIG_FILE,\n-                    # tokenizer_file used to initialize a slow from a fast. Properly copy the `addedTokens` instead of adding in random orders\n-                    \"tokenizer_file\": FULL_TOKENIZER_FILE,\n-                    \"chat_template_file\": CHAT_TEMPLATE_FILE,\n-                }\n-\n-                vocab_files = {**cls.vocab_files_names, **additional_files_names}\n-                if \"tokenizer_file\" in vocab_files:\n-                    # Try to get the tokenizer config to see if there are versioned tokenizer files.\n-                    fast_tokenizer_file = FULL_TOKENIZER_FILE\n-\n-                    try:\n-                        resolved_config_file = cached_file(\n-                            pretrained_model_name_or_path,\n-                            TOKENIZER_CONFIG_FILE,\n-                            cache_dir=cache_dir,\n-                            force_download=force_download,\n-                            proxies=proxies,\n-                            token=token,\n-                            revision=revision,\n-                            local_files_only=local_files_only,\n-                            subfolder=subfolder,\n-                            user_agent=user_agent,\n-                            _raise_exceptions_for_missing_entries=False,\n-                            _commit_hash=commit_hash,\n-                        )\n-                    except OSError:\n-                        # Re-raise any error raised by cached_file in order to get a helpful error message\n-                        raise\n-                    except Exception:\n-                        # For any other exception, we throw a generic error.\n-                        raise OSError(\n-                            f\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n-                            \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n-                            f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n-                            f\"containing all relevant files for a {cls.__name__} tokenizer.\"\n-                        )\n+            # At this point pretrained_model_name_or_path is either a directory or a model identifier name\n+            additional_files_names = {\n+                \"added_tokens_file\": ADDED_TOKENS_FILE,  # kept only for legacy\n+                \"special_tokens_map_file\": SPECIAL_TOKENS_MAP_FILE,  # kept only for legacy\n+                \"tokenizer_config_file\": TOKENIZER_CONFIG_FILE,\n+                # tokenizer_file used to initialize a slow from a fast. Properly copy the `addedTokens` instead of adding in random orders\n+                \"tokenizer_file\": FULL_TOKENIZER_FILE,\n+                \"chat_template_file\": CHAT_TEMPLATE_FILE,\n+            }\n \n-                    commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n-                    if resolved_config_file is not None:\n-                        with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-                            tokenizer_config = json.load(reader)\n-                            if \"fast_tokenizer_files\" in tokenizer_config:\n-                                fast_tokenizer_file = get_fast_tokenizer_file(tokenizer_config[\"fast_tokenizer_files\"])\n-                    vocab_files[\"tokenizer_file\"] = fast_tokenizer_file\n-\n-                    # This block looks for any extra chat template files\n-                    if is_local:\n-                        template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n-                        if template_dir.is_dir():\n-                            for template_file in template_dir.glob(\"*.jinja\"):\n-                                template_name = template_file.name.removesuffix(\".jinja\")\n-                                vocab_files[f\"chat_template_{template_name}\"] = (\n-                                    f\"{CHAT_TEMPLATE_DIR}/{template_file.name}\"\n-                                )\n-                    else:\n-                        for template in list_repo_templates(\n-                            pretrained_model_name_or_path,\n-                            local_files_only=local_files_only,\n-                            revision=revision,\n-                            cache_dir=cache_dir,\n-                            token=token,\n-                        ):\n-                            template = template.removesuffix(\".jinja\")\n-                            vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n+            vocab_files = {**cls.vocab_files_names, **additional_files_names}\n+            if \"tokenizer_file\" in vocab_files:\n+                # Try to get the tokenizer config to see if there are versioned tokenizer files.\n+                fast_tokenizer_file = FULL_TOKENIZER_FILE\n+\n+                try:\n+                    resolved_config_file = cached_file(\n+                        pretrained_model_name_or_path,\n+                        TOKENIZER_CONFIG_FILE,\n+                        cache_dir=cache_dir,\n+                        force_download=force_download,\n+                        proxies=proxies,\n+                        token=token,\n+                        revision=revision,\n+                        local_files_only=local_files_only,\n+                        subfolder=subfolder,\n+                        user_agent=user_agent,\n+                        _raise_exceptions_for_missing_entries=False,\n+                        _commit_hash=commit_hash,\n+                    )\n+                except OSError:\n+                    # Re-raise any error raised by cached_file in order to get a helpful error message\n+                    raise\n+                except Exception:\n+                    # For any other exception, we throw a generic error.\n+                    raise OSError(\n+                        f\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n+                        \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n+                        f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n+                        f\"containing all relevant files for a {cls.__name__} tokenizer.\"\n+                    )\n+\n+                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n+                if resolved_config_file is not None:\n+                    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n+                        tokenizer_config = json.load(reader)\n+                        if \"fast_tokenizer_files\" in tokenizer_config:\n+                            fast_tokenizer_file = get_fast_tokenizer_file(tokenizer_config[\"fast_tokenizer_files\"])\n+                vocab_files[\"tokenizer_file\"] = fast_tokenizer_file\n+\n+                # This block looks for any extra chat template files\n+                if is_local:\n+                    template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n+                    if template_dir.is_dir():\n+                        for template_file in template_dir.glob(\"*.jinja\"):\n+                            template_name = template_file.name.removesuffix(\".jinja\")\n+                            vocab_files[f\"chat_template_{template_name}\"] = f\"{CHAT_TEMPLATE_DIR}/{template_file.name}\"\n+                else:\n+                    for template in list_repo_templates(\n+                        pretrained_model_name_or_path,\n+                        local_files_only=local_files_only,\n+                        revision=revision,\n+                        cache_dir=cache_dir,\n+                        token=token,\n+                    ):\n+                        template = template.removesuffix(\".jinja\")\n+                        vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n \n         remote_files = []\n         if not is_local and not local_files_only:\n@@ -2122,8 +2103,6 @@ def from_pretrained(\n             elif single_file_id == file_id:\n                 if os.path.isfile(file_path):\n                     resolved_vocab_files[file_id] = file_path\n-                elif is_remote_url(file_path):\n-                    resolved_vocab_files[file_id] = download_url(file_path, proxies=proxies)\n             else:\n                 try:\n                     resolved_vocab_files[file_id] = cached_file("
        },
        {
            "sha": "f916d7673e110740d33e896285c8758880359bd1",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -95,12 +95,10 @@\n     cached_file,\n     default_cache_path,\n     define_sagemaker_information,\n-    download_url,\n     extract_commit_hash,\n     has_file,\n     http_user_agent,\n     is_offline_mode,\n-    is_remote_url,\n     list_repo_templates,\n     try_to_load_from_cache,\n )"
        },
        {
            "sha": "7f3b2ed53c8dbce3a87b11ceaa822c02d65d2256",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 2,
            "deletions": 41,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -19,12 +19,10 @@\n import os\n import re\n import sys\n-import tempfile\n import warnings\n from concurrent import futures\n from pathlib import Path\n from typing import TypedDict\n-from urllib.parse import urlparse\n from uuid import uuid4\n \n import httpx\n@@ -44,7 +42,7 @@\n     snapshot_download,\n     try_to_load_from_cache,\n )\n-from huggingface_hub.file_download import REGEX_COMMIT_HASH, http_get\n+from huggingface_hub.file_download import REGEX_COMMIT_HASH\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n     GatedRepoError,\n@@ -60,12 +58,7 @@\n \n from . import __version__, logging\n from .generic import working_or_temp_dir\n-from .import_utils import (\n-    ENV_VARS_TRUE_VALUES,\n-    get_torch_version,\n-    is_torch_available,\n-    is_training_run_on_sagemaker,\n-)\n+from .import_utils import ENV_VARS_TRUE_VALUES, get_torch_version, is_torch_available, is_training_run_on_sagemaker\n \n \n LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE = \"chat_template.json\"\n@@ -202,11 +195,6 @@ def list_repo_templates(\n     return [entry.stem for entry in templates_dir.iterdir() if entry.is_file() and entry.name.endswith(\".jinja\")]\n \n \n-def is_remote_url(url_or_filename):\n-    parsed = urlparse(url_or_filename)\n-    return parsed.scheme in (\"http\", \"https\")\n-\n-\n def define_sagemaker_information():\n     try:\n         instance_data = httpx.get(os.environ[\"ECS_CONTAINER_METADATA_URI\"]).json()\n@@ -583,33 +571,6 @@ def cached_files(\n     return resolved_files\n \n \n-def download_url(url, proxies=None):\n-    \"\"\"\n-    Downloads a given url in a temporary file. This function is not safe to use in multiple processes. Its only use is\n-    for deprecated behavior allowing to download config/models with a single url instead of using the Hub.\n-\n-    Args:\n-        url (`str`): The url of the file to download.\n-        proxies (`dict[str, str]`, *optional*):\n-            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n-            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n-\n-    Returns:\n-        `str`: The location of the temporary file where the url was downloaded.\n-    \"\"\"\n-    warnings.warn(\n-        f\"Using `from_pretrained` with the url of a file (here {url}) is deprecated and won't be possible anymore in\"\n-        \" v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note\"\n-        \" that this is not compatible with the caching system (your file will be downloaded at each execution) or\"\n-        \" multiple processes (each process will download the file in a different temporary file).\",\n-        FutureWarning,\n-    )\n-    tmp_fd, tmp_file = tempfile.mkstemp()\n-    with os.fdopen(tmp_fd, \"wb\") as f:\n-        http_get(url, f, proxies=proxies)\n-    return tmp_file\n-\n-\n def has_file(\n     path_or_repo: str | os.PathLike,\n     filename: str,"
        },
        {
            "sha": "77836a0bf1ad5f5d0bebc5add579afaab115db6d",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -43,9 +43,7 @@\n     TensorType,\n     add_start_docstrings,\n     copy_func,\n-    download_url,\n     is_offline_mode,\n-    is_remote_url,\n     is_torch_available,\n     is_torchcodec_available,\n     is_torchvision_v2_available,\n@@ -638,10 +636,6 @@ def get_video_processor_dict(\n             resolved_video_processor_file = pretrained_model_name_or_path\n             resolved_processor_file = None\n             is_local = True\n-        elif is_remote_url(pretrained_model_name_or_path):\n-            video_processor_file = pretrained_model_name_or_path\n-            resolved_processor_file = None\n-            resolved_video_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n             video_processor_file = VIDEO_PROCESSOR_NAME\n             try:"
        },
        {
            "sha": "4a6a03f813e6687c593245e42324cf6e87480fb4",
            "filename": "tests/utils/test_image_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 54,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/tests%2Futils%2Ftest_image_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/tests%2Futils%2Ftest_image_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -11,17 +11,13 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import codecs\n-import os\n-import tempfile\n import unittest\n-from io import BytesIO\n \n import httpx\n import numpy as np\n import pytest\n-from huggingface_hub.file_download import hf_hub_url, http_get\n+from huggingface_hub.file_download import hf_hub_download\n \n from tests.pipelines.test_pipelines_document_question_answering import INVOICE_URL\n from transformers import is_torch_available, is_vision_available\n@@ -46,8 +42,8 @@\n \n \n def get_image_from_hub_dataset(dataset_id: str, filename: str, revision: str | None = None) -> \"PIL.Image.Image\":\n-    url = hf_hub_url(dataset_id, filename, repo_type=\"dataset\", revision=revision)\n-    return PIL.Image.open(BytesIO(httpx.get(url, follow_redirects=True).content))\n+    path = hf_hub_download(dataset_id, filename, repo_type=\"dataset\", revision=revision)\n+    return PIL.Image.open(path)\n \n \n def get_random_image(height, width):\n@@ -738,54 +734,30 @@ def test_load_img_local(self):\n         )\n \n     def test_load_img_base64_prefix(self):\n-        try:\n-            tmp_file = tempfile.NamedTemporaryFile(delete=False).name\n-            with open(tmp_file, \"wb\") as f:\n-                http_get(\n-                    \"https://huggingface.co/datasets/hf-internal-testing/dummy-base64-images/raw/main/image_0.txt\", f\n-                )\n-\n-            with open(tmp_file, encoding=\"utf-8\") as b64:\n-                img = load_image(b64.read())\n-                img_arr = np.array(img)\n-\n-        finally:\n-            os.remove(tmp_file)\n-\n+        path = hf_hub_download(\n+            repo_id=\"hf-internal-testing/dummy-base64-images\", filename=\"image_0.txt\", repo_type=\"dataset\"\n+        )\n+        with open(path, encoding=\"utf-8\") as b64:\n+            img = load_image(b64.read())\n+            img_arr = np.array(img)\n         self.assertEqual(img_arr.shape, (64, 32, 3))\n \n     def test_load_img_base64(self):\n-        try:\n-            tmp_file = tempfile.NamedTemporaryFile(delete=False).name\n-            with open(tmp_file, \"wb\") as f:\n-                http_get(\n-                    \"https://huggingface.co/datasets/hf-internal-testing/dummy-base64-images/raw/main/image_1.txt\", f\n-                )\n-\n-            with open(tmp_file, encoding=\"utf-8\") as b64:\n-                img = load_image(b64.read())\n-                img_arr = np.array(img)\n-\n-        finally:\n-            os.remove(tmp_file)\n-\n+        path = hf_hub_download(\n+            repo_id=\"hf-internal-testing/dummy-base64-images\", filename=\"image_1.txt\", repo_type=\"dataset\"\n+        )\n+        with open(path, encoding=\"utf-8\") as b64:\n+            img = load_image(b64.read())\n+            img_arr = np.array(img)\n         self.assertEqual(img_arr.shape, (64, 32, 3))\n \n     def test_load_img_base64_encoded_bytes(self):\n-        try:\n-            tmp_file = tempfile.NamedTemporaryFile(delete=False).name\n-            with open(tmp_file, \"wb\") as f:\n-                http_get(\n-                    \"https://huggingface.co/datasets/hf-internal-testing/dummy-base64-images/raw/main/image_2.txt\", f\n-                )\n-\n-            with codecs.open(tmp_file, encoding=\"unicode_escape\") as b64:\n-                img = load_image(b64.read())\n-                img_arr = np.array(img)\n-\n-        finally:\n-            os.remove(tmp_file)\n-\n+        path = hf_hub_download(\n+            repo_id=\"hf-internal-testing/dummy-base64-images\", filename=\"image_2.txt\", repo_type=\"dataset\"\n+        )\n+        with codecs.open(path, encoding=\"unicode_escape\") as b64:\n+            img = load_image(b64.read())\n+            img_arr = np.array(img)\n         self.assertEqual(img_arr.shape, (256, 256, 3))\n \n     def test_load_img_rgba(self):\n@@ -797,11 +769,7 @@ def test_load_img_rgba(self):\n \n         img = load_image(img)  # img with mode RGBA\n         img_arr = np.array(img)\n-\n-        self.assertEqual(\n-            img_arr.shape,\n-            (512, 512, 3),\n-        )\n+        self.assertEqual(img_arr.shape, (512, 512, 3))\n \n     def test_load_img_la(self):\n         # we use revision=\"refs/pr/1\" until the PR is merged"
        },
        {
            "sha": "d34b224068e11b81b5c5265cfd0cfdbf74fb859f",
            "filename": "tests/utils/test_tokenization_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 37,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/tests%2Futils%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1ab70974d87f0aac43a92cf8cdf489ac2e6d485/tests%2Futils%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_tokenization_utils.py?ref=e1ab70974d87f0aac43a92cf8cdf489ac2e6d485",
            "patch": "@@ -20,16 +20,8 @@\n from pathlib import Path\n \n import httpx\n-from huggingface_hub.file_download import http_get\n-\n-from transformers import (\n-    AlbertTokenizer,\n-    AutoTokenizer,\n-    BertTokenizer,\n-    BertTokenizerFast,\n-    GPT2TokenizerFast,\n-    is_tokenizers_available,\n-)\n+\n+from transformers import AutoTokenizer, BertTokenizer, BertTokenizerFast, GPT2TokenizerFast, is_tokenizers_available\n from transformers.testing_utils import TOKEN, TemporaryHubRepo, is_staging_test, require_tokenizers\n from transformers.tokenization_utils import ExtensionsTrie, Trie\n \n@@ -83,33 +75,6 @@ def test_cached_files_are_used_when_internet_is_down_missing_files(self):\n             # This check we did call the fake head request\n             mock_head.assert_called()\n \n-    def test_legacy_load_from_one_file(self):\n-        # This test is for deprecated behavior and can be removed in v5\n-        try:\n-            tmp_file = tempfile.NamedTemporaryFile(delete=False).name\n-            with open(tmp_file, \"wb\") as f:\n-                http_get(\"https://huggingface.co/albert/albert-base-v1/resolve/main/spiece.model\", f)\n-\n-            _ = AlbertTokenizer.from_pretrained(tmp_file)\n-        finally:\n-            os.remove(tmp_file)\n-\n-        # Supporting this legacy load introduced a weird bug where the tokenizer would load local files if they are in\n-        # the current folder and have the right name.\n-        if os.path.isfile(\"tokenizer.json\"):\n-            # We skip the test if the user has a `tokenizer.json` in this folder to avoid deleting it.\n-            self.skipTest(reason=\"Skipping test as there is a `tokenizer.json` file in the current folder.\")\n-        try:\n-            with open(\"tokenizer.json\", \"wb\") as f:\n-                http_get(\"https://huggingface.co/hf-internal-testing/tiny-random-bert/blob/main/tokenizer.json\", f)\n-            tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n-            # The tiny random BERT has a vocab size of 1024, tiny openai-community/gpt2 as a vocab size of 1000\n-            self.assertEqual(tokenizer.vocab_size, 1000)\n-            # Tokenizer should depend on the remote checkpoint, not the local tokenizer.json file.\n-\n-        finally:\n-            os.remove(\"tokenizer.json\")\n-\n \n @is_staging_test\n class TokenizerPushToHubTester(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 359,
        "additions": 94,
        "deletions": 265
    }
}