{
    "author": "Rocketknight1",
    "message": "Delete irrelevant test that sometimes fails (#42515)\n\n* Delete irrelevant test that sometimes fails\n\n* make fixup",
    "sha": "305f82a176ace46166c0bc8f25530b7d15e5c8dd",
    "files": [
        {
            "sha": "df069444f44719fea06c8e9748d11fde93deb2bf",
            "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/305f82a176ace46166c0bc8f25530b7d15e5c8dd/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/305f82a176ace46166c0bc8f25530b7d15e5c8dd/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py?ref=305f82a176ace46166c0bc8f25530b7d15e5c8dd",
            "patch": "@@ -19,7 +19,6 @@\n import unittest\n \n from transformers import AutoTokenizer\n-from transformers.models.bert.tokenization_bert import BertTokenizer\n from transformers.models.bert_japanese.tokenization_bert_japanese import (\n     VOCAB_FILES_NAMES,\n     BertJapaneseTokenizer,\n@@ -447,25 +446,3 @@ def test_tokenizer_bert_japanese(self):\n         EXAMPLE_BERT_JAPANESE_ID = \"cl-tohoku/bert-base-japanese\"\n         tokenizer = AutoTokenizer.from_pretrained(EXAMPLE_BERT_JAPANESE_ID)\n         self.assertIsInstance(tokenizer, BertJapaneseTokenizer)\n-\n-\n-class BertTokenizerMismatchTest(unittest.TestCase):\n-    def test_tokenizer_mismatch_warning(self):\n-        EXAMPLE_BERT_JAPANESE_ID = \"cl-tohoku/bert-base-japanese\"\n-        with self.assertLogs(\"transformers\", level=\"WARNING\") as cm:\n-            BertTokenizer.from_pretrained(EXAMPLE_BERT_JAPANESE_ID)\n-            self.assertTrue(\n-                cm.records[0].message.startswith(\n-                    \"The tokenizer class you load from this checkpoint is not the same type as the class this function\"\n-                    \" is called from.\"\n-                )\n-            )\n-        EXAMPLE_BERT_ID = \"google-bert/bert-base-cased\"\n-        with self.assertLogs(\"transformers\", level=\"WARNING\") as cm:\n-            BertJapaneseTokenizer.from_pretrained(EXAMPLE_BERT_ID)\n-            self.assertTrue(\n-                cm.records[0].message.startswith(\n-                    \"The tokenizer class you load from this checkpoint is not the same type as the class this function\"\n-                    \" is called from.\"\n-                )\n-            )"
        },
        {
            "sha": "c99ef54c2a0732f8299a3e01b5c3db8b5d9e6719",
            "filename": "tests/tokenization/test_tokenization_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/305f82a176ace46166c0bc8f25530b7d15e5c8dd/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/305f82a176ace46166c0bc8f25530b7d15e5c8dd/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_fast.py?ref=305f82a176ace46166c0bc8f25530b7d15e5c8dd",
            "patch": "@@ -86,12 +86,6 @@ def _create_test_tokenizers(cls):\n \n         return paths\n \n-    @unittest.skip(\n-        \"We disable this test for PreTrainedTokenizerFast because it is the only tokenizer that is not linked to any model\"\n-    )\n-    def test_tokenizer_mismatch_warning(self):\n-        pass\n-\n     @unittest.skip(\n         \"We disable this test for PreTrainedTokenizerFast because it is the only tokenizer that is not linked to any model\"\n     )"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 0,
        "deletions": 29
    }
}