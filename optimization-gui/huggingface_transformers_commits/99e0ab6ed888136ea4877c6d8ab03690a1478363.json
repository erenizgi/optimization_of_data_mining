{
    "author": "hiroaki222",
    "message": "Fix typo in /docs/source/ja/model_doc/decision_transformer.md URL (#35705)\n\ndoc: Update original code repository URL",
    "sha": "99e0ab6ed888136ea4877c6d8ab03690a1478363",
    "files": [
        {
            "sha": "fe37feb5a35d5460468ee8da393c116992da5660",
            "filename": "docs/source/ja/model_doc/decision_transformer.md",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/99e0ab6ed888136ea4877c6d8ab03690a1478363/docs%2Fsource%2Fja%2Fmodel_doc%2Fdecision_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/99e0ab6ed888136ea4877c6d8ab03690a1478363/docs%2Fsource%2Fja%2Fmodel_doc%2Fdecision_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdecision_transformer.md?ref=99e0ab6ed888136ea4877c6d8ab03690a1478363",
            "patch": "@@ -23,31 +23,28 @@ Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laski\n \n 論文の要約は次のとおりです。\n \n-*強化学習（RL）をシーケンスモデリング問題として抽象化するフレームワークを紹介します。\n+_強化学習（RL）をシーケンスモデリング問題として抽象化するフレームワークを紹介します。\n これにより、Transformer アーキテクチャのシンプルさとスケーラビリティ、および関連する進歩を活用できるようになります。\n- GPT-x や BERT などの言語モデリングで。特に、Decision Transformer というアーキテクチャを紹介します。\n- RL の問題を条件付きシーケンス モデリングとして投げかけます。値関数に適合する以前の RL アプローチとは異なり、\n- ポリシー勾配を計算すると、Decision Transformer は因果的にマスクされたアルゴリズムを利用して最適なアクションを出力するだけです。\n- 変成器。望ましいリターン (報酬)、過去の状態、アクションに基づいて自己回帰モデルを条件付けすることにより、\n- Decision Transformer モデルは、望ましいリターンを達成する将来のアクションを生成できます。そのシンプルさにも関わらず、\n- Decision Transformer は、最先端のモデルフリーのオフライン RL ベースラインのパフォーマンスと同等、またはそれを超えています。\n- Atari、OpenAI Gym、Key-to-Door タスク*\n+GPT-x や BERT などの言語モデリングで。特に、Decision Transformer というアーキテクチャを紹介します。\n+RL の問題を条件付きシーケンス モデリングとして投げかけます。値関数に適合する以前の RL アプローチとは異なり、\n+ポリシー勾配を計算すると、Decision Transformer は因果的にマスクされたアルゴリズムを利用して最適なアクションを出力するだけです。\n+変成器。望ましいリターン (報酬)、過去の状態、アクションに基づいて自己回帰モデルを条件付けすることにより、\n+Decision Transformer モデルは、望ましいリターンを達成する将来のアクションを生成できます。そのシンプルさにも関わらず、\n+Decision Transformer は、最先端のモデルフリーのオフライン RL ベースラインのパフォーマンスと同等、またはそれを超えています。\n+Atari、OpenAI Gym、Key-to-Door タスク_\n \n このバージョンのモデルは、状態がベクトルであるタスク用です。\n \n-このモデルは、[edbeeching](https://huggingface.co/edbeeching) によって提供されました。元のコードは [ここ](https://github.com/kzl/decion-transformer) にあります。\n+このモデルは、[edbeeching](https://huggingface.co/edbeeching) によって提供されました。元のコードは [ここ](https://github.com/kzl/decision-transformer) にあります。\n \n ## DecisionTransformerConfig\n \n [[autodoc]] DecisionTransformerConfig\n \n-\n ## DecisionTransformerGPT2Model\n \n-[[autodoc]] DecisionTransformerGPT2Model\n-    - forward\n+[[autodoc]] DecisionTransformerGPT2Model - forward\n \n ## DecisionTransformerModel\n \n-[[autodoc]] DecisionTransformerModel\n-    - forward\n+[[autodoc]] DecisionTransformerModel - forward"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 11,
        "deletions": 14
    }
}