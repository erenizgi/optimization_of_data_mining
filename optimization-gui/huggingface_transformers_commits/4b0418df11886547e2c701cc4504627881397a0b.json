{
    "author": "zucchini-nlp",
    "message": "Enable `padding_side` as call time kwargs (#33385)\n\n* fix\r\n\r\n* add padding-side kwarg\r\n\r\n* add padding side in all models & fix tests\r\n\r\n* fix copies\r\n\r\n* fix tests",
    "sha": "4b0418df11886547e2c701cc4504627881397a0b",
    "files": [
        {
            "sha": "c5ec79666deede3824ca738fa10a369f2d8447f5",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -414,6 +414,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -517,6 +518,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -539,6 +541,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -567,6 +570,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -598,6 +602,7 @@ def batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -625,6 +630,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -653,6 +659,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -677,6 +684,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -708,6 +716,7 @@ def _batch_prepare_for_model(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -728,6 +737,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -748,6 +758,7 @@ def encode(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -769,6 +780,7 @@ def encode(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -795,6 +807,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -838,6 +851,7 @@ def encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -861,6 +875,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -891,6 +906,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -914,6 +930,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1100,6 +1117,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1243,6 +1261,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1265,6 +1284,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1288,7 +1310,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1302,7 +1325,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1317,7 +1340,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "a666e3d4ea1a43147ef750380ee6d3de11020c8c",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -165,6 +165,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -268,6 +269,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -290,6 +292,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -318,6 +321,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -349,6 +353,7 @@ def batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -381,6 +386,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -424,6 +430,7 @@ def encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -451,6 +458,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -470,6 +478,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n         )\n \n         if is_pair:\n@@ -603,6 +612,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -631,6 +641,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -663,6 +674,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -685,6 +697,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -708,7 +723,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -722,7 +738,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -737,7 +753,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "248a299c141fd58ce26b3d4c5e7ee6f6a03ac584",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -543,6 +543,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -646,6 +647,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -668,6 +670,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -697,6 +700,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -728,6 +732,7 @@ def batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -756,6 +761,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -784,6 +790,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -809,6 +816,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -840,6 +848,7 @@ def _batch_prepare_for_model(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -860,6 +869,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -881,6 +891,7 @@ def encode(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -902,6 +913,7 @@ def encode(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -929,6 +941,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -972,6 +985,7 @@ def encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -996,6 +1010,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1026,6 +1041,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -1049,6 +1065,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1237,6 +1254,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1382,6 +1400,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1404,6 +1423,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1427,7 +1449,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1441,7 +1464,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1456,6 +1479,6 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs"
        },
        {
            "sha": "63cd1022e52170762c6fccb329f4fe3f29d5ebbf",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -217,6 +217,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -320,6 +321,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -342,6 +344,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -371,6 +374,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -402,6 +406,7 @@ def batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -436,6 +441,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -479,6 +485,7 @@ def encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -506,6 +513,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -525,6 +533,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n         )\n \n         if is_pair:\n@@ -664,6 +673,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -692,6 +702,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -725,6 +736,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -747,6 +759,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -770,7 +785,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -784,7 +800,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -799,7 +815,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "248f16af8441c16eadb41ea6bf1065de35f4d770",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 3,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -447,6 +447,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -550,6 +551,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -572,6 +574,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -599,6 +602,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -627,6 +631,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -651,6 +656,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -682,6 +688,7 @@ def _batch_prepare_for_model(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -702,6 +709,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -721,6 +729,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -751,6 +760,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -774,6 +784,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -947,6 +958,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1090,6 +1102,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1112,6 +1125,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1135,7 +1151,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1149,7 +1166,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1164,6 +1181,6 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs"
        },
        {
            "sha": "7d12cec496ea30e2e76a7afaacc7fb69077fad21",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -277,6 +277,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -380,6 +381,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -402,6 +404,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -442,6 +445,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -462,6 +466,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n         )\n \n         if is_pair:\n@@ -595,6 +600,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -623,6 +629,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -655,6 +662,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -677,6 +685,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -700,7 +711,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -714,7 +726,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -729,7 +741,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "6c1ec9526aefbf12b0e60a2b2dab866612d2bf86",
            "filename": "src/transformers/models/led/tokenization_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -412,13 +412,15 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         encoded_inputs = super()._pad(\n             encoded_inputs=encoded_inputs,\n             max_length=max_length,\n             padding_strategy=padding_strategy,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n "
        },
        {
            "sha": "6ee69fbe7927524ba328a560e304ae1ce7205bd2",
            "filename": "src/transformers/models/led/tokenization_led_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -288,13 +288,15 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         encoded_inputs = super()._pad(\n             encoded_inputs=encoded_inputs,\n             max_length=max_length,\n             padding_strategy=padding_strategy,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n "
        },
        {
            "sha": "e06b9c753fe59682923954bc566ea83540253c2f",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -570,6 +570,7 @@ def __call__(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -662,6 +663,7 @@ def __call__(\n                 stride=stride,\n                 is_split_into_words=is_split_into_words,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -688,6 +690,7 @@ def __call__(\n                 stride=stride,\n                 is_split_into_words=is_split_into_words,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -715,6 +718,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -769,6 +773,7 @@ def _encode_plus(\n             max_entity_length=max_entity_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -796,6 +801,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -876,6 +882,7 @@ def _batch_encode_plus(\n             max_entity_length=max_entity_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -1070,6 +1077,7 @@ def _batch_prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1112,6 +1120,7 @@ def _batch_prepare_for_model(\n                 max_entity_length=max_entity_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -1132,6 +1141,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -1155,6 +1165,7 @@ def prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1357,6 +1368,7 @@ def prepare_for_model(\n                 max_entity_length=max_entity_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1382,6 +1394,7 @@ def pad(\n         max_length: Optional[int] = None,\n         max_entity_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n@@ -1418,6 +1431,9 @@ def pad(\n             pad_to_multiple_of (`int`, *optional*):\n                 If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n                 the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask (`bool`, *optional*):\n                 Whether to return the attention mask. If left to the default, will return the attention mask according\n                 to the specific tokenizer's default, defined by the `return_outputs` attribute. [What are attention\n@@ -1495,6 +1511,7 @@ def pad(\n                 max_entity_length=max_entity_length,\n                 padding_strategy=padding_strategy,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n             return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n@@ -1519,6 +1536,7 @@ def pad(\n                 max_entity_length=max_entity_length,\n                 padding_strategy=padding_strategy,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1536,6 +1554,7 @@ def _pad(\n         max_entity_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1562,6 +1581,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1600,9 +1622,10 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(encoded_inputs[\"input_ids\"])\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n             if entities_provided:\n                 entity_difference = max_entity_length - len(encoded_inputs[\"entity_ids\"])\n-            if self.padding_side == \"right\":\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                     if entities_provided:\n@@ -1633,7 +1656,7 @@ def _pad(\n                             encoded_inputs[\"entity_end_positions\"] + [0] * entity_difference\n                         )\n \n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                     if entities_provided:\n@@ -1664,7 +1687,7 @@ def _pad(\n                             \"entity_end_positions\"\n                         ]\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "e5de1e4e765c93bc33f8f20ad4759f2692eeb850",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -503,6 +503,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -602,6 +603,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -624,6 +626,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -652,6 +655,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -683,6 +687,7 @@ def batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -710,6 +715,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -738,6 +744,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -762,6 +769,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -793,6 +801,7 @@ def _batch_prepare_for_model(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -813,6 +822,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -833,6 +843,7 @@ def encode(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -854,6 +865,7 @@ def encode(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -880,6 +892,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -923,6 +936,7 @@ def encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -946,6 +960,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -976,6 +991,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -999,6 +1015,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1203,6 +1220,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1357,6 +1375,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1376,6 +1395,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1399,7 +1421,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1419,7 +1442,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1440,6 +1463,6 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs"
        },
        {
            "sha": "796459876425b4e0c1a4b6e22e9980ba26d40113",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -286,6 +286,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -385,6 +386,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -407,6 +409,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -435,6 +438,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -466,6 +470,7 @@ def batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -498,6 +503,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -541,6 +547,7 @@ def encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -568,6 +575,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -587,6 +595,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n         )\n \n         if is_pair:\n@@ -721,6 +730,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -749,6 +759,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -781,6 +792,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -800,6 +812,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -823,7 +838,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -843,7 +859,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -864,7 +880,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "f087c0d92fc63f3fbbcb688445aed9a3ce4f2996",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -399,6 +399,7 @@ def __call__(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -491,6 +492,7 @@ def __call__(\n                 stride=stride,\n                 is_split_into_words=is_split_into_words,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -517,6 +519,7 @@ def __call__(\n                 stride=stride,\n                 is_split_into_words=is_split_into_words,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -545,6 +548,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -599,6 +603,7 @@ def _encode_plus(\n             max_entity_length=max_entity_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -627,6 +632,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -707,6 +713,7 @@ def _batch_encode_plus(\n             max_entity_length=max_entity_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -904,6 +911,7 @@ def _batch_prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -946,6 +954,7 @@ def _batch_prepare_for_model(\n                 max_entity_length=max_entity_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -966,6 +975,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -990,6 +1000,7 @@ def prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1192,6 +1203,7 @@ def prepare_for_model(\n                 max_entity_length=max_entity_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1218,6 +1230,7 @@ def pad(\n         max_length: Optional[int] = None,\n         max_entity_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n@@ -1254,6 +1267,9 @@ def pad(\n             pad_to_multiple_of (`int`, *optional*):\n                 If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n                 the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask (`bool`, *optional*):\n                 Whether to return the attention mask. If left to the default, will return the attention mask according\n                 to the specific tokenizer's default, defined by the `return_outputs` attribute. [What are attention\n@@ -1331,6 +1347,7 @@ def pad(\n                 max_entity_length=max_entity_length,\n                 padding_strategy=padding_strategy,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n             return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n@@ -1355,6 +1372,7 @@ def pad(\n                 max_entity_length=max_entity_length,\n                 padding_strategy=padding_strategy,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1373,6 +1391,7 @@ def _pad(\n         max_entity_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1399,6 +1418,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1437,9 +1459,10 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(encoded_inputs[\"input_ids\"])\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n             if entities_provided:\n                 entity_difference = max_entity_length - len(encoded_inputs[\"entity_ids\"])\n-            if self.padding_side == \"right\":\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                     if entities_provided:\n@@ -1470,7 +1493,7 @@ def _pad(\n                             encoded_inputs[\"entity_end_positions\"] + [0] * entity_difference\n                         )\n \n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                     if entities_provided:\n@@ -1501,7 +1524,7 @@ def _pad(\n                             \"entity_end_positions\"\n                         ]\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "3a980c0ae66f68e239f7415c64f63892f7208663",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -210,6 +210,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -283,6 +284,7 @@ def get_input_ids(text):\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -308,6 +310,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -462,6 +465,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -480,6 +484,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         # Load from model defaults\n@@ -502,8 +507,9 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n \n-            if self.padding_side == \"right\":\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -516,7 +522,7 @@ def _pad(\n                     if key in encoded_inputs:\n                         encoded_inputs[key] = encoded_inputs[key] + [self.pad_token_id] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -530,7 +536,7 @@ def _pad(\n                         encoded_inputs[key] = [self.pad_token_id] * difference + encoded_inputs[key]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n \n@@ -551,6 +557,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -627,6 +634,7 @@ def get_input_ids(text):\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -650,6 +658,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -686,6 +695,7 @@ def _batch_prepare_for_model(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -706,6 +716,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n "
        },
        {
            "sha": "867e53ff89078a246419028e910180448c1e2c64",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 24,
            "deletions": 3,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -517,6 +517,7 @@ def __call__(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -581,6 +582,7 @@ def __call__(\n                 truncation=truncation,\n                 max_length=max_length,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -602,6 +604,7 @@ def __call__(\n                 truncation=truncation,\n                 max_length=max_length,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -631,6 +634,7 @@ def batch_encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -699,6 +703,7 @@ def batch_encode_plus(\n             truncation=truncation,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -738,6 +743,7 @@ def _batch_encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = None,\n@@ -768,6 +774,7 @@ def _batch_encode_plus(\n             add_special_tokens=add_special_tokens,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -797,6 +804,7 @@ def _batch_prepare_for_model(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = True,\n@@ -823,6 +831,7 @@ def _batch_prepare_for_model(\n                 truncation=truncation,\n                 max_length=max_length,\n                 pad_to_multiple_of=None,  # we pad in batch afterwards\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterwards\n                 return_token_type_ids=return_token_type_ids,\n                 return_special_tokens_mask=return_special_tokens_mask,\n@@ -844,6 +853,7 @@ def _batch_prepare_for_model(\n             padding=padding,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -912,6 +922,7 @@ def encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -968,6 +979,7 @@ def encode_plus(\n             padding=padding,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -993,6 +1005,7 @@ def _encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = True,\n@@ -1024,6 +1037,7 @@ def _encode_plus(\n             padding=padding,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -1051,6 +1065,7 @@ def prepare_for_model(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = True,\n@@ -1214,6 +1229,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1754,6 +1770,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1776,6 +1793,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1799,7 +1819,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(encoded_inputs[\"input_ids\"])\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1817,7 +1838,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[\"input_ids\"] = encoded_inputs[\"input_ids\"] + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1836,7 +1857,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[\"input_ids\"] = [self.pad_token_id] * difference + encoded_inputs[\"input_ids\"]\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "e40c07a58aceb7c314af58431125adb44486f98e",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 24,
            "deletions": 3,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -551,6 +551,7 @@ def call_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -654,6 +655,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -676,6 +678,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -704,6 +707,7 @@ def batch_encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -746,6 +750,7 @@ def batch_encode_plus_boxes(\n             stride=stride,\n             is_split_into_words=is_split_into_words,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -813,6 +818,7 @@ def encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -865,6 +871,7 @@ def encode_plus_boxes(\n             stride=stride,\n             is_split_into_words=is_split_into_words,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -892,6 +899,7 @@ def _batch_encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -920,6 +928,7 @@ def _batch_encode_plus_boxes(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -944,6 +953,7 @@ def _batch_prepare_for_model_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -975,6 +985,7 @@ def _batch_prepare_for_model_boxes(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -995,6 +1006,7 @@ def _batch_prepare_for_model_boxes(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n \n@@ -1014,6 +1026,7 @@ def _encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1044,6 +1057,7 @@ def _encode_plus_boxes(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -1067,6 +1081,7 @@ def prepare_for_model_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1240,6 +1255,7 @@ def prepare_for_model_boxes(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -1385,6 +1401,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -1407,6 +1424,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -1430,7 +1450,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1444,7 +1465,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -1459,6 +1480,6 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs"
        },
        {
            "sha": "8ee0577fa10e58a03ed6305c7eeb3964851ba682",
            "filename": "src/transformers/models/udop/tokenization_udop_fast.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -286,6 +286,7 @@ def call_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -389,6 +390,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -411,6 +413,7 @@ def _is_valid_text_input(t):\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -453,6 +456,7 @@ def batch_encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -501,6 +505,7 @@ def batch_encode_plus_boxes(\n             stride=stride,\n             is_split_into_words=is_split_into_words,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -528,6 +533,7 @@ def _batch_encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -548,6 +554,7 @@ def _batch_encode_plus_boxes(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n         )\n \n         if is_pair:\n@@ -684,6 +691,7 @@ def _encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -712,6 +720,7 @@ def _encode_plus_boxes(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -794,6 +803,7 @@ def encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -846,6 +856,7 @@ def encode_plus_boxes(\n             stride=stride,\n             is_split_into_words=is_split_into_words,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -864,6 +875,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -886,6 +898,9 @@ def _pad(\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -909,7 +924,8 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n-            if self.padding_side == \"right\":\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -923,7 +939,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -938,7 +954,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n+                raise ValueError(\"Invalid padding strategy:\" + str(padding_side))\n \n         return encoded_inputs\n "
        },
        {
            "sha": "c1a333fe48c6b4e2d55bc2862167f391db7968af",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -781,6 +781,7 @@ def __call__(\n         padding: Union[bool, str, PaddingStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n         **kwargs,\n@@ -794,6 +795,10 @@ def __call__(\n                 The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n                 values, a list of numpy array or a list of list of float values. Must be mono channel audio, not\n                 stereo, i.e. single float per timestep.\n+\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n         \"\"\"\n \n         is_batched_numpy = isinstance(raw_speech, np.ndarray) and len(raw_speech.shape) > 1\n@@ -825,6 +830,7 @@ def __call__(\n             padding=padding,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=self.return_attention_mask,\n             return_tensors=return_tensors,\n             verbose=verbose,"
        },
        {
            "sha": "6a5bff3679f8aa42046a8e9d0ecdcc02d5a054e9",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -749,6 +749,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -806,6 +807,7 @@ def get_input_ids(text):\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             prepend_batch_axis=True,\n             return_attention_mask=return_attention_mask,\n@@ -833,6 +835,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -891,6 +894,7 @@ def get_input_ids(text):\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n             return_token_type_ids=return_token_type_ids,\n             return_overflowing_tokens=return_overflowing_tokens,\n@@ -913,6 +917,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -942,6 +947,7 @@ def _batch_prepare_for_model(\n                 max_length=max_length,\n                 stride=stride,\n                 pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n                 return_attention_mask=False,  # we pad in batch afterward\n                 return_token_type_ids=return_token_type_ids,\n                 return_overflowing_tokens=return_overflowing_tokens,\n@@ -963,6 +969,7 @@ def _batch_prepare_for_model(\n             padding=padding_strategy.value,\n             max_length=max_length,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_attention_mask=return_attention_mask,\n         )\n "
        },
        {
            "sha": "93dea5ba09de36a9b065208e248bc1ed1847f4cd",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 33,
            "deletions": 4,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -1427,6 +1427,9 @@ def all_special_ids(self) -> List[int]:\n                 If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n                 This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors instead of list of python integers. Acceptable values are:\n \n@@ -2767,6 +2770,7 @@ def encode(\n         truncation: Union[bool, str, TruncationStrategy] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> List[int]:\n@@ -2793,6 +2797,7 @@ def encode(\n             truncation=truncation,\n             max_length=max_length,\n             stride=stride,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             **kwargs,\n         )\n@@ -2956,6 +2961,7 @@ def __call__(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -2997,6 +3003,7 @@ def __call__(\n             \"stride\": stride,\n             \"is_split_into_words\": is_split_into_words,\n             \"pad_to_multiple_of\": pad_to_multiple_of,\n+            \"padding_side\": padding_side,\n             \"return_tensors\": return_tensors,\n             \"return_token_type_ids\": return_token_type_ids,\n             \"return_attention_mask\": return_attention_mask,\n@@ -3041,6 +3048,7 @@ def _call_one(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3111,6 +3119,7 @@ def _is_valid_text_input(t):\n                 stride=stride,\n                 is_split_into_words=is_split_into_words,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -3133,6 +3142,7 @@ def _is_valid_text_input(t):\n                 stride=stride,\n                 is_split_into_words=is_split_into_words,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_tensors=return_tensors,\n                 return_token_type_ids=return_token_type_ids,\n                 return_attention_mask=return_attention_mask,\n@@ -3157,6 +3167,7 @@ def encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3207,6 +3218,7 @@ def encode_plus(\n             stride=stride,\n             is_split_into_words=is_split_into_words,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -3230,6 +3242,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3261,6 +3274,7 @@ def batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3307,6 +3321,7 @@ def batch_encode_plus(\n             stride=stride,\n             is_split_into_words=is_split_into_words,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,\n@@ -3336,6 +3351,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3361,6 +3377,7 @@ def pad(\n         padding: Union[bool, str, PaddingStrategy] = True,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n@@ -3409,6 +3426,9 @@ def pad(\n \n                 This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask (`bool`, *optional*):\n                 Whether to return the attention mask. If left to the default, will return the attention mask according\n                 to the specific tokenizer's default, defined by the `return_outputs` attribute.\n@@ -3491,6 +3511,7 @@ def pad(\n                 max_length=max_length,\n                 padding_strategy=padding_strategy,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n             return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n@@ -3512,6 +3533,7 @@ def pad(\n                 max_length=max_length,\n                 padding_strategy=padding_strategy,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -3573,6 +3595,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3686,6 +3709,7 @@ def prepare_for_model(\n                 max_length=max_length,\n                 padding=padding_strategy.value,\n                 pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n                 return_attention_mask=return_attention_mask,\n             )\n \n@@ -3828,6 +3852,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\"\n@@ -3843,13 +3868,16 @@ def _pad(\n                 - PaddingStrategy.LONGEST Pad to the longest sequence in the batch\n                 - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)\n                 - PaddingStrategy.DO_NOT_PAD: Do not pad\n-                The tokenizer padding sides are defined in self.padding_side:\n+                The tokenizer padding sides are defined in `padding_side` argument:\n \n                     - 'left': pads on the left of the sequences\n                     - 'right': pads on the right of the sequences\n             pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n                 This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n                 `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n             return_attention_mask:\n                 (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n         \"\"\"\n@@ -3873,8 +3901,9 @@ def _pad(\n \n         if needs_to_be_padded:\n             difference = max_length - len(required_input)\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n \n-            if self.padding_side == \"right\":\n+            if padding_side == \"right\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -3884,7 +3913,7 @@ def _pad(\n                 if \"special_tokens_mask\" in encoded_inputs:\n                     encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n                 encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n-            elif self.padding_side == \"left\":\n+            elif padding_side == \"left\":\n                 if return_attention_mask:\n                     encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n                 if \"token_type_ids\" in encoded_inputs:\n@@ -3895,7 +3924,7 @@ def _pad(\n                     encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n                 encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n             else:\n-                raise ValueError(f\"Invalid padding strategy:{self.padding_side}\")\n+                raise ValueError(f\"Invalid padding strategy:{padding_side}\")\n \n         return encoded_inputs\n "
        },
        {
            "sha": "724484b3b30b881296c3f0c5b069e3b9794f712b",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -429,6 +429,7 @@ def set_truncation_and_padding(\n         max_length: int,\n         stride: int,\n         pad_to_multiple_of: Optional[int],\n+        padding_side: Optional[bool],\n     ):\n         \"\"\"\n         Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n@@ -450,6 +451,9 @@ def set_truncation_and_padding(\n             pad_to_multiple_of (`int`, *optional*):\n                 If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n                 the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n         \"\"\"\n         _truncation = self._tokenizer.truncation\n         _padding = self._tokenizer.padding\n@@ -484,7 +488,7 @@ def set_truncation_and_padding(\n             length = max_length if padding_strategy == PaddingStrategy.MAX_LENGTH else None\n             target = {\n                 \"length\": length,\n-                \"direction\": self.padding_side,\n+                \"direction\": padding_side if padding_side is not None else self.padding_side,\n                 \"pad_id\": self.pad_token_id,\n                 \"pad_token\": self.pad_token,\n                 \"pad_type_id\": self.pad_token_type_id,\n@@ -505,6 +509,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -527,6 +532,7 @@ def _batch_encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n         )\n \n         if self._tokenizer.encode_special_tokens != split_special_tokens:\n@@ -593,6 +599,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[bool] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -614,6 +621,7 @@ def _encode_plus(\n             max_length=max_length,\n             stride=stride,\n             pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n             return_tensors=return_tensors,\n             return_token_type_ids=return_token_type_ids,\n             return_attention_mask=return_attention_mask,"
        },
        {
            "sha": "19a6aeec46f9359282023e40406e54961f8067b7",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 17,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -21,6 +21,8 @@\n import unittest\n from typing import List\n \n+from parameterized import parameterized\n+\n from transformers import (\n     AddedToken,\n     LayoutLMv2TokenizerFast,\n@@ -393,7 +395,8 @@ def test_right_and_left_truncation(self):\n     def test_split_special_tokens(self):\n         pass\n \n-    def test_encode_plus_with_padding(self):\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encode_plus_with_padding(self, use_padding_as_call_kwarg: bool):\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n@@ -444,15 +447,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n \n                 # Test right padding\n-                tokenizer.padding_side = \"right\"\n+                tokenizer_kwargs_right = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"right\"\n+                else:\n+                    tokenizer_kwargs_right[\"padding_side\"] = \"right\"\n \n-                right_padded_sequence = tokenizer.encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                right_padded_sequence = tokenizer.encode_plus(words, boxes=boxes, **tokenizer_kwargs_right)\n                 right_padded_input_ids = right_padded_sequence[\"input_ids\"]\n \n                 right_padded_special_tokens_mask = right_padded_sequence[\"special_tokens_mask\"]\n@@ -463,14 +469,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n \n                 # Test left padding\n-                tokenizer.padding_side = \"left\"\n-                left_padded_sequence = tokenizer.encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                tokenizer_kwargs_left = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"left\"\n+                else:\n+                    tokenizer_kwargs_left[\"padding_side\"] = \"left\"\n+\n+                left_padded_sequence = tokenizer.encode_plus(words, boxes=boxes, **tokenizer_kwargs_left)\n                 left_padded_input_ids = left_padded_sequence[\"input_ids\"]\n                 left_padded_special_tokens_mask = left_padded_sequence[\"special_tokens_mask\"]\n                 left_padded_sequence_length = len(left_padded_input_ids)"
        },
        {
            "sha": "007e23430b3a5636c7a343c7225051e94f8754d1",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 27,
            "deletions": 17,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -22,6 +22,8 @@\n import unittest\n from typing import List\n \n+from parameterized import parameterized\n+\n from transformers import (\n     AddedToken,\n     LayoutLMv3TokenizerFast,\n@@ -273,7 +275,8 @@ def test_right_and_left_truncation(self):\n     def test_split_special_tokens(self):\n         pass\n \n-    def test_encode_plus_with_padding(self):\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encode_plus_with_padding(self, use_padding_as_call_kwarg: bool):\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n@@ -324,15 +327,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n \n                 # Test right padding\n-                tokenizer.padding_side = \"right\"\n+                tokenizer_kwargs_right = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"right\"\n+                else:\n+                    tokenizer_kwargs_right[\"padding_side\"] = \"right\"\n \n-                right_padded_sequence = tokenizer.encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                right_padded_sequence = tokenizer.encode_plus(words, boxes=boxes, **tokenizer_kwargs_right)\n                 right_padded_input_ids = right_padded_sequence[\"input_ids\"]\n \n                 right_padded_special_tokens_mask = right_padded_sequence[\"special_tokens_mask\"]\n@@ -343,14 +349,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n \n                 # Test left padding\n-                tokenizer.padding_side = \"left\"\n-                left_padded_sequence = tokenizer.encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                tokenizer_kwargs_left = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"left\"\n+                else:\n+                    tokenizer_kwargs_left[\"padding_side\"] = \"left\"\n+\n+                left_padded_sequence = tokenizer.encode_plus(words, boxes=boxes, **tokenizer_kwargs_left)\n                 left_padded_input_ids = left_padded_sequence[\"input_ids\"]\n                 left_padded_special_tokens_mask = left_padded_sequence[\"special_tokens_mask\"]\n                 left_padded_sequence_length = len(left_padded_input_ids)"
        },
        {
            "sha": "8acd3716cf576be68ca5429a33c4ede684c5e475",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 27,
            "deletions": 17,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -19,6 +19,8 @@\n import unittest\n from typing import List\n \n+from parameterized import parameterized\n+\n from transformers import (\n     AddedToken,\n     LayoutXLMTokenizerFast,\n@@ -324,7 +326,8 @@ def test_encode_decode_with_spaces(self):\n                 decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n                 self.assertIn(decoded, [output, output.lower()])\n \n-    def test_encode_plus_with_padding(self):\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encode_plus_with_padding(self, use_padding_as_call_kwarg: bool):\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n@@ -375,15 +378,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n \n                 # Test right padding\n-                tokenizer.padding_side = \"right\"\n+                tokenizer_kwargs_right = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"right\"\n+                else:\n+                    tokenizer_kwargs_right[\"padding_side\"] = \"right\"\n \n-                right_padded_sequence = tokenizer.encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                right_padded_sequence = tokenizer.encode_plus(words, boxes=boxes, **tokenizer_kwargs_right)\n                 right_padded_input_ids = right_padded_sequence[\"input_ids\"]\n \n                 right_padded_special_tokens_mask = right_padded_sequence[\"special_tokens_mask\"]\n@@ -394,14 +400,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n \n                 # Test left padding\n-                tokenizer.padding_side = \"left\"\n-                left_padded_sequence = tokenizer.encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                tokenizer_kwargs_left = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"left\"\n+                else:\n+                    tokenizer_kwargs_left[\"padding_side\"] = \"left\"\n+\n+                left_padded_sequence = tokenizer.encode_plus(words, boxes=boxes, **tokenizer_kwargs_left)\n                 left_padded_input_ids = left_padded_sequence[\"input_ids\"]\n                 left_padded_special_tokens_mask = left_padded_sequence[\"special_tokens_mask\"]\n                 left_padded_sequence_length = len(left_padded_input_ids)"
        },
        {
            "sha": "fcdde2eb8a874bd44a5fa9604ca50267e8071990",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 27,
            "deletions": 17,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -22,6 +22,8 @@\n import unittest\n from typing import List\n \n+from parameterized import parameterized\n+\n from transformers import (\n     AddedToken,\n     MarkupLMTokenizerFast,\n@@ -211,7 +213,8 @@ def test_encode_decode_with_spaces(self):\n     def test_right_and_left_truncation(self):\n         pass\n \n-    def test_encode_plus_with_padding(self):\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encode_plus_with_padding(self, use_padding_as_call_kwarg: bool):\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n@@ -262,15 +265,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n \n                 # Test right padding\n-                tokenizer.padding_side = \"right\"\n+                tokenizer_kwargs_right = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"right\"\n+                else:\n+                    tokenizer_kwargs_right[\"padding_side\"] = \"right\"\n \n-                right_padded_sequence = tokenizer.encode_plus(\n-                    nodes,\n-                    xpaths=xpaths,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, **tokenizer_kwargs_right)\n                 right_padded_input_ids = right_padded_sequence[\"input_ids\"]\n \n                 right_padded_special_tokens_mask = right_padded_sequence[\"special_tokens_mask\"]\n@@ -281,14 +287,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n \n                 # Test left padding\n-                tokenizer.padding_side = \"left\"\n-                left_padded_sequence = tokenizer.encode_plus(\n-                    nodes,\n-                    xpaths=xpaths,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                tokenizer_kwargs_left = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"left\"\n+                else:\n+                    tokenizer_kwargs_left[\"padding_side\"] = \"left\"\n+\n+                left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, **tokenizer_kwargs_left)\n                 left_padded_input_ids = left_padded_sequence[\"input_ids\"]\n                 left_padded_special_tokens_mask = left_padded_sequence[\"special_tokens_mask\"]\n                 left_padded_sequence_length = len(left_padded_input_ids)"
        },
        {
            "sha": "49327a39cd80d368b4e98592dcc58765237408d9",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -21,6 +21,7 @@\n \n import numpy as np\n import pandas as pd\n+from parameterized import parameterized\n \n from transformers import AddedToken, is_torch_available\n from transformers.models.tapas.tokenization_tapas import (\n@@ -494,7 +495,8 @@ def test_encode_decode_with_spaces(self):\n                 decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n                 self.assertIn(decoded, [output, output.lower()])\n \n-    def test_encode_plus_with_padding(self):\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encode_plus_with_padding(self, use_padding_as_call_kwarg: bool):\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n@@ -547,15 +549,18 @@ def test_encode_plus_with_padding(self):\n                 assert special_tokens_mask == not_padded_special_tokens_mask\n \n                 # Test right padding\n-                tokenizer.padding_side = \"right\"\n+                tokenizer_kwargs_right = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"right\"\n+                else:\n+                    tokenizer_kwargs_right[\"padding_side\"] = \"right\"\n \n-                right_padded_sequence = tokenizer.encode_plus(\n-                    table,\n-                    sequence,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                right_padded_sequence = tokenizer.encode_plus(table, sequence, **tokenizer_kwargs_right)\n                 right_padded_input_ids = right_padded_sequence[\"input_ids\"]\n \n                 right_padded_special_tokens_mask = right_padded_sequence[\"special_tokens_mask\"]\n@@ -566,14 +571,18 @@ def test_encode_plus_with_padding(self):\n                 assert special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask\n \n                 # Test left padding\n-                tokenizer.padding_side = \"left\"\n-                left_padded_sequence = tokenizer.encode_plus(\n-                    table,\n-                    sequence,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                tokenizer_kwargs_left = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"left\"\n+                else:\n+                    tokenizer_kwargs_left[\"padding_side\"] = \"left\"\n+\n+                left_padded_sequence = tokenizer.encode_plus(table, sequence, **tokenizer_kwargs_left)\n                 left_padded_input_ids = left_padded_sequence[\"input_ids\"]\n                 left_padded_special_tokens_mask = left_padded_sequence[\"special_tokens_mask\"]\n                 left_padded_sequence_length = len(left_padded_input_ids)"
        },
        {
            "sha": "342254dfbdf0660baeb673825e5c35a4cb38b0ec",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 32,
            "deletions": 17,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b0418df11886547e2c701cc4504627881397a0b/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b0418df11886547e2c701cc4504627881397a0b/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=4b0418df11886547e2c701cc4504627881397a0b",
            "patch": "@@ -2225,7 +2225,15 @@ def test_padding_with_attention_mask(self):\n                 else:\n                     self.assertListEqual(padded_features[\"attention_mask\"], [[1, 1, 1, 1, 1, 0], [0, 0, 0, 1, 1, 0]])\n \n-    def test_encode_plus_with_padding(self):\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encode_plus_with_padding(self, use_padding_as_call_kwarg: bool):\n+        \"\"\"\n+        This test checks that padding works as expected when tokenizing a sequence.\n+        Padding is expected to have no effect when the input is a single sequence and\n+        the padding-strategy is not `max_length`. Otherwise it pads to the specified max-length\n+        using tokenizer classes `padding_side` attribute. Also, we check that passing `padding_side`\n+        as call time kwarg works same way as when one sets `tokenizer.padding_side` attribute.\n+        \"\"\"\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n@@ -2244,8 +2252,6 @@ def test_encode_plus_with_padding(self):\n                 sequence_length = len(input_ids)\n \n                 # Test 'longest' and 'no_padding' don't do anything\n-                tokenizer.padding_side = \"right\"\n-\n                 not_padded_sequence = tokenizer.encode_plus(\n                     sequence,\n                     padding=True,\n@@ -2275,14 +2281,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertEqual(special_tokens_mask, not_padded_special_tokens_mask)\n \n                 # Test right padding\n-                tokenizer.padding_side = \"right\"\n+                tokenizer_kwargs_right = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n \n-                right_padded_sequence = tokenizer.encode_plus(\n-                    sequence,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"right\"\n+                else:\n+                    tokenizer_kwargs_right[\"padding_side\"] = \"right\"\n+\n+                right_padded_sequence = tokenizer.encode_plus(sequence, **tokenizer_kwargs_right)\n                 right_padded_input_ids = right_padded_sequence[\"input_ids\"]\n \n                 right_padded_special_tokens_mask = right_padded_sequence[\"special_tokens_mask\"]\n@@ -2293,13 +2303,18 @@ def test_encode_plus_with_padding(self):\n                 self.assertEqual(special_tokens_mask + [1] * padding_size, right_padded_special_tokens_mask)\n \n                 # Test left padding\n-                tokenizer.padding_side = \"left\"\n-                left_padded_sequence = tokenizer.encode_plus(\n-                    sequence,\n-                    max_length=sequence_length + padding_size,\n-                    padding=\"max_length\",\n-                    return_special_tokens_mask=True,\n-                )\n+                tokenizer_kwargs_left = {\n+                    \"max_length\": sequence_length + padding_size,\n+                    \"padding\": \"max_length\",\n+                    \"return_special_tokens_mask\": True,\n+                }\n+\n+                if not use_padding_as_call_kwarg:\n+                    tokenizer.padding_side = \"left\"\n+                else:\n+                    tokenizer_kwargs_left[\"padding_side\"] = \"left\"\n+\n+                left_padded_sequence = tokenizer.encode_plus(sequence, **tokenizer_kwargs_left)\n                 left_padded_input_ids = left_padded_sequence[\"input_ids\"]\n                 left_padded_special_tokens_mask = left_padded_sequence[\"special_tokens_mask\"]\n                 left_padded_sequence_length = len(left_padded_input_ids)"
        }
    ],
    "stats": {
        "total": 677,
        "additions": 528,
        "deletions": 149
    }
}