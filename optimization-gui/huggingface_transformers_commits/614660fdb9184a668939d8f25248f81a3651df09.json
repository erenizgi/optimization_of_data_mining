{
    "author": "karan-uppal3",
    "message": "Removed unnecessary transpose in Switch Transformer Routing (#33582)\n\nremoved switch transformer routing transpose",
    "sha": "614660fdb9184a668939d8f25248f81a3651df09",
    "files": [
        {
            "sha": "f1495ddc8c00578cf123195d6e0ce038a465876e",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/614660fdb9184a668939d8f25248f81a3651df09/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/614660fdb9184a668939d8f25248f81a3651df09/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=614660fdb9184a668939d8f25248f81a3651df09",
            "patch": "@@ -298,7 +298,7 @@ def forward(self, hidden_states):\n \n         router_mask = router_mask.bool()\n         batch_size, seq_len, num_experts = router_mask.shape\n-        idx_mask = router_mask.transpose(1, 2).reshape(batch_size * seq_len, num_experts).sum(dim=0)\n+        idx_mask = router_mask.reshape(batch_size * seq_len, num_experts).sum(dim=0)\n         idx_mask = torch.nonzero(idx_mask, as_tuple=True)[\n             0\n         ].tolist()  # length: number of \"activated\" expert / value: index"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}