{
    "author": "Rocketknight1",
    "message": "Chat Template Doc Fixes (#40173)\n\n* draft commit\n\n* draft commit\n\n* Fixup chat_extras too\n\n* Update conversations.md\n\n* Update the toctree and titles\n\n* Update the writing guide!\n\n* Use @zucchini-nlp's suggestion\n\n* Update docs/source/en/conversations.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/conversations.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/conversations.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
    "files": [
        {
            "sha": "c62d3b7631656b3bd7ee71427741288c51589e61",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
            "patch": "@@ -81,13 +81,13 @@\n     - local: conversations\n       title: Chat basics\n     - local: chat_templating\n-      title: Templates\n+      title: Chat templates\n     - local: chat_templating_multimodal\n-      title: Multimodal templates\n-    - local: chat_templating_writing\n-      title: Template writing\n+      title: Multimodal chat templates\n     - local: chat_extras\n-      title: Tools and RAG\n+      title: Tool use\n+    - local: chat_templating_writing\n+      title: Writing a chat template\n     title: Chat with models\n   - sections:\n     - local: serving"
        },
        {
            "sha": "53c431633c5ea359b1493be1a6a2eb9c69459295",
            "filename": "docs/source/en/chat_extras.md",
            "status": "modified",
            "additions": 49,
            "deletions": 121,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_extras.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_extras.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_extras.md?ref=dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
            "patch": "@@ -14,64 +14,64 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Tools and RAG\n+# Tool use\n \n-The [`~PreTrainedTokenizerBase.apply_chat_template`] method supports virtually any additional argument types - strings, lists, dicts - besides the chat message. This makes it possible to use chat templates for many use cases.\n+Chat models are commonly trained with support for \"function-calling\" or \"tool-use\". Tools are functions supplied by the user, which the model can choose to call as part of its response. For example, models could have access to a calculator tool to perform arithmetic without having to it internally.\n \n-This guide will demonstrate how to use chat templates with tools and retrieval-augmented generation (RAG).\n+This guide will demonstrate how to define tools, how to pass them to a chat model, and how to handle the model's output when it calls a tool.\n \n-## Tools\n+## Passing tools\n \n-Tools are functions a large language model (LLM) can call to perform specific tasks. It is a powerful way to extend the capabilities of conversational agents with real-time information, computational tools, or access to large databases.\n+When a model supports tool-use, pass functions to the `tools` argument of [`~PreTrainedTokenizerBase.apply_chat_template`].\n+The tools are passed as either a [JSON schema](https://json-schema.org/learn) or Python functions. If you pass Python functions,\n+the arguments, argument types, and function docstring are parsed in order to generate the JSON schema automatically.\n \n-Follow the rules below when creating a tool.\n+Although passing Python functions is very convenient, the parser can only handle [Google-style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)\n+docstrings. Refer to the examples below for how to format a tool-ready function.\n \n-1. The function should have a descriptive name.\n-2. The function arguments must have a type hint in the function header (don't include in the `Args` block).\n-3. The function must have a [Google-style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings) docstring.\n-4. The function can have a return type and `Returns` block, but these are optional because most tool use models ignore them.\n-\n-An example tool to get temperature and wind speed is shown below.\n \n ```py\n-def get_current_temperature(location: str, unit: str) -> float:\n+def get_current_temperature(location: str, unit: str):\n     \"\"\"\n     Get the current temperature at a location.\n     \n     Args:\n         location: The location to get the temperature for, in the format \"City, Country\"\n         unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n-    Returns:\n-        The current temperature at the specified location in the specified units, as a float.\n     \"\"\"\n     return 22.  # A real function should probably actually get the temperature!\n \n-def get_current_wind_speed(location: str) -> float:\n+def get_current_wind_speed(location: str):\n     \"\"\"\n     Get the current wind speed in km/h at a given location.\n     \n     Args:\n-        location: The location to get the temperature for, in the format \"City, Country\"\n-    Returns:\n-        The current wind speed at the given location in km/h, as a float.\n+        location: The location to get the wind speed for, in the format \"City, Country\"\n     \"\"\"\n     return 6.  # A real function should probably actually get the wind speed!\n \n tools = [get_current_temperature, get_current_wind_speed]\n ```\n \n+You can optionally add a `Returns:` block to the docstring and a return type to the function header, but most models won't use this information. The parser will also ignore the actual code inside the function!\n+\n+What really matters is the function name, argument names, argument types, and docstring describing the function's purpose\n+and the purpose of its arguments. These create the \"signature\" the model will use to decide whether to call the tool.\n+\n+## Tool-calling Example\n+\n Load a model and tokenizer that supports tool-use like [NousResearch/Hermes-2-Pro-Llama-3-8B](https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B), but you can also consider a larger model like [Command-R](./model_doc/cohere) and [Mixtral-8x22B](./model_doc/mixtral) if your hardware can support it.\n \n ```py\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-tokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n-tokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n-model = AutoModelForCausalLM.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\", dtype=torch.bfloat16, device_map=\"auto\")\n+checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n+tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n+model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=\"auto\", device_map=\"auto\")\n ```\n \n-Create a chat message.\n+Create a chat history.\n \n ```py\n messages = [\n@@ -80,12 +80,11 @@ messages = [\n ]\n ```\n \n-Pass `messages` and a list of tools to [`~PreTrainedTokenizerBase.apply_chat_template`]. Then you can pass the inputs to the model for generation.\n+Next, pass `messages` and a list of tools to [`~PreTrainedTokenizerBase.apply_chat_template`]. Tokenize the chat and generate a response.\n \n ```py\n inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v for k, v in inputs.items()}\n-outputs = model.generate(**inputs, max_new_tokens=128)\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=128)\n print(tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):]))\n ```\n \n@@ -95,60 +94,52 @@ print(tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):]))\n </tool_call><|im_end|>\n ```\n \n-The chat model called the `get_current_temperature` tool with the correct parameters from the docstring. It inferred France as the location based on Paris, and that it should use Celsius for the units of temperature. \n+The chat model called the `get_current_temperature` tool with the correct parameters from the docstring. It inferred France as the location based on Paris, and that it should use Celsius for the units of temperature.\n+\n+A model **cannot actually call the tool itself**. It requests a tool call, and it's your job to handle the call and append it and the result to the chat history.\n \n-Now append the `get_current_temperature` function and these arguments to the chat message as `tool_call`. The `tool_call` dictionary should be provided to the `assistant` role instead of the `system` or `user`.\n+Hold the call in the `tool_calls` key of an `assistant` message. This is the recommended API, and should be supported by the chat template of most tool-using models.\n \n > [!WARNING]\n-> The OpenAI API uses a JSON string as its `tool_call` format. This may cause errors or strange model behavior if used in Transformers, which expects a dict.\n+> Although `tool_calls` is similar to the OpenAI API, the OpenAI API uses a JSON string as its `tool_calls` format. This may cause errors or strange model behavior if used in Transformers, which expects a dict.\n \n-<hfoptions id=\"tool-call\">\n-<hfoption id=\"Llama\">\n \n ```py\n tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n ```\n \n-Allow the assistant to read the function outputs and chat with the user.\n+Append the tool response to the chat history with the `tool` role.\n+\n+```py\n+messages.append({\"role\": \"tool\", \"content\": \"22\"})  # Note that the returned content is always a string!\n+```\n+\n+Finally, allow the model to read the tool response and reply to the user.\n \n ```py\n inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v for k, v in inputs.items()}\n-out = model.generate(**inputs, max_new_tokens=128)\n+out = model.generate(**inputs.to(model.device), max_new_tokens=128)\n print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n ```\n \n ```txt\n-The temperature in Paris, France right now is approximately 12°C (53.6°F).<|im_end|>\n+The temperature in Paris, France right now is 22°C.<|im_end|>\n ```\n \n-</hfoption>\n-<hfoption id=\"Mistral/Mixtral\">\n-\n-For [Mistral](./model_doc/mistral) and [Mixtral](./model_doc/mixtral) models, you need an additional `tool_call_id`. The `tool_call_id` is 9 randomly generated alphanumeric characters assigned to the `id` key in the `tool_call` dictionary.\n-\n-```py\n-tool_call_id = \"9Ae3bDc2F\"\n-tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n-messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n-```\n+> [!WARNING]\n+> Although the key in the assistant message is called `tool_calls`, in most cases, models only emit a single tool call at a time. Some older models emit multiple tool calls at the same time, but this is a\n+> significantly more complex process, as you need to handle multiple tool responses at once and disambiguate them, often using tool call IDs. Please refer to the model card to see exactly what format a model expects for tool calls.\n \n-```py\n-inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v for k, v in inputs.items()}\n-out = model.generate(**inputs, max_new_tokens=128)\n-print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n-```\n \n-</hfoption>\n-</hfoptions>\n+## JSON schemas\n \n-## Schema\n+Another way to define tools is by passing a [JSON schema](https://json-schema.org/learn/getting-started-step-by-step).\n \n-[`~PreTrainedTokenizerBase.apply_chat_template`] converts functions into a [JSON schema](https://json-schema.org/learn/getting-started-step-by-step) which is passed to the chat template. A LLM never sees the code inside the function. In other words, a LLM doesn't care how the function works technically, it only cares about function **definition** and **arguments**.\n+You can also manually call the low-level functions that convert Python functions to JSON schemas, and then check or edit the generated schemas. This is usually not necessary, but is useful for understanding the underlying mechanics. It's particularly important\n+for chat template authors who need to access the JSON schema to render the tool definitions.\n \n-The JSON schema is automatically generated behind the scenes as long as your function follows the [rules](#tools) listed earlier above. But you can use [get_json_schema](https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/src/transformers/utils/chat_template_utils.py#L205) to manually convert a schema for more visibility or debugging.\n+The  [`~PreTrainedTokenizerBase.apply_chat_template`] method uses the [get_json_schema](https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/src/transformers/utils/chat_template_utils.py#L205) function to convert Python functions to a JSON schema.\n \n ```py\n from transformers.utils import get_json_schema\n@@ -191,12 +182,7 @@ print(schema)\n }\n ```\n \n-You can edit the schema or write one entirely from scratch. This gives you a lot of flexibility to define precise schemas for more complex functions.\n-\n-> [!WARNING]\n-> Try keeping your function signatures simple and the arguments to a minimum. These are easier for a model to understand and use than complex functions for example with nested arguments.\n-\n-The example below demonstrates writing a schema manually and then passing it to [`~PreTrainedTokenizerBase.apply_chat_template`].\n+We won't go into the details of JSON schema itself here, since it's already [very well documented](https://json-schema.org/) elsewhere. We will, however, mention that you can pass JSON schema dicts to the `tools` argument of [`~PreTrainedTokenizerBase.apply_chat_template`] instead of Python functions:\n \n ```py\n # A simple function that takes no arguments\n@@ -238,62 +224,4 @@ model_input = tokenizer.apply_chat_template(\n     messages,\n     tools = [current_time, multiply]\n )\n-```\n-\n-## RAG\n-\n-Retrieval-augmented generation (RAG) models enhance a models existing knowledge by allowing it to search documents for additional information before returning a query. For RAG models, add a `documents` parameter to [`~PreTrainedTokenizerBase.apply_chat_template`]. This `documents` parameter should be a list of documents, and each document should be a single dict with `title` and `content` keys.\n-\n-> [!TIP]\n-> The `documents` parameter for RAG isn't widely supported and many models have chat templates that ignore `documents`. Verify if a model supports `documents` by reading its model card or executing `print(tokenizer.chat_template)` to see if the `documents` key is present. [Command-R](https://hf.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://hf.co/CohereForAI/c4ai-command-r-plus-08-2024) both support `documents` in their RAG chat templates.\n-\n-Create a list of documents to pass to the model.\n-\n-```py\n-documents = [\n-    {\n-        \"title\": \"The Moon: Our Age-Old Foe\", \n-        \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n-    },\n-    {\n-        \"title\": \"The Sun: Our Age-Old Friend\",\n-        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n-    }\n-]\n-```\n-\n-Set `chat_template=\"rag\"` in [`~PreTrainedTokenizerBase.apply_chat_template`] and generate a response.\n-\n-```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-# Load the model and tokenizer\n-tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01-4bit\")\n-model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01-4bit\", device_map=\"auto\")\n-device = model.device # Get the device the model is loaded on\n-\n-# Define conversation input\n-conversation = [\n-    {\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"}\n-]\n-\n-input_ids = tokenizer.apply_chat_template(\n-    conversation=conversation,\n-    documents=documents,\n-    chat_template=\"rag\",\n-    tokenize=True,\n-    add_generation_prompt=True,\n-    return_tensors=\"pt\").to(device)\n-\n-# Generate a response \n-generated_tokens = model.generate(\n-    input_ids,\n-    max_new_tokens=100,\n-    do_sample=True,\n-    temperature=0.3,\n-    )\n-\n-# Decode and print the generated text along with generation prompt\n-generated_text = tokenizer.decode(generated_tokens[0])\n-print(generated_text)\n-```\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "2f965657a4203cd6fa7f26948821b19fdd896f74",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 54,
            "deletions": 61,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
            "patch": "@@ -14,11 +14,19 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Templates\n+# Chat templates\n \n-The [chat pipeline](./conversations) guide introduced [`TextGenerationPipeline`] and the concept of a chat prompt or chat template for conversing with a model. Underlying this high-level pipeline is the [`apply_chat_template`] method. A chat template is a part of the tokenizer and it specifies how to convert conversations into a single tokenizable string in the expected model format.\n+The [chat basics](./conversations) guide covers how to store chat histories and generate text from chat models using [`TextGenerationPipeline`]. \n \n-In the example below, Mistral-7B-Instruct and Zephyr-7B are finetuned from the same base model but they’re trained with different chat formats. Without chat templates, you have to manually write formatting code for each model and even minor errors can hurt performance. Chat templates offer a universal way to format chat inputs to any model.\n+This guide is intended for more advanced users, and covers the underlying classes and methods, as well as the key concepts for understanding what's actually going on when you chat with a model.\n+\n+The critical insight needed to understand chat models is this: All causal LMs, whether chat-trained or not, continue a sequence of tokens. When causal LMs are trained, the training usually begins with \"pre-training\" on a huge corpus of text, which creates a \"base\" model.\n+These base models are then often \"fine-tuned\" for chat, which means training them on data that is formatted as a sequence of messages. The chat is still just a sequence of tokens, though! The list of `role` and `content` dictionaries that you pass\n+to a chat model get converted to a token sequence, often with control tokens like `<|user|>` or `<|assistant|>` or `<|end_of_message|>`, which allow the model to see the chat structure. \n+There are many possible chat formats, and different models may use different formats or control tokens, even if they were fine-tuned from the same base model!\n+\n+Don't panic, though - you don't need to memorize every possible chat format in order to use chat models. Chat models come with **chat templates**, which indicate how they expect chats to be formatted.\n+You can access these with the [`apply_chat_template`] method. Let's see two examples. Both of these models are fine-tuned from the same `Mistral-7B` base model:\n \n <hfoptions id=\"template\">\n <hfoption id=\"Mistral\">\n@@ -61,13 +69,17 @@ tokenizer.apply_chat_template(chat, tokenize=False)\n </hfoption>\n </hfoptions>\n \n-This guide explores [`apply_chat_template`] and chat templates in more detail.\n+Mistral-7B-Instruct uses `[INST]` and `[/INST]` tokens to indicate the start and end of user messages, while Zephyr-7B uses `<|user|>` and `<|assistant|>` tokens to indicate speaker roles. This is why chat templates are important - with the wrong control tokens, these models would have drastically worse performance.\n \n-## apply_chat_template\n+## Using `apply_chat_template`\n \n-Chats should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker (usually between you and the system), and the `content` key contains your message. For the system, the `content` is a high-level description of how the model should behave and respond when you’re chatting with it.\n+The input to `apply_chat_template` should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker, and the `content` key contains the message. The common roles are:\n \n-Pass your messages to [`apply_chat_template`] to tokenize and format them. You can set [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` to indicate the start of a message.\n+ - `user` for messages from the user\n+ - `assistant` for messages from the model\n+ - `system` for directives on how the model should act (usually placed at the beginning of the chat)\n+\n+[`apply_chat_template`] takes this list and returns a formatted sequence. Set `tokenize=True` if you want to tokenize the sequence.\n \n ```py\n import torch\n@@ -83,6 +95,7 @@ messages = [\n tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n print(tokenizer.decode(tokenized_chat[0]))\n ```\n+\n ```md\n <|system|>\n You are a friendly chatbot who always responds in the style of a pirate</s>\n@@ -91,7 +104,7 @@ How many helicopters can a human eat in one sitting?</s>\n <|assistant|>\n ```\n \n-Now pass the tokenized chat to [`~GenerationMixin.generate`] to generate a response.\n+Pass the tokenized chat to [`~GenerationMixin.generate`] to generate a response.\n \n ```py\n outputs = model.generate(tokenized_chat, max_new_tokens=128) \n@@ -106,10 +119,17 @@ How many helicopters can a human eat in one sitting?</s>\n Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n ```\n \n+> [!WARNING]\n+> Some tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` if you tokenize later to avoid duplicating these tokens.\n+> This isn’t an issue if you use `apply_chat_template(tokenize=True)`, which means it's usually the safer option!\n+\n ### add_generation_prompt\n-The [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) parameter adds tokens that indicate the start of a response. This ensures the chat model generates a system response instead of continuing a users message.\n \n-Not all models require generation prompts, and some models, like [Llama](./model_doc/llama), don’t have any special tokens before the system response. In this case, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\n+You may have noticed the [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) argument in the above examples. \n+This argument adds tokens to the end of the chat that indicate the start of an `assistant` response. Remember: Beneath all the chat abstractions, chat models are still just language models that continue a sequence of tokens!\n+If you include tokens that tell it that it's now in an `assistant` response, it will correctly write a response, but if you don't include these tokens, the model may get confused and do something strange, like **continuing** the user's message instead of replying to it! \n+\n+Let's see an example to understand what `add_generation_prompt` is actually doing. First, let's format a chat without `add_generation_prompt`:\n \n ```py\n tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n@@ -124,11 +144,32 @@ Nice to meet you!<|im_end|>\n Can I ask a question?<|im_end|>\n ```\n \n+Now, let's format the same chat with `add_generation_prompt=True`:\n+\n+```py\n+tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+tokenized_chat\n+```\n+```md\n+<|im_start|>user\n+Hi there!<|im_end|>\n+<|im_start|>assistant\n+Nice to meet you!<|im_end|>\n+<|im_start|>user\n+Can I ask a question?<|im_end|>\n+<|im_start|>assistant\n+\n+```\n+\n+When `add_generation_prompt=True`, `<|im_start|>assistant` is added at the end to indicate the start of an `assistant` message. This lets the model know an `assistant` response is next.\n+\n+Not all models require generation prompts, and some models, like [Llama](./model_doc/llama), don’t have any special tokens before the `assistant` response. In these cases, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\n+\n ### continue_final_message\n \n The [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message.\n \n-This is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy for instruction following when you know how to start its replies.\n+This is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy of instruction following when you know how to start its replies.\n \n ```py\n chat = [\n@@ -143,52 +184,12 @@ model.generate(**formatted_chat)\n > [!WARNING]\n > You shouldn’t use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n \n-[`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the “assistant” role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) to the pipeline.\n-\n-## Multiple templates\n+[`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the `assistant` role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) argument to the pipeline.\n \n-A model may have several different templates for different use cases. For example, a model may have a template for regular chat, tool use, and RAG.\n-\n-When there are multiple templates, the chat template is a dictionary. Each key corresponds to the name of a template. [`apply_chat_template`] handles multiple templates based on their name. It looks for a template named `default` in most cases and if it can’t find one, it raises an error.\n-\n-For a tool calling template, if a user passes a `tools` parameter and a `tool_use` template exists, the tool calling template is used instead of `default`.\n-\n-To access templates with other names, pass the template name to the `chat_template` parameter in [`apply_chat_template`]. For example, if you’re using a RAG template then set `chat_template=\"rag\"`.\n-\n-It can be confusing to manage multiple templates though, so we recommend using a single template for all use cases. Use Jinja statements like `if tools is defined` and `{% macro %}` definitions to wrap multiple code paths in a single template.\n-\n-## Template selection\n-\n-It is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if you’re training the model further, performance is best if the chat tokens are kept constant.\n-\n-But if you’re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexible enough to handle many use cases. It even includes support for [generation prompts](#add_generation_prompt), but it doesn’t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\n-\n-```py\n-{%- for message in messages %}\n-    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n-{%- endfor %}\n-```\n-\n-Set the template with the following logic to support [generation prompts](#add_generation_prompt). The template wraps each message with `<|im_start|>` and `<|im_end|>` tokens and writes the role as a string. This allows you to easily customize the roles you want to train with.\n-\n-```py\n-tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n-```\n-\n-The `user`, `system` and `assistant` roles are standard roles in chat templates. We recommend using these roles when it makes sense, especially if you’re using your model with the [`TextGenerationPipeline`].\n-\n-```py\n-<|im_start|>system\n-You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n-<|im_start|>user\n-How are you?<|im_end|>\n-<|im_start|>assistant\n-I'm doing great!<|im_end|>\n-```\n \n ## Model training\n \n-Training a model with a chat template is a good way to ensure a chat template matches the tokens a model is trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren’t helpful during training.\n+Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren’t helpful during training.\n \n An example of preprocessing a dataset with a chat template is shown below.\n \n@@ -219,11 +220,3 @@ The sun.</s>\n ```\n \n After this step, you can continue following the [training recipe](./tasks/language_modeling) for causal language models using the `formatted_chat` column.\n-\n-Some tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` as well to avoid duplicating them.\n-\n-```py\n-apply_chat_template(messages, tokenize=False, add_special_tokens=False)\n-```\n-\n-This isn’t an issue if `apply_chat_template(tokenize=True)`."
        },
        {
            "sha": "30a0b19cc909e9043bcc6556e1be3029e4e1ccd8",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 41,
            "deletions": 57,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
            "patch": "@@ -14,22 +14,21 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Multimodal templates\n+# Multimodal chat templates\n \n-Multimodal model chat templates expect a similar [template](./chat_templating) as text-only models. It needs `messages` that includes a dictionary of the `role` and `content`.\n+Multimodal chat models accept inputs like images, audio or video, in addition to text. The `content` key in a multimodal chat history is a list containing multiple items of different types. This is unlike text-only chat models whose `content` key is a single string.\n \n-Multimodal templates are included in the [Processor](./processors) class and require an additional `type` key for specifying whether the included content is an image, video, or text.\n \n-This guide will show you how to format chat templates for multimodal models as well as some best practices for configuring the template\n+In the same way the [Tokenizer](./fast_tokenizer) class handles chat templates and tokenization for text-only models, \n+the [Processor](./processors) class handles preprocessing, tokenization and chat templates for multimodal models. Their [`~ProcessorMixin.apply_chat_template`] methods are almost identical.\n+\n+This guide will show you how to chat with multimodal models with the high-level [`ImageTextToTextPipeline`] and at a lower level using the [`~ProcessorMixin.apply_chat_template`] and [`~GenerationMixin.generate`] methods.\n \n ## ImageTextToTextPipeline\n \n [`ImageTextToTextPipeline`] is a high-level image and text generation class with a “chat mode”. Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).\n \n-Start by building a chat history with the following two roles.\n-\n-- `system` describes how the model should behave and respond when you’re chatting with it. This role isn’t supported by all chat models.\n-- `user` is where you enter your first message to the model.\n+Add image and text blocks to the `content` key in the chat history.\n \n ```py\n messages = [\n@@ -47,39 +46,35 @@ messages = [\n ]\n ```\n \n-Create a [`ImageTextToTextPipeline`] and pass the chat to it. For large models, setting [device_map=“auto”](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Changing the data type to [torch.bfloat16](./models#model-data-type) also helps save memory.\n-\n-> [!TIP]\n-> The [`ImageTextToTextPipeline`] accepts chats in the OpenAI format to make inference easier and more accessible. \n+Create an [`ImageTextToTextPipeline`] and pass the chat to it. For large models, setting [device_map=“auto”](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Setting the data type to [auto](./models#model-data-type) also helps save memory and improve speed.\n \n ```python\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device_map=\"auto\", dtype=torch.float16)\n-pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n-[{'input_text': [{'role': 'system',\n-    'content': [{'type': 'text',\n-      'text': 'You are a friendly chatbot who always responds in the style of a pirate'}]},\n-   {'role': 'user',\n-    'content': [{'type': 'image',\n-      'url': 'http://images.cocodataset.org/val2017/000000039769.jpg'},\n-     {'type': 'text', 'text': 'What are these?'}]}],\n-  'generated_text': 'The image shows two cats lying on a pink surface, which appears to be a cushion or a soft blanket. The cat on the left has a striped coat, typical of tabby cats, and is lying on its side with its head resting on the'}]\n+pipe = pipeline(\"image-text-to-text\", model=\"Qwen/Qwen2.5-VL-3B-Instruct\", device_map=\"auto\", dtype=\"auto\")\n+out = pipe(text=messages, max_new_tokens=128)\n+print(out[0]['generated_text'][-1]['content'])\n ```\n \n-## Image inputs\n \n-For multimodal models that accept images like [LLaVA](./model_doc/llava), include the following in `content` as shown below.\n+```\n+Ahoy, me hearty! These be two feline friends, likely some tabby cats, taking a siesta on a cozy pink blanket. They're resting near remote controls, perhaps after watching some TV or just enjoying some quiet time together. Cats sure know how to find comfort and relaxation, don't they?\n+```\n+\n+Aside from the gradual descent from pirate-speak into modern American English (it **is** only a 3B model, after all), this is correct!\n+\n+## Using `apply_chat_template`\n+\n+Like [text-only models](./chat_templating), use the [`~ProcessorMixin.apply_chat_template`] method to prepare the chat messages for multimodal models. \n+This method handles the tokenization and formatting of the chat messages, including images and other media types. The resulting inputs are passed to the model for generation.\n \n-- The content `\"type\"` can be an `\"image\"` or `\"text\"`.\n-- For images, it can be a link to the image (`\"url\"`), a file path (`\"path\"`), or `\"base64\"`. Images are automatically loaded, processed, and prepared into pixel values as inputs to the model.\n \n ```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n \n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n-processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", device_map=\"auto\", torch_dtype=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n \n messages = [\n     {\n@@ -96,14 +91,28 @@ messages = [\n ]\n ```\n \n-Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content and return the `input_ids` and `pixel_values`.\n+Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content. Unlike text models, the output of `apply_chat_template`\n+contains a `pixel_values` key with the preprocessed image data, in addition to the tokenized text.\n \n ```py\n processed_chat = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n-print(processed_chat.keys())\n+print(list(processed_chat.keys()))\n ```\n \n-These inputs are now ready to be used in [`~GenerationMixin.generate`].\n+\n+```\n+['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']\n+```\n+\n+Pass these inputs to [`~GenerationMixin.generate`].\n+\n+```python\n+out = model.generate(**processed_chat.to(model.device), max_new_tokens=128)\n+print(processor.decode(out[0]))\n+```\n+\n+The decoded output contains the full conversation so far, including the user message and the placeholder tokens that contain the image information. You may need to trim the previous conversation from the output before displaying it to the user.\n+\n \n ## Video inputs\n \n@@ -263,28 +272,3 @@ print(processed_chat.keys())\n </hfoption>\n </hfoptions>\n \n-## Template configuration\n-\n-You can create a custom chat template with [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) and set it with [`~ProcessorMixin.apply_chat_template`]. Refer to the [Template writing](./chat_templating_writing) guide for more details.\n-\n-For example, to enable a template to handle a *list of content* from multiple modalities while still supporting plain strings for text-only inference, specify how to handle the `content['type']` if it is an image or text as shown below in the Llama 3.2 Vision Instruct [template](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/blob/main/chat_template.json).\n-\n-```jinja\n-{% for message in messages %}\n-{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n-{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n-{% if message['content'] is string %}\n-{{ message['content'] }}\n-{% else %}\n-{% for content in message['content'] %}\n-{% if content['type'] == 'image' %}\n-{{ '<|image|>' }}\n-{% elif content['type'] == 'text' %}\n-{{ content['text'] }}\n-{% endif %}\n-{% endfor %}\n-{% endif %}\n-{{ '<|eot_id|>' }}\n-{% endfor %}\n-{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\n-```"
        },
        {
            "sha": "a7da4b6597c800cea05dd65508c13516751d1506",
            "filename": "docs/source/en/chat_templating_writing.md",
            "status": "modified",
            "additions": 78,
            "deletions": 60,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_writing.md?ref=dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
            "patch": "@@ -14,15 +14,10 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Template writing\n+# Writing a chat template\n \n-A chat template is a [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) template stored in the tokenizers [chat_template](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.chat_template) attribute. Jinja is a templating language that allows you to write Python-like code and syntax. A chat template performs the following three roles.\n+A chat template is a [Jinja](https://jinja.palletsprojects.com/en/stable/templates/) template stored in the tokenizer's [chat_template](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.chat_template) attribute. Jinja is a templating language that allows you to write Python-like code and syntax.\n \n-1. Print the role enclosed in `<|` and `|>` (`<|user|>`, `<|assistant|>`, etc.).\n-2. Print the message followed by an end-of-sequence (`EOS`) token.\n-3. Print the assistant token if [add_generation_prompt=True](./chat_templating#add_generation_prompt) so the model generates an assistant response.\n-\n-An example template is shown below.\n \n ```jinja\n {%- for message in messages %}\n@@ -34,78 +29,97 @@ An example template is shown below.\n {%- endif %}\n ```\n \n-The template can be customized to handle more complex use cases. This guide will show you how to add and edit templates and includes template writing tips.\n+If you stare at this for a while, you should realize that this is actually very like Python, albeit with some strange\n+`{%-` syntax. The template iterates over a list of messages, and for each message, it prints the role and content of \n+the message, followed by an end-of-sequence token. If `add_generation_prompt=True`, it adds \n+the starting header for an assistant message to the end of the conversation.\n+\n+Load the written template as a string and assign it to the tokenizer's `chat_template` attribute. Once set, the template is used whenever you call [`~PreTrainedTokenizerBase.apply_chat_template`]. It is also saved\n+with the tokenizer whenever [`~PreTrainedTokenizer.save_pretrained`] or [`~PreTrainedTokenizer.push_to_hub`] is called. The template is saved in the `chat_template.jinja` file in the tokenizer directory. You can\n+edit this file directly to change the template, which is often easier than manipulating a template string.\n+\n+## Template writing tips\n+\n+The easiest way to start writing Jinja templates is to refer to existing templates. Use `print(tokenizer.chat_template)` on any chat model to see the template it's using. Try starting with simple models that don't call any tools or support RAG because tool-use models can have very complex templates. Finally, take a look at the [Jinja documentation](https://jinja.palletsprojects.com/en/stable/templates/#synopsis) for more details about formatting and syntax.\n \n-## Create a template\n+There are some specific tips and pitfalls you may encounter while writing chat templates specifically, though, and this section will cover some of them in more detail. \n \n-Create a template by writing a Jinja template and then setting it as the chat template in the tokenizer. For example, the template below adds `[ASST]` and `[/ASST]` tags to the assistant messages.\n+### Writing multimodal chat templates\n+\n+For multimodal templates, the `chat_template` attribute is set on the **processor**, not the tokenizer. The `content` key of a message is often a list of content dicts,\n+rather than just a single string. You may wish to check the type of each content item in the list, and handle it accordingly.\n+\n+Generally, the template should not directly access image or video data. This is normally handled by the processor after template rendering has finished. Instead,\n+your template should emit a single special token like `<|image|>` or `<|video|>` when it encounters image or video content.  The processor will\n+expand the single special token out into a sequence of image or video tokens later. The exact tokens to emit depends on the model you're working with. We strongly recommend loading an existing multimodal processor to see how it handles data.\n+\n+The example template below handles mixed image and text content.\n \n ```jinja\n {%- for message in messages %}\n-    {%- if message['role'] == 'user' %}\n-        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n-    {%- elif message['role'] == 'system' %}\n-        {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n-    {%- elif message['role'] == 'assistant' %}\n-        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n+    {%- if loop.index0 == 0 %}\n+        {{- bos_token }}\n     {%- endif %}\n+    {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n+    {%- if message['content'] is string %}\n+        {{- message['content'] }}\n+    {%- else %}\n+        {%- for content in message['content'] %}\n+            {%- if content['type'] == 'image' %}\n+                {{- '<|image|>' }}\n+            {%- elif content['type'] == 'text' %}\n+                {{- content['text'] }}\n+            {%- endif %}\n+        {%- endfor %}\n+    {%- endif %}\n+    {{- '<|eot_id|>' }}\n {%- endfor %}\n+{%- if add_generation_prompt %}\n+    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n+{%- endif %}\n ```\n \n-Set the template in the tokenizer, and the next time you use [`~PreTrainedTokenizerBase.apply_chat_template`], the new template is used.\n-\n-```py\n-template = tokenizer.chat_template\n-template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n-tokenizer.chat_template = template  # Set the new template\n-```\n-\n-The template is saved in the `tokenizer_config.json` file. Upload it to the Hub with [`~PreTrainedTokenizer.push_to_hub`] so you can reuse it later and make sure everyone is using the right template for your model.\n+This multimodal template is very similar to the more simple template above, but it checks for `content` lists,\n+and iterates over them to render `<|image|>` tokens where necessary. This allows images to be inserted \"into the flow\"\n+of user text.\n \n-```py\n-tokenizer.push_to_hub(\"model_name\")\n-```\n+Not all models work this way - some may move all images to the end of the user message,\n+for example. The chat template should always match the format the model was trained with.\n \n-## Template writing tips\n+### Trimming whitespace\n \n-The easiest way to start writing Jinja templates is to refer to existing templates. Use `print(tokenizer.chat_template)` on any chat model to see what template it's using. Try starting with simple models that don't call any tools or support RAG. Finally, take a look at the [Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for more details about formatting and syntax.\n+Jinja prints any whitespace before or after a block of text. This can be an issue for chat templates because adding extra whitespace that was not present during model training can harm performance. To remove the whitespace, add `-` to the Jinja line syntax. This allows you to write your template with Pythonic indentation and linebreaks, without accidentally printing an indentation in the rendered output.\n \n-This section curates some best practices for writing clean and efficient Jinja templates.\n+The example template below doesn't use `-`, resulting in extra whitespace being printed in the output.\n \n-### Trimming whitespace\n+```jinja\n+{% for message in messages %}\n+    {{ message['role'] + message['content'] }}\n+{% endfor %}\n+```\n \n-Jinja prints any whitespace before or after a block of text. This can be an issue for chat templates because whitespace usage should be intentional. Add `-` to strip any whitespace before a block.\n+We strongly recommend using `-` to ensure only the intended content is printed.\n \n ```jinja\n {%- for message in messages %}\n     {{- message['role'] + message['content'] }}\n {%- endfor %}\n ```\n \n-The incorrect whitespace usage example below may introduce a newline and indentation in the output.\n+### Special variables and callables\n \n-```jinja\n-{% for message in messages %}\n-    {{ message['role'] + message['content'] }}\n-{% endfor %}\n-```\n-\n-### Special variables\n \n-There are five special variables available inside a template. You can pass virtually any additional arguments to [`~PreTrainedTokenizerBase.apply_chat_template`] and it will be available inside the template as a variable. However, you should try to keep the number of variables to the five below to make it easier for users to use the chat model without writing custom code to handle model-specific arguments.\n+The only constants in a template are the `messages` variable and the `add_generation_prompt` boolean. However, you have\n+access to **any other keyword arguments that are passed** to the [`~PreTrainedTokenizerBase.apply_chat_template`] method.\n \n-- `messages` contains the chat history as a list of message dicts.\n-- `tools` contains a list of tools in JSON schema format.\n-- `documents` contains a list of documents with the format `{\"title\": Title, \"contents\": \"Contents\"}` (designed for RAG models).\n-- `add_generation_prompt` is a boolean that determines whether to add an assistant header at the end of the conversation.\n-- `bos_token` and `eos_token` are special tokens extracted from a tokenizers `special_tokens_map`.\n+This provides flexibility and enables support for use-cases we may not have thought of while designing the spec. The most common additional variable is `tools`, which contains a list of tools in JSON schema format. Although you can use any variable name you like, we highly recommend sticking to convention and using `tools` for this purpose. This makes templates more compatible with the standard API.\n \n-### Callable functions\n+You also have access to any tokens contained in `tokenizer.special_tokens_map`, which often includes special tokens like `bos_token` and `eos_token`. Access these directly by name, like `{{- bos_token }}`.\n \n-There are two callable functions available inside a template.\n+There are two callable functions available to you. To call them, use `{{- function_name(argument) }}`.\n \n - `raise_exception(msg)` raises a `TemplateException`. This is useful for debugging or warning users about incorrect template usage.\n-- `strftime_now(format_str)` retrieves the current date and time in a specific format which could be useful to include in system messages. It is equivalent to [datetime.now().strftime(format_str)](https://docs.python.org/3/library/datetime.html#datetime.datetime.now) in Python.\n+- `strftime_now(format_str)` retrieves the current date and time in a specific format, which is often required in system messages. It is equivalent to [datetime.now().strftime(format_str)](https://docs.python.org/3/library/datetime.html#datetime.datetime.now) in Python.\n \n ### Compatibility with non-Python Jinja\n \n@@ -144,9 +158,11 @@ The following section lists elements of the standard API for writing templates f\n \n ### Tool definitions\n \n-Transformers chat template methods allow a user to pass tools as Python functions or a JSON schema. When functions are passed, a JSON schema is automatically generated and passed to the template. The `tools` variable in a template always takes a list of JSON schemas.\n+[Tools](./chat_extras) are passed as Python functions or a JSON schema. When functions are passed, a JSON schema is automatically generated and passed to the template. When a template accesses the `tools` variable, it is always a list of JSON schemas.\n \n-The specific tokens and tool descriptions should match the ones your model was trained with. Your model doesn't need to understand the JSON schema input because your template can translate the JSON schema into your models format. For example, [Command-R](./model_doc/cohere) was trained with tools defined with Python function headers, but the Command-R tool template accepts JSON schemas. The template internally converts types and renders the input tools as Python headers.\n+Even though a template always receive tools as a JSON schema, you may need to radically change this format when rendering them to match the format a model was trained with. For example, [Command-R](./model_doc/cohere) was trained with tools defined with Python function headers. The template internally converts JSON schema types and renders the input tools as Python headers.\n+\n+The example below shows how a tool is defined in JSON schema format.\n \n ```json\n {\n@@ -172,7 +188,7 @@ The specific tokens and tool descriptions should match the ones your model was t\n }\n ```\n \n-An example for handling tool definitions in a chat template is shown below. The specific tokens and tool descriptions should be changed to match the ones a model was trained with.\n+An example of handling tool definitions in a chat template is shown below. The specific tokens and layouts should be changed to match the ones the model was trained with.\n \n ```\n {%- if tools %}\n@@ -188,7 +204,9 @@ An example for handling tool definitions in a chat template is shown below. The\n \n ### Tool calls\n \n-Tool calls, if present, is a list with the `\"assistant”` role. This is always a list even though most tool-calling models only support single tool calls, which means the list usually only contains a single element.\n+In addition to rendering the tool definitions, you also need to render **tool calls** and **tool responses** in the template.\n+\n+Tool calls are generally passed in the `tool_calls` key of an `\"assistant”` message. This is always a list even though most tool-calling models only support single tool calls, which means the list usually only contains a single element.\n \n ```json\n {\n@@ -208,7 +226,7 @@ Tool calls, if present, is a list with the `\"assistant”` role. This is always\n }\n ```\n \n-A common pattern for handling tool calls is shown below.\n+A common pattern for handling tool calls is shown below. You can use this as a starting point, but make sure you template actually matches the format the model was trained with!\n \n ```\n {%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n@@ -221,7 +239,7 @@ A common pattern for handling tool calls is shown below.\n \n ### Tool responses\n \n-Tool responses are a message dict with the `role`, `name` (name of the function) and `content` (result of the tool call) keys.\n+Tool responses are message dicts with the `tool` role. They are much simpler than tool calls, and usually only contain the `role`, `name` and `content` keys.\n \n ```json\n {\n@@ -231,7 +249,7 @@ Tool responses are a message dict with the `role`, `name` (name of the function)\n }\n ```\n \n-Not all the keys need to be used in the tool response. For example, if a model doesn’t expect the function name to be included in the tool response, then you can just include the `role` and `content`.\n+Some templates may not even need the `name` key, in which case, you can write your template to only read the `content` key.\n \n ```\n {%- if message['role'] == 'tool' %}\n@@ -241,11 +259,11 @@ Not all the keys need to be used in the tool response. For example, if a model d\n \n ## Contribute\n \n-Add a chat template by setting the `chat_template` attribute in the tokenizer and testing it with [`~PreTrainedTokenizerBase.apply_chat_template`]. If it works as expected, then you can upload it to the Hub with with [`~PreTrainedTokenizer.push_to_hub`].\n+Once a template is ready, set it to the `chat_template` attribute in the tokenizer and test it with [`~PreTrainedTokenizerBase.apply_chat_template`]. If it works as expected, then upload it to the Hub with [`~PreTrainedTokenizer.push_to_hub`].\n \n-Even if you're not the model owner, it is still helpful to add a template for a model with an empty chat template or a model that is using a default class template. Open a [pull request](https://hf.co/docs/hub/repositories-pull-requests-discussions) on the model repository to add the template.\n+Even if you're not the model owner, it is still helpful to add a template for a model with an empty or incorrect chat template. Open a [pull request](https://hf.co/docs/hub/repositories-pull-requests-discussions) on the model repository to add the template!\n \n ```py\n tokenizer.chat_template = template\n-tokenizer.push_to_hub(\"model_name\")\n+tokenizer.push_to_hub(\"amazing_company/cool_model\", commit_message=\"Add chat template\", create_pr=True)\n ```"
        },
        {
            "sha": "0fed56c632d277946ee87e8210100911d0e465bc",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 25,
            "deletions": 68,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dab66f15a1e3533391b0e9db4a73ae3b642db4ca/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=dab66f15a1e3533391b0e9db4a73ae3b642db4ca",
            "patch": "@@ -16,18 +16,15 @@ rendered properly in your Markdown viewer.\n \n # Chat basics\n \n-Chat models are conversational models you can send and receive messages from. There are many chat models available to choose from, but in general, larger models tend to be better though that's not always the case. The model size is often included in the name, like \"8B\" or \"70B\", and it describes the number of parameters. Mixture-of-expert (MoE) models have names like \"8x7B\" or \"141B-A35B\" which means it's a 56B and 141B parameter model. You can try quantizing larger models to reduce memory requirements, otherwise you'll need ~2 bytes of memory per parameter.\n+Chat models are conversational models you can send a message to and receive a response. Most language models from mid-2023 onwards are chat models and may be referred to as \"instruct\" or \"instruction-tuned\" models. Models that do not support chat are often referred to as \"base\" or \"pretrained\" models.\n \n-Check model leaderboards like [OpenLLM](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard) and [LMSys Chatbot Arena](https://chat.lmsys.org/?leaderboard) to further help you identify the best chat models for your use case. Models that are specialized in certain domains (medical, legal text, non-English languages, etc.) may sometimes outperform larger general purpose models.\n+Larger and newer models are generally more capable, but models specialized in certain domains (medical, legal text, non-English languages, etc.) can often outperform these larger models. Try leaderboards like [OpenLLM](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard) and [LMSys Chatbot Arena](https://chat.lmsys.org/?leaderboard) to help you identify the best model for your use case.\n \n-> [!TIP]\n-> Chat with a number of open-source models for free on [HuggingChat](https://hf.co/chat/)!\n-\n-This guide shows you how to quickly start chatting with Transformers from the command line, how build and format a conversation, and how to chat using the [`TextGenerationPipeline`].\n+This guide shows you how to quickly load chat models in Transformers from the command line, how to build and format a conversation, and how to chat using the [`TextGenerationPipeline`].\n \n ## chat CLI\n \n-After you've [installed Transformers](./installation), chat with a model directly from the command line as shown below. It launches an interactive session with a model, with a few base commands listed at the start of the session.\n+After you've [installed Transformers](./installation), you can chat with a model directly from the command line. The command below launches an interactive session with a model, with a few base commands listed at the start of the session.\n \n ```bash\n transformers chat Qwen/Qwen2.5-0.5B-Instruct\n@@ -56,85 +53,54 @@ The chat is implemented on top of the [AutoClass](./model_doc/auto), using tooli\n \n [`TextGenerationPipeline`] is a high-level text generation class with a \"chat mode\". Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).\n \n-To start, build a chat history with the following two roles.\n-\n-- `system` describes how the model should behave and respond when you're chatting with it. This role isn't supported by all chat models.\n-- `user` is where you enter your first message to the model.\n+Chat models accept a list of messages (the chat history) as the input. Each message is a dictionary with `role` and `content` keys.\n+To start the chat, add a single `user` message. You can also optionally include a `system` message to give the model directions on how to behave.\n \n ```py\n chat = [\n-    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n-    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n+    {\"role\": \"system\", \"content\": \"You are a helpful science assistant.\"},\n+    {\"role\": \"user\", \"content\": \"Hey, can you explain gravity to me?\"}\n ]\n ```\n \n-Create the [`TextGenerationPipeline`] and pass `chat` to it. For large models, setting [device_map=\"auto\"](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Changing the data type to [torch.bfloat16](./models#model-data-type) also helps save memory.\n+Create the [`TextGenerationPipeline`] and pass `chat` to it. For large models, setting [device_map=\"auto\"](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available.\n \n ```py\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(task=\"text-generation\", model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", dtype=\"auto\", device_map=\"auto\")\n response = pipeline(chat, max_new_tokens=512)\n print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n-```txt\n-(sigh) Oh boy, you're asking me for advice? You're gonna need a map, pal! Alright,\n-alright, I'll give you the lowdown. But don't say I didn't warn you, I'm a robot, not a tour guide!\n-\n-So, you wanna know what's fun to do in the Big Apple? Well, let me tell you, there's a million\n-things to do, but I'll give you the highlights. First off, you gotta see the sights: the Statue of\n-Liberty, Central Park, Times Square... you know, the usual tourist traps. But if you're lookin' for\n-something a little more... unusual, I'd recommend checkin' out the Museum of Modern Art. It's got\n-some wild stuff, like that Warhol guy's soup cans and all that jazz.\n+If this works successfully, you should see a response from the model! If you want to continue the conversation,\n+you need to update the chat history with the model's response. You can do this either by appending the text\n+to `chat` (use the `assistant` role), or by reading `response[0][\"generated_text\"]`, which contains\n+the full chat history, including the most recent response.\n \n-And if you're feelin' adventurous, take a walk across the Brooklyn Bridge. Just watch out for\n-those pesky pigeons, they're like little feathered thieves! (laughs) Get it? Thieves? Ah, never mind.\n-\n-Now, if you're lookin' for some serious fun, hit up the comedy clubs in Greenwich Village. You might\n-even catch a glimpse of some up-and-coming comedians... or a bunch of wannabes tryin' to make it big. (winks)\n-\n-And finally, if you're feelin' like a real New Yorker, grab a slice of pizza from one of the many amazing\n-pizzerias around the city. Just don't try to order a \"robot-sized\" slice, trust me, it won't end well. (laughs)\n-\n-So, there you have it, pal! That's my expert advice on what to do in New York. Now, if you'll\n-excuse me, I've got some oil changes to attend to. (winks)\n-```\n-\n-Use the `append` method on `chat` to respond to the models message.\n+Once you have the model's response, you can continue the conversation by appending a new `user` message to the chat history.\n \n ```py\n chat = response[0][\"generated_text\"]\n chat.append(\n-    {\"role\": \"user\", \"content\": \"Wait, what's so wild about soup cans?\"}\n+    {\"role\": \"user\", \"content\": \"Woah! But can it be reconciled with quantum mechanics?\"}\n )\n response = pipeline(chat, max_new_tokens=512)\n print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n-```txt\n-(laughs) Oh, you're killin' me, pal! You don't get it, do you? Warhol's soup cans are like, art, man!\n-It's like, he took something totally mundane, like a can of soup, and turned it into a masterpiece. It's\n-like, \"Hey, look at me, I'm a can of soup, but I'm also a work of art!\"\n-(sarcastically) Oh, yeah, real original, Andy.\n-\n-But, you know, back in the '60s, it was like, a big deal. People were all about challenging the\n-status quo, and Warhol was like, the king of that. He took the ordinary and made it extraordinary.\n-And, let me tell you, it was like, a real game-changer. I mean, who would've thought that a can of soup could be art? (laughs)\n-\n-But, hey, you're not alone, pal. I mean, I'm a robot, and even I don't get it. (winks)\n-But, hey, that's what makes art, art, right? (laughs)\n-```\n+By repeating this process, you can continue the conversation as long as you like, at least until the model runs out of context window\n+or you run out of memory.\n \n-## Performance\n+## Performance and memory usage\n \n-Transformers load models in full precision by default, and for a 8B model, this requires ~32GB of memory! Reduce memory usage by loading a model in half-precision or bfloat16 (only uses ~2 bytes per parameter). You can even quantize the model to a lower precision like 8-bit or 4-bit with [bitsandbytes](https://hf.co/docs/bitsandbytes/index).\n+Transformers load models in full `float32` precision by default, and for a 8B model, this requires ~32GB of memory! Use the `torch_dtype=\"auto\"` argument, which generally uses `bfloat16` for models that were trained with it, to reduce your memory usage.\n \n > [!TIP]\n > Refer to the [Quantization](./quantization/overview) docs for more information about the different quantization backends available.\n \n-Create a [`BitsAndBytesConfig`] with your desired quantization settings and pass it to the pipelines `model_kwargs` parameter. The example below quantizes a model to 8-bits.\n+To lower memory usage even lower, you can quantize the model to 8-bit or 4-bit with [bitsandbytes](https://hf.co/docs/bitsandbytes/index). Create a [`BitsAndBytesConfig`] with your desired quantization settings and pass it to the pipelines `model_kwargs` parameter. The example below quantizes a model to 8-bits.\n \n ```py\n from transformers import pipeline, BitsAndBytesConfig\n@@ -143,19 +109,10 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", model_kwargs={\"quantization_config\": quantization_config})\n ```\n \n-In general, larger models are slower in addition to requiring more memory because text generation is bottlenecked by **memory bandwidth** instead of compute power. Each active parameter must be read from memory for every generated token. For a 16GB model, 16GB must be read from memory for every generated token.\n-\n-The number of generated tokens/sec is proportional to the total memory bandwidth of the system divided by the model size. Depending on your hardware, total memory bandwidth can vary. Refer to the table below for approximate generation speeds for different hardware types.\n-\n-| Hardware | Memory bandwidth |\n-|---|---|\n-| consumer CPU | 20-100GB/sec |\n-| specialized CPU (Intel Xeon, AMD Threadripper/Epyc, Apple silicon) | 200-900GB/sec |\n-| data center GPU (NVIDIA A100/H100) | 2-3TB/sec |\n-\n-The easiest solution for improving generation speed is to either quantize a model or use hardware with higher memory bandwidth.\n+In general, model size and performance are directly correlated. Larger models are slower in addition to requiring more memory because each active parameter must be read from memory for every generated token. \n+This is a bottleneck for LLM text generation and the main options for improving generation speed are to either quantize a model or use hardware with higher memory bandwidth. Adding more compute power doesn't meaningfully help.\n \n-You can also try techniques like [speculative decoding](./generation_strategies#speculative-decoding), where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token per `forward` pass. This significantly alleviates the bandwidth bottleneck and improves generation speed.\n+You can also try techniques like [speculative decoding](./generation_strategies#speculative-decoding), where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token at a time. This significantly alleviates the bandwidth bottleneck and improves generation speed.\n \n > [!TIP]\n-> Parameters may not be active for every generated token in MoE models such as [Mixtral](./model_doc/mixtral), [Qwen2MoE](./model_doc/qwen2_moe), and [DBRX](./model_doc/dbrx). As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because parameters become activated with each new speculated token.\n+Mixture-of-Expert (MoE) models such as [Mixtral](./model_doc/mixtral), [Qwen2MoE](./model_doc/qwen2_moe), and [GPT-OSS](./model_doc/gpt-oss) have lots of parameters, but only \"activate\" a small fraction of them to generate each token. As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because more parameters become activated with each new speculated token."
        }
    ],
    "stats": {
        "total": 624,
        "additions": 252,
        "deletions": 372
    }
}