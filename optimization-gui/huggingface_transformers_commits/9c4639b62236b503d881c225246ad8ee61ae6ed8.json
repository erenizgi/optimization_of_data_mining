{
    "author": "zucchini-nlp",
    "message": "Return image hidden states (#33426)\n\n* fix\r\n\r\n* return image hidden states\r\n\r\n* fix copies\r\n\r\n* fix test",
    "sha": "9c4639b62236b503d881c225246ad8ee61ae6ed8",
    "files": [
        {
            "sha": "9ad19ccee7222869c5979eea2bb0ae2221843847",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -43,7 +43,6 @@\n \n \n @dataclass\n-# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->Llava\n class LlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for Llava causal language model (or autoregressive) outputs.\n@@ -70,19 +69,17 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n class LlavaMultiModalProjector(nn.Module):\n@@ -560,6 +557,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "ebb4da3102da42ab612acc0387bd451b77b99eed",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -144,7 +144,6 @@ def unpad_image(tensor, original_size):\n \n \n @dataclass\n-# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->LlavaNext\n class LlavaNextCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for LlavaNext causal language model (or autoregressive) outputs.\n@@ -171,19 +170,17 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n # Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->LlavaNext\n@@ -931,6 +928,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "589bf346ceeb9e7e2e9fb6aa29416072d08f576c",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -150,7 +150,6 @@ def unpad_image(tensor, original_size):\n \n \n @dataclass\n-# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->LlavaNextVideo\n class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for LlavaNextVideo causal language model (or autoregressive) outputs.\n@@ -177,19 +176,21 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n class LlavaNextVideoPooler(nn.Module):\n@@ -1015,6 +1016,8 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "697ea84fea504017dc94a65be86fc47b4619fd25",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -145,7 +145,7 @@ def unpad_image(tensor, original_size):\n \n \n @dataclass\n-# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->LlavaOnevision\n+# Copied from transformers.models.llava_next_video.modeling_llava_next_video.LlavaNextVideoCausalLMOutputWithPast with LlavaNextVideo->LlavaOnevision\n class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for LlavaOnevision causal language model (or autoregressive) outputs.\n@@ -172,19 +172,21 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n # Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->LlavaOnevision\n@@ -690,6 +692,8 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "39ee57d70bb5d8a0a6b83ddf599765a34cdc95b6",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -72,19 +72,17 @@ class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n class PaliGemmaMultiModalProjector(nn.Module):\n@@ -488,6 +486,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "9ae80be65ae4b65f7b80675e952c112caac4ae09",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -40,7 +40,6 @@\n \n \n @dataclass\n-# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->VideoLlava\n class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for VideoLlava causal language model (or autoregressive) outputs.\n@@ -67,19 +66,21 @@ class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n # Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->VideoLlava\n@@ -672,6 +673,8 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values_images is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "53a3213697193e969bb9db8be9fea53f019d27f2",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -40,7 +40,7 @@\n \n \n @dataclass\n-# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->VipLlava\n+# Copied from transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast with Llava->VipLlava\n class VipLlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for VipLlava causal language model (or autoregressive) outputs.\n@@ -67,19 +67,17 @@ class VipLlavaCausalLMOutputWithPast(ModelOutput):\n \n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n class VipLlavaMultiModalProjector(nn.Module):\n@@ -554,6 +552,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n         )\n \n     def prepare_inputs_for_generation("
        },
        {
            "sha": "1f8883453754143c11880df0fc943d440432d0f9",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c4639b62236b503d881c225246ad8ee61ae6ed8/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c4639b62236b503d881c225246ad8ee61ae6ed8/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=9c4639b62236b503d881c225246ad8ee61ae6ed8",
            "patch": "@@ -320,6 +320,10 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                 model_row_output = model(**single_row_input)\n \n             for key in model_batched_output:\n+                # we can't test videos as their output shapes are linked to number of frames\n+                # and we don't have to as it is a CLIP model and can be tested from `ClipModelTester` class\n+                if key == \"video_hidden_states\":\n+                    continue\n                 recursive_check(model_batched_output[key], model_row_output[key], model_name, key)\n \n     # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs"
        }
    ],
    "stats": {
        "total": 104,
        "additions": 56,
        "deletions": 48
    }
}