{
    "author": "vasqu",
    "message": "[`GPTNeoX`] Flex Attention + Refactor (#34896)\n\n* gpt neox flex attention + refactor\n\n* some formatting\n\n* small fix on dropout\n\n* add assertion on flex attn test\n\n* flaky ci :(\n\n* add head mask support\n\n* style\n\n* handle dtype, replace torch where\n\n* fixup flex with output attns\n\n* code review and several other fixes\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* style\n\n* remove unnecessary comment\n\n* remove incorrect comment\n\n* make flex attn check more agnostic tor versions and centralized\n\n* change peft input dtype check to value since q and k could be affected by other stuff like RoPE\n\n* i forgor\n\n* flaky\n\n* code review and small fixes\n\n* Update src/transformers/models/gpt_neox/modeling_gpt_neox.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "46df859975404e475cf5eeae76634f69951abe44",
    "files": [
        {
            "sha": "ec03ba1eb5fd83ae8b9dd667a05edf49292378ae",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 51,
            "deletions": 1,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=46df859975404e475cf5eeae76634f69951abe44",
            "patch": "@@ -20,7 +20,10 @@\n import torch\n import torch.nn.functional as F\n \n-from .utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal\n+from .utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal, logging\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n if is_flash_attn_2_available():\n@@ -180,6 +183,47 @@ def prepare_fa2_from_position_ids(query, key, value, position_ids):\n     return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n \n \n+def fa_peft_integration_check(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    target_dtype: Optional[torch.dtype] = None,\n+):\n+    \"\"\"\n+    PEFT usually casts the layer norms in float32 for training stability reasons\n+    therefore the input hidden states gets silently casted in float32. Hence, we need\n+    cast them back in float16 / bfloat16 just to be sure everything works as expected.\n+    This might slowdown training & inference so it is recommended to not cast the LayerNorms!\n+\n+    Args:\n+        query (`torch.Tensor`):\n+            Input query states to be passed to Flash Attention API\n+        key (`torch.Tensor`):\n+            Input key states to be passed to Flash Attention API\n+        value (`torch.Tensor`):\n+            Input value states to be passed to Flash Attention API\n+        target_dtype (`torch.dtype`, *optional*):\n+            The dtype to convert the attention tensors to. Conversion can be ignored by\n+            not providing the target dtype.\n+    \"\"\"\n+    if target_dtype is None:\n+        return query, key, value\n+\n+    input_dtype = value.dtype\n+    if input_dtype == torch.float32:\n+        logger.warning_once(\n+            f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+            f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+            f\" {target_dtype}.\"\n+        )\n+\n+        query = query.to(target_dtype)\n+        key = key.to(target_dtype)\n+        value = value.to(target_dtype)\n+\n+    return query, key, value\n+\n+\n flash_241 = is_flash_attn_greater_or_equal(\"2.4.1\")\n deterministic_g = os.environ.get(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n \n@@ -202,6 +246,7 @@ def _flash_attention_forward(\n     cu_seq_lens_k: Optional[torch.LongTensor] = None,\n     max_length_q: Optional[int] = None,\n     max_length_k: Optional[int] = None,\n+    target_dtype: Optional[torch.dtype] = None,\n ):\n     \"\"\"\n     Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n@@ -248,6 +293,11 @@ def _flash_attention_forward(\n     if softcap is not None:\n         flash_kwargs[\"softcap\"] = softcap\n \n+    # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op\n+    query_states, key_states, value_states = fa_peft_integration_check(\n+        query_states, key_states, value_states, target_dtype\n+    )\n+\n     # Contains at least one padding token in the sequence\n     if attention_mask is not None:\n         batch_size = query_states.shape[0]"
        },
        {
            "sha": "dae29111c8dcc0ccc6ad63df006317e928ddfe00",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 1,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=46df859975404e475cf5eeae76634f69951abe44",
            "patch": "@@ -89,6 +89,7 @@\n     is_peft_available,\n     is_remote_url,\n     is_safetensors_available,\n+    is_torch_flex_attn_available,\n     is_torch_greater_or_equal,\n     is_torch_sdpa_available,\n     is_torch_xla_available,\n@@ -1342,6 +1343,9 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n     # SDPA support\n     _supports_sdpa = False\n \n+    # Flex Attention support\n+    _supports_flex_attn = False\n+\n     # Has support for a `Cache` instance as `past_key_values`? Does it support a `StaticCache`?\n     _supports_cache_class = False\n     _supports_static_cache = False\n@@ -1548,6 +1552,10 @@ def _autoset_attn_implementation(\n                     message += ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n                 if cls._supports_sdpa:\n                     message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n+                if cls._supports_flex_attn:\n+                    message += (\n+                        ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n+                    )\n                 raise ValueError(message + \".\")\n \n             # If a config is passed with a preset attn_implementation, we skip the automatic dispatch and use the user-provided config, with hard checks that the requested attention implementation is available.\n@@ -1582,6 +1590,8 @@ def _autoset_attn_implementation(\n                 hard_check_only=False,\n                 check_device_map=check_device_map,\n             )\n+        elif requested_attn_implementation == \"flex_attention\":\n+            config = cls._check_and_enable_flex_attn(config, hard_check_only=True)\n         elif requested_attn_implementation in [None, \"sdpa\"] and not is_torch_xla_available():\n             # use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\n             config = cls._check_and_enable_sdpa(\n@@ -1778,7 +1788,7 @@ def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> Pretra\n         \"\"\"\n         Checks the availability of SDPA for a given model.\n \n-        If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"flash_attention_2\" so that the model can initialize the correct attention module.\n+        If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"sdpa\" so that the model can initialize the correct attention module.\n         \"\"\"\n         if hard_check_only:\n             if not cls._supports_sdpa:\n@@ -1803,6 +1813,35 @@ def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> Pretra\n             config._attn_implementation = \"sdpa\"\n         return config\n \n+    @classmethod\n+    def _check_and_enable_flex_attn(cls, config, hard_check_only: bool = False) -> PretrainedConfig:\n+        \"\"\"\n+        Checks the availability of Flex Attention for a given model.\n+\n+        If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"flex_attention\" so that the model can initialize the correct attention module.\n+        \"\"\"\n+        if hard_check_only:\n+            if not cls._supports_flex_attn:\n+                raise ValueError(\n+                    f\"{cls.__name__} does not support an attention implementation through torch's flex_attention.\"\n+                    \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/34809.\"\n+                    \" If you believe this error is a bug, please open an issue in Transformers GitHub repository\"\n+                    ' and load your model with the argument `attn_implementation=\"eager\"` meanwhile.'\n+                    ' Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n+                )\n+            if not is_torch_flex_attn_available():\n+                raise ImportError(\n+                    \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n+                )\n+\n+        if not is_torch_flex_attn_available() or not cls._supports_flex_attn:\n+            return config\n+\n+        if not hard_check_only:\n+            config._attn_implementation = \"flex_attention\"\n+\n+        return config\n+\n     def enable_input_require_grads(self):\n         \"\"\"\n         Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping"
        },
        {
            "sha": "3fdb814ebab51ac170e0b6a27898b3fbe2fdf00d",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 242,
            "deletions": 247,
            "changes": 489,
            "blob_url": "https://github.com/huggingface/transformers/blob/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=46df859975404e475cf5eeae76634f69951abe44",
            "patch": "@@ -18,7 +18,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -42,9 +41,9 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n-    get_torch_version,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n )\n from .configuration_gpt_neox import GPTNeoXConfig\n@@ -53,6 +52,9 @@\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import flex_attention\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\"\n@@ -76,6 +78,7 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -92,6 +95,169 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n+def eager_attention_forward(\n+    query, key, value, attention_mask, head_mask, norm_factor, attention_dropout, training, **_kwargs\n+):\n+    # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n+    batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n+    key_length = key.size(-2)\n+\n+    query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)\n+    key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)\n+    attn_scores = torch.zeros(\n+        batch_size * num_attention_heads,\n+        query_length,\n+        key_length,\n+        dtype=query.dtype,\n+        device=key.device,\n+    )\n+    attn_scores = torch.baddbmm(\n+        attn_scores,\n+        query,\n+        key.transpose(1, 2),\n+        beta=1.0,\n+        alpha=norm_factor,\n+    )\n+    attn_scores = attn_scores.view(batch_size, num_attention_heads, query_length, key_length)\n+\n+    if attention_mask is not None:  # no matter the length, we just slice it\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_scores = attn_scores + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n+    attn_weights = attn_weights.to(value.dtype)\n+\n+    # Mask heads if we want to\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=attention_dropout, training=training)\n+    attn_output = torch.matmul(attn_weights, value)\n+\n+    # Reshape outputs\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def flash_attention_forward(\n+    query,\n+    key,\n+    value,\n+    attention_mask,\n+    norm_factor,\n+    attention_dropout,\n+    training,\n+    target_dtype=None,\n+    **_kwargs,\n+):\n+    query_length = query.shape[-2]\n+\n+    # GPT-neo-X casts query and key in fp32 to apply rotary embedding in full precision\n+    query = query.to(value.dtype)\n+    key = key.to(value.dtype)\n+\n+    # Permute to get the expected shape for Flash Attention\n+    query = query.transpose(1, 2)\n+    key = key.transpose(1, 2)\n+    value = value.transpose(1, 2)\n+\n+    attention_dropout = attention_dropout if training else 0.0\n+    flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    # Compute attention\n+    attn_output = _flash_attention_forward(\n+        query,\n+        key,\n+        value,\n+        attention_mask,\n+        query_length,\n+        dropout=attention_dropout,\n+        softmax_scale=norm_factor,\n+        is_causal=True,\n+        use_top_left_mask=flash_attn_uses_top_left_mask,\n+        target_dtype=target_dtype,\n+    )\n+\n+    return attn_output, None\n+\n+\n+def sdpa_attention_forward(query, key, value, attention_mask, attention_dropout, training, **_kwargs):\n+    q_len = query.shape[-2]\n+\n+    causal_mask = attention_mask\n+    if attention_mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    # GPT-neo-X casts query and key in fp32 to apply rotary embedding in full precision\n+    query = query.to(value.dtype)\n+    key = key.to(value.dtype)\n+\n+    # Avoid torch==2.1.2 specific bug for the memory-efficient backend in SDPA\n+    query = query.contiguous()\n+    key = key.contiguous()\n+    value = value.contiguous()\n+\n+    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+    is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query=query,\n+        key=key,\n+        value=value,\n+        attn_mask=causal_mask,\n+        dropout_p=attention_dropout if training else 0.0,\n+        is_causal=is_causal,\n+    )\n+\n+    # Reshape outputs\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, None\n+\n+\n+def flex_attention_forward(query, key, value, attention_mask, head_mask, norm_factor, **_kwargs):\n+    causal_mask = attention_mask\n+    if causal_mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    def causal_mod(score, b, h, q_idx, kv_idx):\n+        if causal_mask is not None:\n+            score += causal_mask[b][0][q_idx][kv_idx]\n+        if head_mask is not None:\n+            score += head_mask[b][h][0][0]\n+        return score\n+\n+    attn_output, attn_weights = flex_attention(\n+        query,\n+        key,\n+        value,\n+        score_mod=causal_mod,\n+        enable_gqa=True,\n+        scale=norm_factor,\n+        # Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless.\n+        # For simplification, we thus always return it as no additional computations are introduced.\n+        return_lse=True,\n+    )\n+\n+    # lse is returned in float32\n+    attn_weights = attn_weights.to(value.dtype)\n+\n+    # Reshape outputs\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+GPTNEOX_ATTENTION_FUNCTION = {\n+    \"eager\": eager_attention_forward,\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+    \"flex_attention\": flex_attention_forward,\n+}\n+\n+\n class GPTNeoXAttention(nn.Module):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n@@ -147,20 +313,57 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n+        bsz, seq_len, _ = hidden_states.shape\n+\n         # Apply attention-specific projections and rope\n         query, key, value, present = self._attn_projections_and_rope(\n             hidden_states=hidden_states,\n             position_ids=position_ids,\n             layer_past=layer_past,\n             use_cache=use_cache,\n+            cache_position=cache_position,\n             position_embeddings=position_embeddings,\n         )\n \n+        # Checking for fallbacks in case an unsupported feature is requested\n+        attention_type = self.config._attn_implementation\n+        if (output_attentions or head_mask is not None) and self.config._attn_implementation in [\n+            \"sdpa\",\n+            \"flash_attention_2\",\n+        ]:\n+            logger.warning_once(\n+                f\"Setting `attention_type` to `eager` because `{attention_type}` does not support\"\n+                f\" `output_attentions=True` or `head_mask`.\"\n+            )\n+            attention_type = \"eager\"\n+\n+        elif (\n+            self.training\n+            and self.config.attention_dropout > 0\n+            and self.config._attn_implementation == \"flex_attention\"\n+        ):\n+            logger.warning_once(\n+                f\"Setting `attention_type` to `eager` because `dropout` is not supported in `{attention_type}`.\"\n+            )\n+            attention_type = \"eager\"\n+\n         # Compute attention\n-        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+        attn_output, attn_weights = GPTNEOX_ATTENTION_FUNCTION[attention_type](\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            norm_factor=self.norm_factor,\n+            attention_dropout=self.config.attention_dropout,\n+            training=self.training,\n+            # Flash Attention 2 specific PEFT check\n+            target_dtype=self._fa_peft_dtype_check(value),\n+        )\n \n-        # Reshape outputs\n-        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n+        # Reshape outputs and final projection\n+        attn_output = attn_output.contiguous()\n+        attn_output = attn_output.view(bsz, seq_len, -1)\n         attn_output = self.dense(attn_output)\n \n         outputs = (attn_output, present)\n@@ -250,262 +453,47 @@ def _attn_projections_and_rope(\n \n         return query, key, value, layer_past\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n-        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n-        # compute causal mask from causal mask buffer\n-        batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n-        key_length = key.size(-2)\n-\n-        # dynamically increase the causal mask with the key length, if needed.\n-        if key_length > self.bias.shape[-1]:\n-            self._init_bias(key_length, device=key.device)\n-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n-\n-        query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)\n-        key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)\n-        attn_scores = torch.zeros(\n-            batch_size * num_attention_heads,\n-            query_length,\n-            key_length,\n-            dtype=query.dtype,\n-            device=key.device,\n-        )\n-        attn_scores = torch.baddbmm(\n-            attn_scores,\n-            query,\n-            key.transpose(1, 2),\n-            beta=1.0,\n-            alpha=self.norm_factor,\n-        )\n-        attn_scores = attn_scores.view(batch_size, num_attention_heads, query_length, key_length)\n-\n-        mask_value = torch.finfo(attn_scores.dtype).min\n-        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n-        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-        mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n-        attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n-            attn_scores = attn_scores + causal_mask\n-\n-        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n-        attn_weights = attn_weights.to(value.dtype)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n-        attn_weights = self.attention_dropout(attn_weights)\n-\n-        attn_output = torch.matmul(attn_weights, value)\n-        return attn_output, attn_weights\n+    def _fa_peft_dtype_check(self, value):\n+        \"\"\"\n+        PEFT can silently cast the dtype to float32 - this method returns the target dtype to which\n+        FA should convert back to (if necessary). For now, we can not move this to the forward pass\n+        itself due to the dependency on checking on some part of its own weights (last case).\n+        \"\"\"\n+        target_dtype = None\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            input_dtype = value.dtype\n+            if input_dtype == torch.float32:\n+                if torch.is_autocast_enabled():\n+                    target_dtype = torch.get_autocast_gpu_dtype()\n+                # Handle the case where the model is quantized\n+                elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                    target_dtype = self.config._pre_quantization_dtype\n+                else:\n+                    target_dtype = self.query_key_value.weight.dtype\n+        return target_dtype\n \n \n+# TODO Remove in deprecation cycle\n class GPTNeoXFlashAttention2(GPTNeoXAttention):\n-    \"\"\"\n-    GPTNeoX flash attention module. This module inherits from `GPTNeoXAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        attention_mask: torch.FloatTensor,\n-        position_ids: torch.LongTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        layer_past: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n-    ):\n-        # Apply attention-specific projections and rope\n-        query, key, value, present = self._attn_projections_and_rope(\n-            hidden_states=hidden_states,\n-            position_ids=position_ids,\n-            layer_past=layer_past,\n-            use_cache=use_cache,\n-            cache_position=cache_position,\n-            position_embeddings=position_embeddings,\n-        )\n-\n-        query_length = query.shape[-2]\n-\n-        # GPT-neo-X casts query and key in fp32 to apply rotary embedding in full precision\n-        target_dtype = value.dtype\n-        if query.dtype != target_dtype:\n-            query = query.to(target_dtype)\n-        if key.dtype != target_dtype:\n-            key = key.to(target_dtype)\n-\n-        # Permute to get the expected shape for Flash Attention\n-        query = query.permute(0, 2, 1, 3)\n-        key = key.permute(0, 2, 1, 3)\n-        value = value.permute(0, 2, 1, 3)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 / bfloat16 just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        input_dtype = query.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.query_key_value.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query = query.to(target_dtype)\n-            key = key.to(target_dtype)\n-            value = value.to(target_dtype)\n-\n-        attention_dropout = self.config.attention_dropout if self.training else 0.0\n-\n-        # Compute attention\n-        attn_weights = _flash_attention_forward(\n-            query,\n-            key,\n-            value,\n-            attention_mask,\n-            query_length,\n-            dropout=attention_dropout,\n-            softmax_scale=self.norm_factor,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        # Reshape outputs\n-        attn_output = attn_weights.reshape(\n-            attn_weights.shape[0], attn_weights.shape[1], self.num_attention_heads * self.head_size\n+        logger.warning_once(\n+            \"The `GPTNeoXFlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n+            \"attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\"\n         )\n-        attn_output = self.dense(attn_output)\n-\n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n \n \n+# TODO Remove in deprecation cycle\n class GPTNeoXSdpaAttention(GPTNeoXAttention):\n-    \"\"\"\n-    GPTNeoX attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GPTNeoXAttention` as the weights of the module stays untouched. The only changes are on the forward pass\n-    to adapt to the SDPA API.\n-    \"\"\"\n-\n     def __init__(self, config, layer_idx=None):\n         super().__init__(config, layer_idx=layer_idx)\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()`. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        attention_mask: torch.FloatTensor,\n-        position_ids: torch.LongTensor,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n-    ):\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`GPTNeoXSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                head_mask=head_mask,\n-                layer_past=layer_past,\n-                use_cache=use_cache,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        # Apply attention-specific projections and rope\n-        query, key, value, present = self._attn_projections_and_rope(\n-            hidden_states=hidden_states,\n-            position_ids=position_ids,\n-            layer_past=layer_past,\n-            use_cache=use_cache,\n-            cache_position=cache_position,\n-            position_embeddings=position_embeddings,\n-        )\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n-\n-        # GPT-neo-X casts query and key in fp32 to apply rotary embedding in full precision\n-        target_dtype = value.dtype\n-        if query.dtype != target_dtype:\n-            query = query.to(target_dtype)\n-        if key.dtype != target_dtype:\n-            key = key.to(target_dtype)\n-\n-        # Avoid torch==2.1.2 specific bug for the memory-efficient backend in SDPA\n-        if self.require_contiguous_qkv and query.device.type == \"cuda\" and attention_mask is not None:\n-            query = query.contiguous()\n-            key = key.contiguous()\n-            value = value.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query=query,\n-            key=key,\n-            value=value,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout.p if self.training else 0.0,\n-            is_causal=is_causal,\n+        logger.warning_once(\n+            \"The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n+            \"attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\"\n         )\n \n-        # Reshape outputs\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.dense(attn_output)\n-\n-        return attn_output, present, None\n-\n-\n-def attention_mask_func(attention_scores, ltor_mask):\n-    attention_scores.masked_fill_(~ltor_mask, torch.finfo(attention_scores.dtype).min)\n-    return attention_scores\n-\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoX\n class GPTNeoXRotaryEmbedding(nn.Module):\n@@ -675,6 +663,7 @@ def forward(self, hidden_states):\n     \"eager\": GPTNeoXAttention,\n     \"flash_attention_2\": GPTNeoXFlashAttention2,\n     \"sdpa\": GPTNeoXSdpaAttention,\n+    \"flex_attention\": GPTNeoXAttention,\n }\n \n \n@@ -919,7 +908,13 @@ def forward(\n         # attention_probs has shape bsz x n_heads x N x N\n         # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        converted_head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        # Flex Attention converts it to a separate mask\n+        if head_mask is not None:\n+            converted_head_mask = ~converted_head_mask.bool() * torch.finfo(inputs_embeds.dtype).min\n+            converted_head_mask = converted_head_mask.to(dtype=self.dtype, device=self.device)\n+        head_mask = converted_head_mask\n+\n         hidden_states = self.emb_dropout(inputs_embeds)\n \n         # create position embeddings to be shared across the decoder layers"
        },
        {
            "sha": "f7e962bec346fbe67095289b1bbcd6429935602c",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=46df859975404e475cf5eeae76634f69951abe44",
            "patch": "@@ -206,6 +206,7 @@\n     is_torch_compile_available,\n     is_torch_cuda_available,\n     is_torch_deterministic,\n+    is_torch_flex_attn_available,\n     is_torch_fp16_available_on_device,\n     is_torch_fx_available,\n     is_torch_fx_proxy,"
        },
        {
            "sha": "2ce4bd7bc778da5089a8d350c71a8a8819c50156",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46df859975404e475cf5eeae76634f69951abe44/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=46df859975404e475cf5eeae76634f69951abe44",
            "patch": "@@ -358,6 +358,17 @@ def is_torch_sdpa_available():\n     return version.parse(_torch_version) >= version.parse(\"2.1.1\")\n \n \n+def is_torch_flex_attn_available():\n+    if not is_torch_available():\n+        return False\n+    elif _torch_version == \"N/A\":\n+        return False\n+\n+    # TODO check if some bugs cause push backs on the exact version\n+    # NOTE: We require torch>=2.5.0 as it is the first release\n+    return version.parse(_torch_version) >= version.parse(\"2.5.0\")\n+\n+\n def is_torchvision_available():\n     return _torchvision_available\n \n@@ -916,6 +927,7 @@ def is_flash_attn_2_available():\n         return False\n \n \n+@lru_cache()\n def is_flash_attn_greater_or_equal_2_10():\n     if not _is_package_available(\"flash_attn\"):\n         return False"
        },
        {
            "sha": "435133e93860ac80e836d0a4e66c96dd59438d06",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/46df859975404e475cf5eeae76634f69951abe44/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46df859975404e475cf5eeae76634f69951abe44/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=46df859975404e475cf5eeae76634f69951abe44",
            "patch": "@@ -459,6 +459,31 @@ def test_lm_generate_gptneox(self):\n \n             self.assertEqual(output_str, expected_output)\n \n+    @slow\n+    def test_lm_generate_flex_attn_gptneox(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m-deduped\")\n+        for checkpointing in [True, False]:\n+            model = GPTNeoXForCausalLM.from_pretrained(\n+                \"EleutherAI/pythia-410m-deduped\", attn_implementation=\"flex_attention\"\n+            )\n+            self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+\n+            if checkpointing:\n+                model.gradient_checkpointing_enable()\n+            else:\n+                model.gradient_checkpointing_disable()\n+            model.to(torch_device)\n+\n+            inputs = tokenizer(\"My favorite food is\", return_tensors=\"pt\").to(torch_device)\n+            # The hub repo. is updated on 2023-04-04, resulting in poor outputs.\n+            # See: https://github.com/huggingface/transformers/pull/24193\n+            expected_output = \"My favorite food is a good old-fashioned, old-fashioned, old-fashioned.\\n\\nI'm not sure\"\n+\n+            output_ids = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n+            output_str = tokenizer.batch_decode(output_ids)[0]\n+\n+            self.assertEqual(output_str, expected_output)\n+\n     def pythia_integration_test(self):\n         model_name_or_path = \"EleutherAI/pythia-70m\"\n         model = GPTNeoXForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16).to(torch_device)"
        }
    ],
    "stats": {
        "total": 620,
        "additions": 371,
        "deletions": 249
    }
}