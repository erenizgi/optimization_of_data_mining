{
    "author": "gante",
    "message": "[generate] move `SinkCache` to a `custom_generate` repo (#38399)\n\nremove sink cache",
    "sha": "beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
    "files": [
        {
            "sha": "1c17e99d5da38f4ea0dd6921d76fef11700ddd3b",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
            "patch": "@@ -380,11 +380,6 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n [[autodoc]] HQQQuantizedCache\n \n-[[autodoc]] SinkCache\n-    - update\n-    - get_seq_length\n-    - reorder_cache\n-\n [[autodoc]] OffloadedCache\n     - update\n     - prefetch_layer\n@@ -443,4 +438,3 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n [[autodoc]] CompileConfig\n     - __call__\n-"
        },
        {
            "sha": "440ce18e5acb3687983e299b94c7c94865bf5864",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 3,
            "deletions": 31,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
            "patch": "@@ -30,7 +30,6 @@ Transformers offers several [`Cache`] classes that implement different caching m\n | Offloaded Static Cache  | No               | Yes                      | Yes                        | High    | Yes                     |\n | Quantized Cache        | Yes              | No                       | No                         | Low     | Yes                     |\n | Sliding Window Cache   | No               | Yes                      | Yes                        | High    | No                      |\n-| Sink Cache             | Yes              | No                       | Yes                        | Mid     | Yes                     |\n \n This guide introduces you to the different [`Cache`] classes and shows you how to use them for generation.\n \n@@ -174,28 +173,6 @@ I like rock music because it's loud and energetic. It's a great way to express m\n </hfoption>\n </hfoptions>\n \n-### Sink cache\n-\n-[`SinkCache`] is capable of generating very long sequences (\"infinite length\" according to the paper) by only retaining a few initial tokens from the sequence. These are called the *sink tokens* because they account for a significant portion of the attention scores during generation. Subsequent tokens are discarded on a sliding windowed basis, and only the latest `window_size` tokens are kept. This means most of the previous knowledge is discarded.\n-\n-The sink tokens allow a model to maintain stable performance even when it's dealing with very long text sequences.\n-\n-Enable [`SinkCache`] by initializing it first with the [window_length](https://hf.co/docs/transformers/main/en/internal/generation_utils#transformers.SinkCache.window_length) and [num_sink_tokens](https://hf.co/docs/transformers/main/en/internal/generation_utils#transformers.SinkCache.num_sink_tokens) parameters before passing it to [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) in [`~GenerationMixin.generate`].\n-\n-```py\n-import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n-inputs = tokenizer(\"This is a long story about unicorns, fairies and magic.\", return_tensors=\"pt\").to(model.device)\n-\n-past_key_values = SinkCache(window_length=256, num_sink_tokens=4)\n-out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)\n-tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-\"This is a long story about unicorns, fairies and magic. It is a fantasy world where unicorns and fairies live together in harmony. The story follows a young girl named Lily\"\n-```\n-\n ## Speed optimized caches\n \n The default [`DynamicCache`] prevents you from taking advantage of just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation.\n@@ -247,7 +224,7 @@ Enable [`SlidingWindowCache`] by configuring `cache_implementation=\"sliding_wind\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16).to(\"cuda:0\")\n@@ -284,16 +261,13 @@ A cache can also work in iterative generation settings where there is back-and-f\n \n For iterative generation with a cache, start by initializing an empty cache class and then you can feed in your new prompts. Keep track of dialogue history with a [chat template](./chat_templating).\n \n-If you're using [`SinkCache`], the inputs need to be truncated to the maximum length because [`SinkCache`] can generate text that exceeds its maximum window size. However, the first input shouldn't exceed the maximum cache length.\n-\n The example below demonstrates how to use a cache for iterative generation.\n \n ```py\n import torch\n from transformers import AutoTokenizer,AutoModelForCausalLM\n from transformers.cache_utils import (\n     DynamicCache,\n-    SinkCache,\n     StaticCache,\n     SlidingWindowCache,\n     QuantoQuantizedCache,\n@@ -313,8 +287,6 @@ messages = []\n for prompt in user_prompts:\n     messages.append({\"role\": \"user\", \"content\": prompt})\n     inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n-    if isinstance(past_key_values, SinkCache):\n-        inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n     input_length = inputs[\"input_ids\"].shape[1]\n     outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n     completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\n@@ -336,7 +308,7 @@ model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n-# Init StaticCache with big enough max-length (1024 tokens for the below example) \n+# Init StaticCache with big enough max-length (1024 tokens for the below example)\n # You can also init a DynamicCache, if that suits you better\n prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n \n@@ -351,7 +323,7 @@ responses = []\n for prompt in prompts:\n     new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n     past_key_values = copy.deepcopy(prompt_cache)\n-    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20) \n+    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n     response = tokenizer.batch_decode(outputs)[0]\n     responses.append(response)\n "
        },
        {
            "sha": "e4841f0c626af61578fe58b9483789ebfd22055b",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
            "patch": "@@ -366,11 +366,6 @@ generation_output[:2]\n \n [[autodoc]] HQQQuantizedCache\n \n-[[autodoc]] SinkCache\n-    - update\n-    - get_seq_length\n-    - reorder_cache\n-\n [[autodoc]] OffloadedCache\n     - update\n     - prefetch_layer"
        },
        {
            "sha": "be6b18d2162f51c6ae8b9cdaadfb6c45274120ce",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 190,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
            "patch": "@@ -2,7 +2,6 @@\n import importlib.metadata\n import json\n import os\n-import warnings\n from dataclasses import dataclass\n from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n \n@@ -1063,199 +1062,18 @@ def _dequantize(self, qtensor):\n \n class SinkCache(Cache):\n     \"\"\"\n-    Deprecated.\n-\n-    A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453). It allows the model to\n-    generate beyond the length of its context window, without losing fluency in the conversation. As it discards past\n-    tokens, the model will lose the ability to generate tokens that depend on the context that was discarded.\n-\n-    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n-    `[batch_size, num_heads, seq_len, head_dim]`.\n-\n-    Parameters:\n-        window_length (`int`):\n-            The length of the context window.\n-        num_sink_tokens (`int`):\n-            The number of sink tokens. See the original paper for more information.\n-\n-    Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n-\n-        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-\n-        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n-\n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = SinkCache(window_length=256, num_sink_tokens=4)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        SinkCache()\n-        ```\n+    Is its now a `custom_generate` repository on the Hub: https://huggingface.co/transformers-community/sink_cache.\n+    See [these docs](https://huggingface.co/docs/transformers/generation_strategies#custom-decoding-methods) for\n+    general `custom_generate`usage.\n     \"\"\"\n \n-    def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n-        super().__init__()\n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n-        self.window_length = window_length\n-        self.num_sink_tokens = num_sink_tokens\n-        self.cos_sin_rerotation_cache = {}\n-        self._cos_cache = None\n-        self._sin_cache = None\n-        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n-\n-        warnings.warn(\n-            \"`SinkCache` is deprecated and will be removed in v4.53.0. You can achieve similar functionality by \"\n-            \"using a model with a sliding window attention mechanism, or by expanding RoPE and optionally using an \"\n-            \"offloaded cache implementation.\",\n-            FutureWarning,\n+    # TODO (joao, manuel): Remove this class in v4.59.0\n+    def __init__(self, **kwargs) -> None:\n+        raise NotImplementedError(\n+            \"`SinkCache` has been moved as a `custom_generate` repository on the Hub: \"\n+            \"https://huggingface.co/transformers-community/sink_cache. See the repository for usage examples.\"\n         )\n \n-    @staticmethod\n-    def _rotate_half(x):\n-        x1 = x[..., : x.shape[-1] // 2]\n-        x2 = x[..., x.shape[-1] // 2 :]\n-        return torch.cat((-x2, x1), dim=-1)\n-\n-    def _apply_key_rotary_pos_emb(\n-        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n-    ) -> torch.Tensor:\n-        rotated_key_states = (key_states * cos) + (self._rotate_half(key_states) * sin)\n-        return rotated_key_states\n-\n-    def _get_rerotation_cos_sin(\n-        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        if key_states.shape[-2] not in self.cos_sin_rerotation_cache:\n-            # Upcast to float32 temporarily for better accuracy\n-            cos = cos.to(torch.float32)\n-            sin = sin.to(torch.float32)\n-\n-            # Compute the cos and sin required for back- and forward-rotating to one position earlier in the sequence\n-            original_cos = cos[self.num_sink_tokens + key_states.shape[-2] :]\n-            shifted_cos = cos[self.num_sink_tokens : -key_states.shape[-2]]\n-            original_sin = sin[self.num_sink_tokens + key_states.shape[-2] :]\n-            shifted_sin = sin[self.num_sink_tokens : -key_states.shape[-2]]\n-            rerotation_cos = original_cos * shifted_cos + original_sin * shifted_sin\n-            rerotation_sin = -original_sin * shifted_cos + original_cos * shifted_sin\n-\n-            self.cos_sin_rerotation_cache[key_states.shape[-2]] = (\n-                rerotation_cos.to(key_states.dtype).unsqueeze(0),\n-                rerotation_sin.to(key_states.dtype).unsqueeze(0),\n-            )\n-        return self.cos_sin_rerotation_cache[key_states.shape[-2]]\n-\n-    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n-        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n-        # TODO: deprecate this function in favor of `cache_position`\n-        # Workaround to make 'key_states.shape[-2] + past_key_value.get_seq_length(self.layer_idx)' <= window_length\n-        if len(self.key_cache) <= layer_idx:\n-            return 0\n-        return self.key_cache[layer_idx].shape[-2]\n-\n-    def get_max_cache_shape(self) -> Optional[int]:\n-        \"\"\"Returns the maximum sequence length of the cache object, in case of SinkCache it is the window length.\"\"\"\n-        return self.window_length\n-\n-    def update(\n-        self,\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n-\n-        Parameters:\n-            key_states (`torch.Tensor`):\n-                The new key states to cache.\n-            value_states (`torch.Tensor`):\n-                The new value states to cache.\n-            layer_idx (`int`):\n-                The index of the layer to cache the states for.\n-            cache_kwargs (`Dict[str, Any]`, `optional`):\n-                Additional arguments for the cache subclass. The following arguments can be used in `SinkCache`: `sin`,\n-                `cos` and `partial_rotation_size`. These arguments are used with models using RoPE, to recompute the\n-                rotation as the tokens are shifted.\n-\n-        Return:\n-            A tuple containing the updated key and value states.\n-        \"\"\"\n-        # Optional kwargs for `SinkCache` -- needed on models using RoPE. `partial_rotation_size` is used on models\n-        # with partially rotated position embeddings, like Phi or Persimmon.\n-        if cache_kwargs is None:\n-            cache_kwargs = {}\n-        sin = cache_kwargs.get(\"sin\")\n-        cos = cache_kwargs.get(\"cos\")\n-        partial_rotation_size = cache_kwargs.get(\"partial_rotation_size\")\n-        using_rope = cos is not None and sin is not None\n-\n-        # Update the number of seen tokens\n-        if layer_idx == 0:\n-            self._seen_tokens += key_states.shape[-2]\n-\n-        # Update the sin/cos cache, which holds sin/cos values for all possible positions\n-        if using_rope and layer_idx == 0:\n-            # BC: some models still pass `sin`/`cos` with 2 dims. In those models, they are the full sin/cos. Remove\n-            # after all RoPE models have a llama-like cache utilization.\n-            if cos.dim() == 2:\n-                self._cos_cache = cos\n-                self._sin_cache = sin\n-            else:\n-                if self._cos_cache is None:\n-                    self._cos_cache = cos[0, ...]\n-                    self._sin_cache = sin[0, ...]\n-                elif self._cos_cache.shape[0] < self.window_length:\n-                    self._cos_cache = torch.cat([self._cos_cache, cos[0, ...]], dim=0)\n-                    self._sin_cache = torch.cat([self._sin_cache, sin[0, ...]], dim=0)\n-\n-        # [bsz, num_heads, seq_len, head_dim]\n-        if len(self.key_cache) <= layer_idx:\n-            # Empty cache\n-            self.key_cache.append(key_states)\n-            self.value_cache.append(value_states)\n-\n-        elif key_states.shape[-2] + self.get_seq_length(layer_idx) < self.window_length:\n-            # Growing cache\n-            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n-            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n-\n-        else:\n-            # Shifting cache\n-            keys_to_keep = self.key_cache[layer_idx][\n-                :, :, -self.window_length + self.num_sink_tokens + key_states.shape[-2] :\n-            ]\n-\n-            # On RoPE models, we need to recompute the Key rotation as the tokens are shifted\n-            if using_rope:\n-                rerotation_cos, rerotation_sin = self._get_rerotation_cos_sin(\n-                    key_states, self._cos_cache[: self.window_length], self._sin_cache[: self.window_length]\n-                )\n-                if partial_rotation_size is not None:\n-                    keys_to_keep, keys_pass = (\n-                        keys_to_keep[..., :partial_rotation_size],\n-                        keys_to_keep[..., partial_rotation_size:],\n-                    )\n-                keys_to_keep = self._apply_key_rotary_pos_emb(keys_to_keep, rerotation_cos, rerotation_sin)\n-                if partial_rotation_size is not None:\n-                    keys_to_keep = torch.cat((keys_to_keep, keys_pass), dim=-1)\n-\n-            # Concatenate sink tokens, shifted & rotated tokens (if needed), and new tokens\n-            sink_keys = self.key_cache[layer_idx][:, :, : self.num_sink_tokens]\n-            self.key_cache[layer_idx] = torch.cat([sink_keys, keys_to_keep, key_states], dim=-2)\n-\n-            sink_values = self.value_cache[layer_idx][:, :, : self.num_sink_tokens]\n-            values_to_keep = self.value_cache[layer_idx][\n-                :, :, -self.window_length + self.num_sink_tokens + value_states.shape[-2] :\n-            ]\n-            self.value_cache[layer_idx] = torch.cat([sink_values, values_to_keep, value_states], dim=-2)\n-\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n \n class StaticCache(Cache):\n     \"\"\""
        },
        {
            "sha": "28386b9f44956af8dc4dfd1567095553958365ce",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
            "patch": "@@ -34,7 +34,7 @@\n from torch.fx.proxy import ParameterProxy\n \n from .. import logging\n-from ..cache_utils import Cache, DynamicCache, SinkCache, StaticCache\n+from ..cache_utils import Cache, DynamicCache, StaticCache\n from ..modeling_utils import PretrainedConfig, PreTrainedModel\n from ..models.auto import get_values\n from ..models.auto.modeling_auto import (\n@@ -832,12 +832,6 @@ def cache_proxy_factory_fn(n: Node) -> HFCacheProxy:\n     {},\n     proxy_factory_fn=create_cache_proxy_factory_fn(DynamicCache),\n )\n-ProxyableSinkCache = HFProxyableClassMeta(\n-    \"ProxyableSinkCache\",\n-    (SinkCache,),\n-    {},\n-    proxy_factory_fn=create_cache_proxy_factory_fn(SinkCache),\n-)\n ProxyableStaticCache = HFProxyableClassMeta(\n     \"ProxyableStaticCache\",\n     (StaticCache,),\n@@ -880,7 +874,6 @@ class HFTracer(Tracer):\n     _CLASSES_TO_PATCH = {\n         Cache: ProxyableCache,\n         DynamicCache: ProxyableDynamicCache,\n-        SinkCache: ProxyableSinkCache,\n         StaticCache: ProxyableStaticCache,\n     }\n "
        },
        {
            "sha": "58a79cb28940e34455b4c5c76e8cb74a5a830b1f",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beaed8ce01c2f24e1d520efd96e7091f0f0623e2/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=beaed8ce01c2f24e1d520efd96e7091f0f0623e2",
            "patch": "@@ -1050,6 +1050,7 @@ def find_all_documented_objects() -> List[str]:\n     \"VitPoseBackbone\",  # Internal module\n     \"VitPoseBackboneConfig\",  # Internal module\n     \"get_values\",  # Internal object\n+    \"SinkCache\",  # Moved to a custom_generate repository, to be deleted from transformers in v4.59.0\n ]\n \n # This list should be empty. Objects in it should get their own doc page."
        }
    ],
    "stats": {
        "total": 253,
        "additions": 13,
        "deletions": 240
    }
}