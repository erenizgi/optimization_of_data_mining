{
    "author": "lilin-1",
    "message": "ðŸš¨Refactor: Update text2text generation pipelines to use max_new_tokensâ€¦ (#40928)\n\n* Refactor: Update text2text generation pipelines to use max_new_tokens and resolve max_length warning\n\n* docs(text2text_generation): æ›´æ–°å‚æ•°æ³¨é‡Šä»¥åæ˜ çŽ°ä»£ç”Ÿæˆå®žè·µ\n\nå°†max_lengthå‚æ•°æ³¨é‡Šæ›´æ–°ä¸ºmax_new_tokensï¼Œä»¥ç¬¦åˆçŽ°ä»£ç”Ÿæˆå®žè·µä¸­æŒ‡å®šç”Ÿæˆæ–°tokenæ•°é‡çš„æ ‡å‡†åšæ³•\n\n* refactor(text2text_generation): Remove outdated input validation logic\n\n* docs(text2text_generation): Revert incorrectly modified comment\n\n* docs(text2text_generation): Revert incorrectly modified comment",
    "sha": "b33cb700978bbd9d5152fdc650d022ee11619347",
    "files": [
        {
            "sha": "9f98f5725279abc233378fbb98f1fe5069507c8c",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/b33cb700978bbd9d5152fdc650d022ee11619347/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b33cb700978bbd9d5152fdc650d022ee11619347/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=b33cb700978bbd9d5152fdc650d022ee11619347",
            "patch": "@@ -123,7 +123,7 @@ def _sanitize_parameters(\n \n         return preprocess_params, forward_params, postprocess_params\n \n-    def check_inputs(self, input_length: int, min_length: int, max_length: int):\n+    def check_inputs(self, input_length: int, min_length: int, max_new_tokens: int):\n         \"\"\"\n         Checks whether there might be something wrong with given input with regard to the model.\n         \"\"\"\n@@ -198,7 +198,7 @@ def _forward(self, model_inputs, **generate_kwargs):\n         self.check_inputs(\n             input_length,\n             generate_kwargs.get(\"min_length\", self.generation_config.min_length),\n-            generate_kwargs.get(\"max_length\", self.generation_config.max_length),\n+            generate_kwargs.get(\"max_new_tokens\", self.generation_config.max_new_tokens),\n         )\n \n         # User-defined `generation_config` passed to the pipeline call take precedence\n@@ -284,18 +284,18 @@ def __call__(self, *args, **kwargs):\n         \"\"\"\n         return super().__call__(*args, **kwargs)\n \n-    def check_inputs(self, input_length: int, min_length: int, max_length: int) -> bool:\n+    def check_inputs(self, input_length: int, min_length: int, max_new_tokens: int) -> bool:\n         \"\"\"\n         Checks whether there might be something wrong with given input with regard to the model.\n         \"\"\"\n-        if max_length < min_length:\n-            logger.warning(f\"Your min_length={min_length} must be inferior than your max_length={max_length}.\")\n+        if max_new_tokens < min_length:\n+            logger.warning(f\"Your min_length={min_length} must be inferior than your max_new_tokens={max_new_tokens}.\")\n \n-        if input_length < max_length:\n+        if input_length < max_new_tokens:\n             logger.warning(\n-                f\"Your max_length is set to {max_length}, but your input_length is only {input_length}. Since this is \"\n+                f\"Your max_new_tokens is set to {max_new_tokens}, but your input_length is only {input_length}. Since this is \"\n                 \"a summarization task, where outputs shorter than the input are typically wanted, you might \"\n-                f\"consider decreasing max_length manually, e.g. summarizer('...', max_length={input_length // 2})\"\n+                f\"consider decreasing max_new_tokens manually, e.g. summarizer('...', max_new_tokens={input_length // 2})\"\n             )\n \n \n@@ -327,12 +327,10 @@ class TranslationPipeline(Text2TextGenerationPipeline):\n     # Used in the return key of the pipeline.\n     return_name = \"translation\"\n \n-    def check_inputs(self, input_length: int, min_length: int, max_length: int):\n-        if input_length > 0.9 * max_length:\n-            logger.warning(\n-                f\"Your input_length: {input_length} is bigger than 0.9 * max_length: {max_length}. You might consider \"\n-                \"increasing your max_length manually, e.g. translator('...', max_length=400)\"\n-            )\n+    def check_inputs(self, input_length: int, min_length: int, max_new_tokens: int):\n+        \"\"\"\n+        Removed input length check - unnecessary with max_new_tokens (previously relevant for max_length)\n+        \"\"\"\n         return True\n \n     def preprocess(self, *args, truncation=TruncationStrategy.DO_NOT_TRUNCATE, src_lang=None, tgt_lang=None):"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 12,
        "deletions": 14
    }
}