{
    "author": "lucaswychan",
    "message": "Fix Causality Handling in Flash Attention to Support Bidirectional Attention (#39707)\n\nFix the is_causal logic to enable bidirectional attention\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "0ce24f5a885e694e4dfd49cb74336099bab48f40",
    "files": [
        {
            "sha": "552d89bac2f6a32137fe4dc0902621c10c4af189",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ce24f5a885e694e4dfd49cb74336099bab48f40/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ce24f5a885e694e4dfd49cb74336099bab48f40/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=0ce24f5a885e694e4dfd49cb74336099bab48f40",
            "patch": "@@ -58,16 +58,18 @@ def flash_attention_forward(\n         else:\n             target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n \n-    # FA2 always relies on the value set in the module, so remove it if present in kwargs to avoid passing it twice\n-    kwargs.pop(\"is_causal\", None)\n+    # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n+    is_causal = kwargs.pop(\"is_causal\", None)\n+    if is_causal is None:\n+        is_causal = module.is_causal\n \n     attn_output = _flash_attention_forward(\n         query,\n         key,\n         value,\n         attention_mask,\n         query_length=seq_len,\n-        is_causal=module.is_causal,\n+        is_causal=is_causal,\n         dropout=dropout,\n         softmax_scale=scaling,\n         sliding_window=sliding_window,"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 5,
        "deletions": 3
    }
}