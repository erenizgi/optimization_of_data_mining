{
    "author": "stevhliu",
    "message": "[docs] Fix links (#41110)\n\nfix",
    "sha": "4a02bc7004285bdb12cc033e87ad2578ce2fa900",
    "files": [
        {
            "sha": "9b3fffbd5485cfeba496ea80a26943884cddf3bd",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a02bc7004285bdb12cc033e87ad2578ce2fa900/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a02bc7004285bdb12cc033e87ad2578ce2fa900/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=4a02bc7004285bdb12cc033e87ad2578ce2fa900",
            "patch": "@@ -16,25 +16,25 @@ rendered properly in your Markdown viewer.\n \n # Serving\n \n-Transformer models can be efficiently deployed using libraries such as vLLM, Text Generation Inference (TGI), and others. These libraries are designed for production-grade user-facing services, and can scale to multiple servers and millions of concurrent users. Refer to [Transformers as Backend for Inference Servers](./transformers_as_backends) for usage examples.\n+Transformer models can be efficiently deployed using libraries such as vLLM, Text Generation Inference (TGI), and others. These libraries are designed for production-grade user-facing services, and can scale to multiple servers and millions of concurrent users. Refer to [Transformers as Backend for Inference Servers](./transformers_as_backend) for usage examples.\n \n > [!TIP]\n > Responses API is now supported as an experimental API! Read more about it [here](#responses-api).\n \n You can also serve transformer models with the `transformers serve` CLI. With Continuous Batching, `serve` now delivers solid throughput and latency well suited for evaluation, experimentation, and moderate-load local or self-hosted deployments. While vLLM, SGLang, or other inference engines remain our recommendations for large-scale production, `serve` avoids the extra runtime and operational overhead, and is on track to gain more production-oriented features.\n \n In this document, we dive into the different supported endpoints and modalities; we also cover the setup of several user interfaces that can be used on top of `transformers serve` in the following guides:\n-- [Jan (text and MCP user interface)](./jan.md)\n-- [Cursor (IDE)](./cursor.md)\n-- [Open WebUI (text, image, speech user interface)](./open_webui.md)\n-- [Tiny-Agents (text and MCP CLI tool)](./tiny_agents.md)\n+- [Jan (text and MCP user interface)](./jan)\n+- [Cursor (IDE)](./cursor)\n+- [Open WebUI (text, image, speech user interface)](./open_webui)\n+- [Tiny-Agents (text and MCP CLI tool)](./tiny_agents)\n \n ## Serve CLI\n \n > [!WARNING]\n > This section is experimental and subject to change in future versions\n \n-You can serve models of diverse modalities supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers compatibility with the OpenAI SDK, which is the _de facto_ standard for LLM conversations and other related tasks. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations.md#chat-cli)).\n+You can serve models of diverse modalities supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers compatibility with the OpenAI SDK, which is the _de facto_ standard for LLM conversations and other related tasks. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations#chat-cli)).\n \n The server supports the following REST APIs:\n - `/v1/chat/completions`"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}