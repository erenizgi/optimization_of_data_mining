{
    "author": "elvircrn",
    "message": "Efficient Inference Kernel for SpQR  (#34976)\n\n* Resolve vptq conflict\r\n\r\n* Rename spqr package to spqr_quant\r\n\r\n* Get rid of aqlm mention\r\n\r\n* Start working on tests\r\n\r\n* Resolve ruff code checks\r\n\r\n* Ruff format\r\n\r\n* Isort\r\n\r\n* Test updates\r\n\r\n* Add gpu tag\r\n\r\n* Rename to modules_to_not_convert\r\n\r\n* Config update\r\n\r\n* Docs and config update\r\n\r\n* Docs and config update\r\n\r\n* Update to update_torch_dtype\r\n\r\n* spqr config parameter validation\r\n\r\n* Ruff update\r\n\r\n* Apply ruff fixes\r\n\r\n* Test fixes\r\n\r\n* Ruff update\r\n\r\n* Mark tests as @slow again; Ruff; Docstring update\r\n\r\n* Ruff\r\n\r\n* Remove absolute path\r\n\r\n* Resolve typo\r\n\r\n* Remove redundandt log\r\n\r\n* Check accelerate/spqr availability\r\n\r\n* Ruff fix\r\n\r\n* Check if the config contains proper shapes\r\n\r\n* Ruff test\r\n\r\n* Documentation update\r\n\r\n* overview update\r\n\r\n* Ruff checks\r\n\r\n* Ruff code quality\r\n\r\n* Make style\r\n\r\n* Update docs/source/en/quantization/spqr.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update spqr.md\r\n\r\n* Enable gptqmodel (#35012)\r\n\r\n* gptqmodel\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix format\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* update readme\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* gptqmodel need use checkpoint_format (#1)\r\n\r\n* gptqmodel need use checkpoint_format\r\n\r\n* fix quantize\r\n\r\n* Update quantization_config.py\r\n\r\n* Update quantization_config.py\r\n\r\n* Update quantization_config.py\r\n\r\n---------\r\n\r\nCo-authored-by: ZX-ModelCloud <zx@modelcloud.ai>\r\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\r\n\r\n* Revert quantizer_gptq.py (#2)\r\n\r\n* revert quantizer_gptq.py change\r\n\r\n* pass **kwargs\r\n\r\n* limit gptqmodel and optimum version\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix format\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix warning\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix version check\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* revert unrelated changes\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* enable gptqmodel tests\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix requires gptq\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* Fix Transformer compat (#3)\r\n\r\n* revert quantizer_gptq.py change\r\n\r\n* pass **kwargs\r\n\r\n* add meta info\r\n\r\n* cleanup\r\n\r\n* cleanup\r\n\r\n* Update quantization_config.py\r\n\r\n* hf_select_quant_linear pass checkpoint_format and meta\r\n\r\n* fix GPTQTestCUDA\r\n\r\n* Update test_gptq.py\r\n\r\n* gptqmodel.hf_select_quant_linear() now does not select ExllamaV2\r\n\r\n* cleanup\r\n\r\n* add backend\r\n\r\n* cleanup\r\n\r\n* cleanup\r\n\r\n* no need check exllama version\r\n\r\n* Update quantization_config.py\r\n\r\n* lower checkpoint_format and backend\r\n\r\n* check none\r\n\r\n* cleanup\r\n\r\n* Update quantization_config.py\r\n\r\n* fix self.use_exllama == False\r\n\r\n* spell\r\n\r\n* fix unittest\r\n\r\n* fix unittest\r\n\r\n---------\r\n\r\nCo-authored-by: LRL <lrl@lbx.dev>\r\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\r\n\r\n* fix format\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix format again\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* update gptqmodel version (#6)\r\n\r\n* update gptqmodel version\r\n\r\n* update gptqmodel version\r\n\r\n* fix unit test (#5)\r\n\r\n* update gptqmodel version\r\n\r\n* update gptqmodel version\r\n\r\n* \"not self.use_exllama\" is not equivalent to \"self.use_exllama==False\"\r\n\r\n* fix unittest\r\n\r\n* update gptqmodel version\r\n\r\n* backend is loading_attibutes (#7)\r\n\r\n* fix format and tests\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix memory check\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix device mismatch\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix result check\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* Update src/transformers/quantizers/quantizer_gptq.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Update src/transformers/quantizers/quantizer_gptq.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Update src/transformers/quantizers/quantizer_gptq.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* update tests\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* review: update docs (#10)\r\n\r\n* review: update docs (#12)\r\n\r\n* review: update docs\r\n\r\n* fix typo\r\n\r\n* update tests for gptqmodel\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* update document (#9)\r\n\r\n* update overview.md\r\n\r\n* cleanup\r\n\r\n* Update overview.md\r\n\r\n* Update overview.md\r\n\r\n* Update overview.md\r\n\r\n* update gptq.md\r\n\r\n* Update gptq.md\r\n\r\n* Update gptq.md\r\n\r\n* Update gptq.md\r\n\r\n* Update gptq.md\r\n\r\n* Update gptq.md\r\n\r\n* Update gptq.md\r\n\r\n---------\r\n\r\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\r\n\r\n* typo\r\n\r\n* doc note for asymmetric quant\r\n\r\n* typo with apple silicon(e)\r\n\r\n* typo for marlin\r\n\r\n* column name revert: review\r\n\r\n* doc rocm support\r\n\r\n* Update docs/source/en/quantization/gptq.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/quantization/gptq.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/quantization/gptq.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/quantization/gptq.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/quantization/overview.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/quantization/overview.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\nCo-authored-by: LRL-ModelCloud <165116337+LRL-ModelCloud@users.noreply.github.com>\r\nCo-authored-by: ZX-ModelCloud <zx@modelcloud.ai>\r\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\r\nCo-authored-by: ZX-ModelCloud <165115237+ZX-ModelCloud@users.noreply.github.com>\r\nCo-authored-by: LRL <lrl@lbx.dev>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Fix : Nemotron Processor in GGUF conversion (#35708)\r\n\r\n* fixing nemotron processor\r\n\r\n* make style\r\n\r\n* Update docs/source/en/quantization/spqr.md\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Add missing TOC to doc\r\n\r\n---------\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\nCo-authored-by: jiqing-feng <jiqing.feng@intel.com>\r\nCo-authored-by: LRL-ModelCloud <165116337+LRL-ModelCloud@users.noreply.github.com>\r\nCo-authored-by: ZX-ModelCloud <zx@modelcloud.ai>\r\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\r\nCo-authored-by: ZX-ModelCloud <165115237+ZX-ModelCloud@users.noreply.github.com>\r\nCo-authored-by: LRL <lrl@lbx.dev>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "845b0a261601d845d87a186163c303d98100d0b9",
    "files": [
        {
            "sha": "700df877d10fb7a6a2d79345ca137fc9430aee7f",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -53,6 +53,9 @@ RUN python3 -m pip install --no-cache-dir aqlm[gpu]==1.0.2\n # Add vptq for quantization testing\n RUN python3 -m pip install --no-cache-dir vptq\n \n+# Add spqr for quantization testing\n+RUN python3 -m pip install --no-cache-dir spqr_quant[gpu]\n+\n # Add hqq for quantization testing\n RUN python3 -m pip install --no-cache-dir hqq\n "
        },
        {
            "sha": "17a1bb1b3b335eba07da6789213868ee29845314",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -166,6 +166,8 @@\n   - local: quantization/aqlm\n     title: AQLM\n   - local: quantization/vptq\n+    title: SpQR\n+  - local: quantization/spqr\n     title: VPTQ\n   - local: quantization/quanto\n     title: Quanto"
        },
        {
            "sha": "6da5b8ce69b5e3aac0c91700751cb4a789670af2",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -81,6 +81,10 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n \n [[autodoc]] BitNetConfig\n \n+## SpQRConfig\n+\n+[[autodoc]] SpQRConfig\n+\n ## FineGrainedFP8Config\n \n [[autodoc]] FineGrainedFP8Config"
        },
        {
            "sha": "94696e300a57ad35587a3fc87ac2a2ee870fae29",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -61,6 +61,7 @@ Use the table below to help you decide which quantization method to use.\n | [FBGEMM_FP8](./fbgemm_fp8.md)                 | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8             | 游댮               | 游릭                          | 游릭                      | https://github.com/pytorch/FBGEMM       |\n | [torchao](./torchao.md)                       | 游릭                   |                 | 游릭        | 游댮        | 游리 <sub>5</sub> | 游댮              |                 | 4/8         |                  | 游릭游댮                        | 游릭                      | https://github.com/pytorch/ao       |\n | [VPTQ](./vptq.md)                             | 游댮                   | 游댮              |     游릭     | 游리        | 游댮                                 | 游댮              | 游릭              | 1/8         | 游댮               | 游릭                          | 游릭                      | https://github.com/microsoft/VPTQ            |\n+| [SpQR](./spqr.md)                          | 游댮                       |  游댮   | 游릭        | 游댮              |    游댮    | 游댮         |         游릭              | 3              |              游댮                     | 游릭           | 游릭                      | https://github.com/Vahe1994/SpQR/       |\n | [FINEGRAINED_FP8](./finegrained_fp8.md)                 | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8             | 游댮               | 游릭                          | 游릭                      |        |\n <Tip>\n   "
        },
        {
            "sha": "b9ebb99b69cb2aac5da253b36647ff4c76f5efa0",
            "filename": "docs/source/en/quantization/spqr.md",
            "status": "added",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -0,0 +1,35 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# SpQR\n+\n+[SpQR](https://github.com/Vahe1994/SpQR) quantization algorithm involves a 16x16 tiled bi-level group 3-bit quantization structure, with sparse outliers as detailed in [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/abs/2306.03078).\n+\n+To SpQR-quantize a model, refer to the [Vahe1994/SpQR](https://github.com/Vahe1994/SpQR) repository.\n+\n+Load a pre-SpQR-quantized model in [`~PreTrainedModel.from_pretrained`].\n+\n+```python\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+import torch\n+\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\",\n+    torch_dtype=torch.half,\n+    device_map=\"auto\"\n+)\n+tokenizer = AutoTokenizer.from_pretrained(\"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\")\n+```"
        },
        {
            "sha": "8b97168ecf5f9433f0bbff17f4df19985217902f",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -1029,6 +1029,7 @@\n         \"HiggsConfig\",\n         \"HqqConfig\",\n         \"QuantoConfig\",\n+        \"SpQRConfig\",\n         \"TorchAoConfig\",\n         \"VptqConfig\",\n     ],\n@@ -6202,6 +6203,7 @@\n         HiggsConfig,\n         HqqConfig,\n         QuantoConfig,\n+        SpQRConfig,\n         TorchAoConfig,\n         VptqConfig,\n     )"
        },
        {
            "sha": "b545c5da50a52e90a2fe765bcdb972c2258a2d95",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -106,6 +106,7 @@\n     ],\n     \"peft\": [\"PeftAdapterMixin\"],\n     \"quanto\": [\"replace_with_quanto_layers\"],\n+    \"spqr\": [\"replace_with_spqr_linear\"],\n     \"vptq\": [\"replace_with_vptq_linear\"],\n }\n \n@@ -210,6 +211,7 @@\n     )\n     from .peft import PeftAdapterMixin\n     from .quanto import replace_with_quanto_layers\n+    from .spqr import replace_with_spqr_linear\n     from .vptq import replace_with_vptq_linear\n \n     try:"
        },
        {
            "sha": "58b71740d37c77f24fe60ff65fe179b2737c4aa7",
            "filename": "src/transformers/integrations/spqr.py",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fintegrations%2Fspqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fintegrations%2Fspqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fspqr.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -0,0 +1,122 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"SpQR (Sparse-Quantized Representation) integration file\"\n+\n+from ..utils import is_accelerate_available, is_spqr_available, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch.nn as nn\n+\n+\n+def replace_with_spqr_linear(\n+    model,\n+    quantization_config=None,\n+    modules_to_not_convert=None,\n+    current_key_name=None,\n+    has_been_replaced=False,\n+):\n+    \"\"\"\n+    Public method that recursively replaces the Linear layers of the given model with SpQR quantized layers.\n+    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n+    conversion has been successful or not.\n+\n+    Args:\n+        model (`torch.nn.Module`):\n+            The model to convert, can be any `torch.nn.Module` instance.\n+        quantization_config (`SpQRConfig`):\n+            The quantization config object that contains the quantization parameters.\n+        modules_to_not_convert (`list[str]`, *optional*):\n+            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n+            converted.\n+        current_key_name (`list`, *optional*):\n+            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n+        has_been_replaced (`bool`, *optional*):\n+            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n+            should not be passed by the user.\n+    \"\"\"\n+    if modules_to_not_convert is None:\n+        modules_to_not_convert = []\n+\n+    if is_accelerate_available():\n+        from accelerate import init_empty_weights\n+    if is_spqr_available():\n+        from spqr_quant import QuantizedLinear\n+\n+    for name, module in model.named_children():\n+        if current_key_name is None:\n+            current_key_name = []\n+        current_key_name.append(name)\n+\n+        if isinstance(module, nn.Linear):\n+            # Check if the current key is not in the `modules_to_not_convert`\n+            if \".\".join(current_key_name) + \".weight\" not in modules_to_not_convert:\n+                with init_empty_weights():\n+                    tensor_name = \".\".join(current_key_name)\n+\n+                    shapes = quantization_config.shapes\n+                    shapes_keys = shapes.keys()\n+\n+                    shapes_valid = (\n+                        f\"{tensor_name}.dense_weights.shape\" in shapes_keys\n+                        and f\"{tensor_name}.row_offsets.shape\" in shapes_keys\n+                        and f\"{tensor_name}.col_vals.shape\" in shapes_keys\n+                        and f\"{tensor_name}.in_perm.shape\" in shapes_keys\n+                    )\n+\n+                    if not shapes_valid:\n+                        raise ValueError(\n+                            f\"The SpQR quantization config does not contain the shape \"\n+                            f\"configuration for {tensor_name}. This indicates that the \"\n+                            f\"configuration is either invalid or corrupted.\"\n+                        )\n+\n+                    dense_weights_shape = shapes[f\"{tensor_name}.dense_weights.shape\"]\n+                    row_offsets_shape = shapes[f\"{tensor_name}.row_offsets.shape\"]\n+                    col_vals_shape = shapes[f\"{tensor_name}.col_vals.shape\"]\n+                    in_perm_shape = shapes[f\"{tensor_name}.in_perm.shape\"]\n+\n+                    in_features = module.in_features\n+                    out_features = module.out_features\n+\n+                    model._modules[name] = QuantizedLinear.create_placehodler(\n+                        rows=out_features,\n+                        cols=in_features,\n+                        bits=quantization_config.bits,\n+                        beta1=quantization_config.beta1,\n+                        beta2=quantization_config.beta2,\n+                        dense_weights_shape=dense_weights_shape,\n+                        row_offsets_shape=row_offsets_shape,\n+                        col_vals_shape=col_vals_shape,\n+                        in_perm_shape=in_perm_shape,\n+                    )\n+                    has_been_replaced = True\n+\n+                    # Store the module class in case we need to transpose the weight later\n+                    model._modules[name].source_cls = type(module)\n+                    # Force requires grad to False to avoid unexpected errors\n+                    model._modules[name].requires_grad_(False)\n+            else:\n+                pass\n+        if len(list(module.children())) > 0:\n+            _, has_been_replaced = replace_with_spqr_linear(\n+                module,\n+                quantization_config=quantization_config,\n+                modules_to_not_convert=modules_to_not_convert,\n+                current_key_name=current_key_name,\n+                has_been_replaced=has_been_replaced,\n+            )\n+        # Remove the last key for recursion\n+        current_key_name.pop(-1)\n+    return model, has_been_replaced"
        },
        {
            "sha": "ee7c832b1de17a879683b77e5c0a573406395b28",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -31,6 +31,7 @@\n     QuantizationConfigMixin,\n     QuantizationMethod,\n     QuantoConfig,\n+    SpQRConfig,\n     TorchAoConfig,\n     VptqConfig,\n )\n@@ -47,6 +48,7 @@\n from .quantizer_higgs import HiggsHfQuantizer\n from .quantizer_hqq import HqqHfQuantizer\n from .quantizer_quanto import QuantoHfQuantizer\n+from .quantizer_spqr import SpQRHfQuantizer\n from .quantizer_torchao import TorchAoHfQuantizer\n from .quantizer_vptq import VptqHfQuantizer\n \n@@ -66,6 +68,7 @@\n     \"torchao\": TorchAoHfQuantizer,\n     \"bitnet\": BitNetHfQuantizer,\n     \"vptq\": VptqHfQuantizer,\n+    \"spqr\": SpQRHfQuantizer,\n     \"fp8\": FineGrainedFP8HfQuantizer,\n }\n \n@@ -84,6 +87,7 @@\n     \"torchao\": TorchAoConfig,\n     \"bitnet\": BitNetConfig,\n     \"vptq\": VptqConfig,\n+    \"spqr\": SpQRConfig,\n     \"fp8\": FineGrainedFP8Config,\n }\n "
        },
        {
            "sha": "60cc1bca9b279bab45c8134ec540f132b19fb021",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "added",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -0,0 +1,83 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/lic enses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING, Optional\n+\n+from .base import HfQuantizer\n+\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+\n+from ..integrations import replace_with_spqr_linear\n+from ..utils import is_accelerate_available, is_spqr_available, is_torch_available, logging\n+from ..utils.quantization_config import QuantizationConfigMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SpQRHfQuantizer(HfQuantizer):\n+    \"\"\"\n+    Quantizer of the SpQR method. Enables the loading of prequantized models.\n+    \"\"\"\n+\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+        self.quantization_config = quantization_config\n+\n+    def validate_environment(self, *args, **kwargs):\n+        if not torch.cuda.is_available():\n+            raise RuntimeError(\"GPU is required to run SpQR quantized model.\")\n+\n+        if not is_accelerate_available():\n+            raise ImportError(\"Using `spqr` quantization requires Accelerate: `pip install accelerate`\")\n+\n+        if not is_spqr_available():\n+            raise ImportError(\"Using `spqr` quantization requires SpQR: `pip install spqr_quant[gpu]`\")\n+\n+    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        if torch_dtype is None:\n+            torch_dtype = torch.float16\n+            logger.info(\"Assuming SpQR inference on GPU and loading the model in `torch.float16`.\")\n+        elif torch_dtype != torch.float16:\n+            raise ValueError(\n+                \"You cannot use any type other than torch.float16 for SpQR. Please either leave it None or set it to\"\n+                \"torch.float16 explicitly.\"\n+            )\n+        return torch_dtype\n+\n+    def _process_model_before_weight_loading(\n+        self,\n+        model: \"PreTrainedModel\",\n+        **kwargs,\n+    ):\n+        replace_with_spqr_linear(\n+            model,\n+            quantization_config=self.quantization_config,\n+            modules_to_not_convert=self.quantization_config.modules_to_not_convert,\n+        )\n+        model.config.quantization_config = self.quantization_config\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        return model\n+\n+    @property\n+    def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n+        return False\n+\n+    def is_serializable(self, safe_serialization=None):\n+        return True"
        },
        {
            "sha": "14fef29884884f37806df934cfab960b80fffbd8",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -121,6 +121,7 @@\n     is_seqio_available,\n     is_soundfile_available,\n     is_spacy_available,\n+    is_spqr_available,\n     is_sudachi_available,\n     is_sudachi_projection_available,\n     is_tensorflow_probability_available,\n@@ -1191,6 +1192,13 @@ def require_vptq(test_case):\n     return unittest.skipUnless(is_vptq_available(), \"test requires vptq\")(test_case)\n \n \n+def require_spqr(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires spqr\n+    \"\"\"\n+    return unittest.skipUnless(is_spqr_available(), \"test requires spqr\")(test_case)\n+\n+\n def require_eetq(test_case):\n     \"\"\"\n     Decorator marking a test that requires eetq"
        },
        {
            "sha": "cf13060ee3077c2ba933c90b3c04830a208f743b",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -193,6 +193,7 @@\n     is_soundfile_available,\n     is_spacy_available,\n     is_speech_available,\n+    is_spqr_available,\n     is_sudachi_available,\n     is_sudachi_projection_available,\n     is_tensorflow_probability_available,"
        },
        {
            "sha": "bd95b6f282c0951b9b8c269952722d2cda041455",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -201,6 +201,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _blobfile_available = _is_package_available(\"blobfile\")\n _liger_kernel_available = _is_package_available(\"liger_kernel\")\n _triton_available = _is_package_available(\"triton\")\n+_spqr_available = _is_package_available(\"spqr_quant\")\n \n _torch_version = \"N/A\"\n _torch_available = False\n@@ -1213,6 +1214,10 @@ def is_speech_available():\n     return _torchaudio_available\n \n \n+def is_spqr_available():\n+    return _spqr_available\n+\n+\n def is_phonemizer_available():\n     return _phonemizer_available\n "
        },
        {
            "sha": "11415e895d91da9e1f5e8a39cdc4a5e6a330c1a2",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -56,6 +56,7 @@ class QuantizationMethod(str, Enum):\n     FBGEMM_FP8 = \"fbgemm_fp8\"\n     TORCHAO = \"torchao\"\n     BITNET = \"bitnet\"\n+    SPQR = \"spqr\"\n     FP8 = \"fp8\"\n \n \n@@ -1551,6 +1552,75 @@ def post_init(self):\n         pass\n \n \n+@dataclass\n+class SpQRConfig(QuantizationConfigMixin):\n+    \"\"\"\n+    This is a wrapper class about `spqr` parameters. Refer to the original publication for more details.\n+\n+    Args:\n+        bits (`int`, *optional*, defaults to 3):\n+            Specifies the bit count for the weights and first order zero-points and scales.\n+            Currently only bits = 3 is supported.\n+        beta1 (`int`, *optional*, defaults to 16):\n+            SpQR tile width. Currently only beta1 = 16 is supported.\n+        beta2 (`int`, *optional*, defaults to 16):\n+            SpQR tile height. Currently only beta2 = 16 is supported.\n+        shapes (`Optional`, *optional*):\n+            A dictionary holding the shape of each object. We need this because it's impossible\n+            to deduce the exact size of the parameters just from bits, beta1, beta2.\n+        modules_to_not_convert (`Optional[List[str]]`, *optional*):\n+            Optionally, provides a list of full paths of `nn.Linear` weight parameters that shall not be quantized.\n+            Defaults to None.\n+        kwargs (`Dict[str, Any]`, *optional*):\n+            Additional parameters from which to initialize the configuration object.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        bits: int = 3,\n+        beta1: int = 16,\n+        beta2: int = 16,\n+        shapes: Optional[Dict[str, int]] = None,\n+        modules_to_not_convert: Optional[List[str]] = None,\n+        **kwargs,\n+    ):\n+        if shapes is None:\n+            shapes = {}\n+        self.shapes = shapes\n+        self.quant_method = QuantizationMethod.SPQR\n+        self.bits = bits\n+        self.beta1 = beta1\n+        self.beta2 = beta2\n+        if modules_to_not_convert is None:\n+            modules_to_not_convert = []\n+        self.modules_to_not_convert = modules_to_not_convert\n+        self.post_init()\n+\n+    def post_init(self):\n+        r\"\"\"\n+        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n+        \"\"\"\n+        if not isinstance(self.bits, int):\n+            raise TypeError(\"bits must be an int\")\n+        if not isinstance(self.beta1, int):\n+            raise TypeError(\"beta1 must be an int\")\n+        if not isinstance(self.beta2, int):\n+            raise TypeError(\"beta2 must be an int\")\n+\n+        if self.bits != 3:\n+            raise ValueError(\"SpQR currently only supports bits = 3\")\n+        if self.beta1 != 16:\n+            raise ValueError(\"SpQR currently only supports beta1 = 16\")\n+        if self.beta2 != 16:\n+            raise ValueError(\"SpQR currently only supports beta2 = 16\")\n+\n+        if self.modules_to_not_convert is not None and not isinstance(self.modules_to_not_convert, list):\n+            raise ValueError(\"modules_to_not_convert must be a list of strings\")\n+\n+        if not isinstance(self.shapes, dict):\n+            raise TypeError(\"shapes must be a dict\")\n+\n+\n @dataclass\n class FineGrainedFP8Config(QuantizationConfigMixin):\n     \"\"\""
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/spqr_integration/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/tests%2Fquantization%2Fspqr_integration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/tests%2Fquantization%2Fspqr_integration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2F__init__.py?ref=845b0a261601d845d87a186163c303d98100d0b9"
        },
        {
            "sha": "134e57af5de1ced38e2796494984654a95e5b5b0",
            "filename": "tests/quantization/spqr_integration/test_spqr.py",
            "status": "added",
            "additions": 249,
            "deletions": 0,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/845b0a261601d845d87a186163c303d98100d0b9/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/845b0a261601d845d87a186163c303d98100d0b9/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py?ref=845b0a261601d845d87a186163c303d98100d0b9",
            "patch": "@@ -0,0 +1,249 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, SpQRConfig, StaticCache\n+from transformers.testing_utils import (\n+    require_accelerate,\n+    require_spqr,\n+    require_torch_gpu,\n+    require_torch_multi_gpu,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_accelerate_available, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n+\n+@require_torch_gpu\n+class SpQRConfigTest(unittest.TestCase):\n+    def test_to_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a config and converts it to a dict, the dict is the same as the config object\n+        \"\"\"\n+        quantization_config = SpQRConfig()\n+        config_to_dict = quantization_config.to_dict()\n+\n+        for key in config_to_dict:\n+            self.assertEqual(getattr(quantization_config, key), config_to_dict[key])\n+\n+    def test_from_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a dict and converts it to a config object, the config object is the same as the dict\n+        \"\"\"\n+        dict = {\n+            \"beta1\": 16,\n+            \"beta2\": 16,\n+            \"bits\": 3,\n+            \"modules_to_not_convert\": [\"lm_head.weight\"],\n+            \"shapes\": {\"model.layers.0.self_attn.q_proj.dense_weights.shape\": 16},\n+        }\n+        quantization_config = SpQRConfig.from_dict(dict)\n+\n+        self.assertEqual(dict[\"beta1\"], quantization_config.beta1)\n+        self.assertEqual(dict[\"beta2\"], quantization_config.beta2)\n+        self.assertEqual(dict[\"bits\"], quantization_config.bits)\n+        self.assertEqual(dict[\"modules_to_not_convert\"], quantization_config.modules_to_not_convert)\n+        self.assertEqual(dict[\"shapes\"], quantization_config.shapes)\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_spqr\n+@require_accelerate\n+class SpQRTest(unittest.TestCase):\n+    model_name = \"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\"\n+\n+    input_text = \"Hello my name is\"\n+    max_new_tokens = 32\n+\n+    EXPECTED_OUTPUT = (\n+        \"Hello my name is Jesse. (I'm also known as Jesse) I'm a 25 year old male from United States. I'm looking for\"\n+    )\n+    EXPECTED_OUTPUT_COMPILE = \"Hello my name is Jake and I am a 20 year old student at the University of North Texas. (Go Mean Green!) I am a huge fan of the Dallas\"\n+\n+    device_map = \"cuda\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Setup quantized model\n+        \"\"\"\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name,\n+            device_map=cls.device_map,\n+        )\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_quantized_model_conversion(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model has been converted properly\n+        \"\"\"\n+        from spqr_quant import QuantizedLinear\n+\n+        from transformers.integrations import replace_with_spqr_linear\n+\n+        model_id = \"meta-llama/Llama-2-7b-hf\"\n+        config = AutoConfig.from_pretrained(model_id)\n+        quantization_config = AutoConfig.from_pretrained(self.model_name, return_dict=False).quantization_config\n+        quantization_config = SpQRConfig.from_dict(quantization_config)\n+\n+        with init_empty_weights():\n+            model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, config=config)\n+\n+        nb_linears = 0\n+        for module in model.modules():\n+            if isinstance(module, torch.nn.Linear):\n+                nb_linears += 1\n+\n+        model, _ = replace_with_spqr_linear(\n+            model,\n+            quantization_config=quantization_config,\n+            modules_to_not_convert=quantization_config.modules_to_not_convert,\n+        )\n+\n+        nb_spqr_linear = 0\n+        for module in model.modules():\n+            if isinstance(module, QuantizedLinear):\n+                nb_spqr_linear += 1\n+\n+        self.assertEqual(nb_linears - 1, nb_spqr_linear)\n+\n+    def test_quantized_model(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    def test_raise_if_non_quantized(self):\n+        model_id = \"meta-llama/Llama-2-7b-hf\"\n+        quantization_config = SpQRConfig()\n+\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n+\n+    @unittest.skip\n+    def test_save_pretrained(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_quantized_model_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with multiple GPUs\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        quantized_model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\")\n+\n+        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    def test_quantized_model_compile(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+\n+        # Sample tokens greedily\n+        def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_values):\n+            logits = model(\n+                cur_token,\n+                position_ids=input_pos,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+                return_dict=False,\n+                use_cache=True,\n+            )[0]\n+            new_token = torch.argmax(logits[:, [-1]], dim=-1).to(torch.int)\n+\n+            return new_token\n+\n+        # Tokenize the test input\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)[\"input_ids\"]\n+        seq_length = input_ids.shape[1]\n+\n+        # Setup static KV cache for generation\n+        past_key_values = StaticCache(\n+            config=self.quantized_model.config,\n+            batch_size=1,\n+            max_cache_len=seq_length + self.max_new_tokens + 1,\n+            device=torch_device,\n+            dtype=self.quantized_model.config._pre_quantization_dtype,\n+        )\n+\n+        # Allocate token ids to be generated and copy prefix ids\n+        cache_position = torch.arange(seq_length, device=torch_device)\n+        generated_ids = torch.zeros(1, seq_length + self.max_new_tokens, dtype=torch.int, device=torch_device)\n+        generated_ids[:, cache_position] = input_ids.to(torch_device).to(torch.int)\n+\n+        # Do a forward pass to fill the prefix cache and compile the kernels if necessary\n+        logits = self.quantized_model(\n+            input_ids,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            return_dict=False,\n+            use_cache=True,\n+        )[0]\n+        next_token = torch.argmax(logits[:, [-1]], dim=-1).to(torch.int)\n+        generated_ids[:, [seq_length]] = next_token\n+\n+        with torch.no_grad():\n+            # Compile the CUDA graph\n+            decode_one_tokens = torch.compile(decode_one_tokens, mode=\"default\", backend=\"inductor\", fullgraph=True)\n+\n+            # Generate tokens one by one\n+            cache_position = torch.tensor([seq_length + 1], device=torch_device)\n+            for _ in range(1, self.max_new_tokens):\n+                with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n+                    next_token = decode_one_tokens(\n+                        self.quantized_model, next_token.clone(), None, cache_position, past_key_values\n+                    )\n+                    generated_ids.index_copy_(1, cache_position, next_token)\n+                cache_position += 1\n+\n+        # Check generated text\n+        self.assertEqual(\n+            self.tokenizer.decode(generated_ids[0], skip_special_tokens=True), self.EXPECTED_OUTPUT_COMPILE\n+        )"
        }
    ],
    "stats": {
        "total": 591,
        "additions": 591,
        "deletions": 0
    }
}