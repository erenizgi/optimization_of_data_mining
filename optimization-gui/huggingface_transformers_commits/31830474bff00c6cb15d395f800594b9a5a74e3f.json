{
    "author": "dvrogozh",
    "message": "Fix `test_eager_matches_sdpa_inference` for `XPU` backend (#34889)\n\n* Use torch.nn.attention.sdpa_kernel instead of deprecated torch.backends.cuda.sdp_kernel\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\r\n\r\n* Fix test_eager_matches_sdpa_inference for XPU backend\r\n\r\nAs of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\r\nwhich is implemented on PyTorch level using aten operators and is device\r\nagnostic with respect to implementation of each aten operator. Thus, we can\r\nreuse CUDA (or CPU) MATH weights for XPU.\r\n\r\nFixes: #34888\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\r\n\r\n* Use torch.amp.autocast instead of deprecated torch.cuda.amp.autocast in nemotron\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\r\n\r\n---------\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>",
    "sha": "31830474bff00c6cb15d395f800594b9a5a74e3f",
    "files": [
        {
            "sha": "a56b5c68085cb3e9ad8453e2e2480ae4d15ec1b6",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31830474bff00c6cb15d395f800594b9a5a74e3f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31830474bff00c6cb15d395f800594b9a5a74e3f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=31830474bff00c6cb15d395f800594b9a5a74e3f",
            "patch": "@@ -76,7 +76,7 @@ def __init__(\n \n     def forward(self, input: Tensor) -> Tensor:\n         args = _cast_if_autocast_enabled(input, self.normalized_shape, self.weight + 1, self.bias, self.eps)\n-        with torch.cuda.amp.autocast(enabled=False):\n+        with torch.amp.autocast(input.device.type, enabled=False):\n             return F.layer_norm(*args)\n \n "
        },
        {
            "sha": "4f6cfaff7e99abb6818612b2c33f140ae6a26044",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=31830474bff00c6cb15d395f800594b9a5a74e3f",
            "patch": "@@ -41,7 +41,7 @@\n )\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor, sdpa_kernel\n \n \n if is_torch_available():\n@@ -636,7 +636,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                     # TODO: test gradients as well (& for FA2 as well!)\n                                     with torch.no_grad():\n-                                        with torch.backends.cuda.sdp_kernel(\n+                                        with sdpa_kernel(\n                                             enable_flash=enable_kernels,\n                                             enable_math=True,\n                                             enable_mem_efficient=enable_kernels,\n@@ -653,6 +653,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                     if torch_device in [\"cpu\", \"cuda\"]:\n                                         atol = atols[torch_device, enable_kernels, torch_dtype]\n                                         rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                    elif torch_device == \"xpu\":\n+                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                        # which is implemented on PyTorch level using aten operators and is\n+                                        # device agnostic with respect to implementation of each aten operator.\n+                                        atol = atols[\"cuda\", False, torch_dtype]\n+                                        rtol = rtols[\"cuda\", False, torch_dtype]\n                                     else:\n                                         atol = 1e-7\n                                         rtol = 1e-4"
        },
        {
            "sha": "3ea60d550e06cfbacf7e5117ec352c1bc0ad3dc3",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 16,
            "deletions": 4,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=31830474bff00c6cb15d395f800594b9a5a74e3f",
            "patch": "@@ -47,7 +47,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, sdpa_kernel\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -607,7 +607,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 # TODO: test gradients as well (& for FA2 as well!)\n                                 with torch.no_grad():\n-                                    with torch.backends.cuda.sdp_kernel(\n+                                    with sdpa_kernel(\n                                         enable_flash=enable_kernels,\n                                         enable_math=True,\n                                         enable_mem_efficient=enable_kernels,\n@@ -629,6 +629,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                 if torch_device in [\"cpu\", \"cuda\"]:\n                                     atol = atols[torch_device, enable_kernels, torch_dtype]\n                                     rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                elif torch_device == \"xpu\":\n+                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                    # which is implemented on PyTorch level using aten operators and is\n+                                    # device agnostic with respect to implementation of each aten operator.\n+                                    atol = atols[\"cuda\", False, torch_dtype]\n+                                    rtol = rtols[\"cuda\", False, torch_dtype]\n                                 else:\n                                     atol = 1e-7\n                                     rtol = 1e-4\n@@ -1343,7 +1349,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                     if isinstance(inp, torch.Tensor) and inp.dtype in [torch.float32, torch.float16]:\n                         inputs_dict[name] = inp.to(torch.float16)\n \n-                with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+                with sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n     @require_flash_attn\n@@ -1669,7 +1675,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                 # TODO: test gradients as well (& for FA2 as well!)\n                                 # Ignore copy\n                                 with torch.no_grad():\n-                                    with torch.backends.cuda.sdp_kernel(\n+                                    with sdpa_kernel(\n                                         enable_flash=enable_kernels,\n                                         enable_math=True,\n                                         enable_mem_efficient=enable_kernels,\n@@ -1691,6 +1697,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                 if torch_device in [\"cpu\", \"cuda\"]:\n                                     atol = atols[torch_device, enable_kernels, torch_dtype]\n                                     rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                elif torch_device == \"xpu\":\n+                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                    # which is implemented on PyTorch level using aten operators and is\n+                                    # device agnostic with respect to implementation of each aten operator.\n+                                    atol = atols[\"cuda\", False, torch_dtype]\n+                                    rtol = rtols[\"cuda\", False, torch_dtype]\n                                 else:\n                                     atol = 1e-7\n                                     rtol = 1e-4"
        },
        {
            "sha": "bc8baa2746adde407a4f3bbeed3017155890c403",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 16,
            "deletions": 4,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=31830474bff00c6cb15d395f800594b9a5a74e3f",
            "patch": "@@ -48,7 +48,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, sdpa_kernel\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -615,7 +615,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 # TODO: test gradients as well (& for FA2 as well!)\n                                 with torch.no_grad():\n-                                    with torch.backends.cuda.sdp_kernel(\n+                                    with sdpa_kernel(\n                                         enable_flash=enable_kernels,\n                                         enable_math=True,\n                                         enable_mem_efficient=enable_kernels,\n@@ -637,6 +637,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                 if torch_device in [\"cpu\", \"cuda\"]:\n                                     atol = atols[torch_device, enable_kernels, torch_dtype]\n                                     rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                elif torch_device == \"xpu\":\n+                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                    # which is implemented on PyTorch level using aten operators and is\n+                                    # device agnostic with respect to implementation of each aten operator.\n+                                    atol = atols[\"cuda\", False, torch_dtype]\n+                                    rtol = rtols[\"cuda\", False, torch_dtype]\n                                 else:\n                                     atol = 1e-7\n                                     rtol = 1e-4\n@@ -1333,7 +1339,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                     if isinstance(inp, torch.Tensor) and inp.dtype in [torch.float32, torch.float16]:\n                         inputs_dict[name] = inp.to(torch.float16)\n \n-                with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+                with sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n     @require_flash_attn\n@@ -1632,7 +1638,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                 # TODO: test gradients as well (& for FA2 as well!)\n                                 # Ignore copy\n                                 with torch.no_grad():\n-                                    with torch.backends.cuda.sdp_kernel(\n+                                    with sdpa_kernel(\n                                         enable_flash=enable_kernels,\n                                         enable_math=True,\n                                         enable_mem_efficient=enable_kernels,\n@@ -1654,6 +1660,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                 if torch_device in [\"cpu\", \"cuda\"]:\n                                     atol = atols[torch_device, enable_kernels, torch_dtype]\n                                     rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                elif torch_device == \"xpu\":\n+                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                    # which is implemented on PyTorch level using aten operators and is\n+                                    # device agnostic with respect to implementation of each aten operator.\n+                                    atol = atols[\"cuda\", False, torch_dtype]\n+                                    rtol = rtols[\"cuda\", False, torch_dtype]\n                                 else:\n                                     atol = 1e-7\n                                     rtol = 1e-4"
        },
        {
            "sha": "99d0a8058c67f8ecc4956b3ead441f4629eb639c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31830474bff00c6cb15d395f800594b9a5a74e3f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=31830474bff00c6cb15d395f800594b9a5a74e3f",
            "patch": "@@ -187,6 +187,22 @@ def _deepspeed_zero3(ds_config):\n         unset_hf_deepspeed_config()\n \n \n+def sdpa_kernel(enable_flash, enable_math, enable_mem_efficient):\n+    if version.parse(torch.__version__).release < version.parse(\"2.3\").release:\n+        return torch.backends.cuda.sdp_kernel(\n+            enable_flash=enable_flash, enable_math=enable_math, enable_mem_efficient=enable_mem_efficient\n+        )\n+\n+    backends = []\n+    if enable_flash:\n+        backends += [torch.nn.attention.SDPBackend.FLASH_ATTENTION]\n+    if enable_math:\n+        backends += [torch.nn.attention.SDPBackend.MATH]\n+    if enable_mem_efficient:\n+        backends += [torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION]\n+    return torch.nn.attention.sdpa_kernel(backends)\n+\n+\n @require_torch\n class ModelTesterMixin:\n     model_tester = None\n@@ -4175,7 +4191,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                     # TODO: test gradients as well (& for FA2 as well!)\n                                     with torch.no_grad():\n-                                        with torch.backends.cuda.sdp_kernel(\n+                                        with sdpa_kernel(\n                                             enable_flash=enable_kernels,\n                                             enable_math=True,\n                                             enable_mem_efficient=enable_kernels,\n@@ -4198,6 +4214,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                     if torch_device in [\"cpu\", \"cuda\"]:\n                                         atol = atols[torch_device, enable_kernels, torch_dtype]\n                                         rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                                    elif torch_device == \"xpu\":\n+                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                                        # which is implemented on PyTorch level using aten operators and is\n+                                        # device agnostic with respect to implementation of each aten operator.\n+                                        atol = atols[\"cuda\", False, torch_dtype]\n+                                        rtol = rtols[\"cuda\", False, torch_dtype]\n                                     else:\n                                         atol = 1e-7\n                                         rtol = 1e-4"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 64,
        "deletions": 12
    }
}