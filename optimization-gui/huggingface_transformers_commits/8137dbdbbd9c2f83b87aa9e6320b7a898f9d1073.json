{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ [v5] Rename left traces of `past_key_value` in BERT-like models (#41448)\n\nrename everything",
    "sha": "8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
    "files": [
        {
            "sha": "af9337828e3c878c7b5d7afcc6f34b1a1ed3e398",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -470,17 +470,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -490,7 +490,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "6a2005e9e945a117fd7c5462b423ecd8dce6c48c",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -222,17 +222,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -242,7 +242,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "223dd78ebb2284c45d47129a6d24399691187a54",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -175,7 +175,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -187,14 +187,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only bert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -248,7 +248,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -261,22 +261,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -324,7 +324,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -333,7 +333,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -396,14 +396,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -421,7 +421,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -460,7 +460,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "4bfb448f575600c1cd75969d7f124d04bf686a77",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -121,7 +121,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -133,14 +133,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only bert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -195,7 +195,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -208,22 +208,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -258,7 +258,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -267,7 +267,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -333,14 +333,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -358,7 +358,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -399,7 +399,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "739425fa00e9b557a4fdcacbe9c6c3d3236df02d",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -1273,17 +1273,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -1293,7 +1293,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "49f24a7c15be6274528a7e72183820bd5a12fd63",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -197,17 +197,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -217,7 +217,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "5036260977c59cc24d7090fe699e2016fed442e6",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -218,17 +218,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -238,7 +238,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "8d0daafc28704e99ff7c7b263177d47cd82d78c5",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -202,17 +202,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -222,7 +222,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "4c7ee917966907d48ce6712a69ef218c565b8b46",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -155,17 +155,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = curr_past_key_values.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)\n@@ -181,7 +181,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n+                key_layer, value_layer = curr_past_key_values.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "39b88a3acce983f73ec525de619bef340b6bba58",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -466,7 +466,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -478,14 +478,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only roberta can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -540,7 +540,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -553,22 +553,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -603,7 +603,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -612,7 +612,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -643,13 +643,13 @@ def forward(\n         encoder_hidden_states,\n         attention_mask=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         self_attention_output, self_attn_weights = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=None,\n+            past_key_values=None,\n             **kwargs,\n         )\n         attention_output = self_attention_output\n@@ -659,7 +659,7 @@ def forward(\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             **kwargs,\n         )\n         attention_output = cross_attention_output\n@@ -706,15 +706,15 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         outputs = ()\n         self_attention_output, self_attn_weights = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -732,7 +732,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -784,7 +784,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "5c0f67f140cb3bf6963cccef6d7c06b60642a8e8",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -115,7 +115,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -127,14 +127,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only camembert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -188,7 +188,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -201,22 +201,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -264,7 +264,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -273,7 +273,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -336,14 +336,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -361,7 +361,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -569,7 +569,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "c5b4adfd9cd4e8cc01bf621760c04dd25a2a61e8",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -220,7 +220,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -232,14 +232,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only data2vec_text can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -293,7 +293,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -306,22 +306,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -369,7 +369,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -378,7 +378,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -441,14 +441,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -466,7 +466,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -540,7 +540,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "f2bedae2db8b43ca6a5e2d4ba8fa94abe3320196",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -188,11 +188,11 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n@@ -205,8 +205,8 @@ def forward(\n \n             # Try to get key/value states from cache if possible\n             if past_key_values is not None and is_updated:\n-                key_states = curr_past_key_value.layers[self.layer_idx].keys\n-                value_states = curr_past_key_value.layers[self.layer_idx].values\n+                key_states = curr_past_key_values.layers[self.layer_idx].keys\n+                value_states = curr_past_key_values.layers[self.layer_idx].values\n             else:\n                 key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n                 shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n@@ -226,7 +226,7 @@ def forward(\n         ):\n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n             cache_position = cache_position if not is_cross_attention else None\n-            key_states, value_states = curr_past_key_value.update(\n+            key_states, value_states = curr_past_key_values.update(\n                 key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n             # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "d7fc6187e75b8058028c37305e65fa27a1263d24",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -407,11 +407,11 @@ def forward(\n         \"\"\"\n         # Self Attention\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n+        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n         # add present self-attn cache to positions 1,2 of present_key_value tuple\n         atten_out = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_values=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_values,\n             attention_mask=(1 - attention_mask) * torch.finfo(hidden_states.dtype).min,\n             output_attentions=output_attentions,\n         )"
        },
        {
            "sha": "bf617665c542dc0bafac2d5ba288e24ffbaf1485",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -362,12 +362,12 @@ def forward(\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n+        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n             output_attentions=output_attentions,\n-            past_key_values=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_values,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -387,13 +387,13 @@ def forward(\n                 )\n \n             # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n-            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n+            cross_attn_past_key_values = past_key_values[-2:] if past_key_values is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                cross_attn_past_key_value,\n+                cross_attn_past_key_values,\n                 output_attentions,\n             )\n             attention_output = cross_attention_outputs[0]"
        },
        {
            "sha": "86478bcf5a18208a74db7725965fff388b93325b",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -374,12 +374,12 @@ def forward(\n         output_attentions=False,\n     ):\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n+        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n             output_attentions=output_attentions,\n-            past_key_values=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_values,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -399,13 +399,13 @@ def forward(\n                 )\n \n             # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n-            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n+            cross_attn_past_key_values = past_key_values[-2:] if past_key_values is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                cross_attn_past_key_value,\n+                cross_attn_past_key_values,\n                 output_attentions,\n             )\n             attention_output = cross_attention_outputs[0]"
        },
        {
            "sha": "7a135b9fdb5e3a766aa66e364327b8f9ad3f9a5b",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -338,12 +338,12 @@ def forward(\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n+        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n             output_attentions=output_attentions,\n-            past_key_values=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_values,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -363,13 +363,13 @@ def forward(\n                 )\n \n             # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n-            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n+            cross_attn_past_key_values = past_key_values[-2:] if past_key_values is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                cross_attn_past_key_value,\n+                cross_attn_past_key_values,\n                 output_attentions,\n             )\n             attention_output = cross_attention_outputs[0]"
        },
        {
            "sha": "617e4d757c94998bc191cd530f885f9be9490fc7",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -312,11 +312,11 @@ def forward(\n \n         # Self Attention\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n+        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n         # add present self-attn cache to positions 1,2 of present_key_value tuple\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_values=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_values,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n         )\n@@ -331,12 +331,12 @@ def forward(\n             residual = hidden_states\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n+            cross_attn_past_key_values = past_key_values[-2:] if past_key_values is not None else None\n             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                past_key_values=cross_attn_past_key_value,\n+                past_key_values=cross_attn_past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)"
        },
        {
            "sha": "e48bc1182c03d3d8f03cf3e629293f2c10151d96",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -1114,10 +1114,10 @@ def forward(\n     ):\n         # 1st residual block\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n+        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n         ngram_attention_output, self_attn_weights, self_attn_weights_ngram, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_values=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_values,\n             attention_mask=attention_mask,\n             extended_predict_attention_mask=extended_predict_attention_mask,\n             main_relative_position_buckets=main_relative_position_buckets,\n@@ -1127,15 +1127,15 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states + ngram_attention_output)\n \n         # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-        cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n+        cross_attn_past_key_values = past_key_values[-2:] if past_key_values is not None else None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             # 2nd residual block\n             attention_output, cross_attn_weights, cross_attn_present_key_value = self.cross_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attn_mask,\n-                past_key_values=cross_attn_past_key_value,\n+                past_key_values=cross_attn_past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = self.cross_attn_layer_norm(attention_output + hidden_states)"
        },
        {
            "sha": "be8ea4b59735d911c1f17b21a6b1c0d81be1505c",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -182,7 +182,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -194,14 +194,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only bert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -256,7 +256,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -269,22 +269,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -334,7 +334,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -343,7 +343,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -409,14 +409,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -434,7 +434,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -474,7 +474,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "73eaf531290b1745039ea4c9e23bb03fc7ce3462",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -191,7 +191,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -203,14 +203,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only ernie can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -264,7 +264,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -277,22 +277,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -340,7 +340,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -349,7 +349,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -412,14 +412,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -437,7 +437,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -531,7 +531,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "7d89d774a68be00b6cc750eda07fdbcde5377ef1",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -118,17 +118,17 @@ def forward(\n                 is_updated = cache.is_updated.get(self.layer_id)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = cache.cross_attention_cache\n+                    curr_past_key_values = cache.cross_attention_cache\n                 else:\n-                    curr_past_key_value = cache.self_attention_cache\n+                    curr_past_key_values = cache.self_attention_cache\n             else:\n-                curr_past_key_value = cache\n+                curr_past_key_values = cache\n \n         current_states = kv if is_cross_attention else input\n         if is_cross_attention and cache is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            k = curr_past_key_value.key_cache[self.layer_id]\n-            v = curr_past_key_value.value_cache[self.layer_id]\n+            k = curr_past_key_values.key_cache[self.layer_id]\n+            v = curr_past_key_values.value_cache[self.layer_id]\n         else:\n             k = self.k_lin(current_states)\n             v = self.v_lin(current_states)\n@@ -138,7 +138,7 @@ def forward(\n             if cache is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                k, v = curr_past_key_value.update(k, v, self.layer_id, {\"cache_position\": cache_position})\n+                k, v = curr_past_key_values.update(k, v, self.layer_id, {\"cache_position\": cache_position})\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n                     cache.is_updated[self.layer_id] = True"
        },
        {
            "sha": "6d57988c78dc9fe346bfa17c7e6abd18a9abf068",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -738,18 +738,18 @@ def forward(\n                 is_updated = layer_state.is_updated.get(self.layer_idx)\n                 if self.encoder_decoder_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = layer_state.cross_attention_cache\n+                    curr_past_key_values = layer_state.cross_attention_cache\n                 else:\n-                    curr_past_key_value = layer_state.self_attention_cache\n+                    curr_past_key_values = layer_state.self_attention_cache\n             else:\n-                curr_past_key_value = layer_state\n+                curr_past_key_values = layer_state\n \n         # NOTE: FSMT has format (seq_len, BS, model_dim) for inputs\n         current_states = key if self.encoder_decoder_attention else query\n         if self.encoder_decoder_attention and layer_state is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -759,7 +759,7 @@ def forward(\n             if layer_state is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not self.encoder_decoder_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "6443fa10d49a307f1756d1e95df3279cefcaf8ed",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -197,11 +197,11 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n@@ -214,8 +214,8 @@ def forward(\n \n             # Try to get key/value states from cache if possible\n             if past_key_values is not None and is_updated:\n-                key_states = curr_past_key_value.layers[self.layer_idx].keys\n-                value_states = curr_past_key_value.layers[self.layer_idx].values\n+                key_states = curr_past_key_values.layers[self.layer_idx].keys\n+                value_states = curr_past_key_values.layers[self.layer_idx].values\n             else:\n                 key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n                 shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n@@ -235,7 +235,7 @@ def forward(\n         ):\n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n             cache_position = cache_position if not is_cross_attention else None\n-            key_states, value_states = curr_past_key_value.update(\n+            key_states, value_states = curr_past_key_values.update(\n                 key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n             # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "ef1a76d67124ca69440774528a098558440b7f6b",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -185,11 +185,11 @@ def forward(\n                 is_updated = layer_past.is_updated.get(self.layer_idx)\n                 if self.is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = layer_past.cross_attention_cache\n+                    curr_past_key_values = layer_past.cross_attention_cache\n                 else:\n-                    curr_past_key_value = layer_past.self_attention_cache\n+                    curr_past_key_values = layer_past.self_attention_cache\n             else:\n-                curr_past_key_value = layer_past\n+                curr_past_key_values = layer_past\n \n         if self.is_cross_attention:\n             if not hasattr(self, \"q_attn\") or not self.is_cross_attention:\n@@ -199,8 +199,8 @@ def forward(\n                 )\n             if layer_past is not None and is_updated:\n                 # reuse k,v, cross_attentions\n-                key = curr_past_key_value.layers[self.layer_idx].keys\n-                value = curr_past_key_value.layers[self.layer_idx].values\n+                key = curr_past_key_values.layers[self.layer_idx].keys\n+                value = curr_past_key_values.layers[self.layer_idx].values\n             else:\n                 query = self.q_attn(hidden_states).view(*input_shape, -1, self.head_dim).transpose(1, 2)\n                 key, value = self.c_attn(encoder_hidden_states).split((self.head_dim, self.head_dim), dim=-1)\n@@ -221,7 +221,7 @@ def forward(\n         if layer_past is not None:\n             # save all key/value_states to cache to be re-used for fast auto-regressive generation\n             cache_position = cache_position if not self.is_cross_attention else None\n-            key, value = curr_past_key_value.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n+            key, value = curr_past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n             # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n             if self.is_cross_attention:\n                 layer_past.is_updated[self.layer_idx] = True"
        },
        {
            "sha": "c7191aec8de8fc65c39f50e26f3db4737d163f64",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -215,11 +215,11 @@ def forward(\n                 is_updated = layer_past.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = layer_past.cross_attention_cache\n+                    curr_past_key_values = layer_past.cross_attention_cache\n                 else:\n-                    curr_past_key_value = layer_past.self_attention_cache\n+                    curr_past_key_values = layer_past.self_attention_cache\n             else:\n-                curr_past_key_value = layer_past\n+                curr_past_key_values = layer_past\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention:\n@@ -232,8 +232,8 @@ def forward(\n             if layer_past is not None and is_updated:\n                 # reuse k,v, cross_attentions, and compute only q\n                 query = self.q_attn(hidden_states)\n-                key = curr_past_key_value.layers[self.layer_idx].keys\n-                value = curr_past_key_value.layers[self.layer_idx].values\n+                key = curr_past_key_values.layers[self.layer_idx].keys\n+                value = curr_past_key_values.layers[self.layer_idx].values\n             else:\n                 query = self.q_attn(hidden_states)\n                 key, value = self.c_attn(current_states).split(self.split_size, dim=2)\n@@ -247,7 +247,7 @@ def forward(\n         if layer_past is not None:\n             # save all key/value_states to cache to be re-used for fast auto-regressive generation\n             cache_position = cache_position if not is_cross_attention else None\n-            key, value = curr_past_key_value.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n+            key, value = curr_past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n             # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n             if is_cross_attention:\n                 layer_past.is_updated[self.layer_idx] = True"
        },
        {
            "sha": "7b6cbeffa121847bd6a95ce0da313efc8453662d",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -459,17 +459,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -479,7 +479,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n@@ -575,17 +575,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -595,7 +595,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "8dd70019f5f840c81e9a5c06d765392f364174a6",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -266,17 +266,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -286,7 +286,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "66c41e83f37f3166bf56091cc70c3a9a1b7a1e20",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -725,17 +725,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -745,7 +745,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "17c6d5d9ad9b2ff8aaf627352820ea22e681a6d2",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -783,7 +783,7 @@ def forward(\n         hidden_states: torch.Tensor,  # text part\n         encoder_hidden_states: Optional[torch.Tensor] = None,  # image part\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -804,10 +804,10 @@ def forward(\n         # Apply `self.scaling`\n         query_states = self.scaling * query_states\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -859,7 +859,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -873,7 +873,7 @@ def forward(\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1147,7 +1147,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1202,7 +1202,7 @@ def forward(self, features):\n         hidden_states, attn_weights = self.x_attn(\n             hidden_states=latent_query,\n             encoder_hidden_states=key_value_states,\n-            past_key_value=None,\n+            past_key_values=None,\n             attention_mask=None,\n             output_attentions=None,\n             is_causal=False,"
        },
        {
            "sha": "f5b5787a9ddf487ad3b4dde4c244b303c17ea99d",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -791,17 +791,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -811,7 +811,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "62f3f39d11f1d31035b5dedb917c48424f217b75",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -453,17 +453,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -473,7 +473,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "1c7bd1a9ddc5af958da73a33bdbadca7e8e5cac5",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -299,17 +299,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -319,7 +319,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "42bda6bce3594f59914a3f897e2857985ac04bdc",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -219,17 +219,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -239,7 +239,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "1b99da661eac8cd7d94f6358092aacbb21d185db",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -230,17 +230,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -250,7 +250,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "e74ef03ca271af7860233d4bf24bfbc5ccf4d411",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -146,17 +146,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = curr_past_key_values.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n@@ -170,7 +170,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n+                key_layer, value_layer = curr_past_key_values.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "bc5de0b65966e30b0b77a5e6902f4e0d55c27d5c",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -290,17 +290,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -310,7 +310,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "75c8c660c130ea8ac7e9a3756c3b7cfa56157abd",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -252,25 +252,25 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "2f9bcd419ed9e29fe242c09dd9fd81bc3a88be9b",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -259,25 +259,25 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "672235c9624f276b5ab81550f5778b69e220c85a",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -150,17 +150,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -170,7 +170,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "ac801b738acfb0bf2be8d67b80e9e7760a7684a4",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -477,25 +477,25 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "d26dea088444f346162e92482d17359f69d3c691",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -218,17 +218,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -238,7 +238,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "13577571f4cee9d6d7a0ac5ab90d17207fd82368",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -241,17 +241,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -261,7 +261,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "1d0b32fe92f14e8e02da365c44a51cd0e95ad691",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -731,17 +731,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.key(current_states)\n             value_states = self.value(current_states)\n@@ -751,7 +751,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "a3ad21ec63e6903e1c30cc9db5ea2e239b476e91",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -407,17 +407,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -427,7 +427,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "349238a521f32f4127d9bb301b04c24d4e91ccbe",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -296,17 +296,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -316,7 +316,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "74ce73827cfad6ebcd6741b9ffe657ea1303b964",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -461,17 +461,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.key_proj(current_states)\n             value_states = self.value_proj(current_states)\n@@ -481,7 +481,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n@@ -639,10 +639,10 @@ def forward(\n         # We need to obtain the self attention only for this module, if `EncoderDecoderCache`\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n-            main_key_states, main_value_states = curr_past_key_value.update(\n+                curr_past_key_values = past_key_values\n+            main_key_states, main_value_states = curr_past_key_values.update(\n                 main_key_states, main_value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n "
        },
        {
            "sha": "0128f8f1f8d29fdf62b577a6a6c94c0a33611d9a",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -157,17 +157,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = curr_past_key_values.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)\n@@ -183,7 +183,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n+                key_layer, value_layer = curr_past_key_values.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "dba1aa77ba89b928abd6c62e233ed9056f235d31",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -221,7 +221,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -233,14 +233,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only roberta can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -294,7 +294,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -307,22 +307,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -370,7 +370,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -379,7 +379,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -442,14 +442,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -467,7 +467,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -539,7 +539,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "f28c519985bfa40989a9fe4035f32c69945dfb79",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -219,7 +219,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -231,14 +231,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only bert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -293,7 +293,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -306,22 +306,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -369,7 +369,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -379,7 +379,7 @@ def forward(\n             hidden_states_pre_layer_norm,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -444,14 +444,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -469,7 +469,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -511,7 +511,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "ebee4aab2591ab3ac1796e70e6ddb9f25b4b5091",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -238,7 +238,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -250,14 +250,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only bert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -312,7 +312,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -325,22 +325,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -390,7 +390,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -399,7 +399,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -465,14 +465,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -490,7 +490,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -530,7 +530,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "4d2448a39e078051abac6860d45cc27c08901b69",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -161,17 +161,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = curr_past_key_values.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)\n@@ -198,7 +198,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n+                key_layer, value_layer = curr_past_key_values.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "a8386282e528cf7e60c363d9f3c041dd80b66a6e",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -1054,17 +1054,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -1074,7 +1074,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "d6080f6a08e26556859a65445fb5dff5e91f4eed",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -919,17 +919,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -939,7 +939,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "179448c4129b164df4bcf49e8f69f56147a24cc9",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -277,25 +277,25 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "9369c55baf7a139c67375ade4c76d7e042fa21c2",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -902,17 +902,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -922,7 +922,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "9f2af942d80bae1a1ed0f71066af5e2b26f393ea",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -369,17 +369,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -389,7 +389,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "e29d6ca777613d13640434c628b81cccb5dfc7d0",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -300,17 +300,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -320,7 +320,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "759011345057c9d35ee570d08da1a7b016b7fe10",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -328,7 +328,7 @@ def forward(\n \n         if past_key_values is not None:\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n-            curr_past_key_value = past_key_values.cross_attention_cache\n+            curr_past_key_values = past_key_values.cross_attention_cache\n \n         if past_key_values is None or not is_updated:\n             encoder_input_shape = encoder_hidden_states.shape[:-1]\n@@ -337,11 +337,11 @@ def forward(\n             value_states = self.v_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n \n             if past_key_values is not None:\n-                key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n+                key_states, value_states = curr_past_key_values.update(key_states, value_states, self.layer_idx)\n                 past_key_values.is_updated[self.layer_idx] = True\n         else:\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "83375d6ad596b8424ae9304534d242338e2d6c3e",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -400,7 +400,7 @@ def forward(\n \n         if past_key_values is not None:\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n-            curr_past_key_value = past_key_values.cross_attention_cache\n+            curr_past_key_values = past_key_values.cross_attention_cache\n \n         if past_key_values is None or not is_updated:\n             encoder_input_shape = encoder_hidden_states.shape[:-1]\n@@ -409,11 +409,11 @@ def forward(\n             value_states = self.v_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n \n             if past_key_values is not None:\n-                key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n+                key_states, value_states = curr_past_key_values.update(key_states, value_states, self.layer_idx)\n                 past_key_values.is_updated[self.layer_idx] = True\n         else:\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "779a7e96301a51381d9b797d490abefc3096a13c",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -185,17 +185,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = curr_past_key_values.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)\n@@ -211,7 +211,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n+                key_layer, value_layer = curr_past_key_values.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "f23daf2fe4a730fc4f71eb20a1bb42fb6c03b982",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -384,17 +384,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -404,7 +404,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "b6dbf4c287a3882ba6977a01012a8d137bd67915",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -205,17 +205,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -225,7 +225,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "0694e0d91808071f1b21d44963b04c4e58244fa8",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -567,17 +567,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -587,7 +587,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "88d6ff2bdc67eb118e30800a0ccfe1123afdbad2",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -277,17 +277,17 @@ def forward(\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_values.cross_attention_cache\n+                curr_past_key_values = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_values.self_attention_cache\n+                curr_past_key_values = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_values\n+            curr_past_key_values = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)\n@@ -297,7 +297,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "6c0a88c7012e7fdd1b61f16c84a6c53a998aad52",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -159,17 +159,17 @@ def forward(\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n+                    curr_past_key_values = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n+                    curr_past_key_values = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_values\n+                curr_past_key_values = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.layers[self.layer_idx].keys\n-            value_states = curr_past_key_value.layers[self.layer_idx].values\n+            key_states = curr_past_key_values.layers[self.layer_idx].keys\n+            value_states = curr_past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -179,7 +179,7 @@ def forward(\n             if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states = curr_past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls"
        },
        {
            "sha": "3e8c22b54b2fe640df0d1126a1de3a84d6f019ee",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -532,17 +532,17 @@ def forward(\n                 is_updated = cache.is_updated.get(self.layer_id)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = cache.cross_attention_cache\n+                    curr_past_key_values = cache.cross_attention_cache\n                 else:\n-                    curr_past_key_value = cache.self_attention_cache\n+                    curr_past_key_values = cache.self_attention_cache\n             else:\n-                curr_past_key_value = cache\n+                curr_past_key_values = cache\n \n         current_states = kv if is_cross_attention else input\n         if is_cross_attention and cache is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            k = curr_past_key_value.key_cache[self.layer_id]\n-            v = curr_past_key_value.value_cache[self.layer_id]\n+            k = curr_past_key_values.key_cache[self.layer_id]\n+            v = curr_past_key_values.value_cache[self.layer_id]\n         else:\n             k = self.k_lin(current_states)\n             v = self.v_lin(current_states)\n@@ -552,7 +552,7 @@ def forward(\n             if cache is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                k, v = curr_past_key_value.update(k, v, self.layer_id, {\"cache_position\": cache_position})\n+                k, v = curr_past_key_values.update(k, v, self.layer_id, {\"cache_position\": cache_position})\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n                     cache.is_updated[self.layer_id] = True"
        },
        {
            "sha": "8166940bbfd597f3fc48a65124007d874a0bb8ef",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -115,7 +115,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -127,14 +127,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only xlm_roberta can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -188,7 +188,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -201,22 +201,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -264,7 +264,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -273,7 +273,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -336,14 +336,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -361,7 +361,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -569,7 +569,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "dc3cd57a95126a4b74de36e98041b541172f1798",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -223,7 +223,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -235,14 +235,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only xlm_roberta_xl can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -296,7 +296,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -309,22 +309,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -373,7 +373,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -383,7 +383,7 @@ def forward(\n             intermediate,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -444,14 +444,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -469,7 +469,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -510,7 +510,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "ff4a43d4170a17df08f64cae962ee3e59f6d9aad",
            "filename": "src/transformers/models/xlm_roberta_xl/modular_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -145,7 +145,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -155,7 +155,7 @@ def forward(\n             intermediate,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -210,7 +210,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "aecdd008d3ffb687d6f3a9596e9a5bbfff540285",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=8137dbdbbd9c2f83b87aa9e6320b7a898f9d1073",
            "patch": "@@ -218,7 +218,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -230,14 +230,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only roberta can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -292,7 +292,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -305,22 +305,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -371,7 +371,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -384,7 +384,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -508,14 +508,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -533,7 +533,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output"
        }
    ],
    "stats": {
        "total": 1198,
        "additions": 599,
        "deletions": 599
    }
}