{
    "author": "LysandreJik",
    "message": "Transformers cli clean command (#37657)\n\n* transformers-cli -> transformers\n\n* Chat command works with positional argument\n\n* update doc references to transformers-cli\n\n* doc headers\n\n* deepspeed\n\n---------\n\nCo-authored-by: Joao Gante <joao@huggingface.co>",
    "sha": "d538293f62f20d5c756a0a461bb5dbcff1e584a4",
    "files": [
        {
            "sha": "6c3a71de04a109cc030376ed0766e9b364b8749e",
            "filename": ".github/ISSUE_TEMPLATE/bug-report.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/.github%2FISSUE_TEMPLATE%2Fbug-report.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/.github%2FISSUE_TEMPLATE%2Fbug-report.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2FISSUE_TEMPLATE%2Fbug-report.yml?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -16,7 +16,7 @@ body:\n     id: system-info\n     attributes:\n       label: System Info\n-      description: Please share your system info with us. You can run the command `transformers-cli env` and copy-paste its output below.\n+      description: Please share your system info with us. You can run the command `transformers env` and copy-paste its output below.\n       placeholder: transformers version, platform, python version, ...\n     validations:\n       required: true"
        },
        {
            "sha": "081e644ff01c2a45ee48c1438dd0ba6157935525",
            "filename": ".github/ISSUE_TEMPLATE/migration.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/.github%2FISSUE_TEMPLATE%2Fmigration.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/.github%2FISSUE_TEMPLATE%2Fmigration.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2FISSUE_TEMPLATE%2Fmigration.yml?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -6,7 +6,7 @@ body:\n     id: system-info\n     attributes:\n       label: System Info\n-      description: Please share your system info with us. You can run the command `transformers-cli env` and copy-paste its output below.\n+      description: Please share your system info with us. You can run the command `transformers env` and copy-paste its output below.\n       render: shell\n       placeholder: transformers version, platform, python version, ...\n     validations:"
        },
        {
            "sha": "5c9c2ea987373a8138b7d6d5f4969ac03adebc44",
            "filename": ".github/workflows/add-model-like.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/.github%2Fworkflows%2Fadd-model-like.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/.github%2Fworkflows%2Fadd-model-like.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fadd-model-like.yml?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -54,7 +54,7 @@ jobs:\n       - name: Create model files\n         run: |\n           . ~/venv/bin/activate\n-          transformers-cli add-new-model-like --config_file tests/fixtures/add_distilbert_like_config.json --path_to_repo .\n+          transformers add-new-model-like --config_file tests/fixtures/add_distilbert_like_config.json --path_to_repo .\n           make style\n           make fix-copies\n "
        },
        {
            "sha": "5a9e52fd622c281f57c00f8b3fe6cfdedd8594f0",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -78,7 +78,7 @@ Once you've confirmed the bug hasn't already been reported, please include the f\n To get the OS and software versions automatically, run the following command:\n \n ```bash\n-transformers-cli env\n+transformers env\n ```\n \n You can also run the same command from the root of the repository:"
        },
        {
            "sha": "4837cdaee3655657077536071857eb7caadae1ef",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -121,7 +121,7 @@ To chat with a model, the usage pattern is the same. The only difference is you\n > [!TIP]\n > You can also chat with a model directly from the command line.\n > ```shell\n-> transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n+> transformers chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n > ```\n \n ```py"
        },
        {
            "sha": "e5ef4234319fead8fc59766b57b8bfb690e7d810",
            "filename": "docs/source/de/add_new_model.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fde%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fde%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fadd_new_model.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -95,7 +95,7 @@ wie der Code geschrieben werden sollte :-)\n 1. Der Vorwärtsdurchlauf Ihres Modells sollte vollständig in die Modellierungsdatei geschrieben werden und dabei völlig unabhängig von anderen\n    Modellen in der Bibliothek. Wenn Sie einen Block aus einem anderen Modell wiederverwenden möchten, kopieren Sie den Code und fügen ihn mit einem\n    `# Kopiert von` ein (siehe [hier](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)\n-   für ein gutes Beispiel und [hier](pr_checks#check-copies) für weitere Dokumentation zu Copied from). \n+   für ein gutes Beispiel und [hier](pr_checks#check-copies) für weitere Dokumentation zu Copied from).\n 2. Der Code sollte vollständig verständlich sein, auch für einen Nicht-Muttersprachler. Das heißt, Sie sollten\n    beschreibende Variablennamen wählen und Abkürzungen vermeiden. Ein Beispiel: `activation` ist `act` vorzuziehen.\n    Von Variablennamen mit nur einem Buchstaben wird dringend abgeraten, es sei denn, es handelt sich um einen Index in einer for-Schleife.\n@@ -402,7 +402,7 @@ Andernfalls beginnen wir mit der Erstellung eines neuen Modells. Wir empfehlen d\n ein bestehendes Modell:\n \n ```bash\n-transformers-cli add-new-model-like\n+transformers add-new-model-like\n ```\n \n Sie werden mit einem Fragebogen aufgefordert, die grundlegenden Informationen Ihres Modells einzugeben."
        },
        {
            "sha": "f7fc3d1359c3d25460952849ce61b46eac9b1857",
            "filename": "docs/source/de/contributing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fde%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fde%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fcontributing.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -63,7 +63,7 @@ Wenn Sie sich vergewissert haben, dass der Fehler noch nicht gemeldet wurde, geb\n Um das Betriebssystem und die Softwareversionen automatisch auszugeben, führen Sie den folgenden Befehl aus:\n \n ```bash\n-transformers-cli env\n+transformers env\n ```\n \n Sie können denselben Befehl auch im Hauptverzeichnis des Repositorys ausführen:"
        },
        {
            "sha": "45f1706aff9dbebae19e7a541eb11ed4db786caa",
            "filename": "docs/source/en/add_new_model.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_model.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -161,7 +161,7 @@ The downside is that if you aren't used to them, it may take some time to get us\n Run the command below to start and complete the questionnaire with some basic information about the new model. This command jumpstarts the process by automatically generating some model code that you'll need to adapt.\n \n ```bash\n-transformers-cli add-new-model-like\n+transformers add-new-model-like\n ```\n \n ## Create a pull request\n@@ -292,7 +292,7 @@ Once you're able to run the original checkpoint, you're ready to start adapting\n \n ## Adapt the model code\n \n-The `transformers-cli add-new-model-like` command should have generated a model and configuration file.\n+The `transformers add-new-model-like` command should have generated a model and configuration file.\n \n - `src/transformers/models/brand_new_llama/modeling_brand_new_llama.py`\n - `src/transformers/models/brand_new_llama/configuration_brand_new_llama.py`\n@@ -551,10 +551,10 @@ While this example doesn't include an image processor, you may need to implement\n \n If you do need to implement a new image processor, refer to an existing image processor to understand the expected structure. Slow image processors ([`BaseImageProcessor`]) and fast image processors ([`BaseImageProcessorFast`]) are designed differently, so make sure you follow the correct structure based on the processor type you're implementing.\n \n-Run the following command (only if you haven't already created the fast image processor with the `transformers-cli add-new-model-like` command) to generate the necessary imports and to create a prefilled template for the fast image processor. Modify the template to fit your model.\n+Run the following command (only if you haven't already created the fast image processor with the `transformers add-new-model-like` command) to generate the necessary imports and to create a prefilled template for the fast image processor. Modify the template to fit your model.\n \n ```bash\n-transformers-cli add-fast-image-processor --model-name your_model_name\n+transformers add-fast-image-processor --model-name your_model_name\n ```\n \n This command will generate the necessary imports and provide a pre-filled template for the fast image processor. You can then modify it to fit your model's needs."
        },
        {
            "sha": "b102f0c09d6cdab8869c2c80c7c7a843860eb241",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -25,12 +25,12 @@ Check model leaderboards like [OpenLLM](https://hf.co/spaces/HuggingFaceH4/open_\n \n This guide shows you how to quickly start chatting with Transformers from the command line, how build and format a conversation, and how to chat using the [`TextGenerationPipeline`].\n \n-## transformers-cli\n+## transformers CLI\n \n Chat with a model directly from the command line as shown below. It launches an interactive session with a model. Enter `clear` to reset the conversation, `exit` to terminate the session, and `help` to display all the command options.\n \n ```bash\n-transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n+transformers chat Qwen/Qwen2.5-0.5B-Instruct\n ```\n \n <div class=\"flex justify-center\">\n@@ -40,7 +40,7 @@ transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n For a full list of options, run the command below.\n \n ```bash\n-transformers-cli chat -h\n+transformers chat -h\n ```\n \n The chat is implemented on top of the [AutoClass](./model_doc/auto), using tooling from [text generation](./llm_tutorial) and [chat](./chat_templating).\n@@ -76,16 +76,16 @@ print(response[0][\"generated_text\"][-1][\"content\"])\n (sigh) Oh boy, you're asking me for advice? You're gonna need a map, pal! Alright,\n alright, I'll give you the lowdown. But don't say I didn't warn you, I'm a robot, not a tour guide!\n \n-So, you wanna know what's fun to do in the Big Apple? Well, let me tell you, there's a million \n-things to do, but I'll give you the highlights. First off, you gotta see the sights: the Statue of \n-Liberty, Central Park, Times Square... you know, the usual tourist traps. But if you're lookin' for \n-something a little more... unusual, I'd recommend checkin' out the Museum of Modern Art. It's got \n+So, you wanna know what's fun to do in the Big Apple? Well, let me tell you, there's a million\n+things to do, but I'll give you the highlights. First off, you gotta see the sights: the Statue of\n+Liberty, Central Park, Times Square... you know, the usual tourist traps. But if you're lookin' for\n+something a little more... unusual, I'd recommend checkin' out the Museum of Modern Art. It's got\n some wild stuff, like that Warhol guy's soup cans and all that jazz.\n \n-And if you're feelin' adventurous, take a walk across the Brooklyn Bridge. Just watch out for \n+And if you're feelin' adventurous, take a walk across the Brooklyn Bridge. Just watch out for\n those pesky pigeons, they're like little feathered thieves! (laughs) Get it? Thieves? Ah, never mind.\n \n-Now, if you're lookin' for some serious fun, hit up the comedy clubs in Greenwich Village. You might \n+Now, if you're lookin' for some serious fun, hit up the comedy clubs in Greenwich Village. You might\n even catch a glimpse of some up-and-coming comedians... or a bunch of wannabes tryin' to make it big. (winks)\n \n And finally, if you're feelin' like a real New Yorker, grab a slice of pizza from one of the many amazing\n@@ -107,9 +107,9 @@ print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n ```txt\n-(laughs) Oh, you're killin' me, pal! You don't get it, do you? Warhol's soup cans are like, art, man! \n-It's like, he took something totally mundane, like a can of soup, and turned it into a masterpiece. It's \n-like, \"Hey, look at me, I'm a can of soup, but I'm also a work of art!\" \n+(laughs) Oh, you're killin' me, pal! You don't get it, do you? Warhol's soup cans are like, art, man!\n+It's like, he took something totally mundane, like a can of soup, and turned it into a masterpiece. It's\n+like, \"Hey, look at me, I'm a can of soup, but I'm also a work of art!\"\n (sarcastically) Oh, yeah, real original, Andy.\n \n But, you know, back in the '60s, it was like, a big deal. People were all about challenging the"
        },
        {
            "sha": "8ec99b14ab74530ed0a5065295f6cb142928e50c",
            "filename": "docs/source/en/model_doc/bert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -81,10 +81,10 @@ print(f\"The predicted token is: {predicted_token}\")\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model google-bert/bert-base-uncased --device 0\n+echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers run --task fill-mask --model google-bert/bert-base-uncased --device 0\n ```\n \n </hfoption>\n@@ -256,4 +256,4 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n \n [[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput\n \n-[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput\n\\ No newline at end of file\n+[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput"
        },
        {
            "sha": "3be25fb203fcd5a2f3e7bd6fdd2429da3385687a",
            "filename": "docs/source/en/model_doc/code_llama.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -35,7 +35,7 @@ The example below demonstrates how to generate code with [`Pipeline`], or the [`\n \n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n-    \n+\n ```py\n import torch\n from transformers import pipeline\n@@ -76,7 +76,7 @@ prompt = \"# Function to calculate the factorial of a number\\ndef factorial(n):\"\n input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n \n output = model.generate(\n-    **input_ids, \n+    **input_ids,\n     max_new_tokens=256,\n     cache_implementation=\"static\"\n )\n@@ -92,10 +92,10 @@ print(filled_text)\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n-    \n+<hfoption id=\"transformers CLI\">\n+\n ```bash\n-echo -e \"# Function to calculate the factorial of a number\\ndef factorial(n):\" | transformers-cli run --task text-generation --model meta-llama/CodeLlama-7b-hf --device 0\n+echo -e \"# Function to calculate the factorial of a number\\ndef factorial(n):\" | transformers run --task text-generation --model meta-llama/CodeLlama-7b-hf --device 0\n ```\n \n </hfoption>\n@@ -146,7 +146,7 @@ visualizer(\"\"\"def func(a, b):\n - Use the `<FILL_ME>` token where you want your input to be filled. The tokenizer splits this token to create a formatted input string that follows the [original training pattern](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402). This is more robust than preparing the pattern yourself.\n     ```py\n     from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n-    \n+\n     tokenizer = CodeLlamaTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n     model = LlamaForCausalLM.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n     PROMPT = '''def remove_non_ascii(s: str) -> str:\n@@ -155,7 +155,7 @@ visualizer(\"\"\"def func(a, b):\n     '''\n     input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n     generated_ids = model.generate(input_ids, max_new_tokens=128)\n-    \n+\n     filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n     print(PROMPT.replace(\"<FILL_ME>\", filling))\n     ```"
        },
        {
            "sha": "21ae73c94771dee5d456c3c919d77dcc32b1cbdf",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -49,21 +49,21 @@ model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", t\n messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n output = model.generate(\n-    input_ids, \n-    max_new_tokens=100, \n-    do_sample=True, \n+    input_ids,\n+    max_new_tokens=100,\n+    do_sample=True,\n     temperature=0.3,\n     cache_implementation=\"static\",\n )\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers-cli chat --model_name_or_path CohereForAI/c4ai-command-r-v01 --torch_dtype auto --attn_implementation flash_attention_2\n+transformers chat CohereForAI/c4ai-command-r-v01 --torch_dtype auto --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -85,9 +85,9 @@ model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", t\n messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n output = model.generate(\n-    input_ids, \n-    max_new_tokens=100, \n-    do_sample=True, \n+    input_ids,\n+    max_new_tokens=100,\n+    do_sample=True,\n     temperature=0.3,\n     cache_implementation=\"static\",\n )"
        },
        {
            "sha": "0b2162c5e09f5af2a660a59d30fd9113b2b95858",
            "filename": "docs/source/en/model_doc/distilbert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -83,10 +83,10 @@ print(f\"Predicted label: {predicted_label}\")\n \n </hfoption>\n \n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"I love using Hugging Face Transformers!\" | transformers-cli run --task text-classification --model distilbert-base-uncased-finetuned-sst-2-english\n+echo -e \"I love using Hugging Face Transformers!\" | transformers run --task text-classification --model distilbert-base-uncased-finetuned-sst-2-english\n ```\n \n </hfoption>\n@@ -213,7 +213,3 @@ echo -e \"I love using Hugging Face Transformers!\" | transformers-cli run --task\n \n </jax>\n </frameworkcontent>\n-\n-\n-\n-"
        },
        {
            "sha": "1f1f5be076e3949f0ed7dbf73dd4574991cedf3c",
            "filename": "docs/source/en/model_doc/electra.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -45,9 +45,9 @@ import torch\n from transformers import pipeline\n \n classifier = pipeline(\n-    task=\"text-classification\", \n-    model=\"bhadresh-savani/electra-base-emotion\", \n-    torch_dtype=torch.float16, \n+    task=\"text-classification\",\n+    model=\"bhadresh-savani/electra-base-emotion\",\n+    torch_dtype=torch.float16,\n     device=0\n )\n classifier(\"This restaurant has amazing food!\")\n@@ -64,7 +64,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n     \"bhadresh-savani/electra-base-emotion\",\n )\n model = AutoModelForSequenceClassification.from_pretrained(\n-    \"bhadresh-savani/electra-base-emotion\", \n+    \"bhadresh-savani/electra-base-emotion\",\n     torch_dtype=torch.float16\n )\n inputs = tokenizer(\"ELECTRA is more efficient than BERT\", return_tensors=\"pt\")\n@@ -78,10 +78,10 @@ print(f\"Predicted label: {predicted_label}\")\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"This restaurant has amazing food.\" | transformers-cli run --task text-classification --model bhadresh-savani/electra-base-emotion --device 0\n+echo -e \"This restaurant has amazing food.\" | transformers run --task text-classification --model bhadresh-savani/electra-base-emotion --device 0\n ```\n \n </hfoption>\n@@ -96,12 +96,12 @@ echo -e \"This restaurant has amazing food.\" | transformers-cli run --task text-c\n \n     ```py\n     # Example of properly handling padding with attention masks\n-    inputs = tokenizer([\"Short text\", \"This is a much longer text that needs padding\"], \n-                    padding=True, \n+    inputs = tokenizer([\"Short text\", \"This is a much longer text that needs padding\"],\n+                    padding=True,\n                     return_tensors=\"pt\")\n     outputs = model(**inputs)  # automatically uses the attention_mask\n     ```\n-    \n+\n - When using the discriminator for a downstream task, you can load it into any of the ELECTRA model classes ([`ElectraForSequenceClassification`], [`ElectraForTokenClassification`], etc.).\n \n ## ElectraConfig"
        },
        {
            "sha": "b7b87e2ab9ad8158e3006d5ec56ebf04abb743dc",
            "filename": "docs/source/en/model_doc/falcon.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -41,7 +41,7 @@ import torch\n from transformers import pipeline\n \n pipeline = pipeline(\n-    task=\"text-generation\", \n+    task=\"text-generation\",\n     model=\"tiiuae/falcon-7b-instruct\",\n     torch_dtype=torch.bfloat16,\n     device=0\n@@ -76,11 +76,11 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers-cli chat --model_name_or_path tiiuae/falcon-7b-instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+transformers chat tiiuae/falcon-7b-instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n ```\n \n </hfoption>\n@@ -150,4 +150,4 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ## FalconForQuestionAnswering\n \n [[autodoc]] FalconForQuestionAnswering\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "a8d7886894b288dff98dfc55ffb009e6311ed398",
            "filename": "docs/source/en/model_doc/falcon_mamba.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -39,7 +39,7 @@ import torch\n from transformers import pipeline\n \n pipeline = pipeline(\n-    \"text-generation\", \n+    \"text-generation\",\n     model=\"tiiuae/falcon-mamba-7b-instruct\",\n     torch_dtype=torch.bfloat16,\n     device=0\n@@ -73,10 +73,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-transformers-cli chat --model_name_or_path tiiuae/falcon-mamba-7b-instruct --torch_dtype auto --device 0\n+transformers chat tiiuae/falcon-mamba-7b-instruct --torch_dtype auto --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "416d3ac85cf28988de191767f63950c5de90225f",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -80,10 +80,10 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"LLMs generate text through a process known as\" | transformers-cli run --task text-generation --model google/gemma-2b --device 0\n+echo -e \"LLMs generate text through a process known as\" | transformers run --task text-generation --model google/gemma-2b --device 0\n ```\n \n </hfoption>\n@@ -114,8 +114,8 @@ model = AutoModelForCausalLM.from_pretrained(\n input_text = \"LLMs generate text through a process known as.\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n outputs = model.generate(\n-    **input_ids, \n-    max_new_tokens=50, \n+    **input_ids,\n+    max_new_tokens=50,\n     cache_implementation=\"static\"\n )\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n@@ -127,7 +127,7 @@ Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/bl\n from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n \n visualizer = AttentionMaskVisualizer(\"google/gemma-2b\")\n-visualizer(\"LLMs generate text through a process known as\") \n+visualizer(\"LLMs generate text through a process known as\")\n ```\n \n <div class=\"flex justify-center\">"
        },
        {
            "sha": "50c0880300085b385a32c6e56290a2b6d486e122",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -58,7 +58,7 @@ pipe(\"Explain quantum computing simply. \", max_new_tokens=50)\n \n </hfoption>\n <hfoption id=\"AutoModel\">\n-    \n+\n ```python\n import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n@@ -80,16 +80,16 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```\n-echo -e \"Explain quantum computing simply.\" | transformers-cli run --task text-generation --model google/gemma-2-2b --device 0\n+echo -e \"Explain quantum computing simply.\" | transformers run --task text-generation --model google/gemma-2-2b --device 0\n ```\n </hfoption>\n </hfoptions>\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n-\t\n+\n The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n \n ```python\n@@ -118,7 +118,7 @@ Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/bl\n ```python\n from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n visualizer = AttentionMaskVisualizer(\"google/gemma-2b\")\n-visualizer(\"You are an assistant. Make sure you print me\") \n+visualizer(\"You are an assistant. Make sure you print me\")\n ```\n \n <div class=\"flex justify-center\">\n@@ -137,7 +137,7 @@ visualizer(\"You are an assistant. Make sure you print me\")\n \n     inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n     max_generated_length = inputs.input_ids.shape[1] + 10\n-    past_key_values = HybridCache(config=model.config, max_batch_size=1, \n+    past_key_values = HybridCache(config=model.config, max_batch_size=1,\n     max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n     outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     ```"
        },
        {
            "sha": "ac33f5c1da2c221beae9e215ef619ab10ba85b84",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -99,10 +99,10 @@ print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model google/gemma-3-1b-pt --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model google/gemma-3-1b-pt --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "cd2f388e69c359ba7fe88fcc7a95fbfcc3acf9d7",
            "filename": "docs/source/en/model_doc/gpt2.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -64,10 +64,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Hello, I'm a language model\" | transformers-cli run --task text-generation --model openai-community/gpt2 --device 0\n+echo -e \"Hello, I'm a language model\" | transformers run --task text-generation --model openai-community/gpt2 --device 0\n ```\n \n </hfoption>\n@@ -82,16 +82,16 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n \n quantization_config = BitsAndBytesConfig(\n-    load_in_4bit=True,  \n-    bnb_4bit_quant_type=\"nf4\",  \n-    bnb_4bit_compute_dtype=\"float16\",  \n-    bnb_4bit_use_double_quant=True \n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=\"float16\",\n+    bnb_4bit_use_double_quant=True\n )\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"openai-community/gpt2-xl\",\n     quantization_config=quantization_config,\n-    device_map=\"auto\"  \n+    device_map=\"auto\"\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")"
        },
        {
            "sha": "a096f238418630b4f6d35a7f800153efb0e14a63",
            "filename": "docs/source/en/model_doc/jamba.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -75,10 +75,10 @@ output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model ai21labs/AI21-Jamba-Mini-1.6 --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model ai21labs/AI21-Jamba-Mini-1.6 --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "bcdca5583a661a15c97ffbbf24bfb5de0ac51ee6",
            "filename": "docs/source/en/model_doc/llama.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -74,10 +74,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model huggyllama/llama-7b --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model huggyllama/llama-7b --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "5365fa1767f275ce165debef405c7cfd6ea35c30",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -74,10 +74,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-transformers-cli chat --model_name_or_path meta-llama/Llama-2-7b-chat-hf --torch_dtype auto --attn_implementation flash_attention_2\n+transformers chat meta-llama/Llama-2-7b-chat-hf --torch_dtype auto --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -175,4 +175,3 @@ visualizer(\"Plants create energy through a process known as\")\n \n [[autodoc]] LlamaForSequenceClassification\n     - forward\n-"
        },
        {
            "sha": "5426a60d29e009db2b540702bdc5f0f937b62d1d",
            "filename": "docs/source/en/model_doc/longformer.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -76,10 +76,10 @@ tokenizer.decode(predictions).split()\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the <mask> with a torn ligament in his left knee.\" | transformers-cli run --task fill-mask --model allenai/longformer-base-4096 --device 0\n+echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the <mask> with a torn ligament in his left knee.\" | transformers run --task fill-mask --model allenai/longformer-base-4096 --device 0\n ```\n \n </hfoption>\n@@ -147,42 +147,42 @@ echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of t\n \n ## LongformerForMaskedLM\n \n-[[autodoc]] LongformerForMaskedLM \n+[[autodoc]] LongformerForMaskedLM\n     - forward\n \n ## LongformerForSequenceClassification\n \n-[[autodoc]] LongformerForSequenceClassification \n+[[autodoc]] LongformerForSequenceClassification\n     - forward\n \n ## LongformerForMultipleChoice\n \n-[[autodoc]] LongformerForMultipleChoice \n+[[autodoc]] LongformerForMultipleChoice\n     - forward\n \n ## LongformerForTokenClassification\n \n-[[autodoc]] LongformerForTokenClassification \n+[[autodoc]] LongformerForTokenClassification\n     - forward\n \n ## LongformerForQuestionAnswering\n \n-[[autodoc]] LongformerForQuestionAnswering \n+[[autodoc]] LongformerForQuestionAnswering\n     - forward\n \n ## TFLongformerModel\n \n-[[autodoc]] TFLongformerModel    \n+[[autodoc]] TFLongformerModel\n     - call\n \n ## TFLongformerForMaskedLM\n \n-[[autodoc]] TFLongformerForMaskedLM \n+[[autodoc]] TFLongformerForMaskedLM\n     - call\n \n ## TFLongformerForQuestionAnswering\n \n-[[autodoc]] TFLongformerForQuestionAnswering \n+[[autodoc]] TFLongformerForQuestionAnswering\n     - call\n \n ## TFLongformerForSequenceClassification\n@@ -192,10 +192,10 @@ echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of t\n \n ## TFLongformerForTokenClassification\n \n-[[autodoc]] TFLongformerForTokenClassification \n+[[autodoc]] TFLongformerForTokenClassification\n     - call\n \n ## TFLongformerForMultipleChoice\n \n-[[autodoc]] TFLongformerForMultipleChoice \n+[[autodoc]] TFLongformerForMultipleChoice\n     - call"
        },
        {
            "sha": "331449eeacdff272e8d83b717ff14c71b4bb5503",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n # Mistral\n \n-[Mistral](https://huggingface.co/papers/2310.06825) is a 7B parameter language model, available as a pretrained and instruction-tuned variant, focused on balancing \n+[Mistral](https://huggingface.co/papers/2310.06825) is a 7B parameter language model, available as a pretrained and instruction-tuned variant, focused on balancing\n the scaling costs of large models with performance and efficient inference. This model uses sliding window attention (SWA) trained with a 8K context length and a fixed cache size to handle longer sequences more effectively. Grouped-query attention (GQA) speeds up inference and reduces memory requirements. Mistral also features a byte-fallback BPE tokenizer to improve token handling and efficiency by ensuring characters are never mapped to out-of-vocabulary tokens.\n \n You can find all the original Mistral checkpoints under the [Mistral AI_](https://huggingface.co/mistralai) organization.\n@@ -78,10 +78,10 @@ The example below demonstrates how to chat with [`Pipeline`] or the [`AutoModel`\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```python\n-echo -e \"My favorite condiment is\" | transformers-cli chat --model_name_or_path mistralai/Mistral-7B-v0.3 --torch_dtype auto --device 0 --attn_implementation flash_attention_2\n+echo -e \"My favorite condiment is\" | transformers chat mistralai/Mistral-7B-v0.3 --torch_dtype auto --device 0 --attn_implementation flash_attention_2\n ```\n \n </hfoption>"
        },
        {
            "sha": "fcb5c7a1335a0302dfb7cbfb14902c83efcdff37",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -76,10 +76,10 @@ print(f\"The predicted token is: {predicted_token}\")\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"The capital of France is [MASK].\" | transformers-cli run --task fill-mask --model google/mobilebert-uncased --device 0\n+echo -e \"The capital of France is [MASK].\" | transformers run --task fill-mask --model google/mobilebert-uncased --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "b2a57effb24e93ed21dd4f269c5e5688eab488a1",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -79,10 +79,10 @@ print(f\"The predicted token is: {predicted_token}\")\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model answerdotai/ModernBERT-base --device 0\n+echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers run --task fill-mask --model answerdotai/ModernBERT-base --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "27551060c8b802ab31d181618de84c9e84a9cfc7",
            "filename": "docs/source/en/model_doc/openai-gpt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -70,10 +70,10 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"The future of AI is\" | transformers-cli run --task text-generation --model openai-community/openai-gpt --device 0\n+echo -e \"The future of AI is\" | transformers run --task text-generation --model openai-community/openai-gpt --device 0\n \n ```\n </hfoption>"
        },
        {
            "sha": "1fff19ef82919722fe4e859fe1912856d3e5fa13",
            "filename": "docs/source/en/model_doc/phi.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -65,10 +65,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"'''def print_prime(n): \"\"\" Print all primes between 1 and n\"\"\"'''\" | transformers-cli run --task text-classification --model microsoft/phi-1.5 --device 0\n+echo -e \"'''def print_prime(n): \"\"\" Print all primes between 1 and n\"\"\"'''\" | transformers run --task text-classification --model microsoft/phi-1.5 --device 0\n ```\n \n </hfoption>\n@@ -102,20 +102,20 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n     ```py\n     import torch\n     from transformers import AutoTokenizer, AutoModelForCausalLM\n-    \n+\n     tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n     model = AutoModelForCausalLM.from_pretrained(\n         \"microsoft/phi-1\",\n         torch_dtype=torch.float16,\n         device_map=\"auto\",\n         trust_remote_code=True,\n         attn_implementation=\"sdpa\")\n-    \n+\n     input_ids = tokenizer('''def print_prime(n):\n        \"\"\"\n        Print all primes between 1 and n\n        \"\"\"''', return_tensors=\"pt\").to(\"cuda\")\n-    \n+\n     output = model.generate(**input_ids, cache_implementation=\"static\")\n     print(tokenizer.decode(output[0], skip_special_tokens=True))\n     ```"
        },
        {
            "sha": "1d0c2b9a5278df7b18e3ba6a01d3dbfe5331b02d",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -64,7 +64,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen2-1.5B-Instruct\",\n-    torch_dtype=torch.bfloat16, \n+    torch_dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -86,10 +86,10 @@ generated_ids = model.generate(\n     model_inputs.input_ids,\n     cache_implementation=\"static\",\n     max_new_tokens=512,\n-    do_sample=True, \n-    temperature=0.7, \n-    top_k=50,        \n-    top_p=0.95       \n+    do_sample=True,\n+    temperature=0.7,\n+    top_k=50,\n+    top_p=0.95\n )\n generated_ids = [\n     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n@@ -100,11 +100,11 @@ print(response)\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers-cli chat --model_name_or_path Qwen/Qwen2-7B-Instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+transformers chat Qwen/Qwen2-7B-Instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n ```\n \n </hfoption>\n@@ -121,21 +121,21 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n \n quantization_config = BitsAndBytesConfig(\n     load_in_4bit=True,\n-    bnb_4bit_compute_dtype=torch.bfloat16, \n-    bnb_4bit_quant_type=\"nf4\",             \n-    bnb_4bit_use_double_quant=True,       \n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_use_double_quant=True,\n )\n \n-tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B\") \n+tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen2-7B\",\n     torch_dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n-    attn_implementation=\"flash_attention_2\" \n+    attn_implementation=\"flash_attention_2\"\n )\n \n-inputs = tokenizer(\"The Qwen2 model family is\", return_tensors=\"pt\").to(\"cuda\") \n+inputs = tokenizer(\"The Qwen2 model family is\", return_tensors=\"pt\").to(\"cuda\")\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "599f68acd2eae5cfe08dadf291a2035c299f8386",
            "filename": "docs/source/en/model_doc/t5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -75,10 +75,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"translate English to French: The weather is nice today.\" | transformers-cli run --task text2text-generation --model google-t5/t5-base --device 0\n+echo -e \"translate English to French: The weather is nice today.\" | transformers run --task text2text-generation --model google-t5/t5-base --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "290f325b96c74bd9ddb3a69c9b78a59e57f1c729",
            "filename": "docs/source/es/converting_tensorflow_models.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fes%2Fconverting_tensorflow_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fes%2Fconverting_tensorflow_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fconverting_tensorflow_models.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -20,9 +20,9 @@ Te proporcionamos una interfaz de línea de comando (`CLI`, por sus siglas en in\n \n <Tip>\n \n-Desde 2.3.0, el script para convertir es parte de la CLI de transformers (**transformers-cli**) disponible en cualquier instalación de transformers >= 2.3.0.\n+Desde 2.3.0, el script para convertir es parte de la CLI de transformers (**transformers**) disponible en cualquier instalación de transformers >= 2.3.0.\n \n-La siguiente documentación refleja el formato para el comando **transformers-cli convert**.\n+La siguiente documentación refleja el formato para el comando **transformers convert**.\n \n </Tip>\n \n@@ -41,7 +41,7 @@ Aquí hay un ejemplo del proceso para convertir un modelo `BERT-Base Uncased` pr\n ```bash\n export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n \n-transformers-cli convert --model_type bert \\\n+transformers convert --model_type bert \\\n   --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n   --config $BERT_BASE_DIR/bert_config.json \\\n   --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin\n@@ -60,7 +60,7 @@ Aquí hay un ejemplo del proceso para convertir un modelo `ALBERT Base` pre-entr\n ```bash\n export ALBERT_BASE_DIR=/path/to/albert/albert_base\n \n-transformers-cli convert --model_type albert \\\n+transformers convert --model_type albert \\\n   --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\\n   --config $ALBERT_BASE_DIR/albert_config.json \\\n   --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin\n@@ -75,7 +75,7 @@ Este es un ejemplo del proceso para convertir un modelo OpenAI GPT pre-entrenado\n ```bash\n export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights\n \n-transformers-cli convert --model_type gpt \\\n+transformers convert --model_type gpt \\\n   --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n   [--config OPENAI_GPT_CONFIG] \\\n@@ -89,7 +89,7 @@ Aquí hay un ejemplo del proceso para convertir un modelo OpenAI GPT-2 pre-entre\n ```bash\n export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/openai-community/gpt2/pretrained/weights\n \n-transformers-cli convert --model_type gpt2 \\\n+transformers convert --model_type gpt2 \\\n   --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n   [--config OPENAI_GPT2_CONFIG] \\\n@@ -104,7 +104,7 @@ Aquí hay un ejemplo del proceso para convertir un modelo XLNet pre-entrenado:\n export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n \n-transformers-cli convert --model_type xlnet \\\n+transformers convert --model_type xlnet \\\n   --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\\n   --config $TRANSFO_XL_CONFIG_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n@@ -118,7 +118,7 @@ Aquí hay un ejemplo del proceso para convertir un modelo XLM pre-entrenado:\n ```bash\n export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint\n \n-transformers-cli convert --model_type xlm \\\n+transformers convert --model_type xlm \\\n   --tf_checkpoint $XLM_CHECKPOINT_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT\n  [--config XML_CONFIG] \\\n@@ -132,7 +132,7 @@ Aquí hay un ejemplo del proceso para convertir un modelo T5 pre-entrenado:\n ```bash\n export T5=/path/to/t5/uncased_L-12_H-768_A-12\n \n-transformers-cli convert --model_type t5 \\\n+transformers convert --model_type t5 \\\n   --tf_checkpoint $T5/t5_model.ckpt \\\n   --config $T5/t5_config.json \\\n   --pytorch_dump_output $T5/pytorch_model.bin"
        },
        {
            "sha": "1cd5da18c64545a1ae5ba6a343c7a79951487fd5",
            "filename": "docs/source/it/add_new_model.md",
            "status": "modified",
            "additions": 207,
            "deletions": 207,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fit%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fit%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fadd_new_model.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -15,113 +15,113 @@ rendered properly in your Markdown viewer.\n \n # Come aggiungere un modello a 🤗 Transformers?\n \n-Aggiungere un nuovo modello é spesso difficile e richiede una profonda conoscenza della libreria 🤗 Transformers e anche \n-della repository originale del modello. A Hugging Face cerchiamo di dare alla community sempre piú poteri per aggiungere \n-modelli independentemente. Quindi, per alcuni nuovi modelli che la community vuole aggiungere a 🤗 Transformers, abbiamo \n-creato una specifica *call-for-model-addition* che spiega passo dopo passo come aggiungere il modello richiesto. Con \n+Aggiungere un nuovo modello é spesso difficile e richiede una profonda conoscenza della libreria 🤗 Transformers e anche\n+della repository originale del modello. A Hugging Face cerchiamo di dare alla community sempre piú poteri per aggiungere\n+modelli independentemente. Quindi, per alcuni nuovi modelli che la community vuole aggiungere a 🤗 Transformers, abbiamo\n+creato una specifica *call-for-model-addition* che spiega passo dopo passo come aggiungere il modello richiesto. Con\n questo *call-for-model-addition* vogliamo insegnare a volenterosi e esperti collaboratori della community come implementare\n un modello in 🤗 Transformers.\n \n Se questo é qualcosa che può interessarvi, siete liberi di controllare l'attuale “calls-for-model-addition” [qui](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model/open_model_proposals/README.md)\n-e contattarci. \n+e contattarci.\n \n Se il modello sarà selezionato, allora potrete lavorare insieme a un membro di Hugging Face per integrare il modello in 🤗\n-Transformers. Così facendo, ci guadagnerai in una comprensione totale, sia teorica che pratica, del modello proposto. Inoltre, \n+Transformers. Così facendo, ci guadagnerai in una comprensione totale, sia teorica che pratica, del modello proposto. Inoltre,\n sarai l'artefice di un importante contributo open-source a 🤗 Transformers. Durante l'implementazione avrai l'opportunità di:\n \n - ottenere più comprensione delle best practices in open-source\n-- capire i principi di design di una della librerie NLP più popolari \n+- capire i principi di design di una della librerie NLP più popolari\n - capire come efficientemente testare complessi modelli NLP\n-- capire come integrare utilit Python come `black`, `ruff`, `make fix-copies` in una libreria per garantire sempre di avere un codice leggibile e pulito \n+- capire come integrare utilit Python come `black`, `ruff`, `make fix-copies` in una libreria per garantire sempre di avere un codice leggibile e pulito\n \n-Siamo anche contenti se vuoi aggiungere un modello che non può essere trovato nella cartella “calls-for-model-addition”. \n+Siamo anche contenti se vuoi aggiungere un modello che non può essere trovato nella cartella “calls-for-model-addition”.\n Le seguenti sezioni spiegano in dettaglio come aggiungere un nuovo modello. Può anche essere molto utile controllare modelli\n già aggiunti [qui](https://github.com/huggingface/transformers/pulls?q=is%3Apr+label%3A%22PR+for+Model+Addition%22+is%3Aclosed),\n-per capire se richiamano il modello che vorreste aggiungere. \n+per capire se richiamano il modello che vorreste aggiungere.\n \n Per cominciare, vediamo una panoramica general della libreria Transformers.\n \n ## Panoramica generale su 🤗 Transformers\n \n Prima di tutto, vediamo in generale 🤗 Transformers. 🤗 Transformers é una libreria molto strutturata, quindi\n-puà essere che a volte ci sia un disaccordo con alcune filosofie della libreria o scelte di design. Dalla nostra esperienza, \n+puà essere che a volte ci sia un disaccordo con alcune filosofie della libreria o scelte di design. Dalla nostra esperienza,\n tuttavia, abbiamo trovato che le scelte fondamentali di design della libreria sono cruciali per usare 🤗 Transformers efficacemente\n-su larga scala, mantenendo i costi a un livello accettabile.  \n+su larga scala, mantenendo i costi a un livello accettabile.\n \n Un buon primo punto di partenza per capire al meglio la libreria é leggere la [documentazione sulla nostra filosofia](filosofia)\n Da qui, ci sono alcune scelte sul modo di lavorare che cerchiamo di applicare a tutti i modelli:\n \n - La composizione é generalmente favorita sulla sovra-astrazione\n - Duplicare il codice non é sempre male, soprattutto se migliora notevolmente la leggibilità e accessibilità del modello\n-- Tutti i files creati per il nuovo modello devono il piu possibile \"compatti\". Questo vuol dire che quando qualcuno leggerá il codice \n+- Tutti i files creati per il nuovo modello devono il piu possibile \"compatti\". Questo vuol dire che quando qualcuno leggerá il codice\n di uno specifico modello, potrá vedere solo il corrispettivo file `modeling_....py` senza avere multiple dipendenze.\n \n \n-La cosa piú importante, é che consideriamo la libreria non solo un mezzo per dare un prodotto, *per esempio* dare la possibilità \n-di usare BERT per inferenza, ma é anche il prodotto reale che noi vogliamo migliorare sempre più. Quindi, quando aggiungi \n-un modello, non sei solo la persona che userà il modello, ma rappresenti anche tutti coloro che leggeranno, \n+La cosa piú importante, é che consideriamo la libreria non solo un mezzo per dare un prodotto, *per esempio* dare la possibilità\n+di usare BERT per inferenza, ma é anche il prodotto reale che noi vogliamo migliorare sempre più. Quindi, quando aggiungi\n+un modello, non sei solo la persona che userà il modello, ma rappresenti anche tutti coloro che leggeranno,\n cercheranno di capire e modificare il tuo modello.\n \n Tenendo questi principi in mente, immergiamoci nel design generale della libreria.\n \n ### Panoramica sui modelli\n \n Per aggiungere con successo un modello, é importante capire l'interazione tra il tuo modello e la sua configurazione,\n-[`PreTrainedModel`], e [`PretrainedConfig`]. Per dare un esempio, chiameremo il modello da aggiungere a 🤗 Transformers  \n+[`PreTrainedModel`], e [`PretrainedConfig`]. Per dare un esempio, chiameremo il modello da aggiungere a 🤗 Transformers\n `BrandNewBert`.\n \n Diamo un'occhiata:\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png\"/>\n \n-Come potete vedere, ci basiamo sull'ereditarietà in 🤗 Transformers, tenendo però il livello di astrazione a un minimo \n-assoluto.  Non ci sono mai più di due livelli di astrazione per ogni modello nella libreria. `BrandNewBertModel` eredita \n-da `BrandNewBertPreTrainedModel` che, a sua volta, eredita da [`PreTrainedModel`] -  semplice no? \n+Come potete vedere, ci basiamo sull'ereditarietà in 🤗 Transformers, tenendo però il livello di astrazione a un minimo\n+assoluto.  Non ci sono mai più di due livelli di astrazione per ogni modello nella libreria. `BrandNewBertModel` eredita\n+da `BrandNewBertPreTrainedModel` che, a sua volta, eredita da [`PreTrainedModel`] -  semplice no?\n Come regola generale, vogliamo essere sicuri che un nuovo modello dipenda solo da [`PreTrainedModel`]. Le funzionalità\n importanti che sono automaticamente conferite a ogni nuovo modello sono [`~PreTrainedModel.from_pretrained`]\n-e [`~PreTrainedModel.save_pretrained`], che sono usate per serializzazione e deserializzazione. Tutte le altre importanti \n+e [`~PreTrainedModel.save_pretrained`], che sono usate per serializzazione e deserializzazione. Tutte le altre importanti\n funzionalità, come ad esempio `BrandNewBertModel.forward` devono essere definite completamente nel nuovo script\n-`modeling_brand_new_bert.py`. Inoltre, vogliamo essere sicuri che un modello con uno specifico head layer, come \n+`modeling_brand_new_bert.py`. Inoltre, vogliamo essere sicuri che un modello con uno specifico head layer, come\n `BrandNewBertForMaskedLM` non erediti da `BrandNewBertModel`, ma piuttosto usi `BrandNewBertModel`\n-come componente che può essere chiamata nel passaggio forward per mantenere il livello di astrazione basso. Ogni \n-nuovo modello richieste una classe di configurazione, chiamata `BrandNewBertConfig`. Questa configurazione é sempre \n-mantenuta come un attributo in [`PreTrainedModel`], e quindi può essere accessibile tramite l'attributo `config` \n+come componente che può essere chiamata nel passaggio forward per mantenere il livello di astrazione basso. Ogni\n+nuovo modello richieste una classe di configurazione, chiamata `BrandNewBertConfig`. Questa configurazione é sempre\n+mantenuta come un attributo in [`PreTrainedModel`], e quindi può essere accessibile tramite l'attributo `config`\n per tutte le classi che ereditano da `BrandNewBertPreTrainedModel`:\n \n ```python\n model = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\n model.config  # il modello ha accesso al suo config\n ```\n \n-Analogamente al modello, la configurazione eredita le funzionalità base di serializzazione e deserializzazione da \n-[`PretrainedConfig`]. É da notare che la configurazione e il modello sono sempre serializzati in due formati differenti - \n-il modello é serializzato in un file *pytorch_model.bin* mentre la configurazione con *config.json*. Chiamando \n-[`~PreTrainedModel.save_pretrained`] automaticamente chiamerà [`~PretrainedConfig.save_pretrained`], cosicché sia il \n+Analogamente al modello, la configurazione eredita le funzionalità base di serializzazione e deserializzazione da\n+[`PretrainedConfig`]. É da notare che la configurazione e il modello sono sempre serializzati in due formati differenti -\n+il modello é serializzato in un file *pytorch_model.bin* mentre la configurazione con *config.json*. Chiamando\n+[`~PreTrainedModel.save_pretrained`] automaticamente chiamerà [`~PretrainedConfig.save_pretrained`], cosicché sia il\n modello che la configurazione siano salvati.\n \n \n ### Stile per il codice\n \n-Quando codifichi un nuovo modello, tieni presente che Transformers ha una sua struttura di fondo come libreria, perciò \n+Quando codifichi un nuovo modello, tieni presente che Transformers ha una sua struttura di fondo come libreria, perciò\n ci sono alcuni fatti da considerare su come scrivere un codice :-)\n \n-1. Il forward pass del tuo modello dev'essere scritto completamente nel file del modello, mentre dev'essere indipendente \n+1. Il forward pass del tuo modello dev'essere scritto completamente nel file del modello, mentre dev'essere indipendente\n    da altri modelli nella libreria. Se vuoi riutilizzare un blocco di codice da un altro modello, copia e incolla il codice con un commento `# Copied from` in cima al codice (guarda [qui](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)\n    per un ottimo esempio).\n-2. Il codice dev'essere interamente comprensibile, anche da persone che non parlano in inglese. Questo significa che le \n-   variabili devono avere un nome descrittivo e bisogna evitare abbreviazioni. Per esempio, `activation` é molto meglio \n+2. Il codice dev'essere interamente comprensibile, anche da persone che non parlano in inglese. Questo significa che le\n+   variabili devono avere un nome descrittivo e bisogna evitare abbreviazioni. Per esempio, `activation` é molto meglio\n    che `act`. Le variabili con una lettera sono da evitare fortemente, almeno che non sia per un indce in un for loop.\n 3. Generamente é meglio avere un codice esplicito e piú lungo che un codice corto e magico.\n-4. Evita di subclassare `nn.Sequential` in Pytorch, puoi subclassare `nn.Module` e scrivere il forward pass, cosicché \n-   chiunque può effettuare debug sul tuo codice, aggiungendo print o breaking points. \n-5. La tua function-signature dev'essere type-annoted. Per il resto, é meglio preferire variabili con un nome accettabile \n+4. Evita di subclassare `nn.Sequential` in Pytorch, puoi subclassare `nn.Module` e scrivere il forward pass, cosicché\n+   chiunque può effettuare debug sul tuo codice, aggiungendo print o breaking points.\n+5. La tua function-signature dev'essere type-annoted. Per il resto, é meglio preferire variabili con un nome accettabile\n    piuttosto che annotazioni per aumentare la comprensione e leggibilità del codice.\n \n ### Panoramica sui tokenizers\n \n Questa sezione sarà creata al piu presto :-(\n \n-## Aggiungere un modello a 🤗 Transformers passo dopo passo \n+## Aggiungere un modello a 🤗 Transformers passo dopo passo\n \n Ci sono differenti modi per aggiungere un modello a Hugging Face. Qui trovi una lista di blog posts da parte della community su come aggiungere un modello:\n \n@@ -141,11 +141,11 @@ La lista seguente é un sommario di tutto quello che é stato fatto per aggiunge\n \n -  1. ☐ (Opzionale) Capire gli aspetti teorici del modello\n -  2. ☐ Preparare l'ambiente dev per transformers\n--  3. ☐ Preparare l'ambiente debugging della repository originale \n--  4. ☐ Create uno script che gestisca con successo il forward pass usando la repository originale e checkpoint \n+-  3. ☐ Preparare l'ambiente debugging della repository originale\n+-  4. ☐ Create uno script che gestisca con successo il forward pass usando la repository originale e checkpoint\n -  5. ☐ Aggiungere con successo lo scheletro del modello a Transformers\n -  6. ☐ Convertire i checkpoint original a Transformers checkpoint\n--  7. ☐ Effettuare con successo la forward pass in Transformers, di modo che dia un output identico al checkpoint originale \n+-  7. ☐ Effettuare con successo la forward pass in Transformers, di modo che dia un output identico al checkpoint originale\n -  8. ☐ Finire i tests per il modello in Transformers\n -  9. ☐ Aggiungere con successo Tokenizer in Transformers\n -  10. ☐ Testare e provare gli integration tests da capo a fine\n@@ -156,22 +156,22 @@ La lista seguente é un sommario di tutto quello che é stato fatto per aggiunge\n \n Per cominciare di solito consigliamo `BrandNewBert`, partendo dalla teoria, di modo da avere una buona comprensione della teoria generale. TUttavia, se preferisci imparare l'aspetto teorico del modello mentre *lavori* sul modello é ok immergersi direttamente nel codice di `BrandNewBert`. Questa opzione puó essere buona se le tue skills ingegneristiche sono meglio che quelle teoriche, o se il paper `BrandNewBert` ti dá problemi, o se semplicemente ti piace programmare piú che leggere articoli scientifici.\n \n-### 1. (Opzionale) Aspetti teorici di BrandNewBert \n+### 1. (Opzionale) Aspetti teorici di BrandNewBert\n \n Allora con calma, prendi un po' di tempo per leggere l'articolo su *BrandNewBert* . Sicuramente, alcune sezioni dell'articolo sono molto complesse, ma non preoccuparti! L'obiettivo non é avere una compresione immensa della teoria alla base, ma estrarre le informazioni necessarie per re-implementare con successo il modello in 🤗 Transformers. Quindi, non impazzire sugli aspetti teorici, ma piuttosto focalizzati su quelli pratici, ossia:\n \n-- Che tipo di modello é *brand_new_bert*? É solo un encoder in stile BERT? O tipo decoder come GPT2? O encoder e decoder stile BART? Dai un'occhiata a [model_summary](model_summary) se non sei famigliare con le differenze tra questi modelli \n-- Quali sono le applicazioni di *brand_new_bert*? Classificazione di testo? Generazione di testo? O per tasks del genere seq2seq? \n-- Quali sono le nuove aggiunte al modello che lo rendono diverso da BERT/GPT-2/BART? \n+- Che tipo di modello é *brand_new_bert*? É solo un encoder in stile BERT? O tipo decoder come GPT2? O encoder e decoder stile BART? Dai un'occhiata a [model_summary](model_summary) se non sei famigliare con le differenze tra questi modelli\n+- Quali sono le applicazioni di *brand_new_bert*? Classificazione di testo? Generazione di testo? O per tasks del genere seq2seq?\n+- Quali sono le nuove aggiunte al modello che lo rendono diverso da BERT/GPT-2/BART?\n - Quali modelli estistenti in [🤗 Transformers models](https://huggingface.co/transformers/#contents) sono molto simili a *brand_new_bert*?\n-- Che tipo di tokenizer si usa in questo caso? Un sentencepiece tokenizer? O un word piece tokenizer? Il tokenizer é lo stesso di BERT o BART? \n+- Che tipo di tokenizer si usa in questo caso? Un sentencepiece tokenizer? O un word piece tokenizer? Il tokenizer é lo stesso di BERT o BART?\n \n-Una volta che senti che hai avuto una bella overview dell'architettura del modello, puoi scrivere senza problemi al team di Hugging Face per ogni domanda che tu hai. Questo puó includere domande sull'architettura del modello, o sull'attention layer, etc. Saremo molto felici di aiutarti :) \n+Una volta che senti che hai avuto una bella overview dell'architettura del modello, puoi scrivere senza problemi al team di Hugging Face per ogni domanda che tu hai. Questo puó includere domande sull'architettura del modello, o sull'attention layer, etc. Saremo molto felici di aiutarti :)\n \n \n ### 2. Prepare il tuo ambiente\n \n-1. Forka la [repository](https://github.com/huggingface/transformers) cliccando sul tasto ‘Fork' nella pagina della repository. Questo crea una copia del codice nel tuo account GitHub \n+1. Forka la [repository](https://github.com/huggingface/transformers) cliccando sul tasto ‘Fork' nella pagina della repository. Questo crea una copia del codice nel tuo account GitHub\n \n 2. Clona il tuo fork `transfomers` sul tuo dico locale, e aggiungi la repository base come remota:\n \n@@ -190,7 +190,7 @@ source .env/bin/activate\n pip install -e \".[dev]\"\n ```\n \n-quindi torna alla directory principale: \n+quindi torna alla directory principale:\n \n ```bash\n cd ..\n@@ -205,39 +205,39 @@ cd ..\n 5. Per trasferire *brand_new_bert* To port *brand_new_bert* avrai bisogno anche accesso alla sua repository originale:\n \n ```bash\n-git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git \n+git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git\n cd brand_new_bert\n pip install -e .\n ```\n \n Ok, ora hai un ambiente di sviluppo per portare *brand_new_bert* in 🤗 Transformers.\n \n \n-### 3.-4. Provare un pretrained checkpoint usando la repo originale \n+### 3.-4. Provare un pretrained checkpoint usando la repo originale\n \n-Per cominciare, comincerai a lavorare sulla repo originale di *brand_new_bert*. Come spesso accade, l'implementazione originale é molto sullo stile \"ricerca\". Questo significa che a volte la documentazione non é al top, magari manca qualche cosa e il codice puó essere difficile da capire. Tuttavia, questa é e dev'essere la motivazione per reimplementare *brand_new_bert*. In Hugging Face, uno degli obiettivi principali é di *mettere le persone sulle spalle dei giganti*, il che si traduce, in questo contesto, di prendere un modello funzionante e riscriverlo e renderlo il piú possibile **accessibile, user-friendly, e leggibile**. Questa é la top motivazione per re-implementare modelli in 🤗 Transformers - cercare di creare nuove complesse tecnologie NLP accessibili a **chiunque**. \n+Per cominciare, comincerai a lavorare sulla repo originale di *brand_new_bert*. Come spesso accade, l'implementazione originale é molto sullo stile \"ricerca\". Questo significa che a volte la documentazione non é al top, magari manca qualche cosa e il codice puó essere difficile da capire. Tuttavia, questa é e dev'essere la motivazione per reimplementare *brand_new_bert*. In Hugging Face, uno degli obiettivi principali é di *mettere le persone sulle spalle dei giganti*, il che si traduce, in questo contesto, di prendere un modello funzionante e riscriverlo e renderlo il piú possibile **accessibile, user-friendly, e leggibile**. Questa é la top motivazione per re-implementare modelli in 🤗 Transformers - cercare di creare nuove complesse tecnologie NLP accessibili a **chiunque**.\n \n Riuscire a far girare il modello pretrained originale dalla repository ufficiale é spesso il passo **piu arduo**. Dalla nostra esperienza, é molto importante spendere un p' di tempo per diventare familiari con il codice base originale. Come test, prova a capire i seguenti punti:\n \n-- Dove si trovano i pretrained weights? \n-- Come caricare i pretrained weights nel modello corrispondente? \n-- Come girare un tokenizer independentemente dal modello? \n-- Prova a tracciare un singolo forward pass, cosicché potrai sapere che classi e funzioni sono richieste per un semplice forward pass. Di solito, dovrai reimplementare queste funzioni e basta \n+- Dove si trovano i pretrained weights?\n+- Come caricare i pretrained weights nel modello corrispondente?\n+- Come girare un tokenizer independentemente dal modello?\n+- Prova a tracciare un singolo forward pass, cosicché potrai sapere che classi e funzioni sono richieste per un semplice forward pass. Di solito, dovrai reimplementare queste funzioni e basta\n - Prova a localizzare i componenti importanti del modello: Dove si trova la classe del modello? Ci sono sotto classi nel modello *per esempio* EngoderModel, DecoderMOdel? Dove si trova il self-attention layer? Ci sono molteplici differenti layer di attention, *per esempio * *self-attention*, *cross-attention*...?\n - Come puoi fare debug sul modello nell'ambiente originale della repo? Devi aggiungere dei *print* o puoi usare *ipdb* come debugger interattivo, o vabene anche un IDE efficiente per debug come PyCharm?\n \n É molto importante che prima di cominciare a trasferire il modello nuovo tu spenda tempo a fare debug del codice originale in maniera **efficiente**! Inoltre, ricorda che tutta la library é open-soruce, quindi non temere di aprire issue o fare una pull request nella repo originale. Tutti coloro che mantengono la repository saranno piú che felici di avere qualcuno che guarda e gioca con i loro codici!\n \n A questo punto, sta a te decidere quale ambiente per debug vuoi usare. Noi consilgiamo di evitare setup con GPU, che potrebbero costare assai, lavorare su una CPU puó essere un ottimo punto di partenza per indagare la repository originale e per cominciare a scrivere il codice per 🤗 Transformers. Solo alla fine, quando il modello é stato portato con successo in  🤗 Transformers, allora si potrá verificare il suo funzionamento su GPU.\n \n-In generale ci sono due possibili ambienti di debug per il testare il modello originale: \n+In generale ci sono due possibili ambienti di debug per il testare il modello originale:\n \n - [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)\n-- Scripts locali in Python \n+- Scripts locali in Python\n \n Il vantaggio dei Jupyter notebooks é la possibilità di eseguire cella per cella, il che può essere utile per decomporre tutte le componenti logiche, cosi da a vere un ciclo di debug più rapido, siccome si possono salvare i risultati da steps intermedi. Inoltre, i notebooks spesso sono molto facili da condividere con altri contributors, il che può essere molto utile se vuoi chiedere aiuto al team di Hugging Face. Se sei famigliare con Jupyter notebooks allora racommandiamo di lavorare in questa maniera.\n \n-Ovviamente se non siete abituati a lavorare con i notebook, questo può essere uno svantaggio nell'usare questa tecnologia, sprecando un sacco di tempo per setup e portare tutto al nuovo ambiente, siccome non potreste neanche usare dei tools di debug come `ipdb`. \n+Ovviamente se non siete abituati a lavorare con i notebook, questo può essere uno svantaggio nell'usare questa tecnologia, sprecando un sacco di tempo per setup e portare tutto al nuovo ambiente, siccome non potreste neanche usare dei tools di debug come `ipdb`.\n \n Per ogni pratica code-base, é sempre meglio come primo step caricare un **piccolo** checkpoint pretrained e cercare di riprodurre un singolo forward pass usando un vettore fittizio di IDs fatti da numeri interi. Un esempio per uno script simile, in pseudocodice é:\n \n@@ -249,42 +249,42 @@ original_output = model.predict(input_ids)\n \n Per quanto riguarda la strategia di debugging, si può scegliere tra:\n \n-- Decomporre il modello originario in piccole componenenti e testare ognuna di esse \n-- Decomporre il modello originario nel *tokenizer* originale e nel *modello* originale, testare un forward pass su questi, \n+- Decomporre il modello originario in piccole componenenti e testare ognuna di esse\n+- Decomporre il modello originario nel *tokenizer* originale e nel *modello* originale, testare un forward pass su questi,\n e usare dei print statement o breakpoints intermedi per verificare\n \n-Ancora una volta, siete liberi di scegliere quale strategia sia ottimale per voi. Spesso una strategia é piu \n+Ancora una volta, siete liberi di scegliere quale strategia sia ottimale per voi. Spesso una strategia é piu\n avvantaggiosa di un'altra, ma tutto dipende dall'code-base originario.\n \n-Se il code-base vi permette di decomporre il modello in piccole sub-componenenti, *per esempio* se il code-base \n-originario può essere facilmente testato in eager mode, allora vale la pena effettuare un debugging di questo genere. \n-Ricordate che ci sono dei vantaggi nel decidere di prendere la strada piu impegnativa sin da subito: \n+Se il code-base vi permette di decomporre il modello in piccole sub-componenenti, *per esempio* se il code-base\n+originario può essere facilmente testato in eager mode, allora vale la pena effettuare un debugging di questo genere.\n+Ricordate che ci sono dei vantaggi nel decidere di prendere la strada piu impegnativa sin da subito:\n \n - negli stage piu finali, quando bisognerà comparare il modello originario all'implementazione in Hugging Face, potrete verificare\n automaticamente ogni componente, individualmente, di modo che ci sia una corrispondenza 1:1\n - avrete l'opportunità di decomporre un problema molto grande in piccoli passi, così da strutturare meglio il vostro lavoro\n-- separare il modello in componenti logiche vi aiuterà ad avere un'ottima overview sul design del modello, quindi una migliore \n-comprensione del modello stesso \n+- separare il modello in componenti logiche vi aiuterà ad avere un'ottima overview sul design del modello, quindi una migliore\n+comprensione del modello stesso\n - verso gli stage finali i test fatti componente per componente vi aiuterà ad essere sicuri di non andare avanti e indietro\n nell'implementazione, così da continuare la modifica del codice senza interruzione\n \n-Un ottimo esempio di come questo può essere fatto é dato da [Lysandre](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed) \n+Un ottimo esempio di come questo può essere fatto é dato da [Lysandre](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed)\n per il modello ELECTRA\n \n-Tuttavia, se il code-base originale é molto complesso o le componenti intermedie possono essere testate solo in tramite \n-compilazione, potrebbe richiedere parecchio tempo o addirittura essere impossibile separare il modello in piccole sotto-componenti. \n-Un buon esempio é [MeshTensorFlow di T5](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow). Questa libreria \n-é molto complessa e non offre un metodo semplice di decomposizione in sotto-componenti. Per simili librerie, potrete fare \n+Tuttavia, se il code-base originale é molto complesso o le componenti intermedie possono essere testate solo in tramite\n+compilazione, potrebbe richiedere parecchio tempo o addirittura essere impossibile separare il modello in piccole sotto-componenti.\n+Un buon esempio é [MeshTensorFlow di T5](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow). Questa libreria\n+é molto complessa e non offre un metodo semplice di decomposizione in sotto-componenti. Per simili librerie, potrete fare\n affidamento ai print statements.\n \n-In ogni caso, indipendentemente da quale strategia scegliete, la procedura raccomandata é di cominciare a fare debug dal \n-primo layer al layer finale. \n+In ogni caso, indipendentemente da quale strategia scegliete, la procedura raccomandata é di cominciare a fare debug dal\n+primo layer al layer finale.\n É consigliato recuperare gli output dai layers, tramite print o sotto-componenti, nel seguente ordine:\n \n 1. Recuperare gli IDs di input dati al modello\n 2. Recuperare i word embeddings\n-3. Recuperare l'input del primo Transformer layer \n-4. Recuperare l'output del primo Transformer layer \n+3. Recuperare l'input del primo Transformer layer\n+4. Recuperare l'output del primo Transformer layer\n 5. Recuperare l'output dei seguenti `n - 1` Transformer layers\n 6. Recuperare l'output dell'intero BrandNewBert Model\n \n@@ -303,36 +303,36 @@ Gli output dei seguenti layer di solito dovrebbero essere degli array di float m\n  [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],\n ```\n \n-Ci aspettiamo che ogni modello aggiunto a 🤗 Transformers passi con successo un paio di test d'integrazione. Questo \n-significa che il modello originale e la sua implementazione in 🤗 Transformers abbiano lo stesso output con una precisione \n-di 0.001! Siccome é normale che lo stesso esatto modello, scritto in librerie diverse, possa dare output leggermente \n-diversi, la tolleranza accettata é 1e-3 (0.001). Ricordate che i due modelli devono dare output quasi identici. Dunque, \n-é molto conveniente comparare gli output intermedi di 🤗 Transformers molteplici volte con gli output intermedi del \n+Ci aspettiamo che ogni modello aggiunto a 🤗 Transformers passi con successo un paio di test d'integrazione. Questo\n+significa che il modello originale e la sua implementazione in 🤗 Transformers abbiano lo stesso output con una precisione\n+di 0.001! Siccome é normale che lo stesso esatto modello, scritto in librerie diverse, possa dare output leggermente\n+diversi, la tolleranza accettata é 1e-3 (0.001). Ricordate che i due modelli devono dare output quasi identici. Dunque,\n+é molto conveniente comparare gli output intermedi di 🤗 Transformers molteplici volte con gli output intermedi del\n modello originale di *brand_new_bert*. Di seguito vi diamo alcuni consigli per avere un ambiente di debug il piu efficiente\n possibile:\n \n - Trovate la migliore strategia per fare debug dei risultati intermedi. Per esempio, é la repository originale scritta in PyTorch?\n-Se si, molto probabilmente dovrete dedicare un po' di tempo per scrivere degli script piu lunghi, così da decomporre il \n-modello originale in piccole sotto-componenti, in modo da poter recuperare i valori intermedi. Oppure, la repo originale \n-é scritta in Tensorflow 1? Se é così dovrete fare affidamento ai print di Tensorflow [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) \n-per avere i valori intermedi. Altro caso, la repo é scritta in Jax? Allora assicuratevi che il modello non sia in **jit** \n-quanto testate il foward pass, *per esempio* controllate [questo link](https://github.com/google/jax/issues/196). \n-- Usate i più piccoli pretrained checkpoint che potete trovare. Piu piccolo é il checkpoint, piu velocemente sarà il vostro \n-ciclo di debug. Non é efficiente avere un pretrained model così gigante che per il forward pass impieghi piu di 10 secondi. \n+Se si, molto probabilmente dovrete dedicare un po' di tempo per scrivere degli script piu lunghi, così da decomporre il\n+modello originale in piccole sotto-componenti, in modo da poter recuperare i valori intermedi. Oppure, la repo originale\n+é scritta in Tensorflow 1? Se é così dovrete fare affidamento ai print di Tensorflow [tf.print](https://www.tensorflow.org/api_docs/python/tf/print)\n+per avere i valori intermedi. Altro caso, la repo é scritta in Jax? Allora assicuratevi che il modello non sia in **jit**\n+quanto testate il foward pass, *per esempio* controllate [questo link](https://github.com/google/jax/issues/196).\n+- Usate i più piccoli pretrained checkpoint che potete trovare. Piu piccolo é il checkpoint, piu velocemente sarà il vostro\n+ciclo di debug. Non é efficiente avere un pretrained model così gigante che per il forward pass impieghi piu di 10 secondi.\n Nel caso in cui i checkpoints siano molto grandi, e non si possa trovare di meglio, allora é buona consuetudine ricorrere\n-a fare un dummy model nel nuovo ambiente, con weights inizializzati random e salvare quei weights per comprare la versione 🤗 Transformers \n+a fare un dummy model nel nuovo ambiente, con weights inizializzati random e salvare quei weights per comprare la versione 🤗 Transformers\n con il vostro modello\n-- Accertatevi di usare la via piu semplice per chiamare il forward pass nella repo originale. Sarebbe opportuno trovare \n-la funzione originaria che chiami **solo** un singolo forward pass, *per esempio* questa funzione spesso viene chiamata \n-`predict`, `evaluate`, `forward` o `__call__`. Siate sicuri di non fare debug su una funzione che chiami `forward` molteplici \n+- Accertatevi di usare la via piu semplice per chiamare il forward pass nella repo originale. Sarebbe opportuno trovare\n+la funzione originaria che chiami **solo** un singolo forward pass, *per esempio* questa funzione spesso viene chiamata\n+`predict`, `evaluate`, `forward` o `__call__`. Siate sicuri di non fare debug su una funzione che chiami `forward` molteplici\n volte, *per esempio* per generare testo, come `autoregressive_sample`, `generate`.\n-- Cercate di separare la tokenization dal forward pass del modello. Se la repo originaria mostra esempio dove potete dare \n-come input una stringa, provate a cercare dove nella forward call la stringa viene cambiata in input ids e cominciate il \n-debug da questo punto. Questo vi garantisce un ottimo punto di partenza per scrivere un piccolo script personale dove dare \n-gli input al modello, anziche delle stringhe in input. \n-- Assicuratevi che il debugging **non** sia in training mode. Spesso questo potra il modello a dare degli output random, per \n-via dei molteplici dropout layers. Assicuratevi che il forward pass nell'ambiente di debug sia **deterministico**, cosicche \n-i dropout non siano usati. Alternativamente, potete usare *transformers.utils.set_seed* se la vecchia e nuova implementazione \n+- Cercate di separare la tokenization dal forward pass del modello. Se la repo originaria mostra esempio dove potete dare\n+come input una stringa, provate a cercare dove nella forward call la stringa viene cambiata in input ids e cominciate il\n+debug da questo punto. Questo vi garantisce un ottimo punto di partenza per scrivere un piccolo script personale dove dare\n+gli input al modello, anziche delle stringhe in input.\n+- Assicuratevi che il debugging **non** sia in training mode. Spesso questo potra il modello a dare degli output random, per\n+via dei molteplici dropout layers. Assicuratevi che il forward pass nell'ambiente di debug sia **deterministico**, cosicche\n+i dropout non siano usati. Alternativamente, potete usare *transformers.utils.set_seed* se la vecchia e nuova implementazione\n sono nello stesso framework.\n \n La seguente sezione vi da ulteriori dettagli e accorgimenti su come potete fare tutto questo per *brand_new_bert*.\n@@ -343,7 +343,7 @@ La seguente sezione vi da ulteriori dettagli e accorgimenti su come potete fare\n Allora cominciamo ad aggiungere un nuovo codice in 🤗 Transformers. Andate nel vostro fork clone di 🤗 Transformers:\n \n \n-```bash \n+```bash\n cd transformers\n ```\n \n@@ -355,116 +355,116 @@ Se questo non é il caso, cominciamo con il generare un nuovo modello. Ti consig\n un modello esistente:\n \n ```bash\n-transformers-cli add-new-model-like\n+transformers add-new-model-like\n ```\n \n Ti verrà richiesto con un questionario di compilare le informazioni di base del tuo modello.\n \n **Aprire una Pull Request in main huggingface/transformers repo**\n \n-Prime di cominciare ad adattare il codice automaticamente generato, aprite una nuova PR come \"Work in progress (WIP)\", \n+Prime di cominciare ad adattare il codice automaticamente generato, aprite una nuova PR come \"Work in progress (WIP)\",\n *per esempio* \"[WIP] Aggiungere *brand_new_bert*\", cosicché il team di Hugging Face possa lavorare al vostro fianco nell'\n integrare il modello in 🤗 Transformers.\n \n Questi sarebbero gli step generali da seguire:\n \n-1. Creare un branch dal main branch con un nome descrittivo \n+1. Creare un branch dal main branch con un nome descrittivo\n \n-```bash \n-git checkout -b add_brand_new_bert \n+```bash\n+git checkout -b add_brand_new_bert\n ```\n \n-2. Commit del codice automaticamente generato \n+2. Commit del codice automaticamente generato\n \n-```bash \n-git add . \n-git commit \n+```bash\n+git add .\n+git commit\n ```\n \n 3. Fare fetch e rebase del main esistente\n \n-```bash \n-git fetch upstream \n-git rebase upstream/main \n+```bash\n+git fetch upstream\n+git rebase upstream/main\n ```\n \n-4. Push dei cambiamenti al proprio account: \n+4. Push dei cambiamenti al proprio account:\n \n ```bash\n git push -u origin a-descriptive-name-for-my-changes\n ```\n \n-5. Una volte che siete soddisfatti dei nuovi cambiamenti, andate sulla webpage del vostro fork su GitHub. Cliccate \"Pull request\". \n-Assiuratevi di aggiungere alcuni membri di Hugging Face come reviewers, nel riguardo alla destra della pagina della PR, cosicche il team \n-Hugging Face verrà notificato anche per i futuri cambiamenti. \n+5. Una volte che siete soddisfatti dei nuovi cambiamenti, andate sulla webpage del vostro fork su GitHub. Cliccate \"Pull request\".\n+Assiuratevi di aggiungere alcuni membri di Hugging Face come reviewers, nel riguardo alla destra della pagina della PR, cosicche il team\n+Hugging Face verrà notificato anche per i futuri cambiamenti.\n \n 6. Cambiare la PR a draft, cliccando su \"Convert to draft\" alla destra della pagina della PR\n \n-Da quel punto in poi, ricordate di fare commit di ogni progresso e cambiamento, cosicche venga mostrato nella PR. Inoltre, \n+Da quel punto in poi, ricordate di fare commit di ogni progresso e cambiamento, cosicche venga mostrato nella PR. Inoltre,\n ricordatevi di tenere aggiornato il vostro lavoro con il main esistente:\n \n ```bash\n git fetch upstream\n git merge upstream/main\n ```\n \n-In generale, tutte le domande che avrete riguardo al modello o l'implementazione dovranno essere fatte nella vostra PR \n-e discusse/risolte nella PR stessa. In questa maniera, il team di Hugging Face sarà sempre notificato quando farete commit \n-di un nuovo codice o se avrete qualche domanda. É molto utile indicare al team di Hugging Face il codice a cui fate riferimento \n-nella domanda, cosicche il team potra facilmente capire il problema o la domanda. \n+In generale, tutte le domande che avrete riguardo al modello o l'implementazione dovranno essere fatte nella vostra PR\n+e discusse/risolte nella PR stessa. In questa maniera, il team di Hugging Face sarà sempre notificato quando farete commit\n+di un nuovo codice o se avrete qualche domanda. É molto utile indicare al team di Hugging Face il codice a cui fate riferimento\n+nella domanda, cosicche il team potra facilmente capire il problema o la domanda.\n \n-Per fare questo andate sulla tab \"Files changed\", dove potrete vedere tutti i vostri cambiamenti al codice, andate sulla linea \n-dove volete chiedere una domanda, e cliccate sul simbolo \"+\" per aggiungere un commento. Ogni volta che una domanda o problema \n+Per fare questo andate sulla tab \"Files changed\", dove potrete vedere tutti i vostri cambiamenti al codice, andate sulla linea\n+dove volete chiedere una domanda, e cliccate sul simbolo \"+\" per aggiungere un commento. Ogni volta che una domanda o problema\n é stato risolto, cliccate sul bottone \"Resolve\".\n \n-In questa stessa maniera, Hugging Face aprirà domande o commenti nel rivedere il vostro codice. Mi raccomando, chiedete più \n-domande possibili nella pagina della vostra PR. Se avete domande molto generali, non molto utili per il pubblico, siete liberi \n+In questa stessa maniera, Hugging Face aprirà domande o commenti nel rivedere il vostro codice. Mi raccomando, chiedete più\n+domande possibili nella pagina della vostra PR. Se avete domande molto generali, non molto utili per il pubblico, siete liberi\n di chiedere al team Hugging Face direttamente su slack o email.\n \n \n **5. Adattare i codici per brand_new_bert**\n \n-Per prima cosa, ci focalizzeremo sul modello e non sui tokenizer. Tutto il codice relative dovrebbe trovarsi in  \n+Per prima cosa, ci focalizzeremo sul modello e non sui tokenizer. Tutto il codice relative dovrebbe trovarsi in\n `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` e\n `src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.\n \n-Ora potete finalmente cominciare il codice :). Il codice generato in \n-`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` avrà sia la stessa architettura di BERT se é un \n-modello encoder-only o BART se é encoder-decoder. A questo punto, ricordatevi cio che avete imparato all'inizio, riguardo \n-agli aspetti teorici del modello: *In che maniera il modello che sto implmementando é diverso da BERT o BART?*. Implementare \n-questi cambi  spesso vuol dire cambiare il layer *self-attention*, l'ordine dei layer di normalizzazione e così via... \n-Ancora una volta ripetiamo, é molto utile vedere architetture simili di modelli gia esistenti in Transformers per avere \n-un'idea migliore su come implementare il modello. \n+Ora potete finalmente cominciare il codice :). Il codice generato in\n+`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` avrà sia la stessa architettura di BERT se é un\n+modello encoder-only o BART se é encoder-decoder. A questo punto, ricordatevi cio che avete imparato all'inizio, riguardo\n+agli aspetti teorici del modello: *In che maniera il modello che sto implmementando é diverso da BERT o BART?*. Implementare\n+questi cambi  spesso vuol dire cambiare il layer *self-attention*, l'ordine dei layer di normalizzazione e così via...\n+Ancora una volta ripetiamo, é molto utile vedere architetture simili di modelli gia esistenti in Transformers per avere\n+un'idea migliore su come implementare il modello.\n \n-**Notate** che a questo punto non dovete avere subito un codice tutto corretto o pulito. Piuttosto, é consigliato cominciare con un \n-codice poco pulito, con copia-incolla del codice originale in `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` \n-fino a che non avrete tutto il codice necessario. In base alla nostra esperienza, é molto meglio aggiungere una prima bozza \n-del codice richiesto e poi correggere e migliorare iterativamente. L'unica cosa essenziale che deve funzionare qui é la seguente \n-instanza: \n+**Notate** che a questo punto non dovete avere subito un codice tutto corretto o pulito. Piuttosto, é consigliato cominciare con un\n+codice poco pulito, con copia-incolla del codice originale in `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`\n+fino a che non avrete tutto il codice necessario. In base alla nostra esperienza, é molto meglio aggiungere una prima bozza\n+del codice richiesto e poi correggere e migliorare iterativamente. L'unica cosa essenziale che deve funzionare qui é la seguente\n+instanza:\n \n ```python\n from transformers import BrandNewBertModel, BrandNewBertConfig\n \n model = BrandNewBertModel(BrandNewBertConfig())\n ```\n \n-Questo comando creerà un modello con i parametri di default definiti in `BrandNewBergConfig()` e weights random. Questo garantisce \n+Questo comando creerà un modello con i parametri di default definiti in `BrandNewBergConfig()` e weights random. Questo garantisce\n che `init()` di tutte le componenti funzioni correttamente.\n \n \n **6. Scrivere uno script di conversione**\n \n-Il prossimo step é scrivere uno script per convertire il checkpoint che avete usato per fare debug su *brand_new_berts* nella \n-repo originale in un checkpoint per la nuova implementazione di *brand_new_bert* in 🤗 Transformers. Non é consigliato scrivere \n+Il prossimo step é scrivere uno script per convertire il checkpoint che avete usato per fare debug su *brand_new_berts* nella\n+repo originale in un checkpoint per la nuova implementazione di *brand_new_bert* in 🤗 Transformers. Non é consigliato scrivere\n lo script di conversione da zero, ma piuttosto cercate e guardate script gia esistenti in 🤗 Transformers, così da trovarne\n-uno simile al vostro modello. Di solito basta fare una copia di uno script gia esistente e adattarlo al vostro caso. \n+uno simile al vostro modello. Di solito basta fare una copia di uno script gia esistente e adattarlo al vostro caso.\n Non esistate a chiedre al team di Hugging Face a riguardo.\n \n - Se state convertendo un modello da TensorFlow a PyTorch, un ottimo inizio é vedere [questo script di conversione per BERT](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)\n - Se state convertendo un modello da PyTorch a PyTorch, [lo script di conversione di BART può esservi utile](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)\n \n-Qui di seguito spiegheremo come i modelli PyTorch salvano i weights per ogni layer e come i nomi dei layer sono definiti. In PyTorch, \n-il nomde del layer é definito dal nome della class attribute che date al layer. Definiamo un modello dummy in PyTorch, \n+Qui di seguito spiegheremo come i modelli PyTorch salvano i weights per ogni layer e come i nomi dei layer sono definiti. In PyTorch,\n+il nomde del layer é definito dal nome della class attribute che date al layer. Definiamo un modello dummy in PyTorch,\n chiamato `SimpleModel`:\n \n ```python\n@@ -497,7 +497,7 @@ SimpleModel(\n )\n ```\n \n-Si può vedere come i nomi dei layers siano definiti dal nome della class attribute in PyTorch. I valori dei weights di uno \n+Si può vedere come i nomi dei layers siano definiti dal nome della class attribute in PyTorch. I valori dei weights di uno\n specifico layer possono essere visualizzati:\n \n \n@@ -530,7 +530,7 @@ tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n           0.2220,  0.2358]]).\n ```\n \n-Nello script di conversione, dovreste riempire quei valori di inizializzazione random con gli stessi weights del corrispondente \n+Nello script di conversione, dovreste riempire quei valori di inizializzazione random con gli stessi weights del corrispondente\n layer nel checkpoint. *Per esempio*\n \n ```python\n@@ -544,8 +544,8 @@ model_pointer = getattr(model, \"dense\")\n model_pointer.weight.data = torch.from_numpy(pretrained_weight)\n ```\n \n-Così facendo, dovete verificare che ogni inizializzazione random di un peso del modello PyTorch e il suo corrispondente peso nel pretrained checkpoint \n-siano esattamente gli stessi e uguali in **dimensione/shape e nome**. Per fare questo, é **necessario** aggiungere un `assert` \n+Così facendo, dovete verificare che ogni inizializzazione random di un peso del modello PyTorch e il suo corrispondente peso nel pretrained checkpoint\n+siano esattamente gli stessi e uguali in **dimensione/shape e nome**. Per fare questo, é **necessario** aggiungere un `assert`\n per la dimensione/shape e nome:\n \n ```python\n@@ -560,19 +560,19 @@ Inoltre, dovrete fare il print sia dei nomi che dei weights per essere sicuri ch\n logger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")\n ```\n \n-Se la dimensione o il nome non sono uguali, probabilmente avete sbagliato ad assegnare il peso nel checkpoint o nel layer costrutture di \n+Se la dimensione o il nome non sono uguali, probabilmente avete sbagliato ad assegnare il peso nel checkpoint o nel layer costrutture di\n  🤗 Transformers.\n \n-Una dimensione sbagliata può essere dovuta ad un errore nei parameteri in `BrandNewBertConfig()`. Tuttavia, può essere anche \n-che l'implementazione del layer in PyTorch richieda di fare una transposizione della matrice dei weights. \n+Una dimensione sbagliata può essere dovuta ad un errore nei parameteri in `BrandNewBertConfig()`. Tuttavia, può essere anche\n+che l'implementazione del layer in PyTorch richieda di fare una transposizione della matrice dei weights.\n \n-Infine, controllate **tutti** che tutti i weights inizializzati e fate print di tutti i weights del checkpoint che non sono stati \n-usati per l'inizializzazione, di modo da essere sicuri che il modello sia correttamente convertito. É normale che ci siano \n-errori nel test di conversione, fai per un errore in `BrandNewBertConfig()`, o un errore nell'architettura in 🤗 Transformers, \n-o un bug in `init()`. \n+Infine, controllate **tutti** che tutti i weights inizializzati e fate print di tutti i weights del checkpoint che non sono stati\n+usati per l'inizializzazione, di modo da essere sicuri che il modello sia correttamente convertito. É normale che ci siano\n+errori nel test di conversione, fai per un errore in `BrandNewBertConfig()`, o un errore nell'architettura in 🤗 Transformers,\n+o un bug in `init()`.\n \n-Questo step dev'essere fatto tramite iterazioni fino a che non si raggiungano gli stessi valori per i weights. Una volta che \n-il checkpoint é stato correttamente caricato in 🤗 Transformers, potete salvare il modello in una cartella di vostra scelta \n+Questo step dev'essere fatto tramite iterazioni fino a che non si raggiungano gli stessi valori per i weights. Una volta che\n+il checkpoint é stato correttamente caricato in 🤗 Transformers, potete salvare il modello in una cartella di vostra scelta\n `/path/to/converted/checkpoint/folder` che contenga sia\n `pytorch_model.bin` che `config.json`:\n \n@@ -583,9 +583,9 @@ model.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n \n **7. Implementare il forward pass**\n \n-Una volta che i weights pretrained sono stati correttamente caricati in 🤗 Transformers, dovrete assicurarvi che il forward pass \n+Una volta che i weights pretrained sono stati correttamente caricati in 🤗 Transformers, dovrete assicurarvi che il forward pass\n sia correttamente implementato. [Qui](#3-4-provare-un-pretrained-checkpoint-usando-la-repo-originale), avete give creato e provato\n-uno script che testi il forward pass del modello usando la repo originaria. Ora dovrete fare lo stesso con uno script analogo \n+uno script che testi il forward pass del modello usando la repo originaria. Ora dovrete fare lo stesso con uno script analogo\n usando l'implementazione in 🤗 Transformers anziché l'originale. Piu o meno lo script dovrebbe essere:\n \n ```python\n@@ -594,37 +594,37 @@ input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\n output = model(input_ids).last_hidden_states\n ```\n \n-Di solito l'output da 🤗 Transformers non é uguale uguale all'output originario, sopratto la prima volta. Non vi abbattete - \n-é normale! Prima di tutto assicuratevi che non ci siano errori o che non vengano segnalati degli errori nella forward pass. \n-Spesso capita che ci siano dimensioni sbagliate o data type sbagliati, *ad esempio* `torch.long` anziche `torch.float32`. \n+Di solito l'output da 🤗 Transformers non é uguale uguale all'output originario, sopratto la prima volta. Non vi abbattete -\n+é normale! Prima di tutto assicuratevi che non ci siano errori o che non vengano segnalati degli errori nella forward pass.\n+Spesso capita che ci siano dimensioni sbagliate o data type sbagliati, *ad esempio* `torch.long` anziche `torch.float32`.\n Non esistate a chiedere al team Hugging Face!\n \n-Nella parte finale assicuratevi che l'implementazione 🤗 Transformers funzioni correttamente cosi da testare che gli output \n-siano equivalenti a una precisione di `1e-3`. Controllate che `outputs.shape` siano le stesse tra 🤗 Transformers e l'implementazione \n-originaria. Poi, controllate che i valori in output siano identici. Questa é sicuramente la parte più difficile, qui una serie \n+Nella parte finale assicuratevi che l'implementazione 🤗 Transformers funzioni correttamente cosi da testare che gli output\n+siano equivalenti a una precisione di `1e-3`. Controllate che `outputs.shape` siano le stesse tra 🤗 Transformers e l'implementazione\n+originaria. Poi, controllate che i valori in output siano identici. Questa é sicuramente la parte più difficile, qui una serie\n di errori comuni quando gli output non sono uguali:\n \n-- Alcuni layers non sono stati aggiunti, *ad esempio* un *activation* layer non é stato aggiunto, o ci si é scordati di una connessione \n-- La matrice del word embedding non é stata ripareggiata \n-- Ci sono degli embeddings posizionali sbagliati perché l'implementazione originaria ha un offset \n-- Il dropout é in azione durante il forward pass. Per sistemare questo errore controllate che *model.training = False* e che \n+- Alcuni layers non sono stati aggiunti, *ad esempio* un *activation* layer non é stato aggiunto, o ci si é scordati di una connessione\n+- La matrice del word embedding non é stata ripareggiata\n+- Ci sono degli embeddings posizionali sbagliati perché l'implementazione originaria ha un offset\n+- Il dropout é in azione durante il forward pass. Per sistemare questo errore controllate che *model.training = False* e che\n il dropout non sia stato attivato nel forward pass, * per esempio * passate *self.training* a [PyTorch's functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)\n \n-La miglior maniera per sistemare il problema é di vedere all'implementazione originaria del forward pass e in 🤗 Transformers \n-fianco a fianco e vedere se ci sono delle differenze. In teoria, con debug e print degli output intermedie di entrambe le \n-implementazioni nel forward pass nell'esatta posizione del network dovrebbe aiutarvi a vedere dove ci sono differenze tra \n-i due frameworks. Come prima mossa controllate che `input_ids` siano identici in entrambi gli scripts. Da lì andate fino \n-all'ultimo layer. Potrete notare una differenza tra le due implementazioni a quel punto. \n+La miglior maniera per sistemare il problema é di vedere all'implementazione originaria del forward pass e in 🤗 Transformers\n+fianco a fianco e vedere se ci sono delle differenze. In teoria, con debug e print degli output intermedie di entrambe le\n+implementazioni nel forward pass nell'esatta posizione del network dovrebbe aiutarvi a vedere dove ci sono differenze tra\n+i due frameworks. Come prima mossa controllate che `input_ids` siano identici in entrambi gli scripts. Da lì andate fino\n+all'ultimo layer. Potrete notare una differenza tra le due implementazioni a quel punto.\n \n Una volta che lo stesso output é stato ragguingi, verificate gli output con `torch.allclose(original_output, output, atol=1e-3)`.\n A questo punto se é tutto a posto: complimenti! Le parti seguenti saranno una passeggiata 😊.\n \n \n **8. Aggiungere i test necessari per il modello**\n \n-A questo punto avete aggiunto con successo il vostro nuovo modello. Tuttavia, é molto probabile che il modello non sia \n+A questo punto avete aggiunto con successo il vostro nuovo modello. Tuttavia, é molto probabile che il modello non sia\n del tutto ok con il design richiesto. Per essere sicuri che l'implementazione sia consona e compatibile con 🤗 Transformers é\n-necessario implementare dei tests. Il Cookiecutter dovrebbe fornire automaticamente dei file per test per il vostro modello, \n+necessario implementare dei tests. Il Cookiecutter dovrebbe fornire automaticamente dei file per test per il vostro modello,\n di solito nella folder `tests/test_modeling_brand_new_bert.py`. Provate questo per verificare l'ok nei test piu comuni:\n \n ```bash\n@@ -636,8 +636,8 @@ Una volta sistemati i test comuni, bisogna assicurarsi che il vostro lavoro sia\n - a) La community puo capire in maniera semplice il vostro lavoro controllando tests specifici del modello *brand_new_bert*,\n - b) Implementazioni future del vostro modello non rompano alcune feature importante del modello.\n \n-Per prima cosa agguingete dei test d'integrazione. Questi sono essenziali perche fanno la stessa funzione degli scripts di \n-debug usati precedentemente. Un template per questi tests esiste gia nel Cookiecutter ed é sotto il nome di `BrandNewBertModelIntegrationTests`, \n+Per prima cosa agguingete dei test d'integrazione. Questi sono essenziali perche fanno la stessa funzione degli scripts di\n+debug usati precedentemente. Un template per questi tests esiste gia nel Cookiecutter ed é sotto il nome di `BrandNewBertModelIntegrationTests`,\n voi dovrete solo completarlo. Una volta che questi tests sono OK, provate:\n \n ```bash\n@@ -650,7 +650,7 @@ Nel caso siate su Windows, sostituite `RUN_SLOW=1` con `SET RUN_SLOW=1`\n \n </Tip>\n \n-Di seguito, tutte le features che sono utili e necessarire per *brand_new_bert* devono essere testate in test separati, \n+Di seguito, tutte le features che sono utili e necessarire per *brand_new_bert* devono essere testate in test separati,\n contenuti in `BrandNewBertModelTester`/ `BrandNewBertModelTest`. spesso la gente si scorda questi test, ma ricordate che sono utili per:\n \n \n@@ -664,7 +664,7 @@ A questo punto avremo bisogno un tokenizer per *brand_new_bert*. Di solito il to\n \n É importante che troviate il file con il tokenizer originale e che lo carichiate in 🤗 Transformers.\n \n-Per controllare che il tokenizer funzioni in modo corretto, create uno script nella repo originaria che riceva come input \n+Per controllare che il tokenizer funzioni in modo corretto, create uno script nella repo originaria che riceva come input\n una stringa e ritorni gli `input_ids`. Piu o meno questo potrebbe essere il codice:\n \n ```python\n@@ -673,8 +673,8 @@ model = BrandNewBertModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\n input_ids = model.tokenize(input_str)\n ```\n \n-Potrebbe richiedere un po' di tempo, ma guardate ancora alla repo originaria per trovare la funzione corretta del tokenizer. \n-A volte capita di dover riscrivere il tokenizer nella repo originaria, di modo da avere come output gli `input_ids`. \n+Potrebbe richiedere un po' di tempo, ma guardate ancora alla repo originaria per trovare la funzione corretta del tokenizer.\n+A volte capita di dover riscrivere il tokenizer nella repo originaria, di modo da avere come output gli `input_ids`.\n A quel punto uno script analogo é necessario in 🤗 Transformers:\n \n ```python\n@@ -687,7 +687,7 @@ tokenizer = BrandNewBertTokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\n input_ids = tokenizer(input_str).input_ids\n ```\n \n-Una volta che `input_ids` sono uguali, bisogna aggiungere un test per il tokenizer. \n+Una volta che `input_ids` sono uguali, bisogna aggiungere un test per il tokenizer.\n \n Il file test per tokenizer di *brand_new_brand* dovrebbe avere un paio di hard-coded test d'integrazione.\n \n@@ -696,22 +696,22 @@ Il file test per tokenizer di *brand_new_brand* dovrebbe avere un paio di hard-c\n \n Ora che avete il tokenizer, dovrete aggiungere dei test d'integrazione per l'intero workflow in `tests/test_modeling_brand_new_bert.py` in 🤗 Transformer.\n Questi test devono mostrare che un significante campione text-to-text funzioni come ci si aspetta nell'implementazione di  🤗 Transformers.\n-*Per esempio* potreste usare dei source-to-target-translation, o un sommario di un articolo, o un domanda-risposta e cosi via. \n-Se nessuno dei checkpoints é stato ultra parametrizzato per task simili, allora i tests per il modello sono piu che sufficienti. \n-Nello step finale dovete assicurarvi che il modello sia totalmente funzionale, e consigliamo anche di provare a testare su GPU. \n+*Per esempio* potreste usare dei source-to-target-translation, o un sommario di un articolo, o un domanda-risposta e cosi via.\n+Se nessuno dei checkpoints é stato ultra parametrizzato per task simili, allora i tests per il modello sono piu che sufficienti.\n+Nello step finale dovete assicurarvi che il modello sia totalmente funzionale, e consigliamo anche di provare a testare su GPU.\n Puo succedere che ci si scordi un `.to(self.device)` ad esempio. Se non avete accesso a GPU, il team Hugging Face puo provvedere\n-a testare questo aspetto per voi. \n+a testare questo aspetto per voi.\n \n **11. Aggiungere una Docstring**\n \n-Siete quasi alla fine! L'ultima cosa rimasta é avere una bella docstring e una pagina doc. Il Cookiecutter dovrebbe provvedere già \n-un template chiamato `docs/source/model_doc/brand_new_bert.rst`, che dovrete compilare. La prima cosa che un utente farà \n-per usare il vostro modello sarà dare una bella lettura al doc. Quindi proponete una documentazione chiara e concisa. É molto \n-utile per la community avere anche delle *Tips* per mostrare come il modello puo' essere usato. Non esitate a chiedere a Hugging Face \n-riguardo alle docstirng. \n+Siete quasi alla fine! L'ultima cosa rimasta é avere una bella docstring e una pagina doc. Il Cookiecutter dovrebbe provvedere già\n+un template chiamato `docs/source/model_doc/brand_new_bert.rst`, che dovrete compilare. La prima cosa che un utente farà\n+per usare il vostro modello sarà dare una bella lettura al doc. Quindi proponete una documentazione chiara e concisa. É molto\n+utile per la community avere anche delle *Tips* per mostrare come il modello puo' essere usato. Non esitate a chiedere a Hugging Face\n+riguardo alle docstirng.\n \n-Quindi, assicuratevi che la docstring sia stata aggiunta a `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`. \n-Assicuratevi che la docstring sia corretta e che includa tutti i necessari input e output. Abbiamo una guida dettagliata per \n+Quindi, assicuratevi che la docstring sia stata aggiunta a `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`.\n+Assicuratevi che la docstring sia corretta e che includa tutti i necessari input e output. Abbiamo una guida dettagliata per\n scrivere la documentazione e docstring.\n \n \n@@ -729,19 +729,19 @@ E che il codice passi i quality check:\n make quality\n ```\n \n-A volte capita che manchino delle informazioninella docstring o alcuni nomi sbagliati, questo farà fallire i tests sopra. \n-Ripetiamo: chiedete pure a Hugging Face, saremo lieti di aiutarvi. \n+A volte capita che manchino delle informazioninella docstring o alcuni nomi sbagliati, questo farà fallire i tests sopra.\n+Ripetiamo: chiedete pure a Hugging Face, saremo lieti di aiutarvi.\n \n Per ultimo, fare del refactoring del codice una volta che é stato creato.\n \n Avete finito con il codice, congratulazioni! 🎉 Siete fantasticiiiiiii! 😎\n \n **12. Caricare il modello sul model hub**\n \n-In questa ultima parte dovrete convertire e caricare il modello, con tutti i checkpoints, nel model hub e aggiungere una \n-model card per ogni checkpoint caricato. Leggete la nostra guida [Model sharing and uploading Page](model_sharing) per \n-avere familiarità con l'hub. Di solito in questa parte lavorate a fianco di Hugging face per decidere un nome che sia ok \n-per ogni checkpoint, per ottenere i permessi necessari per caricare il modello nell'organizzazione dell'autore di *brand_new_bert*. \n+In questa ultima parte dovrete convertire e caricare il modello, con tutti i checkpoints, nel model hub e aggiungere una\n+model card per ogni checkpoint caricato. Leggete la nostra guida [Model sharing and uploading Page](model_sharing) per\n+avere familiarità con l'hub. Di solito in questa parte lavorate a fianco di Hugging face per decidere un nome che sia ok\n+per ogni checkpoint, per ottenere i permessi necessari per caricare il modello nell'organizzazione dell'autore di *brand_new_bert*.\n Il metodo `push_to_hub`, presente in tutti i modelli `transformers`, é una maniera rapida e indolore per caricare il vostro checkpoint sull'hub:\n \n ```python\n@@ -754,27 +754,27 @@ brand_new_bert.push_to_hub(\n )\n ```\n \n-Vale la pena spendere un po' di tempo per creare una model card ad-hoc per ogni checkpoint. Le model cards dovrebbero \n-suggerire le caratteristiche specifiche del checkpoint, *per esempio* su che dataset il checkpoint é stato pretrained o fine-tuned. \n+Vale la pena spendere un po' di tempo per creare una model card ad-hoc per ogni checkpoint. Le model cards dovrebbero\n+suggerire le caratteristiche specifiche del checkpoint, *per esempio* su che dataset il checkpoint é stato pretrained o fine-tuned.\n O che su che genere di task il modello lavoro? E anche buona pratica includere del codice su come usare il modello correttamente.\n \n \n **13. (Opzionale) Aggiungere un notebook**\n \n-É molto utile aggiungere un notebook, che dimostri in dettaglio come *brand_new_bert* si utilizzi per fare inferenza e/o \n+É molto utile aggiungere un notebook, che dimostri in dettaglio come *brand_new_bert* si utilizzi per fare inferenza e/o\n fine-tuned su specifiche task. Non é una cosa obbligatoria da avere nella vostra PR, ma é molto utile per la community.\n \n **14. Sottomettere la PR**\n \n-L'ultimissimo step! Ovvero il merge della PR nel main. Di solito il team Hugging face a questo punto vi avrà gia aiutato, \n+L'ultimissimo step! Ovvero il merge della PR nel main. Di solito il team Hugging face a questo punto vi avrà gia aiutato,\n ma é ok prendere un po' di tempo per pulire la descirzione e commenti nel codice.\n \n \n ### Condividete il vostro lavoro!!\n \n-É ora tempo di prendere un po' di credito dalla communità per il vostro lavoro! Caricare e implementare un nuovo modello \n-é un grandissimo contributo per Transformers e l'intera community NLP. Il codice e la conversione dei modelli pre-trained sara \n-sicuramente utilizzato da centinaia o migliaia di sviluppatori e ricercatori. Siate fieri e orgogliosi di condividere il vostro \n-traguardo con l'intera community :) \n+É ora tempo di prendere un po' di credito dalla communità per il vostro lavoro! Caricare e implementare un nuovo modello\n+é un grandissimo contributo per Transformers e l'intera community NLP. Il codice e la conversione dei modelli pre-trained sara\n+sicuramente utilizzato da centinaia o migliaia di sviluppatori e ricercatori. Siate fieri e orgogliosi di condividere il vostro\n+traguardo con l'intera community :)\n \n ** Avete create un altro modello che é super facile da usare per tutti quanti nella community! 🤯**"
        },
        {
            "sha": "dace244fa6dd069aab5e61432aa9c3387f0408e4",
            "filename": "docs/source/it/converting_tensorflow_models.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fit%2Fconverting_tensorflow_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fit%2Fconverting_tensorflow_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fconverting_tensorflow_models.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -18,10 +18,10 @@ in modelli che possono essere caricati utilizzando i metodi `from_pretrained` de\n \n <Tip>\n \n-A partire dalla versione 2.3.0 lo script di conversione è parte di transformers CLI (**transformers-cli**), disponibile in ogni installazione\n+A partire dalla versione 2.3.0 lo script di conversione è parte di transformers CLI (**transformers**), disponibile in ogni installazione\n di transformers >=2.3.0.\n \n-La seguente documentazione riflette il formato dei comandi di **transformers-cli convert**.\n+La seguente documentazione riflette il formato dei comandi di **transformers convert**.\n \n </Tip>\n \n@@ -49,7 +49,7 @@ Questo è un esempio del processo di conversione per un modello `BERT-Base Uncas\n \n ```bash\n export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n-transformers-cli convert --model_type bert \\\n+transformers convert --model_type bert \\\n   --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n   --config $BERT_BASE_DIR/bert_config.json \\\n   --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin\n@@ -70,7 +70,7 @@ Ecco un esempio del procedimento di conversione di un modello `ALBERT Base` pre-\n \n ```bash\n export ALBERT_BASE_DIR=/path/to/albert/albert_base\n-transformers-cli convert --model_type albert \\\n+transformers convert --model_type albert \\\n   --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\\n   --config $ALBERT_BASE_DIR/albert_config.json \\\n   --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin\n@@ -84,7 +84,7 @@ Ecco un esempio del processo di conversione di un modello OpenAI GPT pre-allenat\n sia salvato nello stesso formato dei modelli pre-allenati OpenAI (vedi [qui](https://github.com/openai/finetune-transformer-lm)):\n ```bash\n export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights\n-transformers-cli convert --model_type gpt \\\n+transformers convert --model_type gpt \\\n   --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n   [--config OPENAI_GPT_CONFIG] \\\n@@ -97,7 +97,7 @@ Ecco un esempio del processo di conversione di un modello OpenAI GPT-2 pre-allen\n \n ```bash\n export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/openai-community/gpt2/pretrained/weights\n-transformers-cli convert --model_type gpt2 \\\n+transformers convert --model_type gpt2 \\\n   --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n   [--config OPENAI_GPT2_CONFIG] \\\n@@ -111,7 +111,7 @@ Ecco un esempio del processo di conversione di un modello XLNet pre-allenato:\n ```bash\n export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n-transformers-cli convert --model_type xlnet \\\n+transformers convert --model_type xlnet \\\n   --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\\n   --config $TRANSFO_XL_CONFIG_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n@@ -124,7 +124,7 @@ Ecco un esempio del processo di conversione di un modello XLM pre-allenato:\n \n ```bash\n export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint\n-transformers-cli convert --model_type xlm \\\n+transformers convert --model_type xlm \\\n   --tf_checkpoint $XLM_CHECKPOINT_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT\n  [--config XML_CONFIG] \\\n@@ -137,7 +137,7 @@ Ecco un esempio del processo di conversione di un modello T5 pre-allenato:\n \n ```bash\n export T5=/path/to/t5/uncased_L-12_H-768_A-12\n-transformers-cli convert --model_type t5 \\\n+transformers convert --model_type t5 \\\n   --tf_checkpoint $T5/t5_model.ckpt \\\n   --config $T5/t5_config.json \\\n   --pytorch_dump_output $T5/pytorch_model.bin"
        },
        {
            "sha": "000e4fd8592431c1d14629dd96e736e0f336fa79",
            "filename": "docs/source/ja/add_new_model.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fja%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fja%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fadd_new_model.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -312,7 +312,7 @@ cd transformers\n 既存のモデル:\n \n ```bash\n-transformers-cli add-new-model-like\n+transformers add-new-model-like\n ```\n \n モデルの基本情報を入力するためのアンケートが表示されます。\n@@ -517,7 +517,7 @@ tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n \n スクリプト内の変換スクリプトでは、ランダムに初期化された重みを、対応するチェックポイント内の正確な重みで埋める必要があります。例えば、以下のように翻訳します：\n \n- \n+\n ```python\n # retrieve matching layer weights, e.g. by\n # recursive algorithm\n@@ -747,5 +747,3 @@ brand_new_bert.push_to_hub(\"brand_new_bert\")\n さあ、コミュニティからあなたの作業に対する評価を得る時が来ました！モデルの追加を完了することは、TransformersおよびNLPコミュニティにとって重要な貢献です。あなたのコードとポートされた事前学習済みモデルは、何百人、何千人という開発者や研究者によって確実に使用されるでしょう。あなたの仕事に誇りを持ち、コミュニティとあなたの成果を共有しましょう。\n \n **あなたはコミュニティの誰でも簡単にアクセスできる別のモデルを作成しました！ 🤯**\n-\n-"
        },
        {
            "sha": "e30c2dc9f0d223a2d457ae73ce14cb2a9d6a6ab8",
            "filename": "docs/source/ko/add_new_model.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fko%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fko%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fadd_new_model.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -73,7 +73,7 @@ model.config  # model has access to its config\n 5. 함수 시그니처에는 타입 주석을 사용해야 합니다. 그 외에는 타입 주석보다 변수 이름이 훨씬 읽기 쉽고 이해하기 쉽습니다.\n \n ### 토크나이저 개요 [[overview-of-tokenizers]]\n- \n+\n 아직 준비되지 않았습니다 :-( 이 섹션은 곧 추가될 예정입니다!\n \n ## 🤗 Transformers에 모델 추가하는 단계별 방법  [[stepbystep-recipe-to-add-a-model-to-transformers]]\n@@ -272,7 +272,7 @@ cd transformers\n 기존 모델:\n \n ```bash\n-transformers-cli add-new-model-like\n+transformers add-new-model-like\n ```\n \n 모델의 기본 정보를 입력하는 설문지가 표시됩니다."
        },
        {
            "sha": "f005ac0a569a70daee671ed482009147fa10d8d0",
            "filename": "docs/source/ko/contributing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fko%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fko%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcontributing.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -63,7 +63,7 @@ limitations under the License.\n 운영체제와 소프트웨어 버전을 자동으로 가져오려면 다음 명령을 실행하세요:\n \n ```bash\n-transformers-cli env\n+transformers env\n ```\n \n 저장소의 루트 디렉터리에서도 같은 명령을 실행할 수 있습니다:"
        },
        {
            "sha": "6a4f7a5102ce3199264783460abd5556fa4cea11",
            "filename": "docs/source/ko/deepspeed.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fdeepspeed.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -1165,7 +1165,7 @@ python -c 'import deepspeed; print(f\"deepspeed: {deepspeed.__version__}\")'\n \n ### DeepSpeed 프로세스가 시작 단계에서 종료되었을 경우[[deepspeed-process-killed-at-startup]]\n \n-실행 중에 트레이스백 없이 DeepSpeed 프로세스가 종료되면 일반적으로 프로그램이 시스템보다 많은 CPU 메모리를 할당하려고 시도했거나 프로세스가 허용된 것보다 많은 CPU 메모리를 할당하려고 시도하여 OS 커널이 프로세스를 종료했음을 의미합니다. 이 경우 구성 파일에 `offload_optimizer`, `offload_param` 또는 둘 다 CPU로 오프로드하도록 구성되어 있는지 확인하세요.  \n+실행 중에 트레이스백 없이 DeepSpeed 프로세스가 종료되면 일반적으로 프로그램이 시스템보다 많은 CPU 메모리를 할당하려고 시도했거나 프로세스가 허용된 것보다 많은 CPU 메모리를 할당하려고 시도하여 OS 커널이 프로세스를 종료했음을 의미합니다. 이 경우 구성 파일에 `offload_optimizer`, `offload_param` 또는 둘 다 CPU로 오프로드하도록 구성되어 있는지 확인하세요.\n \n NVMe 및 ZeRO-3를 설정한 경우 NVMe로 오프로드를 실험해 보세요(모델의 메모리 요구 사항을 [확인](https://deepspeed.readthedocs.io/en/latest/memory.html)하세요).\n \n@@ -1211,7 +1211,7 @@ NVMe 및 ZeRO-3를 설정한 경우 NVMe로 오프로드를 실험해 보세요(\n \n ## 리소스[[resources]]\n \n-DeepSpeed ZeRO는 제한된 GPU 리소스로 추론을 위해 매우 큰 모델을 훈련하고 로드하는 강력한 기술로, 누구나 쉽게 사용할 수 있습니다. DeepSpeed에 대해 자세히 알아보려면 [블로그 포스트](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [공식 문서](https://www.deepspeed.ai/getting-started/), [깃허브 리포지토리](https://github.com/deepspeedai/DeepSpeed)를 참조하세요. \n+DeepSpeed ZeRO는 제한된 GPU 리소스로 추론을 위해 매우 큰 모델을 훈련하고 로드하는 강력한 기술로, 누구나 쉽게 사용할 수 있습니다. DeepSpeed에 대해 자세히 알아보려면 [블로그 포스트](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [공식 문서](https://www.deepspeed.ai/getting-started/), [깃허브 리포지토리](https://github.com/deepspeedai/DeepSpeed)를 참조하세요.\n \n 다음 문서도 ZeRO에 대해 자세히 알아볼 수 있는 훌륭한 자료입니다:\n "
        },
        {
            "sha": "446acd62ea8ffd2b422f9f9a84b5091c4788156d",
            "filename": "docs/source/pt/converting_tensorflow_models.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fpt%2Fconverting_tensorflow_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fpt%2Fconverting_tensorflow_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fconverting_tensorflow_models.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -21,10 +21,10 @@ que podem ser carregados usando os métodos `from_pretrained` da biblioteca.\n \n <Tip>\n \n-A partir da versão 2.3.0 o script de conversão agora faz parte do transformers CLI (**transformers-cli**) disponível em qualquer instalação\n+A partir da versão 2.3.0 o script de conversão agora faz parte do transformers CLI (**transformers**) disponível em qualquer instalação\n transformers >= 2.3.0.\n \n-A documentação abaixo reflete o formato do comando **transformers-cli convert**.\n+A documentação abaixo reflete o formato do comando **transformers convert**.\n \n </Tip>\n \n@@ -49,7 +49,7 @@ Aqui está um exemplo do processo de conversão para um modelo `BERT-Base Uncase\n ```bash\n export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n \n-transformers-cli convert --model_type bert \\\n+transformers convert --model_type bert \\\n   --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n   --config $BERT_BASE_DIR/bert_config.json \\\n   --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin\n@@ -71,7 +71,7 @@ Aqui está um exemplo do processo de conversão para o modelo `ALBERT Base` pré\n ```bash\n export ALBERT_BASE_DIR=/path/to/albert/albert_base\n \n-transformers-cli convert --model_type albert \\\n+transformers convert --model_type albert \\\n   --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\\n   --config $ALBERT_BASE_DIR/albert_config.json \\\n   --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin\n@@ -88,7 +88,7 @@ foi salvo com o mesmo formato do modelo pré-treinado OpenAI (veja [aqui](https:\n ```bash\n export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights\n \n-transformers-cli convert --model_type gpt \\\n+transformers convert --model_type gpt \\\n   --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n   [--config OPENAI_GPT_CONFIG] \\\n@@ -102,7 +102,7 @@ Aqui está um exemplo do processo de conversão para um modelo OpenAI GPT-2 pré\n ```bash\n export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/openai-community/gpt2/pretrained/weights\n \n-transformers-cli convert --model_type gpt2 \\\n+transformers convert --model_type gpt2 \\\n   --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n   [--config OPENAI_GPT2_CONFIG] \\\n@@ -117,7 +117,7 @@ Aqui está um exemplo do processo de conversão para um modelo XLNet pré-treina\n export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n \n-transformers-cli convert --model_type xlnet \\\n+transformers convert --model_type xlnet \\\n   --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\\n   --config $TRANSFO_XL_CONFIG_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n@@ -131,7 +131,7 @@ Aqui está um exemplo do processo de conversão para um modelo XLM pré-treinado\n ```bash\n export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint\n \n-transformers-cli convert --model_type xlm \\\n+transformers convert --model_type xlm \\\n   --tf_checkpoint $XLM_CHECKPOINT_PATH \\\n   --pytorch_dump_output $PYTORCH_DUMP_OUTPUT\n  [--config XML_CONFIG] \\\n@@ -145,7 +145,7 @@ Aqui está um exemplo do processo de conversão para um modelo T5 pré-treinado:\n ```bash\n export T5=/path/to/t5/uncased_L-12_H-768_A-12\n \n-transformers-cli convert --model_type t5 \\\n+transformers convert --model_type t5 \\\n   --tf_checkpoint $T5/t5_model.ckpt \\\n   --config $T5/t5_config.json \\\n   --pytorch_dump_output $T5/pytorch_model.bin"
        },
        {
            "sha": "797e5b73c0e3bff939468f6a545c7683ae81480d",
            "filename": "docs/source/zh/contributing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fzh%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/docs%2Fsource%2Fzh%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcontributing.md?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -63,7 +63,7 @@ limitations under the License.\n 想要自动获取操作系统和软件版本，请运行以下命令：\n \n ```bash\n-transformers-cli env\n+transformers env\n ```\n \n 你也可以从代码仓库的根目录下运行相同的命令："
        },
        {
            "sha": "31c1e84e14ce4e54a87925893105e6d4bc9bd384",
            "filename": "examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -99,7 +99,7 @@ class ModelArguments:\n     use_auth_token: bool = field(\n         default=False,\n         metadata={\n-            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n+            \"help\": \"Will use the token generated when running `transformers login` (necessary to use this script \"\n             \"with private models).\"\n         },\n     )"
        },
        {
            "sha": "70aaa1876207d4d81721dc7d5a8315c5ac2d50b1",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -466,7 +466,7 @@ def run(self):\n     package_data={\"\": [\"**/*.cu\", \"**/*.cpp\", \"**/*.cuh\", \"**/*.h\", \"**/*.pyx\", \"py.typed\"]},\n     zip_safe=False,\n     extras_require=extras,\n-    entry_points={\"console_scripts\": [\"transformers-cli=transformers.commands.transformers_cli:main\"]},\n+    entry_points={\"console_scripts\": [\"transformers=transformers.commands.transformers_cli:main\", \"transformers-cli=transformers.commands.transformers_cli:main_cli\"]},\n     python_requires=\">=3.9.0\",\n     install_requires=list(install_requires),\n     classifiers=["
        },
        {
            "sha": "cd7adbe4f6fd4b8d2a3ae43954795a6de08ab4e2",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -358,7 +358,7 @@ class ChatArguments:\n     \"\"\"\n \n     # General settings\n-    model_name_or_path: str = field(metadata={\"help\": \"Name of the pre-trained model.\"})\n+    model_name_or_path: Optional[str] = field(default=None, metadata={\"help\": \"Name of the pre-trained model.\"})\n     user: Optional[str] = field(default=None, metadata={\"help\": \"Username to display in chat interface.\"})\n     system_prompt: Optional[str] = field(default=None, metadata={\"help\": \"System prompt.\"})\n     save_folder: str = field(default=\"./chat_history/\", metadata={\"help\": \"Folder to save chat history.\"})\n@@ -435,9 +435,20 @@ def register_subcommand(parser: ArgumentParser):\n         \"\"\"\n         dataclass_types = (ChatArguments,)\n         chat_parser = parser.add_parser(\"chat\", help=HELP_STRING, dataclass_types=dataclass_types)\n+\n+        group = chat_parser.add_argument_group(\"Positional arguments\")\n+        group.add_argument(\n+            \"model_name_or_path_positional\", type=str, nargs=\"?\", default=None, help=\"Name of the pre-trained model.\"\n+        )\n+\n         chat_parser.set_defaults(func=chat_command_factory)\n \n     def __init__(self, args):\n+        args.model_name_or_path = args.model_name_or_path_positional or args.model_name_or_path\n+\n+        if args.model_name_or_path is None:\n+            raise ValueError(\"--model_name_or_path required for chat command.\")\n+\n         self.args = args\n \n     @staticmethod"
        },
        {
            "sha": "220d1d44b1aa281e3b3af9ad584ad3ad49b6beb0",
            "filename": "src/transformers/commands/convert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fconvert.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -73,7 +73,7 @@ def __init__(\n         finetuning_task_name: str,\n         *args,\n     ):\n-        self._logger = logging.get_logger(\"transformers-cli/converting\")\n+        self._logger = logging.get_logger(\"transformers/converting\")\n \n         self._logger.info(f\"Loading model {model_type}\")\n         self._model_type = model_type"
        },
        {
            "sha": "212a4e8710be642822a1c237bc929df2f378ef11",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -37,7 +37,7 @@ def Body(*x, **y):\n     _serve_dependencies_installed = False\n \n \n-logger = logging.get_logger(\"transformers-cli/serving\")\n+logger = logging.get_logger(\"transformers/serving\")\n \n \n def serve_command_factory(args: Namespace):"
        },
        {
            "sha": "06e95443df24c87f86b4e2dc96d811dee515e267",
            "filename": "src/transformers/commands/train.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Ftrain.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Ftrain.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftrain.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -91,7 +91,7 @@ def register_subcommand(parser: ArgumentParser):\n         train_parser.set_defaults(func=train_command_factory)\n \n     def __init__(self, args: Namespace):\n-        self.logger = logging.get_logger(\"transformers-cli/training\")\n+        self.logger = logging.get_logger(\"transformers/training\")\n \n         self.framework = \"tf\" if is_tf_available() else \"torch\"\n "
        },
        {
            "sha": "00eaff01a4efbeb6f9deaad147ab22669dc112f9",
            "filename": "src/transformers/commands/transformers_cli.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -12,6 +12,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n \n from transformers import HfArgumentParser\n from transformers.commands.add_fast_image_processor import AddFastImageProcessorCommand\n@@ -24,9 +25,17 @@\n from transformers.commands.serving import ServeCommand\n \n \n+def main_cli():\n+    warnings.warn(\n+        \"`transformers-cli` is deprecated in favour of `transformers` directly and will be removed in v5.\",\n+        DeprecationWarning,\n+    )\n+    main()\n+\n+\n def main():\n-    parser = HfArgumentParser(prog=\"Transformers CLI tool\", usage=\"transformers-cli <command> [<args>]\")\n-    commands_parser = parser.add_subparsers(help=\"transformers-cli command helpers\")\n+    parser = HfArgumentParser(prog=\"Transformers CLI tool\", usage=\"transformers <command> [<args>]\")\n+    commands_parser = parser.add_subparsers(help=\"transformers command helpers\")\n \n     # Register commands\n     ChatCommand.register_subcommand(commands_parser)"
        },
        {
            "sha": "c86afddc79303b32a330b62fb880cb7668a49b2a",
            "filename": "src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -257,7 +257,7 @@ def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder\n     print(\"Conversion is done!\")\n     print(\"\\nLast step is to upload the files to s3\")\n     print(f\"cd {data_root}\")\n-    print(f\"transformers-cli upload {model_dir}\")\n+    print(f\"transformers upload {model_dir}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "e9467e518d17dd9045b7f42d17ac287b30c602ad",
            "filename": "tests/utils/test_cli.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d538293f62f20d5c756a0a461bb5dbcff1e584a4/tests%2Futils%2Ftest_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d538293f62f20d5c756a0a461bb5dbcff1e584a4/tests%2Futils%2Ftest_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cli.py?ref=d538293f62f20d5c756a0a461bb5dbcff1e584a4",
            "patch": "@@ -23,7 +23,7 @@\n class CLITest(unittest.TestCase):\n     @patch(\"sys.argv\", [\"fakeprogrampath\", \"env\"])\n     def test_cli_env(self):\n-        # test transformers-cli env\n+        # test transformers env\n         import transformers.commands.transformers_cli\n \n         with CaptureStd() as cs:"
        }
    ],
    "stats": {
        "total": 785,
        "additions": 399,
        "deletions": 386
    }
}