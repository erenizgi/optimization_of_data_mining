{
    "author": "gante",
    "message": "[tests] Smaller model in slow cache tests (#37922)",
    "sha": "9981214d32c9d0777e1d67cd68d4dd4009712e75",
    "files": [
        {
            "sha": "691c5aa535782024e3994464efdac1e75b4267b5",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 58,
            "deletions": 40,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/9981214d32c9d0777e1d67cd68d4dd4009712e75/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9981214d32c9d0777e1d67cd68d4dd4009712e75/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=9981214d32c9d0777e1d67cd68d4dd4009712e75",
            "patch": "@@ -24,6 +24,7 @@\n     cleanup,\n     get_gpu_count,\n     is_torch_available,\n+    require_read_token,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n@@ -301,79 +302,95 @@ def test_cache_extra_left_padding(self, cache_implementation):\n class CacheHardIntegrationTest(unittest.TestCase):\n     \"\"\"Hard cache integration tests that require loading different models\"\"\"\n \n-    def tearDown(self):\n-        # Some tests use large models, which might result in suboptimal torch re-allocation if we run multiple tests\n-        # in a row\n+    def setUp(self):\n+        # Clears memory before each test. Some tests use large models, which might result in suboptimal torch\n+        # re-allocation if we run multiple tests in a row without clearing memory.\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Clears memory after the last test. See `setUp` for more details.\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n     def test_dynamic_cache_hard(self):\n         \"\"\"Hard test for base cache implementation -- minor numerical fluctuations will cause this test to fail\"\"\"\n-        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"meta-llama/Llama-2-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16\n-        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\", padding_side=\"left\")\n+        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-4B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n         inputs = tokenizer([\"Here's everything I know about cats. Cats\"], return_tensors=\"pt\").to(model.device)\n \n         set_seed(0)\n-        gen_out = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n-\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        expected_text = (\n-            \"Here's everything I know about cats. Cats are mysterious creatures. They can't talk, and they don't like \"\n-            \"to be held. They don't play fetch, and they don't like to be hugged. But they do like to be petted.\\n\"\n-            \"Cats are also very independent. They don't like to be told what to do, and they don't like to be told \"\n-            \"what to eat. They are also very territorial. They don't like to share their food or their toys.\\nCats \"\n-            \"are also very curious. They like to explore, and they like to play. They are also very fast. They can \"\n-            \"run very fast, and they can jump very high.\\nCats are also very smart. They can learn tricks, and they \"\n-            \"can solve problems. They are also very playful. They like to play with toys, and they like to play with \"\n-            \"other cats.\\nCats are also very affectionate. They like to be petted, and they like to be held. They \"\n-            \"also like to be scratched.\\nCats are also very clean. They like to groom themselves, and they like to \"\n-            \"clean their litter box.\\nCats are also very independent. They don't\"\n+        gen_out = model.generate(\n+            **inputs, do_sample=True, max_new_tokens=256, return_dict_in_generate=True, output_scores=True\n+        )\n+        decoded = tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n+        # sum of the scores for the generated tokens\n+        input_length = inputs.input_ids.shape[1]\n+        score_sum = sum(\n+            [score[0][gen_out.sequences[0][input_length + idx]] for idx, score in enumerate(gen_out.scores)]\n         )\n-        self.assertEqual(decoded[0], expected_text)\n+\n+        EXPECTED_GENERATION = (\n+            \"Here's everything I know about cats. Cats are mammals, they have four legs, they have a tail, they have \"\n+            \"a face with a nose, eyes, and mouth. They have fur, they have claws, and they have a body that is \"\n+            \"covered in fur. They are carnivores, so they eat meat. They are also very clean animals, they groom \"\n+            \"themselves. They have a lot of different breeds. Some are small, some are large. Some are friendly, \"\n+            \"some are not. They have a lot of different personalities. They can be very independent, or they can be \"\n+            \"very affectionate. They can be very playful, or they can be very lazy. They can be very intelligent, or \"\n+            \"they can be very silly. They have a lot of different behaviors. They can be very curious, or they can \"\n+            \"be very cautious. They can be very vocal, or they can be very quiet. They can be very social, or they \"\n+            \"can be very solitary. They can be very active, or they can be very inactive. They can be very \"\n+            \"affectionate, or they can be very aloof. They can be very playful, or they can be very lazy. They can \"\n+            \"be very intelligent, or they can be very silly. They have a lot of different behaviors. They can be \"\n+            \"very curious, or they can\"\n+        )\n+        EXPECTED_SCORE_SUM = 11017.4971\n+        self.assertEqual(decoded[0], EXPECTED_GENERATION)\n+        self.assertAlmostEqual(score_sum, EXPECTED_SCORE_SUM, places=2)\n+        self.assertIsInstance(gen_out.past_key_values, DynamicCache)  # sanity check\n \n     @parameterized.expand([(\"eager\"), (\"sdpa\")])\n     @require_torch_gpu\n     @slow\n     def test_static_cache_greedy_decoding_pad_left(self, attn_implementation):\n         \"\"\"Tests that different cache implementations work well with eager and SDPA inference\"\"\"\n         EXPECTED_GENERATION = [\n-            \"The best color is the one that complements the skin tone of the\",\n-            \"We should not undermind the issues at hand.\\nWe should not undermind the issues\",\n+            \"The best color is the one that is most suitable for the purpose.\",\n+            \"We should not undermind the issues at hand, but instead, we should focus on the things\",\n         ]\n \n-        tokenizer = AutoTokenizer.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\", padding_side=\"left\", pad_token=\"<s>\"\n-        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\", padding_side=\"left\")\n         model = AutoModelForCausalLM.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\",\n+            \"Qwen/Qwen3-4B\",\n             torch_dtype=torch.bfloat16,\n             attn_implementation=attn_implementation,\n-        ).to(torch_device)\n+            device_map=\"auto\",\n+        )\n         inputs = tokenizer(\n             [\"The best color is\", \"We should not undermind the issues at hand\"], padding=True, return_tensors=\"pt\"\n         ).to(model.device)\n+        generation_kwargs = {\"do_sample\": False, \"max_new_tokens\": 10, \"return_dict_in_generate\": True}\n \n         set_seed(0)\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n+        gen_out = model.generate(**inputs, **generation_kwargs)\n+        decoded = tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         with self.subTest(f\"{attn_implementation}, dynamic\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n+            self.assertIsInstance(gen_out.past_key_values, DynamicCache)  # sanity check\n \n         set_seed(0)\n-        gen_out = model.generate(\n-            **inputs, do_sample=False, max_new_tokens=10, cache_implementation=\"static\", disable_compile=True\n-        )\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n+        gen_out = model.generate(**inputs, **generation_kwargs, cache_implementation=\"static\", disable_compile=True)\n+        decoded = tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         with self.subTest(f\"{attn_implementation}, static, eager\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n+            self.assertIsInstance(gen_out.past_key_values, StaticCache)  # sanity check\n \n         set_seed(0)\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10, cache_implementation=\"static\")\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n+        gen_out = model.generate(**inputs, **generation_kwargs, cache_implementation=\"static\")\n+        decoded = tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         with self.subTest(f\"{attn_implementation}, static, compiled\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n+            self.assertIsInstance(gen_out.past_key_values, StaticCache)  # sanity check\n \n     @require_torch_accelerator\n     @slow\n@@ -446,9 +463,9 @@ def test_cache_copy(self):\n             responses.append(response)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTraveling is a wonderful \"\n-            \"way to explore new places, cultures, and experiences. Whether you are a seasoned traveler or a \"\n-            \"first-time adventurer, there is always something\",\n+            \"You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTraveling is an \"\n+            \"enriching experience that broadens our horizons and allows us to explore the world beyond our comfort \"\n+            \"zones. Whether it's a short weekend getaway\",\n             \"You are a helpful assistant. What is the capital of France?\\n\\n\\n## Response:Paris is the capital \"\n             \"of France.\\n\\n\\n\\n\\n\\n\\n<|endoftext|>\",\n         ]\n@@ -506,6 +523,7 @@ def test_static_cache_no_cuda_graph_skips(self):\n \n     @require_torch_multi_gpu\n     @slow\n+    @require_read_token\n     def test_static_cache_multi_gpu(self):\n         \"\"\"Regression test for #35164: static cache with multi-gpu\"\"\"\n "
        }
    ],
    "stats": {
        "total": 98,
        "additions": 58,
        "deletions": 40
    }
}