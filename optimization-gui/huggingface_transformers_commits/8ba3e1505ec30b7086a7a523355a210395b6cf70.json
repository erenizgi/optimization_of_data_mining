{
    "author": "lewtun",
    "message": "Retain newlines in chat template when `continue_final_message=True` (#34253)\n\n* Retain newlines in chat template when\r\n\r\n* Add try/except\r\n\r\n* Add regression test\r\n\r\n* Simplify test\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "8ba3e1505ec30b7086a7a523355a210395b6cf70",
    "files": [
        {
            "sha": "03df02d21ff32b04476073daee5e6b0464ad26bc",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ba3e1505ec30b7086a7a523355a210395b6cf70/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ba3e1505ec30b7086a7a523355a210395b6cf70/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=8ba3e1505ec30b7086a7a523355a210395b6cf70",
            "patch": "@@ -1690,8 +1690,12 @@ def apply_chat_template(\n                 final_message = chat[-1][\"content\"]\n                 if isinstance(final_message, (list, tuple)):\n                     final_message = final_message[-1][\"text\"]\n-                final_message = final_message.strip()\n-                rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)].rstrip()\n+                try:\n+                    rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)]\n+                except:  # noqa: E722\n+                    # Some chat templates like Llama-3.1 trim messages before rendering, so we must do the same here.\n+                    final_message = final_message.strip()\n+                    rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)]\n             rendered.append(rendered_chat)\n \n         if not is_batched:"
        },
        {
            "sha": "f04a4255556baf2e0fbff5e3551ce570008116ed",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ba3e1505ec30b7086a7a523355a210395b6cf70/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ba3e1505ec30b7086a7a523355a210395b6cf70/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=8ba3e1505ec30b7086a7a523355a210395b6cf70",
            "patch": "@@ -1461,6 +1461,38 @@ def test_continue_final_message(self):\n                     \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nuser message<|im_end|>\\n<|im_start|>assistant\\nassistant message\",\n                 )\n \n+    @require_jinja\n+    def test_continue_final_message_with_trim(self):\n+        \"\"\"Regression test for chat templates with trimming: https://github.com/huggingface/transformers/pull/34214\"\"\"\n+\n+        dummy_template = \"\"\"\n+        {%- for message in messages %}\n+            {{- \"<|im_start|>\" + message['role'] + \"\\n\" + message['content'] | trim + \"<|im_end|>\" + \"\\n\"}}\n+        {%- endfor %}\"\"\"\n+        dummy_conversation = [\n+            {\"role\": \"system\", \"content\": \"system message\"},\n+            {\"role\": \"user\", \"content\": \"user message\"},\n+            {\"role\": \"assistant\", \"content\": \"assistant message \"},  # Note the trailing whitespace\n+        ]\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                output = tokenizer.apply_chat_template(\n+                    dummy_conversation, chat_template=dummy_template, tokenize=False, continue_final_message=False\n+                )\n+                self.assertEqual(\n+                    output,\n+                    \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nuser message<|im_end|>\\n<|im_start|>assistant\\nassistant message<|im_end|>\\n\",\n+                )\n+                prefill_output = tokenizer.apply_chat_template(\n+                    dummy_conversation, chat_template=dummy_template, tokenize=False, continue_final_message=True\n+                )\n+                # Assert that the final message is unterminated\n+                self.assertEqual(\n+                    prefill_output,\n+                    \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nuser message<|im_end|>\\n<|im_start|>assistant\\nassistant message\",\n+                )\n+\n     @require_jinja\n     def test_chat_template_dict(self):\n         dummy_template_1 = \"{{'a'}}\""
        }
    ],
    "stats": {
        "total": 40,
        "additions": 38,
        "deletions": 2
    }
}