{
    "author": "insujang",
    "message": "Fix number of patch check for different vision feature select strategy (#32494)\n\n* Fix number of patch check for different vision feature select strategy\r\n\r\n* add test\r\n\r\n---------\r\n\r\nCo-authored-by: raushan <raushan@huggingface.co>",
    "sha": "bcf8946f0acb578c534b1d33d534450d1fc88507",
    "files": [
        {
            "sha": "c1d1ca8c276d7aa6e1fddc079d19d17281ad5980",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcf8946f0acb578c534b1d33d534450d1fc88507/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcf8946f0acb578c534b1d33d534450d1fc88507/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=bcf8946f0acb578c534b1d33d534450d1fc88507",
            "patch": "@@ -645,7 +645,7 @@ def _merge_input_ids_with_image_features(\n \n         return final_embedding, final_attention_mask, position_ids, final_labels, final_input_ids\n \n-    def pack_image_features(self, image_features, image_sizes, image_newline=None):\n+    def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n \n@@ -654,6 +654,8 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None):\n                 List of image feature tensor, each contains all the visual feature of all patches.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n+            vision_feature_select_strategy (`str`)\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n             image_newline (`torch.Tensor` of shape `(embed_dim)`)\n                 New line embedding vector.\n         Returns:\n@@ -668,8 +670,14 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None):\n                 base_image_feature = image_feature[0]\n                 image_feature = image_feature[1:]\n                 height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n-                if height * width != base_image_feature.shape[0]:\n+\n+                if vision_feature_select_strategy == \"default\":\n+                    expected_num_patches = height * width\n+                elif vision_feature_select_strategy == \"full\":\n+                    expected_num_patches = height * width + 1\n+                if expected_num_patches != base_image_feature.shape[0]:\n                     raise ValueError(\"The number of patches is not consistent with the image size.\")\n+\n                 num_patch_height, num_patch_width = get_anyres_image_grid_shape(\n                     image_sizes[image_idx],\n                     self.config.image_grid_pinpoints,\n@@ -825,6 +833,7 @@ def forward(\n             image_features, feature_lens = self.pack_image_features(\n                 image_features,\n                 image_sizes,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n                 image_newline=self.image_newline,\n             )\n             if legacy_processing:"
        },
        {
            "sha": "bd0b5a19064650031000004d0566f6cf1e149a29",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcf8946f0acb578c534b1d33d534450d1fc88507/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcf8946f0acb578c534b1d33d534450d1fc88507/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=bcf8946f0acb578c534b1d33d534450d1fc88507",
            "patch": "@@ -620,3 +620,24 @@ def test_expansion_in_processing(self):\n \n         # check that both inputs are handled correctly and generate the same output\n         self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_full_vision_state_selection(self):\n+        model = LlavaNextForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-v1.6-mistral-7b-hf\",\n+            load_in_4bit=True,\n+        )\n+        # test that changing `strategy` won't error out\n+        model.vision_feature_select_strategy = \"full\"\n+\n+        inputs = self.processor(self.prompt, self.image, return_tensors=\"pt\")\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=30)\n+        EXPECTED_DECODED_TEXT = '[INST]  \\nWhat is shown in this image? [/INST] The image appears to be a radar chart, which is a type of multi-dimensional plot that displays values for multiple quantitative variables represented on axes'  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 32,
        "deletions": 2
    }
}