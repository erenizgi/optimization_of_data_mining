{
    "author": "techkang",
    "message": "Fix GA loss bugs and add unit test (#35121)\n\n* fix GA bugs and add unit test\n\n* narrow down model loss unit test diff gap\n\n* format code to make ruff happy\n\n* send num_items_in_batch argument to decoder\n\n* fix GA loss bug in BertLMHeadModel\n\n* use TinyStories-33M to narrow down diff gap\n\n* fotmat code\n\n* missing .config\n\n* avoid add extra args\n\n---------\n\nCo-authored-by: kangsheng <kangsheng@meituan.com>",
    "sha": "1ccca8f48c493a4804921da66f65023e3f9c8d9c",
    "files": [
        {
            "sha": "e311f93b6c81edc081cc367e5e516481ca770c97",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ccca8f48c493a4804921da66f65023e3f9c8d9c/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ccca8f48c493a4804921da66f65023e3f9c8d9c/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=1ccca8f48c493a4804921da66f65023e3f9c8d9c",
            "patch": "@@ -1325,6 +1325,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **loss_kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1375,11 +1376,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(prediction_scores, labels, self.config.vocab_size, **loss_kwargs)\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "3bff8f6acd290d5e9512fe2540907351bd089c9d",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ccca8f48c493a4804921da66f65023e3f9c8d9c/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ccca8f48c493a4804921da66f65023e3f9c8d9c/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=1ccca8f48c493a4804921da66f65023e3f9c8d9c",
            "patch": "@@ -491,6 +491,8 @@ def forward(\n         kwargs_decoder = {\n             argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n         }\n+        if \"num_items_in_batch\" in kwargs_encoder:\n+            kwargs_decoder[\"num_items_in_batch\"] = kwargs_encoder.pop(\"num_items_in_batch\", None)\n \n         if encoder_outputs is None:\n             if inputs is None:"
        },
        {
            "sha": "f7d79481809807f597f67dd911736d096689ebea",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ccca8f48c493a4804921da66f65023e3f9c8d9c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ccca8f48c493a4804921da66f65023e3f9c8d9c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1ccca8f48c493a4804921da66f65023e3f9c8d9c",
            "patch": "@@ -3649,10 +3649,7 @@ def training_step(\n             return loss_mb.reduce_mean().detach().to(self.args.device)\n \n         with self.compute_loss_context_manager():\n-            if self.model_accepts_loss_kwargs:\n-                loss = self.compute_loss(model, inputs)\n-            else:\n-                loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n+            loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n \n         del inputs\n         if (\n@@ -5132,10 +5129,6 @@ def get_batch_samples(self, epoch_iterator, num_batches):\n             except StopIteration:\n                 break\n \n-        # Keep default behavior the same\n-        if not self.model_accepts_loss_kwargs:\n-            return batch_samples, None\n-\n         if len(batch_samples) > 0 and \"labels\" in batch_samples[0]:\n             # For now we don't support object detection\n             try:"
        },
        {
            "sha": "d33be2789761dad4f7f96de14610110157de7c60",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 103,
            "deletions": 11,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ccca8f48c493a4804921da66f65023e3f9c8d9c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ccca8f48c493a4804921da66f65023e3f9c8d9c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=1ccca8f48c493a4804921da66f65023e3f9c8d9c",
            "patch": "@@ -750,11 +750,102 @@ def test_model_init(self):\n         self.check_trained_model(trainer.model, alternate_seed=True)\n \n     @slow\n-    def test_gradient_accumulation_loss_alignment(self):\n+    def test_gradient_accumulation_loss_alignment_with_model_loss(self):\n         set_seed(42)\n         import datasets\n \n-        model_name = \"distilgpt2\"\n+        model_name = \"nickypro/tinyllama-110M\"\n+        dataset_name = \"wikitext\"\n+        dataset_config = \"wikitext-2-raw-v1\"\n+        dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:500]\")\n+        dataset = dataset.train_test_split(test_size=0.2)\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+        tokenizer.pad_token = tokenizer.eos_token\n+\n+        def tokenize_function(examples):\n+            return tokenizer(examples[\"text\"], max_length=128, padding=\"max_length\", truncation=True)\n+\n+        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n+\n+        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n+\n+        model = AutoModelForCausalLM.from_pretrained(model_name)\n+\n+        base_loss_callback = StoreLossCallback()\n+\n+        args_kwargs = {\n+            \"report_to\": \"none\",\n+            \"logging_steps\": 1,\n+            \"max_steps\": 20,\n+            \"learning_rate\": 3e-4,\n+            \"disable_tqdm\": True,\n+        }\n+\n+        args = TrainingArguments(\n+            \"./generation\",\n+            **args_kwargs,\n+        )\n+        trainer = Trainer(\n+            model,\n+            args,\n+            train_dataset=tokenized_dataset[\"train\"],\n+            callbacks=[base_loss_callback],\n+            data_collator=data_collator,\n+        )\n+        assert trainer.model_accepts_loss_kwargs\n+        trainer.train()\n+\n+        grad_accum_loss_callback = StoreLossCallback()\n+        args = TrainingArguments(\n+            \"./generation\",\n+            **args_kwargs,\n+            gradient_accumulation_steps=2,\n+            per_device_train_batch_size=4,\n+        )\n+        set_seed(42)\n+        model = AutoModelForCausalLM.from_pretrained(model_name)\n+        trainer = Trainer(\n+            model,\n+            args,\n+            train_dataset=tokenized_dataset[\"train\"],\n+            callbacks=[grad_accum_loss_callback],\n+            data_collator=data_collator,\n+        )\n+        trainer.train()\n+\n+        set_seed(42)\n+        model = AutoModelForCausalLM.from_pretrained(model_name)\n+        broken_loss_callback = StoreLossCallback()\n+        trainer = Trainer(\n+            model,\n+            args,\n+            train_dataset=tokenized_dataset[\"train\"],\n+            callbacks=[broken_loss_callback],\n+            data_collator=data_collator,\n+        )\n+        # disable model_accepts_loss_kwargs\n+        trainer.model_accepts_loss_kwargs = False\n+        trainer.train()\n+\n+        # Calculate the difference between the base loss and the grad_accum loss\n+        diff_truth = [\n+            abs(base - grad) for base, grad in zip(base_loss_callback.losses, grad_accum_loss_callback.losses)\n+        ]\n+        diff_broken = [abs(base - grad) for base, grad in zip(base_loss_callback.losses, broken_loss_callback.losses)]\n+\n+        # all diff truth should be quite close\n+        self.assertLess(max(diff_truth), 0.01, f\"Difference {max(diff_truth)} is not within 0.01\")\n+\n+        # max diff broken should be very off\n+        self.assertGreater(max(diff_broken), 3, f\"Difference {max(diff_broken)} is not greater than 3\")\n+\n+    @slow\n+    def test_gradient_accumulation_loss_alignment_with_loss_func(self):\n+        set_seed(42)\n+        import datasets\n+\n+        model_name = \"roneneldan/TinyStories-33M\"\n         dataset_name = \"wikitext\"\n         dataset_config = \"wikitext-2-raw-v1\"\n         dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:500]\")\n@@ -836,15 +927,16 @@ def compute_loss(logits, labels, vocab_size, num_items_in_batch, disable_num_ite\n         trainer.train()\n \n         # Calculate the difference between the base loss and the grad_accum loss\n-        diff_truth = [base - grad for base, grad in zip(base_loss_callback.losses, grad_accum_loss_callback.losses)]\n-        diff_broken = [base - grad for base, grad in zip(base_loss_callback.losses, broken_loss_callback.losses)]\n-        # These should be quite close\n-        for diff in diff_truth:\n-            self.assertLess(abs(diff), 0.1, f\"Difference {diff} is not within 0.1\")\n-\n-        # These should be very off\n-        for diff in diff_broken:\n-            self.assertGreater(abs(diff), 0.1, f\"Difference {diff} is not greater than 0.1\")\n+        diff_truth = [\n+            abs(base - grad) for base, grad in zip(base_loss_callback.losses, grad_accum_loss_callback.losses)\n+        ]\n+        diff_broken = [abs(base - grad) for base, grad in zip(base_loss_callback.losses, broken_loss_callback.losses)]\n+\n+        # all diff truth should be quite close\n+        self.assertLess(max(diff_truth), 0.01, f\"Difference {max(diff_truth)} is not within 0.01\")\n+\n+        # max diff broken should be very off\n+        self.assertGreater(max(diff_broken), 3, f\"Difference {max(diff_broken)} is not greater than 3\")\n \n     def test_gradient_accumulation(self):\n         # Training with half the batch size but accumulation steps as 2 should give the same training losses."
        }
    ],
    "stats": {
        "total": 132,
        "additions": 108,
        "deletions": 24
    }
}