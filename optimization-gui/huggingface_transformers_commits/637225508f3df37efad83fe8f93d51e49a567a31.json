{
    "author": "faaany",
    "message": "[docs] add explanation to `release_memory()` (#34911)\n\n* explain release_memory\r\n\r\n* Update docs/source/en/llm_tutorial_optimization.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "637225508f3df37efad83fe8f93d51e49a567a31",
    "files": [
        {
            "sha": "3414725fc37087d20d8a759f9c82415dcf755ab4",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/637225508f3df37efad83fe8f93d51e49a567a31/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/637225508f3df37efad83fe8f93d51e49a567a31/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=637225508f3df37efad83fe8f93d51e49a567a31",
            "patch": "@@ -147,7 +147,7 @@ Let's call it now for the next experiment.\n ```python\n flush()\n ```\n-In the recent version of the accelerate library, you can also use a utility method called `release_memory()`\n+From the Accelerate library, you can also use a device-agnostic utility method called [release_memory](https://github.com/huggingface/accelerate/blob/29be4788629b772a3b722076e433b5b3b5c85da3/src/accelerate/utils/memory.py#L63), which takes various hardware backends like XPU, MLU, NPU, MPS, and more into account.\n \n ```python\n from accelerate.utils import release_memory"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}