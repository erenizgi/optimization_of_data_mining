{
    "author": "Rocketknight1",
    "message": "Fix donut backtracking (#37788)\n\n* Fix donut backtracking\n\n* make fixup\n\n* Trigger tests\n\n* Remove old line\n\n* Update code\n\n* Fix reversed slice",
    "sha": "ebbe9b12dd75b69f92100d684c47f923ee262a93",
    "files": [
        {
            "sha": "d6438df1be603eb6fb59f0d36168e462e31df53f",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebbe9b12dd75b69f92100d684c47f923ee262a93/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebbe9b12dd75b69f92100d684c47f923ee262a93/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=ebbe9b12dd75b69f92100d684c47f923ee262a93",
            "patch": "@@ -156,14 +156,18 @@ def token2json(self, tokens, is_inner_value=False, added_vocab=None):\n         output = {}\n \n         while tokens:\n-            start_token = re.search(r\"<s_(.*?)>\", tokens, re.IGNORECASE)\n-            if start_token is None:\n+            # We want r\"<s_(.*?)>\" but without ReDOS risk, so do it manually in two parts\n+            potential_start = re.search(r\"<s_\", tokens, re.IGNORECASE)\n+            if potential_start is None:\n                 break\n-            key = start_token.group(1)\n+            start_token = tokens[potential_start.start() :]\n+            if \">\" not in start_token:\n+                break\n+            start_token = start_token[: start_token.index(\">\") + 1]\n+            key = start_token[len(\"<s_\") : -len(\">\")]\n             key_escaped = re.escape(key)\n \n             end_token = re.search(rf\"</s_{key_escaped}>\", tokens, re.IGNORECASE)\n-            start_token = start_token.group()\n             if end_token is None:\n                 tokens = tokens.replace(start_token, \"\")\n             else:"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}