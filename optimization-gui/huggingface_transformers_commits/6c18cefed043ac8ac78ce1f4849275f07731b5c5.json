{
    "author": "yijun-lee",
    "message": "ğŸŒ [i18n-KO] Translated `gguf.md` to Korean (#33764)\n\n* docs: ko: gguf.md\r\n\r\n* feat nmt draft\r\n\r\n* fix: manual edits\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>\r\n\r\n---------\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>",
    "sha": "6c18cefed043ac8ac78ce1f4849275f07731b5c5",
    "files": [
        {
            "sha": "81e692f5fca95503f61fe8273bdd2b960e93a10d",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c18cefed043ac8ac78ce1f4849275f07731b5c5/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c18cefed043ac8ac78ce1f4849275f07731b5c5/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=6c18cefed043ac8ac78ce1f4849275f07731b5c5",
            "patch": "@@ -135,8 +135,8 @@\n     title: ì»¤ë®¤ë‹ˆí‹° ë¦¬ì†ŒìŠ¤\n   - local: troubleshooting\n     title: ë¬¸ì œ í•´ê²°\n-  - local: in_translation\n-    title: (ë²ˆì—­ì¤‘) Interoperability with GGUF files\n+  - local: gguf\n+    title: GGUF íŒŒì¼ë“¤ê³¼ì˜ ìƒí˜¸ ìš´ìš©ì„±\n   title: (ë²ˆì—­ì¤‘) ê°œë°œì ê°€ì´ë“œ\n - sections:\n   - local: in_translation"
        },
        {
            "sha": "03bd7c08bb2691c5214999cbe00d9e3cdacadfe2",
            "filename": "docs/source/ko/gguf.md",
            "status": "added",
            "additions": 100,
            "deletions": 0,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c18cefed043ac8ac78ce1f4849275f07731b5c5/docs%2Fsource%2Fko%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c18cefed043ac8ac78ce1f4849275f07731b5c5/docs%2Fsource%2Fko%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fgguf.md?ref=6c18cefed043ac8ac78ce1f4849275f07731b5c5",
            "patch": "@@ -0,0 +1,100 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# GGUFì™€ Transformersì˜ ìƒí˜¸ì‘ìš© [[gguf-and-interaction-with-transformers]]\n+\n+GGUF íŒŒì¼ í˜•ì‹ì€ [GGML](https://github.com/ggerganov/ggml)ê³¼ ê·¸ì— ì˜ì¡´í•˜ëŠ” ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬, ì˜ˆë¥¼ ë“¤ì–´ ë§¤ìš° ì¸ê¸° ìˆëŠ” [llama.cpp](https://github.com/ggerganov/llama.cpp)ì´ë‚˜ [whisper.cpp](https://github.com/ggerganov/whisper.cpp)ì—ì„œ ì¶”ë¡ ì„ ìœ„í•œ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n+\n+ì´ íŒŒì¼ í˜•ì‹ì€ [Hugging Face Hub](https://huggingface.co/docs/hub/en/gguf)ì—ì„œ ì§€ì›ë˜ë©°, íŒŒì¼ ë‚´ì˜ í…ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì‹ ì†í•˜ê²Œ ê²€ì‚¬í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n+\n+ì´ í˜•ì‹ì€ \"ë‹¨ì¼ íŒŒì¼ í˜•ì‹(single-file-format)\"ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, í•˜ë‚˜ì˜ íŒŒì¼ì— ì„¤ì • ì†ì„±, í† í¬ë‚˜ì´ì € ì–´íœ˜, ê¸°íƒ€ ì†ì„±ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë¸ì—ì„œ ë¡œë“œë˜ëŠ” ëª¨ë“  í…ì„œê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ íŒŒì¼ë“¤ì€ íŒŒì¼ì˜ ì–‘ìí™” ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì–‘ìí™” ìœ í˜•ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì€ [ì—¬ê¸°](https://huggingface.co/docs/hub/en/gguf#quantization-types)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## Transformers ë‚´ ì§€ì› [[support-within-transformers]]\n+\n+`transformers` ë‚´ì—ì„œ `gguf` íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ GGUF ëª¨ë¸ì˜ ì¶”ê°€ í•™ìŠµ/ë¯¸ì„¸ ì¡°ì •ì„ ì œê³µí•œ í›„ `ggml` ìƒíƒœê³„ì—ì„œ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ `gguf` íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ ë¨¼ì € FP32ë¡œ ì—­ì–‘ìí™”í•œ í›„, PyTorchì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n+\n+> [!NOTE]\n+> ì§€ì›ì€ ì•„ì§ ì´ˆê¸° ë‹¨ê³„ì— ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ì–‘ìí™” ìœ í˜•ê³¼ ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ì´ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•œ ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤.\n+\n+í˜„ì¬ ì§€ì›ë˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ì–‘ìí™” ìœ í˜•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+### ì§€ì›ë˜ëŠ” ì–‘ìí™” ìœ í˜• [[supported-quantization-types]]\n+\n+ì´ˆê¸°ì— ì§€ì›ë˜ëŠ” ì–‘ìí™” ìœ í˜•ì€ Hubì—ì„œ ê³µìœ ëœ ì¸ê¸° ìˆëŠ” ì–‘ìí™” íŒŒì¼ì— ë”°ë¼ ê²°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+- F32\n+- F16\n+- BF16\n+- Q4_0\n+- Q4_1\n+- Q5_0\n+- Q5_1\n+- Q8_0\n+- Q2_K\n+- Q3_K\n+- Q4_K\n+- Q5_K\n+- Q6_K\n+- IQ1_S\n+- IQ1_M\n+- IQ2_XXS\n+- IQ2_XS\n+- IQ2_S\n+- IQ3_XXS\n+- IQ3_S\n+- IQ4_XS\n+- IQ4_NL\n+\n+> [!NOTE]\n+> GGUF ì—­ì–‘ìí™”ë¥¼ ì§€ì›í•˜ë ¤ë©´ `gguf>=0.10.0` ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n+\n+### ì§€ì›ë˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ [[supported-model-architectures]]\n+\n+í˜„ì¬ ì§€ì›ë˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” Hubì—ì„œ ë§¤ìš° ì¸ê¸°ê°€ ë§ì€ ì•„í‚¤í…ì²˜ë“¤ë¡œ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n+\n+- LLaMa\n+- Mistral\n+- Qwen2\n+- Qwen2Moe\n+- Phi3\n+- Bloom\n+\n+## ì‚¬ìš© ì˜ˆì‹œ [[example-usage]]\n+\n+`transformers`ì—ì„œ `gguf` íŒŒì¼ì„ ë¡œë“œí•˜ë ¤ë©´ `from_pretrained` ë©”ì†Œë“œì— `gguf_file` ì¸ìˆ˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. ë™ì¼í•œ íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: \n+\n+```python\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n+filename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n+model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n+```\n+\n+ì´ì œ PyTorch ìƒíƒœê³„ì—ì„œ ëª¨ë¸ì˜ ì–‘ìí™”ë˜ì§€ ì•Šì€ ì „ì²´ ë²„ì „ì— ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ë¥¸ ì—¬ëŸ¬ ë„êµ¬ë“¤ê³¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+`gguf` íŒŒì¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ë ¤ë©´ llama.cppì˜ [`convert-hf-to-gguf.py`](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n+\n+ìœ„ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì™„ë£Œí•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‹¤ì‹œ `gguf`ë¡œ ë‚´ë³´ë‚´ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+```python\n+tokenizer.save_pretrained('directory')\n+model.save_pretrained('directory')\n+\n+!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}\n+```"
        }
    ],
    "stats": {
        "total": 104,
        "additions": 102,
        "deletions": 2
    }
}