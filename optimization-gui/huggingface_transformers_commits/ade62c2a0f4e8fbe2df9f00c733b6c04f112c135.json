{
    "author": "vasqu",
    "message": "[`Tokenizers`] Change treatment of special tokens (#42903)\n\n* fix\n\n* test",
    "sha": "ade62c2a0f4e8fbe2df9f00c733b6c04f112c135",
    "files": [
        {
            "sha": "540683aff3e5d896fb169ce35051cb6e7717b6aa",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ade62c2a0f4e8fbe2df9f00c733b6c04f112c135/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ade62c2a0f4e8fbe2df9f00c733b6c04f112c135/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=ade62c2a0f4e8fbe2df9f00c733b6c04f112c135",
            "patch": "@@ -2152,9 +2152,10 @@ def save_pretrained(\n         # Add tokenizer class to the tokenizer config to be able to reload it with from_pretrained\n         tokenizer_class = self.__class__.__name__\n \n-        # tokenizers backend don't need to save added_tokens_decoder\n+        # tokenizers backend don't need to save added_tokens_decoder and additional_special_tokens\n         if any(base.__name__ == \"TokenizersBackend\" for base in self.__class__.__mro__):\n             tokenizer_config.pop(\"added_tokens_decoder\", None)\n+            tokenizer_config.pop(\"additional_special_tokens\", None)\n \n         # Remove the Fast at the end if we can save the slow tokenizer\n         if tokenizer_class.endswith(\"Fast\") and getattr(self, \"can_save_slow_tokenizer\", False):"
        },
        {
            "sha": "ccc30f133ec3ee9904b3761022e1c634f05b7f0b",
            "filename": "src/transformers/tokenization_utils_tokenizers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ade62c2a0f4e8fbe2df9f00c733b6c04f112c135/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ade62c2a0f4e8fbe2df9f00c733b6c04f112c135/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_tokenizers.py?ref=ade62c2a0f4e8fbe2df9f00c733b6c04f112c135",
            "patch": "@@ -339,7 +339,7 @@ def __init__(self, *args, **kwargs):\n                 tokens.append(token)\n             if tokens:\n                 # These tokens are from the special tokens map\n-                self.add_tokens(tokens, special_tokens=True)\n+                self.add_tokens(tokens)\n \n         try:\n             vocab_size = self._tokenizer.get_vocab_size()"
        },
        {
            "sha": "e2195077aea51ac843fa2a58dce83f0c733e8b82",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ade62c2a0f4e8fbe2df9f00c733b6c04f112c135/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ade62c2a0f4e8fbe2df9f00c733b6c04f112c135/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=ade62c2a0f4e8fbe2df9f00c733b6c04f112c135",
            "patch": "@@ -44,7 +44,7 @@\n \n if is_tokenizers_available():\n     import tokenizers\n-    from tokenizers import Tokenizer\n+    from tokenizers import AddedToken, Tokenizer\n     from tokenizers.models import WordPiece\n \n \n@@ -330,3 +330,23 @@ def test_encode_message_raises_on_add_generation_prompt(self):\n         ]\n         with self.assertRaises(ValueError):\n             tokenizer.encode_message_with_chat_template(conversation[0], add_generation_prompt=True)\n+\n+    @require_tokenizers\n+    def test_special_tokens_overwrite(self):\n+        text_with_nonspecial_tokens = \"there are 2 cats\"  # '2' is originally special\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/Ernie4_5_Tokenizer\")\n+        # Overwrite special tokens 0-9 to non-special\n+        tokenizer.add_tokens([AddedToken(f\"{i}\", normalized=False, special=False) for i in range(10)])\n+        self.assertTrue(\n+            tokenizer.decode(tokenizer.encode(text_with_nonspecial_tokens), skip_special_tokens=True)\n+            == text_with_nonspecial_tokens\n+        )\n+\n+        # Checking if this carries over even after saving and relaoding\n+        tokenizer.save_pretrained(\"/tmp/ernie_tokenizer\")\n+        new_tokenizer = AutoTokenizer.from_pretrained(\"/tmp/ernie_tokenizer\")\n+        self.assertTrue(\n+            new_tokenizer.decode(new_tokenizer.encode(text_with_nonspecial_tokens), skip_special_tokens=True)\n+            == text_with_nonspecial_tokens\n+        )"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 24,
        "deletions": 3
    }
}