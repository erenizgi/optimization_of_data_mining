{
    "author": "ArthurZucker",
    "message": "Modular `transformers`: modularity and inheritance for new model additions (#33248)\n\n* update exampel\r\n\r\n* update\r\n\r\n* push the converted diff files for testing and ci\r\n\r\n* correct one example\r\n\r\n* fix class attributes and docstring\r\n\r\n* nits\r\n\r\n* oups\r\n\r\n* fixed config!\r\n\r\n* update\r\n\r\n* nitd\r\n\r\n* class attributes are not matched against the other, this is missing\r\n\r\n* fixed overwriting self.xxx now onto the attributes I think\r\n\r\n* partial fix, now order with docstring\r\n\r\n* fix docstring order?\r\n\r\n* more fixes\r\n\r\n* update\r\n\r\n* fix missing docstrings!\r\n\r\n* examples don't all work yet\r\n\r\n* fixup\r\n\r\n* nit\r\n\r\n* updated\r\n\r\n* hick\r\n\r\n* update\r\n\r\n* delete\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* fix\r\n\r\n* all default\r\n\r\n* no local import\r\n\r\n* fix more diff\r\n\r\n* some fix related to \"safe imports\"\r\n\r\n* push fixed\r\n\r\n* add helper!\r\n\r\n* style\r\n\r\n* add a check\r\n\r\n* all by default\r\n\r\n* add the\r\n\r\n* update\r\n\r\n* FINALLY!\r\n\r\n* nit\r\n\r\n* fix config dependencies\r\n\r\n* man that is it\r\n\r\n* fix fix\r\n\r\n* update diffs\r\n\r\n* fix the last issue\r\n\r\n* re-default to all\r\n\r\n* alll the fixes\r\n\r\n* nice\r\n\r\n* fix properties vs setter\r\n\r\n* fixup\r\n\r\n* updates\r\n\r\n* update dependencies\r\n\r\n* make sure to install what needs to be installed\r\n\r\n* fixup\r\n\r\n* quick fix for now\r\n\r\n* fix!\r\n\r\n* fixup\r\n\r\n* update\r\n\r\n* update\r\n\r\n* updates\r\n\r\n* whitespaces\r\n\r\n* nit\r\n\r\n* fix\r\n\r\n* simplify everything, and make it file agnostic (should work for image processors)\r\n\r\n* style\r\n\r\n* finish fixing all import issues\r\n\r\n* fixup\r\n\r\n* empty modeling should not be written!\r\n\r\n* Add logic to find who depends on what\r\n\r\n* update\r\n\r\n* cleanup\r\n\r\n* update\r\n\r\n* update gemma to support positions\r\n\r\n* some small nits\r\n\r\n* this is the correct docstring for gemma2\r\n\r\n* fix merging of docstrings\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* update\r\n\r\n* take doc into account\r\n\r\n* styling\r\n\r\n* update\r\n\r\n* fix hidden activation\r\n\r\n* more fixes\r\n\r\n* final fixes!\r\n\r\n* fixup\r\n\r\n* fixup instruct  blip video\r\n\r\n* update\r\n\r\n* fix bugs\r\n\r\n* align gemma2 with the rest as well\r\n\r\n* updats\r\n\r\n* revert\r\n\r\n* update\r\n\r\n* more reversiom\r\n\r\n* grind\r\n\r\n* more\r\n\r\n* arf\r\n\r\n* update\r\n\r\n* order will matter\r\n\r\n* finish del stuff\r\n\r\n* update\r\n\r\n* rename to modular\r\n\r\n* fixup\r\n\r\n* nits\r\n\r\n* update makefile\r\n\r\n* fixup\r\n\r\n* update order of the checks!\r\n\r\n* fix\r\n\r\n* fix docstring that has a call inside\r\n\r\n* fiix conversion check\r\n\r\n* style\r\n\r\n* add some initial documentation\r\n\r\n* update\r\n\r\n* update doc\r\n\r\n* some fixup\r\n\r\n* updates\r\n\r\n* yups\r\n\r\n* Mostly todo gimme a minut\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* revert some stuff\r\n\r\n* Review docs for the modular transformers (#33472)\r\n\r\nDocs\r\n\r\n* good update\r\n\r\n* fixup\r\n\r\n* mmm current updates lead to this code\r\n\r\n* okay, this fixes it\r\n\r\n* cool\r\n\r\n* fixes\r\n\r\n* update\r\n\r\n* nit\r\n\r\n* updates\r\n\r\n* nits\r\n\r\n* fix doc\r\n\r\n* update\r\n\r\n* revert bad changes\r\n\r\n* update\r\n\r\n* updates\r\n\r\n* proper update\r\n\r\n* update\r\n\r\n* update?\r\n\r\n* up\r\n\r\n* update\r\n\r\n* cool\r\n\r\n* nits\r\n\r\n* nits\r\n\r\n* bon bon\r\n\r\n* fix\r\n\r\n* ?\r\n\r\n* minimise changes\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* updates?\r\n\r\n* fixed gemma2\r\n\r\n* kind of a hack\r\n\r\n* nits\r\n\r\n* update\r\n\r\n* remove `diffs` in favor of `modular`\r\n\r\n* fix make fix copies\r\n\r\n---------\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>",
    "sha": "317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
    "files": [
        {
            "sha": "ca2afc67c10e3e34e52921bf2cb4d6f950126cf6",
            "filename": ".circleci/config.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/.circleci%2Fconfig.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/.circleci%2Fconfig.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fconfig.yml?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -137,7 +137,7 @@ jobs:\n         parallelism: 1\n         steps:\n             - checkout\n-            - run: uv pip install -e .\n+            - run: uv pip install -e \".[quality]\"\n             - run:\n                 name: Show installed libraries and their versions\n                 command: pip freeze | tee installed.txt\n@@ -162,13 +162,14 @@ jobs:\n         parallelism: 1\n         steps:\n             - checkout\n-            - run: uv pip install -e .\n+            - run: uv pip install -e \".[quality]\"\n             - run:\n                 name: Show installed libraries and their versions\n                 command: pip freeze | tee installed.txt\n             - store_artifacts:\n                   path: ~/transformers/installed.txt\n             - run: python utils/check_copies.py\n+            - run: python utils/check_modular_conversion.py\n             - run: python utils/check_table.py\n             - run: python utils/check_dummies.py\n             - run: python utils/check_repo.py"
        },
        {
            "sha": "710c555b74f6df662a21823351d51c732bda8d2c",
            "filename": "Makefile",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -36,6 +36,7 @@ autogenerate_code: deps_table_update\n \n repo-consistency:\n \tpython utils/check_copies.py\n+\tpython utils/check_modular_conversion.py\n \tpython utils/check_table.py\n \tpython utils/check_dummies.py\n \tpython utils/check_repo.py\n@@ -80,6 +81,7 @@ fixup: modified_only_fixup extra_style_checks autogenerate_code repo-consistency\n \n fix-copies:\n \tpython utils/check_copies.py --fix_and_overwrite\n+\tpython utils/check_modular_conversion.py  --fix_and_overwrite\n \tpython utils/check_table.py --fix_and_overwrite\n \tpython utils/check_dummies.py --fix_and_overwrite\n \tpython utils/check_doctest_list.py --fix_and_overwrite"
        },
        {
            "sha": "482974a837de2e135c0c391c28c30aeb8aba86da",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -5,6 +5,8 @@\n     title: Quick tour\n   - local: installation\n     title: Installation\n+  - local: add_new_model\n+    title: Adding a new model to `transformers`\n   title: Get started\n - sections:\n   - local: pipeline_tutorial\n@@ -149,6 +151,8 @@\n     title: Interoperability with GGUF files\n   - local: tiktoken\n     title: Interoperability with TikToken files\n+  - local: modular_transformers\n+    title: Modularity in `transformers`\n   title: Developer guides\n - sections:\n   - local: quantization/overview"
        },
        {
            "sha": "33d2bb9483482a8c81ddbb3006c8929af969d8e3",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "added",
            "additions": 121,
            "deletions": 0,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,121 @@\n+# Modular transformers\n+\n+`transformers` is an opinionated framework; our philosophy is defined in the following [conceptual guide](./philosophy).\n+\n+The core of that philosophy is exemplified by the [single model, single file](https://huggingface.co/blog/transformers-design-philosophy)\n+aspect of the library. This component's downside is that it limits the inheritance and importability of components from\n+files to others in the toolkit.\n+\n+As a result, model components tend to be repeated across many files. There are as many attention layers defined\n+in `transformers` as there are models, and a significant number of those are identical to each other. \n+The unfortunate consequence is that independent implementations tend to diverge as fixes and changes get applied\n+to specific parts of the code.\n+\n+In order to balance this issue, we introduced the concept of \"copies\" across the library. By adding a comment indicating\n+that code is a copy of another, we can enforce through CI and local commands that copies do not diverge. However,\n+while the complexity is low, this is often quite tedious to do.\n+\n+And, finally, this contributes to adding a significant overhead to contributing models which we would like to remove.\n+This approach often requires model contributions to add modeling code (~1k lines), processor (~500 lines), tests, docs,\n+etc. Model contribution PRs rarely add less than 3-5k lines of code, with much of this code being boilerplate.\n+\n+This raises the bar for contributions, and with Modular Transformers, we're aiming to lower the bar to a much more\n+acceptable point.\n+\n+## What is it?\n+\n+Modular Transformers introduces the concept of a \"modular\" file to a model folder. This modular file accepts code\n+that isn't typically accepted in modeling/processing files, as it allows importing from neighbouring models as well\n+as inheritance from classes to others.\n+\n+This modular file defines models, processors, and the configuration class that would otherwise be defined in their\n+respective modules.\n+\n+Finally, this feature introduces a new `linter` which will \"unravel\" the modular file into the \"single model, single \n+file\" directory structure. These files will get auto-generated every time the script is run; reducing the required\n+contributions to the modular file, and therefore only to the changes between the contributed model and others.\n+\n+Model users will end up importing and using the single-file interface, so no change is expected here. Doing this, we\n+hope to combine the best of both worlds: enabling simple contributions while sticking to our philosophy.\n+\n+This is therefore a replacement for the `# Copied from` markers, and previously contributed models can be expected to\n+be moved to the new Modular Transformers format in the coming months.\n+\n+### Details \n+\n+The \"linter\", which unravels the inheritance and creates all single-files from the modular file, will flatten the \n+inheritance while trying to be invisible to Python users. At this time, the linter flattens a **single** level of\n+inheritance.\n+\n+For example:\n+- If a configuration class inherits from another and adds/deletes an argument, the generated file will either directly \n+  reference it (in case of addition) or completely remove it (in case of deletion).\n+- If a class inherits from another, for example: class GemmaModel(LlamaModel):, dependencies are automatically \n+  inferred. All submodules will be automatically inferred from the superclass.\n+\n+You should be able to write everything (the tokenizer, the image processor, the model, the config) in this `modular` \n+file, and the corresponding files will be created for you. \n+\n+### Enforcement\n+\n+[TODO] We are introducing a new test, that makes sure the generated content matches what is present in the `modular_xxxx.py`\n+\n+### Examples\n+\n+Here is a quick example with BERT and RoBERTa. The two models are intimately related: their modeling implementation \n+differs solely by a change in the embedding layer.\n+\n+Instead of redefining the model entirely, here is what the `modular_roberta.py` file looks like for the modeling &\n+configuration classes (for the sake of the example, the tokenizer is ignored at this time as very different).\n+\n+```python\n+from torch import nn\n+from ..bert.configuration_bert import BertConfig\n+from ..bert.modeling_bert import (\n+    BertModel,\n+    BertEmbeddings,\n+    BertForMaskedLM\n+)\n+\n+# The RoBERTa config is identical to BERT's config\n+class RobertaConfig(BertConfig):\n+  model_type = 'roberta'\n+\n+# We redefine the embeddings here to highlight the padding ID difference, and we redefine the position embeddings\n+class RobertaEmbeddings(BertEmbeddings):\n+    def __init__(self, config):\n+        super().__init__(config())\n+\n+        self.padding_idx = config.pad_token_id\n+        self.position_embeddings = nn.Embedding(\n+            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n+        )\n+\n+# The RoBERTa model is identical to the BERT model, except for the embedding layer. \n+# We redefine the embeddings above, so here there is no need to do additional work\n+class RobertaModel(BertModel):\n+  def __init__(self, config):\n+    super().__init__(config)\n+    self.embeddings = RobertaEmbeddings(config)\n+\n+      \n+# The heads now only need to redefine the model inside to the correct `RobertaModel`\n+class RobertaForMaskedLM(BertForMaskedLM):\n+  def __init__(self, config):\n+    super().__init__(config)\n+    self.model = RobertaModel(config)\n+```\n+\n+Note that if you do not use the dependency that you defined, you will have the following error:\n+\n+```bash\n+ValueError: You defined `RobertaEmbeddings` in the modular_roberta.py, it should be used\n+                                    when you define `BertModel`, as it is one of it's direct dependencies. Make sure\n+                                    you use it in the `__init__` function.\n+```\n+\n+Additionally, you may find a list of examples here:\n+\n+## What it is not\n+\n+It is not a replacement for the modeling code (yet?), and if your model is not based on anything else that ever existed, then you can add a `modeling` file as usual.\n\\ No newline at end of file"
        },
        {
            "sha": "a575a83b015c636d59d1990fa2f7a21e3ce74f43",
            "filename": "examples/diff-conversion/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/75b7485cc72bf7122094b943af9f7d26d69ff827/examples%2Fdiff-conversion%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/75b7485cc72bf7122094b943af9f7d26d69ff827/examples%2Fdiff-conversion%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fdiff-conversion%2FREADME.md?ref=75b7485cc72bf7122094b943af9f7d26d69ff827",
            "patch": "@@ -1,20 +0,0 @@\n-# Using the `diff_converter` linter\n-\n-`pip install libcst` is a must!\n-\n-# `sh examples/diff-conversion/convert_examples.sh` to get the converted outputs\n-\n-The diff converter is a new `linter` specific to `transformers`. It allows us to unpack inheritance in python to convert a modular `diff` file like `diff_gemma.py` into a `single model single file`. \n-\n-Examples of possible usage are available in the `examples/diff-conversion`, or `diff_gemma` for a full model usage.\n-\n-`python utils/diff_model_converter.py --files_to_parse \"/Users/arthurzucker/Work/transformers/examples/diff-conversion/diff_my_new_model2.py\"`\n-\n-## How it works\n-We use the `libcst` parser to produce an AST representation of the `diff_xxx.py` file. For any imports that are made from `transformers.models.modeling_xxxx` we parse the source code of that module, and build a class dependency mapping, which allows us to unpack the difference dependencies.\n-\n-The code from the `diff` file and the class dependency mapping are \"merged\" to produce the single model single file. \n-We use ruff to automatically remove the potential duplicate imports.\n-\n-## Why we use libcst instead of the native AST?\n-AST is super powerful, but it does not keep the `docstring`, `comment` or code formatting. Thus we decided to go with `libcst`\n\\ No newline at end of file"
        },
        {
            "sha": "4eba1d03aebc8b40a015d04c56e67b0c138f6084",
            "filename": "examples/modular-transformers/README.md",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2FREADME.md?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,20 @@\n+# Using the `modular_converter` linter\n+\n+`pip install libcst` is a must!\n+\n+# `sh examples/modular-transformers/convert_examples.sh` to get the converted outputs\n+\n+The modular converter is a new `linter` specific to `transformers`. It allows us to unpack inheritance in python to convert a modular file like `modular_gemma.py` into a `single model single file`. \n+\n+Examples of possible usage are available in the `examples/modular-transformers`, or `modular_gemma` for a full model usage.\n+\n+`python utils/modular_model_converter.py --files_to_parse \"/Users/arthurzucker/Work/transformers/examples/modular-transformers/modular_my_new_model2.py\"`\n+\n+## How it works\n+We use the `libcst` parser to produce an AST representation of the `modular_xxx.py` file. For any imports that are made from `transformers.models.modeling_xxxx` we parse the source code of that module, and build a class dependency mapping, which allows us to unpack the modularerence dependencies.\n+\n+The code from the `modular` file and the class dependency mapping are \"merged\" to produce the single model single file. \n+We use ruff to automatically remove the potential duplicate imports.\n+\n+## Why we use libcst instead of the native AST?\n+AST is super powerful, but it does not keep the `docstring`, `comment` or code formatting. Thus we decided to go with `libcst`\n\\ No newline at end of file"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/modular-transformers/configuration_dummy.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_dummy.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3"
        },
        {
            "sha": "d7c946dbe318574843d8c347787e01a1723ca78c",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "added",
            "additions": 196,
            "deletions": 0,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,196 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class MyNewModelConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MyNewModelModel`]. It is used to instantiate an MyNewModel\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the MyNewModel-7B.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the MyNewModel model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MyNewModelModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. MyNewModel 1 supports up to 2048 tokens,\n+            MyNewModel 2 up to 4096, CodeMyNewModel up to 16384.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n+            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n+            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'my_new_model3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'my_new_model3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'my_new_model3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'my_new_model3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_heads\n+        new_param (`int`, *optional*, defaults to `False`):\n+            A fun new parameter\n+\n+    ```python\n+    >>> from transformers import MyNewModelModel, MyNewModelConfig\n+\n+    >>> # Initializing a MyNewModel my_new_model-7b style configuration\n+    >>> configuration = MyNewModelConfig()\n+\n+    >>> # Initializing a model from the my_new_model-7b style configuration\n+    >>> model = MyNewModelModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"my_new_model\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=32000,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        pretraining_tp=1,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=True,\n+        head_dim=None,\n+        new_param=0,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.pretraining_tp = pretraining_tp\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.mlp_bias = mlp_bias\n+        self.new_param = new_param"
        },
        {
            "sha": "b940d8d93b303386c5dd444a351e38fb3eadc359",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,97 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class MyNewModel2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GemmaModel`]. It is used to instantiate an Gemma\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Gemma-7B.\n+    e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`GemmaModel`]\n+    ```python\n+    >>> from transformers import GemmaModel, GemmaConfig\n+    >>> # Initializing a Gemma gemma-7b style configuration\n+    >>> configuration = GemmaConfig()\n+    >>> # Initializing a model from the gemma-7b style configuration\n+    >>> model = GemmaModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"my_new_model2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=32000,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        pretraining_tp=1,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        head_dim=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.pretraining_tp = pretraining_tp\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )"
        },
        {
            "sha": "7d57f9fe25b0414c25fb19be2f3161fea01f5e50",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,134 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Example where we only want to overwrite the defaults of an init\n+\n+from transformers import PretrainedConfig\n+\n+\n+class NewModelConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`NewModelModel`]. It is used to instantiate an NewModel\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the NewModel-7B.\n+    e.g. [google/new_model-7b](https://huggingface.co/google/new_model-7b)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the NewModel model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`NewModelModel`]\n+        hidden_size (`int`, *optional*, defaults to 3072):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 24576):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 28):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 16):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The legacy activation function. It is overwritten by the `hidden_activation`.\n+        hidden_activation (`str` or `function`, *optional*):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+    ```python\n+    >>> from transformers import NewModelModel, NewModelConfig\n+    >>> # Initializing a NewModel new_model-7b style configuration\n+    >>> configuration = NewModelConfig()\n+    >>> # Initializing a model from the new_model-7b style configuration\n+    >>> model = NewModelModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"new_model\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=256030,\n+        hidden_size=64,\n+        intermediate_size=90,\n+        num_hidden_layers=28,\n+        num_attention_heads=16,\n+        num_key_value_heads=16,\n+        head_dim=256,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        hidden_activation=None,\n+        max_position_embeddings=1500,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=0,\n+        eos_token_id=1,\n+        bos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.hidden_activation = hidden_activation\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    @property\n+    def num_heads(self):\n+        return self.num_attention_heads"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/modular-transformers/configuration_super.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconfiguration_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_super.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3"
        },
        {
            "sha": "4af31f1b4268de07d3fc0fc8b1a18b2ac0d1b0e7",
            "filename": "examples/modular-transformers/convert_examples.sh",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconvert_examples.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fconvert_examples.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconvert_examples.sh?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,7 +1,7 @@\n #!/bin/bash\n \n # Iterate over each file in the current directory\n-for file in examples/diff-conversion/diff_*; do\n+for file in examples/modular-transformers/modular_*; do\n     # Check if it's a regular file\n     if [ -f \"$file\" ]; then\n         # Call the Python script with the file name as an argument",
            "previous_filename": "examples/diff-conversion/convert_examples.sh"
        },
        {
            "sha": "5dd76c603035a4d717f1b9bb6b52f88e402c02b9",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "added",
            "additions": 1053,
            "deletions": 0,
            "changes": 1053,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,1053 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+from math import log\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+)\n+from .configuration_dummy import DummyConfig\n+\n+\n+def _pre_process_input(input_ids):\n+    print(log(input_ids))\n+    return input_ids\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+\n+    return causal_mask\n+\n+\n+class DummyRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        DummyRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class DummyRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[DummyConfig] = None,\n+    ):\n+        super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`DummyRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.45\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+class DummyMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        if self.config.pretraining_tp > 1:\n+            slice = self.intermediate_size // self.config.pretraining_tp\n+            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n+            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n+            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n+\n+            gate_proj = torch.cat(\n+                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n+            )\n+            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n+\n+            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n+            down_proj = [\n+                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n+            ]\n+            down_proj = sum(down_proj)\n+        else:\n+            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+\n+        return down_proj\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+class DummyAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: DummyConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+\n+        # TODO (joao): remove in v4.45 (RoPE is computed in the model, not in the decoder layers)\n+        self.rotary_emb = DummyRotaryEmbedding(config=self.config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        if self.config.pretraining_tp > 1:\n+            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n+            query_slices = self.q_proj.weight.split(\n+                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n+            )\n+            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n+            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n+\n+            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n+            query_states = torch.cat(query_states, dim=-1)\n+\n+            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n+            key_states = torch.cat(key_states, dim=-1)\n+\n+            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n+            value_states = torch.cat(value_states, dim=-1)\n+\n+        else:\n+            query_states = self.q_proj(hidden_states)\n+            key_states = self.k_proj(hidden_states)\n+            value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n+\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n+\n+        if self.config.pretraining_tp > 1:\n+            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n+            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n+            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n+        else:\n+            attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class DummyFlashAttention2(DummyAttention):\n+    \"\"\"\n+    Dummy flash attention module. This module inherits from `DummyAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if isinstance(past_key_value, StaticCache):\n+            raise ValueError(\n+                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n+                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n+            )\n+\n+        output_attentions = False\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        dropout_rate = self.attention_dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (DummyRMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self, \"sliding_window\", None),\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class DummySdpaAttention(DummyAttention):\n+    \"\"\"\n+    Dummy attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `DummyAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from DummyAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"DummyModel is using DummySdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+DUMMY_ATTENTION_CLASSES = {\n+    \"eager\": DummyAttention,\n+    \"flash_attention_2\": DummyFlashAttention2,\n+    \"sdpa\": DummySdpaAttention,\n+}\n+\n+\n+class DummyDecoderLayer(nn.Module):\n+    def __init__(self, config: DummyConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = DUMMY_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+\n+        self.mlp = DummyMLP(config)\n+        self.input_layernorm = DummyRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = DummyRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+DUMMY_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`DummyConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Dummy Model outputting raw hidden-states without any specific head on top.\",\n+    DUMMY_START_DOCSTRING,\n+)\n+class DummyPreTrainedModel(PreTrainedModel):\n+    config_class = DummyConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"DummyDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+DUMMY_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance;\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Dummy Model outputting raw hidden-states without any specific head on top.\",\n+    DUMMY_START_DOCSTRING,\n+)\n+class DummyModel(DummyPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DummyDecoderLayer`]\n+\n+    Args:\n+        config: DummyConfig\n+    \"\"\"\n+\n+    def __init__(self, config: DummyConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [DummyDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DummyRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = DummyRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(DUMMY_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        input_ids = _pre_process_input(input_ids)\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        return_legacy_cache = False\n+        if (\n+            use_cache and not isinstance(past_key_values, Cache) and not self.training\n+        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            logger.warning_once(\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_length()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            min_dtype=min_dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask"
        },
        {
            "sha": "bdedd1f5f5a273742f4c63b0c8e3cf7aa1609ba9",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "added",
            "additions": 1038,
            "deletions": 0,
            "changes": 1038,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,1038 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+import os\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from packaging import version\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    get_torch_version,\n+    logging,\n+)\n+from .configuration_dummy_bert import DummyBertConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"google-dummy_bert/dummy_bert-base-uncased\"\n+_CONFIG_FOR_DOC = \"DummyBertConfig\"\n+\n+\n+def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n+    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n+    try:\n+        import re\n+\n+        import numpy as np\n+        import tensorflow as tf\n+    except ImportError:\n+        logger.error(\n+            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n+            \"https://www.tensorflow.org/install/ for installation instructions.\"\n+        )\n+        raise\n+    tf_path = os.path.abspath(tf_checkpoint_path)\n+    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n+    # Load weights from TF model\n+    init_vars = tf.train.list_variables(tf_path)\n+    names = []\n+    arrays = []\n+    for name, shape in init_vars:\n+        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n+        array = tf.train.load_variable(tf_path, name)\n+        names.append(name)\n+        arrays.append(array)\n+\n+    for name, array in zip(names, arrays):\n+        name = name.split(\"/\")\n+        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n+        # which are not required for using pretrained model\n+        if any(\n+            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n+            for n in name\n+        ):\n+            logger.info(f\"Skipping {'/'.join(name)}\")\n+            continue\n+        pointer = model\n+        for m_name in name:\n+            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n+                scope_names = re.split(r\"_(\\d+)\", m_name)\n+            else:\n+                scope_names = [m_name]\n+            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n+                pointer = getattr(pointer, \"weight\")\n+            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n+                pointer = getattr(pointer, \"bias\")\n+            elif scope_names[0] == \"output_weights\":\n+                pointer = getattr(pointer, \"weight\")\n+            elif scope_names[0] == \"squad\":\n+                pointer = getattr(pointer, \"classifier\")\n+            else:\n+                try:\n+                    pointer = getattr(pointer, scope_names[0])\n+                except AttributeError:\n+                    logger.info(f\"Skipping {'/'.join(name)}\")\n+                    continue\n+            if len(scope_names) >= 2:\n+                num = int(scope_names[1])\n+                pointer = pointer[num]\n+        if m_name[-11:] == \"_embeddings\":\n+            pointer = getattr(pointer, \"weight\")\n+        elif m_name == \"kernel\":\n+            array = np.transpose(array)\n+        try:\n+            if pointer.shape != array.shape:\n+                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n+        except ValueError as e:\n+            e.args += (pointer.shape, array.shape)\n+            raise\n+        logger.info(f\"Initialize PyTorch weight {name}\")\n+        pointer.data = torch.from_numpy(array)\n+    return model\n+\n+\n+class DummyBertEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n+        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n+\n+        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n+        # any TensorFlow checkpoint file\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+        self.register_buffer(\n+            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n+        if input_ids is not None:\n+            input_shape = input_ids.size()\n+        else:\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        seq_length = input_shape[1]\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n+\n+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n+        # issue #5664\n+        if token_type_ids is None:\n+            if hasattr(self, \"token_type_ids\"):\n+                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n+                token_type_ids = buffered_token_type_ids_expanded\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+\n+        embeddings = inputs_embeds + token_type_embeddings\n+        if self.position_embedding_type == \"absolute\":\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings += position_embeddings\n+        embeddings = self.LayerNorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+\n+class DummyBertSelfAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n+\n+        self.is_decoder = config.is_decoder\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        # If this is instantiated as a cross-attention module, the keys\n+        # and values come from an encoder; the attention mask needs to be\n+        # such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        if is_cross_attention and past_key_value is not None:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value[0]\n+            value_layer = past_key_value[1]\n+            attention_mask = encoder_attention_mask\n+        elif is_cross_attention:\n+            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n+            attention_mask = encoder_attention_mask\n+        elif past_key_value is not None:\n+            key_layer = self.transpose_for_scores(self.key(hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(hidden_states))\n+            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(hidden_states))\n+\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        use_cache = past_key_value is not None\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n+            if use_cache:\n+                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n+                    -1, 1\n+                )\n+            else:\n+                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n+            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n+            distance = position_ids_l - position_ids_r\n+\n+            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n+            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n+\n+            if self.position_embedding_type == \"relative_key\":\n+                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores\n+            elif self.position_embedding_type == \"relative_key_query\":\n+                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+        if attention_mask is not None:\n+            # Apply the attention mask is (precomputed for all layers in DummyBertModel forward() function)\n+            attention_scores = attention_scores + attention_mask\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n+class DummyBertSdpaSelfAttention(DummyBertSelfAttention):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n+\n+    # Adapted from DummyBertSelfAttention\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n+            logger.warning_once(\n+                \"DummyBertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n+        # mask needs to be such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n+        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+            key_layer, value_layer = past_key_value\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(current_states))\n+            value_layer = self.transpose_for_scores(self.value(current_states))\n+            if past_key_value is not None and not is_cross_attention:\n+                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n+        # a causal mask in case tgt_len == 1.\n+        is_causal = (\n+            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n+        )\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n+\n+        outputs = (attn_output,)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n+class DummyBertSelfOutput(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n+        return hidden_states\n+\n+\n+DUMMY_BERT_SELF_ATTENTION_CLASSES = {\n+    \"eager\": DummyBertSelfAttention,\n+    \"sdpa\": DummyBertSdpaSelfAttention,\n+}\n+\n+\n+class DummyBertAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__()\n+        self.self = DUMMY_BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n+            config, position_embedding_type=position_embedding_type\n+        )\n+        self.output = DummyBertSelfOutput(config)\n+        self.pruned_heads = set()\n+\n+    def prune_heads(self, heads):\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.self.query = prune_linear_layer(self.self.query, index)\n+        self.self.key = prune_linear_layer(self.self.key, index)\n+        self.self.value = prune_linear_layer(self.self.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n+        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        self_outputs = self.self(\n+            hidden_states,\n+            attention_mask,\n+            head_mask,\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            past_key_value,\n+            output_attentions,\n+        )\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+class DummyBertIntermediate(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n+        if isinstance(config.hidden_act, str):\n+            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n+        else:\n+            self.intermediate_act_fn = config.hidden_act\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.intermediate_act_fn(hidden_states)\n+        return hidden_states\n+\n+\n+class DummyBertOutput(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n+        return hidden_states\n+\n+\n+class DummyBertLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n+        self.seq_len_dim = 1\n+        self.attention = DummyBertAttention(config)\n+        self.is_decoder = config.is_decoder\n+        self.add_cross_attention = config.add_cross_attention\n+        if self.add_cross_attention:\n+            if not self.is_decoder:\n+                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n+            self.crossattention = DummyBertAttention(config, position_embedding_type=\"absolute\")\n+        self.intermediate = DummyBertIntermediate(config)\n+        self.output = DummyBertOutput(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n+        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attention_outputs = self.attention(\n+            hidden_states,\n+            attention_mask,\n+            head_mask,\n+            output_attentions=output_attentions,\n+            past_key_value=self_attn_past_key_value,\n+        )\n+        attention_output = self_attention_outputs[0]\n+\n+        # if decoder, the last output is tuple of self-attn cache\n+        if self.is_decoder:\n+            outputs = self_attention_outputs[1:-1]\n+            present_key_value = self_attention_outputs[-1]\n+        else:\n+            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        cross_attn_present_key_value = None\n+        if self.is_decoder and encoder_hidden_states is not None:\n+            if not hasattr(self, \"crossattention\"):\n+                raise ValueError(\n+                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n+                    \" by setting `config.add_cross_attention=True`\"\n+                )\n+\n+            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n+            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+            cross_attention_outputs = self.crossattention(\n+                attention_output,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                cross_attn_past_key_value,\n+                output_attentions,\n+            )\n+            attention_output = cross_attention_outputs[0]\n+            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n+\n+            # add cross-attn cache to positions 3,4 of present_key_value tuple\n+            cross_attn_present_key_value = cross_attention_outputs[-1]\n+            present_key_value = present_key_value + cross_attn_present_key_value\n+\n+        layer_output = apply_chunking_to_forward(\n+            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n+        )\n+        outputs = (layer_output,) + outputs\n+\n+        # if decoder, return the attn key/values as the last output\n+        if self.is_decoder:\n+            outputs = outputs + (present_key_value,)\n+\n+        return outputs\n+\n+    def feed_forward_chunk(self, attention_output):\n+        intermediate_output = self.intermediate(attention_output)\n+        layer_output = self.output(intermediate_output, attention_output)\n+        return layer_output\n+\n+\n+class DummyBertEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([DummyBertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        return_dict: Optional[bool] = True,\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        next_decoder_cache = () if use_cache else None\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+            past_key_value = past_key_values[i] if past_key_values is not None else None\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    past_key_value,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = layer_module(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    past_key_value,\n+                    output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+            if use_cache:\n+                next_decoder_cache += (layer_outputs[-1],)\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+                if self.config.add_cross_attention:\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    next_decoder_cache,\n+                    all_hidden_states,\n+                    all_self_attentions,\n+                    all_cross_attentions,\n+                ]\n+                if v is not None\n+            )\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_decoder_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+class DummyBertPooler(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.activation = nn.Tanh()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        # We \"pool\" the model by simply taking the hidden state corresponding\n+        # to the first token.\n+        first_token_tensor = hidden_states[:, 0]\n+        pooled_output = self.dense(first_token_tensor)\n+        pooled_output = self.activation(pooled_output)\n+        return pooled_output\n+\n+\n+class DummyBertPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = DummyBertConfig\n+    load_tf_weights = load_tf_weights_in_dummy_bert\n+    base_model_prefix = \"dummy_bert\"\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+DUMMY_BERT_START_DOCSTRING = r\"\"\"\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`DummyBertConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+DUMMY_BERT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `({0})`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare DummyBert Model transformer outputting raw hidden-states without any specific head on top.\",\n+    DUMMY_BERT_START_DOCSTRING,\n+)\n+class DummyBertModel(DummyBertPreTrainedModel):\n+    \"\"\"\n+\n+    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n+    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n+    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n+    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n+\n+    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n+    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n+    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n+    \"\"\"\n+\n+    _no_split_modules = [\"DummyBertEmbeddings\", \"DummyBertLayer\"]\n+\n+    def __init__(self, config, add_pooling_layer=True):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = DummyBertEmbeddings(config)\n+        self.encoder = DummyBertEncoder(config)\n+\n+        self.pooler = DummyBertPooler(config) if add_pooling_layer else None\n+\n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.word_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.word_embeddings = value\n+\n+    def _prune_heads(self, heads_to_prune):\n+        \"\"\"\n+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n+        class PreTrainedModel\n+        \"\"\"\n+        for layer, heads in heads_to_prune.items():\n+            self.encoder.layer[layer].attention.prune_heads(heads)\n+\n+    @add_start_docstrings_to_model_forward(DUMMY_BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+        r\"\"\"\n+        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n+            the model is configured as a decoder.\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n+            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n+            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        \"\"\"\n+        r\"\"\"\n+        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n+            the model is configured as a decoder.\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n+            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n+            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if self.config.is_decoder:\n+            use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        else:\n+            use_cache = False\n+\n+        if input_ids is not None and inputs_embeds is not None:\n+            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n+        elif input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+            input_shape = input_ids.size()\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+        else:\n+            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n+\n+        batch_size, seq_length = input_shape\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n+        # past_key_values_length\n+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+\n+        if token_type_ids is None:\n+            if hasattr(self.embeddings, \"token_type_ids\"):\n+                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n+                token_type_ids = buffered_token_type_ids_expanded\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n+\n+        use_sdpa_attention_masks = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        # Expand the attention mask\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n+            # Expand the attention mask for SDPA.\n+            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+            if self.config.is_decoder:\n+                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                    attention_mask,\n+                    input_shape,\n+                    embedding_output,\n+                    past_key_values_length,\n+                )\n+            else:\n+                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        # If a 2D or 3D attention mask is provided for the cross-attention\n+        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+        if self.config.is_decoder and encoder_hidden_states is not None:\n+            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n+            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n+            if encoder_attention_mask is None:\n+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n+\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n+                # Expand the attention mask for SDPA.\n+                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+            else:\n+                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        else:\n+            encoder_extended_attention_mask = None\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            attention_mask=extended_attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_extended_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n+\n+        if not return_dict:\n+            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPoolingAndCrossAttentions(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            past_key_values=encoder_outputs.past_key_values,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+            cross_attentions=encoder_outputs.cross_attentions,\n+        )"
        },
        {
            "sha": "fea7994a53eefd9ac0198a25edbbd4a1ed262e2b",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "added",
            "additions": 1059,
            "deletions": 0,
            "changes": 1059,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,1059 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    SequenceClassifierOutputWithPast,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+)\n+from .configuration_my_new_model2 import MyNewModel2Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+\n+    return causal_mask\n+\n+\n+class MyNewModel2RMSNorm(nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.zeros(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float())\n+        # Llama does x.to(float16) * w whilst MyNewModel2 is (x * w).to(float16)\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        output = output * (1.0 + self.weight.float())\n+        return output.type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n+class MyNewModel2RotaryEmbedding(nn.Module):\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+        super().__init__()\n+\n+        self.dim = dim\n+        self.max_position_embeddings = max_position_embeddings\n+        self.base = base\n+        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n+        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids, seq_len=None):\n+        # x: [bs, num_attention_heads, seq_len, head_size]\n+        self.inv_freq.to(x.device)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 since bfloat16 loses precision on long contexts\n+        # See https://github.com/huggingface/transformers/pull/29285\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class MyNewModel2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        if config.hidden_activation is None:\n+            logger.warning_once(\n+                \"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\\n\"\n+                \"MyNewModel2's activation function will be set to `gelu_pytorch_tanh`. Please, use\\n\"\n+                \"`config.hidden_activation` if you want to override this behaviour.\\n\"\n+                \"See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n+            )\n+            config.hidden_activation = \"gelu_pytorch_tanh\"\n+        hidden_activation = config.hidden_activation\n+        self.act_fn = ACT2FN[hidden_activation]\n+\n+    def forward(self, x):\n+        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+class MyNewModel2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: MyNewModel2Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.head_dim\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+        self.scaling = 1 / math.sqrt(config.head_dim)\n+\n+        if self.hidden_size % self.num_heads != 0:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n+            )\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+        self.rotary_emb = MyNewModel2RotaryEmbedding(\n+            self.head_dim,\n+            max_position_embeddings=self.max_position_embeddings,\n+            base=self.rope_theta,\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n+\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class MyNewModel2FlashAttention2(MyNewModel2Attention):\n+    \"\"\"\n+    MyNewModel2 flash attention module. This module inherits from `MyNewModel2Attention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if isinstance(past_key_value, StaticCache):\n+            raise ValueError(\n+                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n+                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n+            )\n+\n+        output_attentions = False\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        dropout_rate = self.attention_dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (MyNewModel2RMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self, \"sliding_window\", None),\n+            is_causal=self.is_causal,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class MyNewModel2SdpaAttention(MyNewModel2Attention):\n+    \"\"\"\n+    MyNewModel2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `MyNewModel2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from MyNewModel2Attention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"MyNewModel2Model is using MyNewModel2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+MY_NEW_MODEL2_ATTENTION_CLASSES = {\n+    \"eager\": MyNewModel2Attention,\n+    \"flash_attention_2\": MyNewModel2FlashAttention2,\n+    \"sdpa\": MyNewModel2SdpaAttention,\n+}\n+\n+\n+class MyNewModel2DecoderLayer(nn.Module):\n+    def __init__(self, config: MyNewModel2Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MY_NEW_MODEL2_ATTENTION_CLASSES[config._attn_implementation](\n+            config=config, layer_idx=layer_idx\n+        )\n+\n+        self.mlp = MyNewModel2MLP(config)\n+        self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+MY_NEW_MODEL2_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`MyNewModel2Config`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare MyNewModel2 Model outputting raw hidden-states without any specific head on top.\",\n+    MY_NEW_MODEL2_START_DOCSTRING,\n+)\n+class MyNewModel2PreTrainedModel(PreTrainedModel):\n+    config_class = MyNewModel2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"MyNewModel2DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+MY_NEW_MODEL2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance;\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare MyNewModel2 Model outputting raw hidden-states without any specific head on top.\",\n+    MY_NEW_MODEL2_START_DOCSTRING,\n+)\n+class MyNewModel2Model(MyNewModel2PreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MyNewModel2DecoderLayer`]\n+\n+    Args:\n+        config: MyNewModel2Config\n+    \"\"\"\n+\n+    def __init__(self, config: MyNewModel2Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [MyNewModel2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        return_legacy_cache = False  # noqa: F841\n+        if (\n+            use_cache and not isinstance(past_key_values, Cache) and not self.training\n+        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+            return_legacy_cache = True  # noqa: F841\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        # embed positions\n+        hidden_states = inputs_embeds\n+\n+        # normalized\n+        # MyNewModel2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n+        hidden_states = hidden_states * normalizer\n+        if (\n+            use_cache and not isinstance(past_key_values, Cache) and not self.training\n+        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            logger.warning_once(\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+            )\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_length()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            min_dtype=min_dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The MyNewModel2 Model transformer with a sequence classification head on top (linear layer).\n+\n+    [`MyNewModel2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n+    (e.g. GPT-2) do.\n+\n+    Since it does classification on the last token, it requires to know the position of the last token. If a\n+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n+    each row of the batch).\n+    \"\"\",\n+    MY_NEW_MODEL2_START_DOCSTRING,\n+)\n+class MyNewModel2ForSequenceClassification(MyNewModel2PreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.model = MyNewModel2Model(config)\n+        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        transformer_outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        hidden_states = transformer_outputs[0]\n+        logits = self.score(hidden_states)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            sequence_lengths = -1\n+        else:\n+            if input_ids is not None:\n+                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n+                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n+                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n+                sequence_lengths = sequence_lengths.to(logits.device)\n+            else:\n+                sequence_lengths = -1\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+\n+        loss = None\n+        if labels is not None:\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(pooled_logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(pooled_logits, labels)\n+        if not return_dict:\n+            output = (pooled_logits,) + transformer_outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return SequenceClassifierOutputWithPast(\n+            loss=loss,\n+            logits=pooled_logits,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n+        )"
        },
        {
            "sha": "d91bdb1820c2a31a5af3ea4e75c9adaf51ae0e7b",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "added",
            "additions": 953,
            "deletions": 0,
            "changes": 953,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,953 @@\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#               This file was automatically generated from <path_to_diff_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the diff. If any change should be done, please apply the change to the\n+#                           diff.py file directly. One of our CI enforces this\n+#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, StaticCache\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+)\n+from .configuration_super import SuperConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+\n+    return causal_mask\n+\n+\n+class SuperRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        SuperRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class SuperRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[SuperConfig] = None,\n+    ):\n+        super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`SuperRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.45\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+class SuperMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        if self.config.pretraining_tp > 1:\n+            slice = self.intermediate_size // self.config.pretraining_tp\n+            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n+            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n+            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n+\n+            gate_proj = torch.cat(\n+                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n+            )\n+            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n+\n+            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n+            down_proj = [\n+                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n+            ]\n+            down_proj = sum(down_proj)\n+        else:\n+            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+\n+        return down_proj\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+class SuperAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: SuperConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+\n+        # TODO (joao): remove in v4.45 (RoPE is computed in the model, not in the decoder layers)\n+        self.rotary_emb = SuperRotaryEmbedding(config=self.config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        if self.config.pretraining_tp > 1:\n+            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n+            query_slices = self.q_proj.weight.split(\n+                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n+            )\n+            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n+            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n+\n+            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n+            query_states = torch.cat(query_states, dim=-1)\n+\n+            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n+            key_states = torch.cat(key_states, dim=-1)\n+\n+            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n+            value_states = torch.cat(value_states, dim=-1)\n+\n+        else:\n+            query_states = self.q_proj(hidden_states)\n+            key_states = self.k_proj(hidden_states)\n+            value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n+\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n+\n+        if self.config.pretraining_tp > 1:\n+            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n+            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n+            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n+        else:\n+            attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class SuperFlashAttention2(SuperAttention):\n+    \"\"\"\n+    Super flash attention module. This module inherits from `SuperAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if isinstance(past_key_value, StaticCache):\n+            raise ValueError(\n+                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n+                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n+            )\n+\n+        output_attentions = False\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        dropout_rate = self.attention_dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (SuperRMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self, \"sliding_window\", None),\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class SuperSdpaAttention(SuperAttention):\n+    \"\"\"\n+    Super attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `SuperAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from SuperAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"SuperModel is using SuperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+SUPER_ATTENTION_CLASSES = {\n+    \"eager\": SuperAttention,\n+    \"flash_attention_2\": SuperFlashAttention2,\n+    \"sdpa\": SuperSdpaAttention,\n+}\n+\n+\n+class SuperDecoderLayer(nn.Module):\n+    def __init__(self, config: SuperConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = SUPER_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+\n+        self.mlp = SuperMLP(config)\n+        self.input_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+SUPER_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`SuperConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Super Model outputting raw hidden-states without any specific head on top.\",\n+    SUPER_START_DOCSTRING,\n+)\n+class SuperPreTrainedModel(PreTrainedModel):\n+    config_class = SuperConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"SuperDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+SUPER_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance;\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Super Model outputting raw hidden-states without any specific head on top.\",\n+    SUPER_START_DOCSTRING,\n+)\n+class SuperModel(SuperPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`SuperDecoderLayer`]\n+\n+    Args:\n+        config: SuperConfig\n+    \"\"\"\n+\n+    def __init__(self, config: SuperConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [SuperDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = SuperRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(SUPER_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        out = super().forward(\n+            input_ids,\n+            attention_mask,\n+            position_ids,\n+            past_key_values,\n+            inputs_embeds,\n+            use_cache,\n+            output_attentions,\n+            output_hidden_states,\n+            return_dict,\n+            cache_position,\n+        )\n+        out.logits *= 2**4\n+        return out\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_length()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            min_dtype=min_dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask"
        },
        {
            "sha": "33dc38d0b44745afe33f837deea007bb3283104b",
            "filename": "examples/modular-transformers/modular_dummy.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -3,10 +3,11 @@\n \n import torch\n \n-from transformers import Cache\n from transformers.modeling_outputs import CausalLMOutputWithPast\n from transformers.models.llama.modeling_llama import LlamaModel\n \n+from ...cache_utils import Cache\n+\n \n def _pre_process_input(input_ids):\n     print(log(input_ids))",
            "previous_filename": "examples/diff-conversion/diff_dummy.py"
        },
        {
            "sha": "7a83a2e0ed2fc238d564749fe91ba30dd8a19c2d",
            "filename": "examples/modular-transformers/modular_dummy_bert.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,27 @@\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+\n+from transformers.models.bert.modeling_bert import BertModel\n+\n+from ...modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n+\n+\n+class DummyBertModel(BertModel):\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+        return super().forward(input_ids)"
        },
        {
            "sha": "c1ea8b0a72490b19412ed909c045660ddc5dc015",
            "filename": "examples/modular-transformers/modular_my_new_model.py",
            "status": "renamed",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_my_new_model.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -5,10 +5,11 @@\n # here there is no `ARG` so we are gonna take parent doc\n class MyNewModelConfig(LlamaConfig):\n     r\"\"\"\n-    mlp_bias (`bool`, *optional*, defaults to `False`)\n+    new_param (`int`, *optional*, defaults to `False`):\n+        A fun new parameter\n     \"\"\"\n \n     def __init__(self, mlp_bias=True, new_param=0, **super_kwargs):\n+        super().__init__(self, **super_kwargs)\n         self.mlp_bias = mlp_bias\n         self.new_param = new_param\n-        super().__init__(self, **super_kwargs)",
            "previous_filename": "examples/diff-conversion/diff_my_new_model.py"
        },
        {
            "sha": "2e449e06b16225a9dd45ab7c1ce81cb918404780",
            "filename": "examples/modular-transformers/modular_my_new_model2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_my_new_model2.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "previous_filename": "examples/diff-conversion/diff_my_new_model2.py"
        },
        {
            "sha": "166c7955c1b5a91839a5b72ecdfb5c4b48660813",
            "filename": "examples/modular-transformers/modular_new_model.py",
            "status": "renamed",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_model.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -26,5 +26,10 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n+        **kwargs,\n     ):\n-        super().__init__(self)\n+        super().__init__(self, **kwargs)\n+\n+    @property\n+    def num_heads(self):\n+        return self.num_attention_heads",
            "previous_filename": "examples/diff-conversion/diff_new_model.py"
        },
        {
            "sha": "a3e0218f9320a0c81d5b0f281729ac8d8a91133f",
            "filename": "examples/modular-transformers/modular_roberta.py",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_roberta.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,20 @@\n+import torch.nn as nn\n+\n+from transformers.models.bert.modeling_bert import BertEmbeddings, BertModel\n+\n+\n+class RobertaEmbeddings(BertEmbeddings):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.pad_token_id = config.pad_token_id\n+        self.position_embeddings = nn.Embedding(\n+            config.max_position_embeddings, config.hidden_size, config.pad_token_id\n+        )\n+\n+\n+class RobertaModel(BertModel):\n+    def __init__(self, config):\n+        super().__init__(self, config)\n+        # Error out here. Why? Because `RobertaEmbeddings` is defined but not used.\n+        # no, because it's defined, and RobertaModel should use RobertaEmbedding\n+        # here if initialized that way it won't use the new embedding."
        },
        {
            "sha": "59909a41e4dcc6452a060c8b2414f2faccae07b3",
            "filename": "examples/modular-transformers/modular_super.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/examples%2Fmodular-transformers%2Fmodular_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_super.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -2,10 +2,11 @@\n \n import torch\n \n-from transformers import Cache\n from transformers.modeling_outputs import CausalLMOutputWithPast\n from transformers.models.llama.modeling_llama import LlamaModel\n \n+from ...cache_utils import Cache\n+\n \n # example where we need some deps and some functions\n class SuperModel(LlamaModel):",
            "previous_filename": "examples/diff-conversion/diff_super.py"
        },
        {
            "sha": "6ea9b192618e571fefa0f8d81346405c69e109b1",
            "filename": "setup.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -192,6 +192,8 @@\n     \"urllib3<2.0.0\",\n     \"uvicorn\",\n     \"pytest-rich\",\n+    \"libcst\",\n+    \"rich\",\n ]\n \n \n@@ -345,7 +347,7 @@ def run(self):\n \n extras[\"deepspeed-testing\"] = extras[\"deepspeed\"] + extras[\"testing\"] + extras[\"optuna\"] + extras[\"sentencepiece\"]\n extras[\"ruff\"] = deps_list(\"ruff\")\n-extras[\"quality\"] = deps_list(\"datasets\", \"isort\", \"ruff\", \"GitPython\", \"urllib3\")\n+extras[\"quality\"] = deps_list(\"datasets\", \"isort\", \"ruff\", \"GitPython\", \"urllib3\", \"libcst\", \"rich\")\n \n extras[\"all\"] = (\n     extras[\"tf\"]"
        },
        {
            "sha": "2634a7b6b3f2f7562e1b428aec613ca342545004",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -97,4 +97,6 @@\n     \"urllib3\": \"urllib3<2.0.0\",\n     \"uvicorn\": \"uvicorn\",\n     \"pytest-rich\": \"pytest-rich\",\n+    \"libcst\": \"libcst\",\n+    \"rich\": \"rich\",\n }"
        },
        {
            "sha": "3ab61c522eff411945a3265ab341ff83dc02619d",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n@@ -21,7 +21,7 @@\n # limitations under the License.\n \n \n-from transformers import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig\n \n \n class GemmaConfig(PretrainedConfig):"
        },
        {
            "sha": "948dd8287b617d0947c4a0c62f70682f474a4a64",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 167,
            "deletions": 173,
            "changes": 340,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n@@ -39,7 +39,6 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -51,63 +50,6 @@\n from .configuration_gemma import GemmaConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-    return causal_mask\n-\n-\n class GemmaRMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -128,7 +70,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(GemmaRMSNorm)\n+logger = logging.get_logger(__name__)\n \n \n class GemmaRotaryEmbedding(nn.Module):\n@@ -159,30 +101,6 @@ def forward(self, x, position_ids, seq_len=None):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-class GemmaMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        if config.hidden_activation is None:\n-            logger.warning_once(\n-                \"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\\n\"\n-                \"Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\\n\"\n-                \"`config.hidden_activation` if you want to override this behaviour.\\n\"\n-                \"See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n-            )\n-            config.hidden_activation = \"gelu_pytorch_tanh\"\n-        hidden_activation = config.hidden_activation\n-        self.act_fn = ACT2FN[hidden_activation]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n class GemmaLinearScalingRotaryEmbedding(GemmaRotaryEmbedding):\n     \"\"\"GemmaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n@@ -212,6 +130,30 @@ def forward(self, x, position_ids):\n         return cos, sin\n \n \n+class GemmaMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        if config.hidden_activation is None:\n+            logger.warning_once(\n+                \"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\\n\"\n+                \"Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\\n\"\n+                \"`config.hidden_activation` if you want to override this behaviour.\\n\"\n+                \"See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n+            )\n+            config.hidden_activation = \"gelu_pytorch_tanh\"\n+        hidden_activation = config.hidden_activation\n+        self.act_fn = ACT2FN[hidden_activation]\n+\n+    def forward(self, x):\n+        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -358,6 +300,94 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n+class GemmaSdpaAttention(GemmaAttention):\n+    \"\"\"\n+    Gemma attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `GemmaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from GemmaAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n class GemmaFlashAttention2(GemmaAttention):\n     \"\"\"\n     Gemma flash attention module. This module inherits from `GemmaAttention` as the weights of the module stays\n@@ -458,7 +488,6 @@ def forward(\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )\n-\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n@@ -468,92 +497,57 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class GemmaSdpaAttention(GemmaAttention):\n-    \"\"\"\n-    Gemma attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GemmaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n     \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n-    # Adapted from GemmaAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n         causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n         if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n \n-        return attn_output, None, past_key_value\n+    return causal_mask\n \n \n GEMMA_ATTENTION_CLASSES = {\n@@ -567,9 +561,7 @@ class GemmaDecoderLayer(nn.Module):\n     def __init__(self, config: GemmaConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n         self.self_attn = GEMMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n-\n         self.mlp = GemmaMLP(config)\n         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -830,9 +822,9 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n+        return_legacy_cache = False  # noqa: F841\n         if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+            return_legacy_cache = True  # noqa: F841\n             if past_key_values is None:\n                 past_key_values = DynamicCache()\n             else:\n@@ -975,6 +967,7 @@ def _update_causal_mask(\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n+\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n@@ -1149,6 +1142,7 @@ def prepare_inputs_for_generation(\n             position_ids.masked_fill_(attention_mask == 0, 1)\n             if past_key_values:\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n                 # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n                 position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n \n@@ -1230,7 +1224,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "ca89b6cf2a6da822f6d5a52e9948b60ffeafc159",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "renamed",
            "additions": 298,
            "deletions": 40,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -21,8 +21,15 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n-from transformers import PretrainedConfig\n-from transformers.models.llama.modeling_llama import (\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n+from ...utils import is_torchdynamo_compiling, logging\n+from ..llama.modeling_llama import (\n+    LlamaDecoderLayer,\n     LlamaFlashAttention2,\n     LlamaForCausalLM,\n     LlamaForSequenceClassification,\n@@ -32,14 +39,6 @@\n     repeat_kv,\n )\n \n-from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n-from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...modeling_outputs import CausalLMOutputWithPast\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n-from ...utils import logging\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -216,6 +215,35 @@ def forward(self, x, position_ids, seq_len=None):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n+class GemmaLinearScalingRotaryEmbedding(GemmaRotaryEmbedding):\n+    \"\"\"GemmaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n+\n+    def forward(self, x, position_ids):\n+        # difference to the original RoPE: a scaling factor is aplied to the position ids\n+        position_ids = position_ids.float() / self.scaling_factor\n+        cos, sin = super().forward(x, position_ids)\n+        return cos, sin\n+\n+\n+class GemmaDynamicNTKScalingRotaryEmbedding(GemmaRotaryEmbedding):\n+    \"\"\"GemmaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n+\n+    def forward(self, x, position_ids):\n+        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_position_embeddings:\n+            base = self.base * (\n+                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n+            ) ** (self.dim / (self.dim - 2))\n+            inv_freq = 1.0 / (\n+                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n+\n+        cos, sin = super().forward(x, position_ids)\n+        return cos, sin\n+\n+\n class GemmaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -340,8 +368,95 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# TODO felix: does this inheritance really work out in the end to GemmaFlashAttention2 inheriting form GemmaAttention?\n-class GemmaFlashAttention2(LlamaFlashAttention2):\n+class GemmaSdpaAttention(GemmaAttention):\n+    \"\"\"\n+    Gemma attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `GemmaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from GemmaAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+class GemmaFlashAttention2(LlamaFlashAttention2, GemmaAttention):\n     \"\"\"\n     Gemma flash attention module. This module inherits from `GemmaAttention` as the weights of the module stays\n     untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n@@ -427,12 +542,12 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n+            position_ids=position_ids,\n             dropout=dropout_rate,\n             sliding_window=getattr(self, \"sliding_window\", None),\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )\n-\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n@@ -442,7 +557,95 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n+GEMMA_ATTENTION_CLASSES = {\n+    \"eager\": GemmaAttention,\n+    \"flash_attention_2\": GemmaFlashAttention2,\n+    \"sdpa\": GemmaSdpaAttention,\n+}\n+\n+\n+class GemmaDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: GemmaConfig, layer_idx: int):\n+        super().__init__(config)\n+        self.self_attn = GEMMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.mlp = GemmaMLP(config)\n+        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n class GemmaModel(LlamaModel):\n+    def __init__(self, config: GemmaConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        del self.rotary_emb  # Gemma does not implement rotary emb at the modeling level yet!\n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -455,7 +658,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -513,22 +716,72 @@ def forward(\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n \n-        return super().forward(\n-            causal_mask,\n-            position_ids,\n-            past_key_values,\n-            use_cache,\n-            output_attentions,\n-            output_hidden_states,\n-            return_dict,\n-            cache_position,\n-            input_ids=None,\n-            inputs_embeds=hidden_states,\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n         )\n \n \n # Example where we ony modify the docstring and call super\n-class GemmaForCausalLM(LlamaForCausalLM, GenerationMixin):\n+class GemmaForCausalLM(LlamaForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = GemmaModel(config)\n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -542,18 +795,9 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n-\n-        Example:\n-\n         ```python\n         >>> from transformers import AutoTokenizer, GemmaForCausalLM\n \n@@ -589,10 +833,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+\n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -618,8 +870,14 @@ def forward(\n \n \n class GemmaForSequenceClassification(LlamaForSequenceClassification):\n-    pass\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = GemmaModel(config)\n+        self.post_init()\n \n \n class GemmaForTokenClassification(LlamaForTokenClassification):\n-    pass\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = GemmaModel(config)\n+        self.post_init()",
            "previous_filename": "src/transformers/models/gemma/diff_gemma.py"
        },
        {
            "sha": "6f4b2eaf2a45d7c2707ea44daae4c770e1755e46",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 23,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n@@ -19,7 +19,9 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from transformers import PretrainedConfig\n+\n+\n+from ...configuration_utils import PretrainedConfig\n \n \n class Gemma2Config(PretrainedConfig):\n@@ -53,7 +55,8 @@ class Gemma2Config(PretrainedConfig):\n         head_dim (`int`, *optional*, defaults to 256):\n             The attention head dimension.\n         hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n-            The non-linear activation function (function or string) in the decoder.\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n         max_position_embeddings (`int`, *optional*, defaults to 8192):\n             The maximum sequence length that this model might ever be used with.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -77,23 +80,25 @@ class Gemma2Config(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n-        attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n         query_pre_attn_scalar (`float`, *optional*, defaults to 224): scaling factor used on the attention scores\n         sliding_window (`int`, *optional*, defaults to 4096): in Gemma2, every other layer uses sliding window attention. This is the\n             size of the sliding window.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n+\n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n-    >>> # Initializing a Gemma2 gemma2-9b style configuration\n+    >>> # Initializing a Gemma2 gemma2-7b style configuration\n     >>> configuration = Gemma2Config()\n-    >>> # Initializing a model from the gemma2-9b style configuration\n+    >>> # Initializing a model from the gemma2-7b style configuration\n     >>> model = Gemma2Model(configuration)\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\"\"\"\n \n     model_type = \"gemma2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    cache_implementation = \"hybrid\"\n \n     def __init__(\n         self,\n@@ -116,12 +121,19 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n-        final_logit_softcapping=30.0,\n-        attn_logit_softcapping=50.0,\n         query_pre_attn_scalar=224,\n         sliding_window=4096,\n+        final_logit_softcapping=30.0,\n+        attn_logit_softcapping=50.0,\n         **kwargs,\n     ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -130,23 +142,14 @@ def __init__(\n         self.num_attention_heads = num_attention_heads\n         self.head_dim = head_dim\n         self.num_key_value_heads = num_key_value_heads\n-        self.hidden_activation = hidden_activation\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        self.attn_logit_softcapping = attn_logit_softcapping\n-\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n-        self.final_logit_softcapping = final_logit_softcapping\n+        self.hidden_activation = hidden_activation\n         self.query_pre_attn_scalar = query_pre_attn_scalar\n         self.sliding_window = sliding_window\n-        self.cache_implementation = \"hybrid\"\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping"
        },
        {
            "sha": "22438ccc80a6d5d610d06487a6f46742784314d3",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 118,
            "deletions": 94,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n@@ -22,13 +22,14 @@\n from typing import List, Optional, Tuple, Union\n \n import torch\n+import torch.nn as nn\n import torch.utils.checkpoint\n-from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -39,7 +40,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n     is_torchdynamo_compiling,\n@@ -49,66 +49,6 @@\n from .configuration_gemma2 import Gemma2Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-    return causal_mask\n-\n-\n class Gemma2RMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -129,6 +69,24 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+class Gemma2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_activation]\n+\n+    def forward(self, x):\n+        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n class Gemma2RotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -191,21 +149,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class Gemma2MLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_activation]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -253,12 +196,12 @@ def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n         self.rotary_emb = Gemma2RotaryEmbedding(\n             self.head_dim,\n             max_position_embeddings=self.max_position_embeddings,\n             base=self.rope_theta,\n         )\n-        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n \n     def forward(\n         self,\n@@ -502,9 +445,11 @@ def forward(\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n         causal_mask = attention_mask\n         if attention_mask is not None:\n             causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n         # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n         # Reference: https://github.com/pytorch/pytorch/issues/112577.\n         if query_states.device.type == \"cuda\" and causal_mask is not None:\n@@ -515,6 +460,7 @@ def forward(\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         is_causal = True if causal_mask is None and q_len > 1 else False\n+\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n@@ -533,6 +479,59 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+\n+    return causal_mask\n+\n+\n GEMMA2_ATTENTION_CLASSES = {\n     \"eager\": Gemma2Attention,\n     \"flash_attention_2\": Gemma2FlashAttention2,\n@@ -543,19 +542,16 @@ def forward(\n class Gemma2DecoderLayer(nn.Module):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n-        self.config = config\n         self.hidden_size = config.hidden_size\n-\n         self.self_attn = GEMMA2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n-\n         self.mlp = Gemma2MLP(config)\n         self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.config = config\n         self.is_sliding = not bool(layer_idx % 2)\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window\n+        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -567,6 +563,25 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n             # Flash-attn is a 2D tensor\n             if self.config._attn_implementation == \"flash_attention_2\":\n@@ -580,6 +595,7 @@ def forward(\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 if attention_mask.shape[-1] <= 1:  # when decoding\n                     attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -711,13 +727,20 @@ def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`HybridCache`, *optional*):\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Gemma 2 uses a unique cache class, [`HybridCache`], and does not guarantee full compatibility with other\n-            cache classes.\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -812,8 +835,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # Instantiate an empty cache if needed.\n-        if use_cache and past_key_values is None:\n+        if use_cache and past_key_values is None and not self.training:\n             batch_size, seq_len, _ = inputs_embeds.shape\n             past_key_values = HybridCache(\n                 self.config,\n@@ -828,6 +850,7 @@ def forward(\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n@@ -844,6 +867,7 @@ def forward(\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n \n+        # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n \n@@ -880,7 +904,6 @@ def forward(\n \n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n@@ -1009,6 +1032,7 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n+\n         if self.training and self.config._attn_implementation != \"eager\":\n             logger.warning_once(\n                 \"It is strongly recommended to train Gemma2 models with the `eager` attention implementation \"\n@@ -1187,10 +1211,10 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "7aca6650961e6833fe12e06b0ebf84ca9b9117c8",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "renamed",
            "additions": 424,
            "deletions": 66,
            "changes": 490,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -13,30 +13,41 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Tuple, Union\n \n import torch\n+import torch.nn as nn\n import torch.utils.checkpoint\n from torch.nn import CrossEntropyLoss\n \n-from transformers.models.gemma.configuration_gemma import GemmaConfig\n-from transformers.models.gemma.modeling_gemma import (\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, HybridCache\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n+)\n+from ...utils import (\n+    is_flash_attn_2_available,\n+    is_flash_attn_greater_or_equal,\n+    is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n+from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaDecoderLayer,\n     GemmaForCausalLM,\n     GemmaForSequenceClassification,\n     GemmaForTokenClassification,\n     GemmaModel,\n+    GemmaPreTrainedModel,\n     GemmaRMSNorm,\n+    _prepare_4d_causal_attention_mask_with_cache_position,\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n \n-from ...cache_utils import Cache\n-from ...generation import GenerationMixin\n-from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n-\n \n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n@@ -45,33 +56,230 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma2Config(GemmaConfig):\n-    cache_implementation = \"hybrid\"  # TODO this is not properly ported, but cls attr is better\n+class Gemma2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Gemma2Model`]. It is used to instantiate an Gemma2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Gemma2-7B.\n+    e.g. [google/gemma2-7b](https://huggingface.co/google/gemma2-7b)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the Gemma2 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Gemma2Model`]\n+        hidden_size (`int`, *optional*, defaults to 3072):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 24576):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 28):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 16):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 224): scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096): in Gemma2, every other layer uses sliding window attention. This is the\n+            size of the sliding window.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n+\n+    ```python\n+    >>> from transformers import Gemma2Model, Gemma2Config\n+    >>> # Initializing a Gemma2 gemma2-7b style configuration\n+    >>> configuration = Gemma2Config()\n+    >>> # Initializing a model from the gemma2-7b style configuration\n+    >>> model = Gemma2Model(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"gemma2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    cache_implementation = \"hybrid\"\n \n     def __init__(\n         self,\n+        vocab_size=256000,\n+        hidden_size=3072,\n+        intermediate_size=24576,\n+        num_hidden_layers=28,\n+        num_attention_heads=16,\n+        num_key_value_heads=16,\n+        head_dim=256,\n+        hidden_activation=\"gelu_pytorch_tanh\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=0,\n+        eos_token_id=1,\n+        bos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n         query_pre_attn_scalar=224,\n         sliding_window=4096,\n         final_logit_softcapping=30.0,\n-        **super_kwargs,\n+        attn_logit_softcapping=50.0,\n+        **kwargs,\n     ):\n-        super().__init__(self, **super_kwargs)\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n         self.query_pre_attn_scalar = query_pre_attn_scalar\n         self.sliding_window = sliding_window\n-        self.cache_implementation = \"hybrid\"\n         self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n \n \n class Gemma2RMSNorm(GemmaRMSNorm):\n     pass\n \n \n+class Gemma2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_activation]\n+\n+    def forward(self, x):\n+        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+\n+\n class Gemma2Attention(GemmaAttention):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n         super().__init__(config, layer_idx)\n         self.scaling = config.query_pre_attn_scalar**-0.5\n+        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"sliding_window\": self.sliding_window,\n+                \"cache_position\": cache_position,\n+            }\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n+\n+        if self.config.attn_logit_softcapping is not None:\n+            attn_weights = attn_weights / self.config.attn_logit_softcapping\n+            attn_weights = torch.tanh(attn_weights)\n+            attn_weights = attn_weights * self.config.attn_logit_softcapping\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n \n \n class Gemma2FlashAttention2(Gemma2Attention):\n@@ -119,9 +327,19 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"sliding_window\": self.sliding_window,\n+                \"cache_position\": cache_position,\n+            }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n+        if attention_mask is not None:\n+            seq_len = attention_mask.shape[1]\n+            key_states = key_states[:, :, :seq_len]\n+            value_states = value_states[:, :, :seq_len]\n+\n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n         query_states = query_states.transpose(1, 2)\n@@ -156,7 +374,6 @@ def forward(\n             key_states = key_states.to(target_dtype)\n             value_states = value_states.to(target_dtype)\n \n-        ########### ONLY DIFFERENCE IS WE USE SLIDING AND PASS THE SOFTMAX SCALING\n         attn_output = _flash_attention_forward(\n             query_states,\n             key_states,\n@@ -166,7 +383,9 @@ def forward(\n             dropout=dropout_rate,\n             softmax_scale=self.scaling,\n             is_causal=self.is_causal,\n+            sliding_window=self.sliding_window,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            softcap=self.config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n@@ -227,7 +446,12 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"sliding_window\": self.sliding_window,\n+                \"cache_position\": cache_position,\n+            }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -269,8 +493,9 @@ def forward(\n class Gemma2DecoderLayer(GemmaDecoderLayer):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n-\n-        self.is_sliding = bool(layer_idx % 2)\n+        self.config = config\n+        self.is_sliding = not bool(layer_idx % 2)\n+        self.mlp = Gemma2MLP(config)\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window\n@@ -286,11 +511,18 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            attention_mask = attention_mask * torch.tril(\n-                torch.ones_like(attention_mask), diagonal=(self.sliding_window - cache_position[-1])\n-            )\n-            if cache_position[0] > 0:\n-                attention_mask = attention_mask[:, -self.sliding_window :]\n+            # Flash-attn is a 2D tensor\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                if past_key_value is not None:  # when decoding\n+                    attention_mask = attention_mask[:, -self.sliding_window :]\n+            else:\n+                min_dtype = torch.finfo(hidden_states.dtype).min\n+                sliding_window_mask = torch.tril(\n+                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n+                )\n+                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n+                if attention_mask.shape[-1] <= 1:  # when decoding\n+                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n \n         residual = hidden_states\n \n@@ -326,13 +558,38 @@ def forward(\n         return outputs\n \n \n-class Gemma2Model(GemmaModel):\n+class Gemma2PreTrainedModel(GemmaPreTrainedModel):\n+    _supports_quantized_cache = False\n+\n+    @classmethod\n+    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n+        \"\"\"\n+        Overloads `PreTrainedModel._check_and_enable_sdpa` so as to DISABLE torch SDPA by default on Gemma2 models.\n+        SDPA reduces the model performance on Gemma2 because of the logits softcapping.\n+        \"\"\"\n+        config = super()._check_and_enable_sdpa(config, hard_check_only=hard_check_only)\n+\n+        # if using the default path -> swap sdpa by eager\n+        if not hard_check_only and config._attn_implementation == \"sdpa\":\n+            config._attn_implementation = \"eager\"\n+\n+        return config\n+\n+\n+class Gemma2Model(GemmaModel, Gemma2PreTrainedModel):\n+    def __init__(self, config: Gemma2Config):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[HybridCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -361,8 +618,21 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if use_cache and past_key_values is None and not self.training:\n+            batch_size, seq_len, _ = inputs_embeds.shape\n+            past_key_values = HybridCache(\n+                self.config,\n+                batch_size=batch_size,\n+                max_cache_len=seq_len,\n+                device=self.device,\n+                dtype=inputs_embeds.dtype,\n+            )\n+\n         if cache_position is None:\n-            cache_position = torch.arange(0, inputs_embeds.shape[1], device=inputs_embeds.device)\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n \n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n@@ -437,69 +707,60 @@ def _update_causal_mask(\n         attention_mask: torch.Tensor,\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n-        past_key_values: Cache,\n+        past_key_values: HybridCache,\n         output_attentions: bool,\n     ):\n+        # Flash Attention currently doesn't support static cache but Gemma2 work only with static cache.\n+        # So we will pass in attention mask as is in any case, not only when ther's padding. Then we'll use its shape\n+        # to cut out keys/values trailing 0 used in static cache. This workaround should be compile compatible\n+        # as it doesn't cause dynamic control issues.\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n+            return attention_mask\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if past_key_values is not None:\n+        if isinstance(past_key_values, HybridCache):\n             target_length = past_key_values.get_max_length()\n         else:\n-            target_length = attention_mask.shape[-1]\n+            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n \n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            causal_mask = attention_mask\n-        else:\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            min_dtype=min_dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n         return causal_mask\n \n \n-class Gemma2ForCausalLM(GemmaForCausalLM, GenerationMixin):\n+class Gemma2ForCausalLM(GemmaForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Gemma2Model(config)\n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[HybridCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n-\n-        Example:\n-\n         ```python\n         >>> from transformers import AutoTokenizer, GemmaForCausalLM\n \n@@ -514,12 +775,17 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n+\n+        if self.training and self.config._attn_implementation != \"eager\":\n+            logger.warning_once(\n+                \"It is strongly recommended to train Gemma2 models with the `eager` attention implementation \"\n+                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n+            )\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n@@ -535,15 +801,23 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n         if self.config.final_logit_softcapping is not None:\n             logits = logits / self.config.final_logit_softcapping\n             logits = torch.tanh(logits)\n             logits = logits * self.config.final_logit_softcapping\n \n+        # TODO: remove the float() operation in v4.46\n         logits = logits.float()\n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -567,10 +841,94 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s\n+                # `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride\n+                # during the decoding. Here, simply using `.contiguous()` is not sufficient as in the\n+                # batch size = 1 case, `position_ids` is already contiguous but with varying stride\n+                # which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # The clone here is for the same reason as for `position_ids`.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n+\n+        if (\n+            isinstance(past_key_values, HybridCache)\n+            and attention_mask.ndim == 2\n+            and not self.config._attn_implementation == \"flash_attention_2\"\n+        ):\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+            dtype = self.lm_head.weight.dtype\n+            min_dtype = torch.finfo(dtype).min\n+            attention_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_length(),\n+                dtype=dtype,\n+                device=device,\n+                min_dtype=min_dtype,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n \n class Gemma2ForSequenceClassification(GemmaForSequenceClassification):\n-    pass\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Gemma2Model(config)\n+        self.post_init()\n \n \n class Gemma2ForTokenClassification(GemmaForTokenClassification):\n-    pass\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Gemma2Model(config)\n+        self.post_init()",
            "previous_filename": "src/transformers/models/gemma2/diff_gemma2.py"
        },
        {
            "sha": "02672bdce83ad94ba757b71b1eceff837c52a59a",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n@@ -24,9 +24,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n-from ...utils import (\n-    logging,\n-)\n+from ...utils import logging\n from ..auto import CONFIG_MAPPING\n \n \n@@ -36,8 +34,8 @@\n class InstructBlipVideoVisionConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InstructBlipVideoVisionModel`]. It is used to\n-    instantiate a Instructblipvideo vision encoder according to the specified arguments, defining the model architecture.\n-    Instantiating a configuration defaults will yield a similar configuration to that of the Instructblipvideo\n+    instantiate a InstructBlipVideo vision encoder according to the specified arguments, defining the model architecture.\n+    Instantiating a configuration defaults will yield a similar configuration to that of the InstructBlipVideo\n     [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n@@ -58,7 +56,7 @@ class InstructBlipVideoVisionConfig(PretrainedConfig):\n             The size (resolution) of each patch.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n-            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"gelu\"` are supported. to 1e-5): The epsilon used by the layer\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"gelu\"` are supported. to 1e-5): The epsilon used by the layer\n             normalization layers.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n             The epsilon used by the layer normalization layers.\n@@ -137,9 +135,9 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike],\n class InstructBlipVideoQFormerConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InstructBlipVideoQFormerModel`]. It is used to\n-    instantiate a Instructblipvideo Querying Transformer (Q-Former) model according to the specified arguments, defining the\n+    instantiate a InstructBlipVideo Querying Transformer (Q-Former) model according to the specified arguments, defining the\n     model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n-    the Instructblipvideo [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)\n+    the InstructBlipVideo [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)\n     architecture. Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs.\n     Read the documentation from [`PretrainedConfig`] for more information.\n \n@@ -189,7 +187,7 @@ class InstructBlipVideoQFormerConfig(PretrainedConfig):\n     ```python\n     >>> from transformers import InstructBlipVideoQFormerConfig, InstructBlipVideoQFormerModel\n \n-    >>> # Initializing a Instructblipvideo Salesforce/instruct-blip-flan-t5 style configuration\n+    >>> # Initializing a InstructBlipVideo Salesforce/instruct-blip-flan-t5 style configuration\n     >>> configuration = InstructBlipVideoQFormerConfig()\n \n     >>> # Initializing a model (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n@@ -360,7 +358,7 @@ def from_vision_qformer_text_configs(\n         **kwargs,\n     ):\n         r\"\"\"\n-        Instantiate a [`InstructBlipVideoConfig`] (or a derived class) from a Instructblipvideo vision model, Q-Former and\n+        Instantiate a [`InstructBlipVideoConfig`] (or a derived class) from a InstructBlipVideo vision model, Q-Former and\n         language model configurations.\n \n         Returns:"
        },
        {
            "sha": "e165fb411af8319f471f33f8210c8594e40c573c",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 88,
            "deletions": 91,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import math\n from dataclasses import dataclass\n from typing import Any, Optional, Tuple, Union\n@@ -354,6 +353,21 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n+INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n INSTRUCTBLIPVIDEO_VISION_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n@@ -371,6 +385,71 @@ def _init_weights(self, module):\n             Whether to interpolate the pre-trained position encodings.\n \"\"\"\n \n+INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n+            [`InstructBlipVideoProcessor.__call__`] for details.\n+\n+        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n+            to serve as text prompt, which the Q-Former model will encode.\n+\n+            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n+            encoder-decoder language model (like T5) is used.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            Only relevant in case an encoder-decoder language model (like T5) is used.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+\"\"\"\n+\n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlipVideo\n class InstructBlipVideoEncoder(nn.Module):\n@@ -459,87 +538,6 @@ def forward(\n         )\n \n \n-INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n-            [`InstructBlipVideoProcessor.__call__`] for details.\n-\n-        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n-            to serve as text prompt, which the Q-Former model will encode.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n-            encoder-decoder language model (like T5) is used.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            Only relevant in case an encoder-decoder language model (like T5) is used.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlipVideo, BLIP->INSTRUCTBLIPVIDEO\n class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n     main_input_name = \"pixel_values\"\n@@ -1089,7 +1087,7 @@ def forward(\n \n class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     \"\"\"\n-    Querying Transformer (Q-Former), used in Instructblipvideo. Slightly modified from BLIP-2 as it also takes the\n+    Querying Transformer (Q-Former), used in InstructBlipVideo. Slightly modified from BLIP-2 as it also takes the\n     instruction as input.\n     \"\"\"\n \n@@ -1285,7 +1283,7 @@ def forward(\n \n @add_start_docstrings(\n     \"\"\"\n-    Instructblipvideo Model for generating text given an image and an optional text prompt. The model consists of a vision\n+    InstructBlipVideo Model for generating text given an image and an optional text prompt. The model consists of a vision\n     encoder, Querying Transformer (Q-Former) and a language model.\n \n     One can optionally pass `input_ids` to the model, which serve as a text prompt, to make the language model continue\n@@ -1358,7 +1356,7 @@ def _preprocess_accelerate(self):\n         hf_device_map = self.hf_device_map\n \n         if len(hf_device_map) > 1 and \"language_model\" not in hf_device_map and torch.cuda.device_count() > 1:\n-            # warn users about unexpected behavior when using multi-GPU + Instructblipvideo + `accelerate`.\n+            # warn users about unexpected behavior when using multi-GPU + InstructBlipVideo + `accelerate`.\n             logger.warning(\n                 \"The `language_model` is not in the `hf_device_map` dictionary and you are running your script\"\n                 \" in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`.\"\n@@ -1505,7 +1503,6 @@ def forward(\n         )\n \n         inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n@@ -1584,11 +1581,11 @@ def generate(\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n-        \"\"\"\n+        r\"\"\"\n         Overrides `generate` function to be able to use the model as a conditional generator.\n \n         Args:\n-           pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width) or\n+            pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width) or\n                 (batch_size, num_frames, num_channels, height, width)): Input images or videos to be processed.\n             qformer_input_ids (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                 The sequence used as a prompt to be fed to the Q-Former module."
        },
        {
            "sha": "2128f25df6625d8c38e284e8b9ea9ef9c0f09cb7",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "renamed",
            "additions": 111,
            "deletions": 78,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -21,32 +21,18 @@\n from torch.nn import CrossEntropyLoss\n \n from transformers.models.instructblip.configuration_instructblip import (\n-    InstructBlipConfig,\n     InstructBlipQFormerConfig,\n     InstructBlipVisionConfig,\n )\n from transformers.models.instructblip.modeling_instructblip import (\n-    InstructBlipAttention,\n-    InstructBlipEncoder,\n-    InstructBlipEncoderLayer,\n     InstructBlipForConditionalGeneration,\n     InstructBlipForConditionalGenerationModelOutput,\n-    InstructBlipMLP,\n-    InstructBlipPreTrainedModel,\n-    InstructBlipQFormerAttention,\n-    InstructBlipQFormerEmbeddings,\n-    InstructBlipQFormerEncoder,\n-    InstructBlipQFormerIntermediate,\n-    InstructBlipQFormerLayer,\n-    InstructBlipQFormerModel,\n-    InstructBlipQFormerOutput,\n-    InstructBlipQFormerSelfOutput,\n-    InstructBlipVisionEmbeddings,\n-    InstructBlipVisionModel,\n )\n \n-from ...generation import GenerationMixin\n+from ...configuration_utils import PretrainedConfig\n+from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n+from ..auto import CONFIG_MAPPING\n \n \n logger = logging.get_logger(__name__)\n@@ -60,76 +46,132 @@ class InstructBlipVideoQFormerConfig(InstructBlipQFormerConfig):\n     pass\n \n \n-class InstructBlipVideoConfig(InstructBlipConfig):\n-    pass\n-\n-\n-@dataclass\n-class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForConditionalGenerationModelOutput):\n-    pass\n-\n-\n-class InstructBlipVideoVisionEmbeddings(InstructBlipVisionEmbeddings):\n-    pass\n-\n-\n-class InstructBlipVideoAttention(InstructBlipAttention):\n-    pass\n-\n-\n-class InstructBlipVideoMLP(InstructBlipMLP):\n-    pass\n-\n-\n-class InstructBlipVideoEncoderLayer(InstructBlipEncoderLayer):\n-    pass\n-\n-\n-class InstructBlipVideoPreTrainedModel(InstructBlipPreTrainedModel):\n-    pass\n-\n-\n-class InstructBlipVideoEncoder(InstructBlipEncoder):\n-    pass\n-\n+class InstructBlipVideoConfig(PretrainedConfig):\n+    r\"\"\"\n+    [`InstructBlipVideoConfig`] is the configuration class to store the configuration of a\n+    [`InstructBlipVideoForConditionalGeneration`]. It is used to instantiate a Instructblipvideo model according to the specified\n+    arguments, defining the vision model, Q-Former model and language model configs. Instantiating a configuration with\n+    the defaults will yield a similar configuration to that of the Instructblipvideo\n+    [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n-class InstructBlipVideoVisionModel(InstructBlipVisionModel):\n-    pass\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n \n+    Args:\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`InstructBlipVideoVisionConfig`].\n+        qformer_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`InstructBlipVideoQFormerConfig`].\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+        num_query_tokens (`int`, *optional*, defaults to 32):\n+            The number of query tokens passed through the Transformer.\n \n-class InstructBlipVideoQFormerSelfOutput(InstructBlipQFormerSelfOutput):\n-    pass\n+        video_token_index (`int`, *optional*):\n+            Token index of special video token.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n \n+    Example:\n \n-class InstructBlipVideoQFormerAttention(InstructBlipQFormerAttention):\n-    pass\n+    ```python\n+    >>> from transformers import (\n+    ...     InstructBlipVideoVisionConfig,\n+    ...     InstructBlipVideoQFormerConfig,\n+    ...     OPTConfig,\n+    ...     InstructBlipVideoConfig,\n+    ...     InstructBlipVideoForConditionalGeneration,\n+    ... )\n \n+    >>> # Initializing a InstructBlipVideoConfig with Salesforce/instruct-blip-flan-t5 style configuration\n+    >>> configuration = InstructBlipVideoConfig()\n \n-class InstructBlipVideoQFormerIntermediate(InstructBlipQFormerIntermediate):\n-    pass\n+    >>> # Initializing a InstructBlipVideoForConditionalGeneration (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n+    >>> model = InstructBlipVideoForConditionalGeneration(configuration)\n \n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n \n-class InstructBlipVideoQFormerOutput(InstructBlipQFormerOutput):\n-    pass\n+    >>> # We can also initialize a InstructBlipVideoConfig from a InstructBlipVideoVisionConfig, InstructBlipVideoQFormerConfig and any PretrainedConfig\n \n+    >>> # Initializing Instructblipvideo vision, Instructblipvideo Q-Former and language model configurations\n+    >>> vision_config = InstructBlipVideoVisionConfig()\n+    >>> qformer_config = InstructBlipVideoQFormerConfig()\n+    >>> text_config = OPTConfig()\n \n-class InstructBlipVideoQFormerLayer(InstructBlipQFormerLayer):\n-    pass\n+    >>> config = InstructBlipVideoConfig.from_text_vision_configs(vision_config, qformer_config, text_config)\n+    ```\"\"\"\n \n+    model_type = \"instructblipvideo\"\n \n-class InstructBlipVideoQFormerEncoder(InstructBlipQFormerEncoder):\n-    pass\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        qformer_config=None,\n+        text_config=None,\n+        num_query_tokens=32,\n+        video_token_index=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"vision_config is None. initializing the InstructBlipVideoVisionConfig with default values.\")\n+\n+        if qformer_config is None:\n+            qformer_config = {}\n+            logger.info(\"qformer_config is None. Initializing the InstructBlipVideoQFormerConfig with default values.\")\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+\n+        self.vision_config = InstructBlipVideoVisionConfig(**vision_config)\n+        self.qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n+        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n+        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n+\n+        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n+        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n+\n+        self.num_query_tokens = num_query_tokens\n+        self.video_token_index = video_token_index\n+        self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n+        self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n+        self.initializer_factor = 1.0\n+        self.initializer_range = 0.02\n+\n+    @classmethod\n+    def from_vision_qformer_text_configs(\n+        cls,\n+        vision_config: InstructBlipVideoVisionConfig,\n+        qformer_config: InstructBlipVideoQFormerConfig,\n+        text_config: PretrainedConfig,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Instantiate a [`InstructBlipVideoConfig`] (or a derived class) from a InstructBlipVideo vision model, Q-Former and\n+        language model configurations.\n \n+        Returns:\n+            [`InstructBlipVideoConfig`]: An instance of a configuration object\n+        \"\"\"\n \n-class InstructBlipVideoQFormerEmbeddings(InstructBlipQFormerEmbeddings):\n-    pass\n+        return cls(\n+            vision_config=vision_config.to_dict(),\n+            qformer_config=qformer_config.to_dict(),\n+            text_config=text_config.to_dict(),\n+            **kwargs,\n+        )\n \n \n-class InstructBlipVideoQFormerModel(InstructBlipQFormerModel):\n+@dataclass\n+class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForConditionalGenerationModelOutput):\n     pass\n \n \n-class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration, GenerationMixin):\n+class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration):\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -146,15 +188,6 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the language modeling loss. Indices should be in `[-100, 0, ..., config.vocab_size -\n-            1]`. All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n-            config.vocab_size]`\n-\n-        Returns:\n-\n-        Examples:\n-\n         ```python\n         >>> from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration\n         >>> import torch\n@@ -339,11 +372,11 @@ def generate(\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n-        \"\"\"\n+        r\"\"\"\n         Overrides `generate` function to be able to use the model as a conditional generator.\n \n         Args:\n-           pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width) or\n+            pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width) or\n                 (batch_size, num_frames, num_channels, height, width)): Input images or videos to be processed.\n             qformer_input_ids (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                 The sequence used as a prompt to be fed to the Q-Former module.",
            "previous_filename": "src/transformers/models/instructblipvideo/diff_instructblipvideo.py"
        },
        {
            "sha": "1631c1018306d24b84f6c9f8594cc5ce5ea0219f",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n@@ -20,17 +20,10 @@\n # limitations under the License.\n \n \n-from transformers import PretrainedConfig\n-\n-from ...utils import (\n-    logging,\n-)\n+from ...configuration_utils import PretrainedConfig\n from ..auto import CONFIG_MAPPING\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class LlavaNextVideoConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaNextVideoForConditionalGeneration`]. It is used to instantiate an\n@@ -48,7 +41,7 @@ class LlavaNextVideoConfig(PretrainedConfig):\n         ignore_index (`int`, *optional*, defaults to -100):\n             The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32001):\n-           The image token index to encode the image prompt.\n+            The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n             The activation function used by the multimodal projector.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):"
        },
        {
            "sha": "b73b2f6994d9a84c972bd4a0d56d6656cb050a62",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 21,
            "deletions": 7,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -1,8 +1,8 @@\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import math\n from dataclasses import dataclass\n from typing import List, Optional, Tuple, Union\n@@ -130,6 +129,12 @@ def unpad_image(tensor, original_size):\n     Returns:\n         `torch.Tensor`: The unpadded image tensor.\n     \"\"\"\n+    if not isinstance(original_size, (list, tuple)):\n+        if not isinstance(original_size, (torch.Tensor, np.ndarray)):\n+            raise TypeError(\n+                f\"image_size invalid type: {type(original_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n+            )\n+        original_size = original_size.tolist()\n     original_height, original_width = original_size\n     current_height, current_width = tensor.shape[1:]\n \n@@ -180,6 +185,7 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n             A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+\n         video_hidden_states (`torch.FloatTensor`, *optional*):\n             A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n             video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n@@ -191,6 +197,7 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n+\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -455,7 +462,6 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n         self.vocab_size = model_embeds.num_embeddings\n         return model_embeds\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration._merge_input_ids_with_image_features\n     def _merge_input_ids_with_image_features(\n         self,\n         image_features,\n@@ -695,7 +701,7 @@ def _merge_input_ids_with_image_features(\n \n         return final_embedding, final_attention_mask, position_ids, final_labels, final_input_ids\n \n-    def pack_image_features(self, image_features, image_sizes, image_newline=None):\n+    def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n \n@@ -704,6 +710,8 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None):\n                 List of image feature tensor, each contains all the visual feature of all patches.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n+            vision_feature_select_strategy (`str`)\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n             image_newline (`torch.Tensor` of shape `(embed_dim)`)\n                 New line embedding vector.\n         Returns:\n@@ -718,8 +726,14 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None):\n                 base_image_feature = image_feature[0]\n                 image_feature = image_feature[1:]\n                 height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n-                if height * width != base_image_feature.shape[0]:\n+\n+                if vision_feature_select_strategy == \"default\":\n+                    expected_num_patches = height * width\n+                elif vision_feature_select_strategy == \"full\":\n+                    expected_num_patches = height * width + 1\n+                if expected_num_patches != base_image_feature.shape[0]:\n                     raise ValueError(\"The number of patches is not consistent with the image size.\")\n+\n                 num_patch_height, num_patch_width = get_anyres_image_grid_shape(\n                     image_sizes[image_idx],\n                     self.config.image_grid_pinpoints,"
        },
        {
            "sha": "f0ec4578e4883dc6aa147ce4b42f579bcfb707a3",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "renamed",
            "additions": 32,
            "deletions": 21,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -21,15 +21,13 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from transformers import PretrainedConfig\n from transformers.models.llava_next.modeling_llava_next import (\n     LlavaNextCausalLMOutputWithPast,\n     LlavaNextForConditionalGeneration,\n-    LlavaNextMultiModalProjector,\n     image_size_to_num_patches,\n )\n \n-from ...generation import GenerationMixin\n+from ...configuration_utils import PretrainedConfig\n from ...utils import (\n     logging,\n     replace_return_docstrings,\n@@ -56,18 +54,8 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The config object or dictionary of the text backbone.\n         ignore_index (`int`, *optional*, defaults to -100):\n             The ignore index for the loss function.\n-        video_token_index (`int`, *optional*, defaults to 32000):\n-            The video token index to encode the image prompt.\n         image_token_index (`int`, *optional*, defaults to 32001):\n-           The image token index to encode the image prompt.\n-        spatial_pool_mode (`str`, *optional*, defaults to `\"average\"`):\n-            Pooling mode to use for videos. Can be \"average\", \"max\" or \"conv\".\n-        spatial_pool_stride (`int`, *optional*, defaults to 2):\n-            Stride used in the pooling layer for videos.\n-        image_seq_length (`int`, *optional*, defaults to 576):\n-            Sequence length of one image embedding.\n-        video_seq_length (`int`, *optional*, defaults to 288):\n-            Sequence length of one video embedding.\n+            The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n             The activation function used by the multimodal projector.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n@@ -81,6 +69,16 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             of the form `(height, width)`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n+        video_token_index (`int`, *optional*, defaults to 32000):\n+            The video token index to encode the image prompt.\n+        spatial_pool_mode (`str`, *optional*, defaults to `\"average\"`):\n+            Pooling mode to use for videos. Can be \"average\", \"max\" or \"conv\".\n+        spatial_pool_stride (`int`, *optional*, defaults to 2):\n+            Stride used in the pooling layer for videos.\n+        image_seq_length (`int`, *optional*, defaults to 576):\n+            Sequence length of one image embedding.\n+        video_seq_length (`int`, *optional*, defaults to 288):\n+            Sequence length of one video embedding.\n \n     Example:\n \n@@ -178,7 +176,13 @@ def __init__(\n \n @dataclass\n class LlavaNextVideoCausalLMOutputWithPast(LlavaNextCausalLMOutputWithPast):\n-    pass\n+    \"\"\"\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n class LlavaNextVideoPooler(nn.Module):\n@@ -215,11 +219,7 @@ def forward(self, image_features):\n         return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n \n \n-class LlavaNextVideoMultiModalProjector(LlavaNextMultiModalProjector):\n-    pass\n-\n-\n-class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration, GenerationMixin):\n+class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n     def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         super().__init__(config, **super_kwargs)\n         self.vision_resampler = LlavaNextVideoPooler(config)\n@@ -287,6 +287,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -298,6 +300,10 @@ def forward(\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n \n         Returns:\n \n@@ -329,7 +335,7 @@ def forward(\n         ...             frames.append(frame)\n         ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n \n-        >>> model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", device_map=\"auto)\n+        >>> model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", device_map=\"auto\")\n         >>> processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n \n         >>> prompt = \"USER: <video>\\nWhy is this video funny? ASSISTANT:\"\n@@ -499,6 +505,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -529,6 +536,8 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -541,6 +550,7 @@ def prepare_inputs_for_generation(\n         image_sizes=None,\n         attention_mask=None,\n         cache_position=None,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         if input_ids is not None:\n@@ -560,6 +570,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n ",
            "previous_filename": "src/transformers/models/llava_next_video/diff_llava_next_video.py"
        },
        {
            "sha": "c378ff09f1e4adb47a46af40cbd98978dcf5c78b",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -182,6 +182,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n             A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+\n         video_hidden_states (`torch.FloatTensor`, *optional*):\n             A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n             video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state."
        },
        {
            "sha": "09b237c1e6c67346b8dbed54efc384720dc0392a",
            "filename": "utils/check_modular_conversion.py",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,76 @@\n+import argparse\n+import difflib\n+import glob\n+import logging\n+from io import StringIO\n+\n+# Console for rich printing\n+from modular_model_converter import convert_modular_file\n+from rich.console import Console\n+from rich.syntax import Syntax\n+\n+\n+logging.basicConfig()\n+logging.getLogger().setLevel(logging.ERROR)\n+console = Console()\n+\n+\n+def process_file(modular_file_path, generated_modeling_content, file_type=\"modeling_\", fix_and_overwrite=False):\n+    file_path = modular_file_path.replace(\"modular_\", f\"{file_type}_\")\n+    # Read the actual modeling file\n+    with open(file_path, \"r\") as modeling_file:\n+        content = modeling_file.read()\n+    output_buffer = StringIO(generated_modeling_content[file_type][0])\n+    output_buffer.seek(0)\n+    output_content = output_buffer.read()\n+    diff = difflib.unified_diff(\n+        output_content.splitlines(),\n+        content.splitlines(),\n+        fromfile=f\"{file_path}_generated\",\n+        tofile=f\"{file_path}\",\n+        lineterm=\"\",\n+    )\n+    diff_list = list(diff)\n+    # Check for differences\n+    if diff_list:\n+        if fix_and_overwrite:\n+            with open(file_path, \"w\") as modeling_file:\n+                modeling_file.write(generated_modeling_content[file_type][0])\n+            console.print(f\"[bold blue]Overwritten {file_path} with the generated content.[/bold blue]\")\n+        else:\n+            console.print(f\"\\n[bold red]Differences found between the generated code and {file_path}:[/bold red]\\n\")\n+            diff_text = \"\\n\".join(diff_list)\n+            syntax = Syntax(diff_text, \"diff\", theme=\"ansi_dark\", line_numbers=True)\n+            console.print(syntax)\n+        return 1\n+    else:\n+        console.print(f\"[bold green]No differences found for {file_path}.[/bold green]\")\n+        return 0\n+\n+\n+def compare_files(modular_file_path, fix_and_overwrite=False):\n+    # Generate the expected modeling content\n+    generated_modeling_content = convert_modular_file(modular_file_path)\n+    diff = 0\n+    for file_type in generated_modeling_content.keys():\n+        diff += process_file(modular_file_path, generated_modeling_content, file_type, fix_and_overwrite)\n+    return diff\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser(description=\"Compare modular_xxx.py files with modeling_xxx.py files.\")\n+    parser.add_argument(\n+        \"--files\", default=[\"all\"], type=list, nargs=\"+\", help=\"List of modular_xxx.py files to compare.\"\n+    )\n+    parser.add_argument(\n+        \"--fix_and_overwrite\", action=\"store_true\", help=\"Overwrite the modeling_xxx.py file if differences are found.\"\n+    )\n+    args = parser.parse_args()\n+    if args.files == [\"all\"]:\n+        args.files = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n+    non_matching_files = 0\n+    for modular_file_path in args.files:\n+        non_matching_files += compare_files(modular_file_path, args.fix_and_overwrite)\n+\n+    if non_matching_files and not args.fix_and_overwrite:\n+        raise ValueError(\"Some diff and their modeling code did not match.\")"
        },
        {
            "sha": "f25a8fb5ca6ff1011e126539adb3d3775b157ab4",
            "filename": "utils/create_dependency_mapping.py",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/utils%2Fcreate_dependency_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/utils%2Fcreate_dependency_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dependency_mapping.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -0,0 +1,69 @@\n+import ast\n+from collections import defaultdict, deque\n+\n+\n+# Function to perform topological sorting\n+def topological_sort(dependencies):\n+    # Create a graph and in-degree count for each node\n+    graph = defaultdict(list)\n+    in_degree = defaultdict(int)\n+\n+    # Build the graph\n+    for node, deps in dependencies.items():\n+        for dep in deps:\n+            graph[dep].append(node)  # node depends on dep\n+            in_degree[node] += 1  # increase in-degree of node\n+\n+    # Add all nodes with zero in-degree to the queue\n+    zero_in_degree_queue = deque([node for node in dependencies if in_degree[node] == 0])\n+\n+    sorted_list = []\n+    # Perform topological sorting\n+    while zero_in_degree_queue:\n+        current = zero_in_degree_queue.popleft()\n+        sorted_list.append(current)\n+\n+        # For each node that current points to, reduce its in-degree\n+        for neighbor in graph[current]:\n+            in_degree[neighbor] -= 1\n+            if in_degree[neighbor] == 0:\n+                zero_in_degree_queue.append(neighbor)\n+\n+    # Handle nodes that have no dependencies and were not initially part of the loop\n+    for node in dependencies:\n+        if node not in sorted_list:\n+            sorted_list.append(node)\n+\n+    return sorted_list\n+\n+\n+# Function to extract class and import info from a file\n+def extract_classes_and_imports(file_path):\n+    with open(file_path, \"r\") as file:\n+        tree = ast.parse(file.read(), filename=file_path)\n+    imports = set()\n+\n+    for node in ast.walk(tree):\n+        if isinstance(node, (ast.Import, ast.ImportFrom)):\n+            module = node.module if isinstance(node, ast.ImportFrom) else None\n+            if module and \"transformers\" in module:\n+                imports.add(module)\n+    return imports\n+\n+\n+# Function to map dependencies between classes\n+def map_dependencies(py_files):\n+    dependencies = defaultdict(set)\n+    # First pass: Extract all classes and map to files\n+    for file_path in py_files:\n+        dependencies[file_path].add(None)\n+        class_to_file = extract_classes_and_imports(file_path)\n+        for module in class_to_file:\n+            dependencies[file_path].add(module)\n+    return dependencies\n+\n+\n+def find_priority_list(py_files):\n+    dependencies = map_dependencies(py_files)\n+    ordered_classes = topological_sort(dependencies)\n+    return ordered_classes[::-1]"
        },
        {
            "sha": "b6e99fbf5eddfde8d28a5725f36ecda77008d9c8",
            "filename": "utils/modular_model_converter.py",
            "status": "renamed",
            "additions": 307,
            "deletions": 127,
            "changes": 434,
            "blob_url": "https://github.com/huggingface/transformers/blob/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/317e069ee7f4d6d6595b1b03b5d9adcaede043e3/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=317e069ee7f4d6d6595b1b03b5d9adcaede043e3",
            "patch": "@@ -20,21 +20,23 @@\n \n import libcst as cst\n from check_copies import run_ruff\n+from create_dependency_mapping import find_priority_list\n from libcst import ClassDef, CSTTransformer, CSTVisitor\n from libcst import matchers as m\n from libcst.metadata import MetadataWrapper, ParentNodeProvider, PositionProvider, ScopeProvider\n \n from transformers import logging\n+from transformers.models.auto.configuration_auto import CONFIG_MAPPING_NAMES\n \n \n logger = logging.get_logger(__name__)\n \n \n AUTO_GENERATED_MESSAGE = \"\"\"#           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                                    diff.py file directly.\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n \"\"\"\n \n@@ -82,12 +84,16 @@ def __init__(self, python_module: cst.Module):\n         self.function_def = {}                          # stores global scope function definition\n         self.assignments = {}                           # LLAMA_DOCSTRING\n         self.class_dependency_mapping = {}              # \"LlamaModel\":[\"LlamaDecoderLayer, \"LlamaRMSNorm\", \"LlamaPreTrainedModel\"], \"LlamaDecoderLayer\":[\"LlamaAttention\",\"Llama\"]\n+        self.first_lvl_dependency_mapping = {}              # \"LlamaModel\":[\"LlamaDecoderLayer, \"LlamaRMSNorm\", \"LlamaPreTrainedModel\"], \"LlamaDecoderLayer\":[\"LlamaAttention\",\"Llama\"]\n         # fmt: on\n \n     def _update_class_dependency(self, name, value):\n         \"\"\"Update the dependency mapping for `name` with `value` by appending the previous\n         dependencies to the new `value`.\n         \"\"\"\n+        dep = set(self.first_lvl_dependency_mapping.get(name, set())) | set({value})\n+        self.first_lvl_dependency_mapping[name] = dep\n+\n         dep = set(self.class_dependency_mapping.get(value, set()))\n         dep |= set(self.class_dependency_mapping.get(name, {})) | set({value})\n         self.class_dependency_mapping[name] = dep\n@@ -146,7 +152,16 @@ def leave_Dict(self, node):\n     def leave_Decorator(self, node):\n         if hasattr(node.decorator, \"args\"):\n             for k in node.decorator.args:\n-                if k.value.value in self.assignments:\n+                if m.matches(k.value, m.Call(func=m.Attribute(value=m.Name()))):  # and k.value.func.value.value:\n+                    if k.value.func.value.value not in self.assignments:\n+                        raise ValueError(\n+                            f\"We detected a call to {k.value.func.value.value}, but it was not assigned. See the list of assigments {self.assignments.keys()}\"\n+                        )\n+                    parent = self.get_metadata(cst.metadata.ParentNodeProvider, node)\n+                    scope = self.get_metadata(cst.metadata.ScopeProvider, node)\n+                    name = scope._name_prefix.split(\".\")[0] if scope._name_prefix != \"\" else parent.name.value\n+                    self._update_class_dependency(name, k.value.func.value.value)\n+                elif m.matches(k, m.Arg(value=m.Name())) and k.value.value in self.assignments:\n                     parent = self.get_metadata(cst.metadata.ParentNodeProvider, node)\n                     scope = self.get_metadata(cst.metadata.ScopeProvider, node)\n                     name = scope._name_prefix.split(\".\")[0] if scope._name_prefix != \"\" else parent.name.value\n@@ -178,6 +193,10 @@ def __init__(self, old_name, new_name, given_old_name=None, given_new_name=None)\n         self.old_name = old_name\n         self.new_name = new_name\n         self.default_name = \"\".join(x.title() for x in new_name.split(\"_\"))\n+        if self.new_name in CONFIG_MAPPING_NAMES:\n+            self.default_name = CONFIG_MAPPING_NAMES[self.new_name].replace(\n+                \"Config\", \"\"\n+            )  # the best source of truth for class names. Could also just use the ones de\n         self.patterns = {\n             old_name: new_name,\n             old_name.upper(): new_name.upper(),\n@@ -193,7 +212,8 @@ def preserve_case_replace(self, text):\n \n         def replace(match):\n             word = match.group(0)\n-            return self.patterns.get(word, self.default_name)\n+            result = self.patterns.get(word, self.default_name)\n+            return result\n \n         return compiled_regex.sub(replace, text)\n \n@@ -227,62 +247,140 @@ def find_classes_in_file(module: cst.Module, old_id=\"llama\", new_id=\"gemma\", giv\n )\n \n \n+def SUPER_CALL_NODE(func_name):\n+    return m.Call(func=m.Attribute(value=m.Call(func=m.Name(\"super\")), attr=m.Name(func_name)))\n+\n+\n+def get_docstring_indent(docstring):\n+    # Match the first line after the opening triple quotes\n+    match = re.search(r'(?:\"\"\"|\\'\\'\\'|```)\\n(\\s+)', docstring)\n+    if match:\n+        # Return the indentation spaces captured\n+        return len(match.group(1))\n+    return 0\n+\n+\n+def merge_docstrings(original_docstring, updated_docstring):\n+    # indent_level = get_docstring_indent(updated_docstring)\n+    original_level = get_docstring_indent(original_docstring)\n+    if \"        Args:\\n        \" not in updated_docstring:\n+        # Split the docstring at the example section, assuming `\"\"\"` is used to define the docstring\n+        parts = original_docstring.split(\"```\")\n+        if \"```\" in updated_docstring and len(parts) > 1:\n+            updated_docstring = updated_docstring.lstrip('r\"')\n+            new_parts = updated_docstring.split(\"```\")\n+            if len(new_parts) != 3:\n+                raise ValueError(\"There should only be one example, and it should have opening and closing '```'\")\n+            parts[1] = new_parts[1]\n+            updated_docstring = \"\".join(\n+                [\n+                    parts[0].rstrip(\" \\n\") + new_parts[0],\n+                    f\"\\n{original_level*' '}```\",\n+                    parts[1],\n+                    \"```\",\n+                    parts[2],\n+                ]\n+            )\n+        elif updated_docstring not in original_docstring:\n+            # add tabulation if we are at the lowest level.\n+            if re.search(r\"\\n\\s*.*\\(.*\\)\\:\\n\\s*\\w\", updated_docstring):\n+                updated_docstring = updated_docstring.replace(\"\\n    \", \"\\n        \")\n+            updated_docstring = original_docstring.rstrip('\"') + \"\\n\" + updated_docstring.lstrip('r\"\\n')\n+    return updated_docstring\n+\n+\n class SuperTransformer(cst.CSTTransformer):\n     METADATA_DEPENDENCIES = (ParentNodeProvider,)\n \n-    def __init__(self, python_module: cst.Module, original_methods, updated_methods):\n+    def __init__(self, python_module: cst.Module, original_methods, updated_methods, class_name=\"\"):\n         self.python_module = python_module\n         self.original_methods = original_methods\n         self.updated_methods = updated_methods\n+        self.all_assign_target = {}\n+        self.deleted_targets = {}  # child node can delete some arguments\n+        self.class_name = class_name\n \n     def update_body(self, existing_body, new_statements):\n         \"\"\"\n         Helper method to update the body by removing duplicates before adding new statements.\n+        `existing_body` is the body of the original method, the parent class\n+        `new_statements` are the additional statements\n         \"\"\"\n         deduplicated_new_body = []\n         existing_nodes = set()\n         for node in new_statements:\n-            code = self.python_module.code_for_node(node)\n-            comment_less_code = re.sub(r\"#.*\", \"\", code).strip()\n-            comment_less_code = re.sub(r\"\\ *\\n\", \"\\n\", comment_less_code).strip()\n-            existing_nodes.add(comment_less_code)\n+            if m.matches(node, m.SimpleStatementLine(body=[m.Assign()])):\n+                target = self.python_module.code_for_node(node.body[0].targets[0].target)\n+                self.all_assign_target[target] = node\n+            if m.matches(node, m.SimpleStatementLine(body=[m.Del()])):\n+                target = self.python_module.code_for_node(node.body[0].target)\n+                self.deleted_targets[target] = node\n+                continue\n+\n         for stmt in existing_body:\n+            if m.matches(stmt, m.SimpleStatementLine(body=[m.Assign()])):\n+                target = self.python_module.code_for_node(stmt.body[0].targets[0].target)\n+                if target in self.deleted_targets:\n+                    logger.warning(f\"Deleted the assign for {target}\")\n+                    continue\n+                if target in self.all_assign_target:\n+                    stmt = self.all_assign_target[target]\n             comment_less_code = re.sub(r\"#.*\", \"\", self.python_module.code_for_node(stmt)).strip()\n             comment_less_code = re.sub(r\"\\ *\\n\", \"\\n\", comment_less_code).strip()\n-            if comment_less_code not in existing_nodes:\n-                if m.matches(stmt, DOCSTRING_NODE) and self.has_docstring:\n-                    continue\n-                deduplicated_new_body.append(stmt)\n-                existing_nodes.add(stmt)\n-            else:\n-                logger.info(f\"\\nFound duplicate {self.python_module.code_for_node(stmt)}\")\n+            deduplicated_new_body.append(stmt)\n+            existing_nodes.add(comment_less_code)\n+\n+        for node in new_statements:\n+            code = self.python_module.code_for_node(node)\n+            comment_less_code = re.sub(r\"#.*\", \"\", code).strip()\n+            comment_less_code = re.sub(r\"\\ *\\n\", \"\\n\", comment_less_code).strip()\n+            if (\n+                node not in deduplicated_new_body\n+                and \"super().__init__\" not in comment_less_code\n+                and comment_less_code not in existing_nodes\n+            ):\n+                if not m.matches(node, m.SimpleStatementLine(body=[m.Del()])):\n+                    # HACK here to fix the pos_init() that has to be last we kinda do this.\n+                    deduplicated_new_body = deduplicated_new_body[:-1] + [node] + deduplicated_new_body[-1:]\n+                    existing_nodes.add(comment_less_code)\n         return deduplicated_new_body\n \n     def replace_super_calls(self, node: cst.IndentedBlock, func_name: str) -> cst.CSTNode:\n         \"\"\"Updates the body of the input `node`'s `func_name` function by replacing calls\n         to super().func_name() with the source code of the parent class' `func_name`.\n         It keeps everything that is defined before `super().func_name()`.\n         \"\"\"\n-        new_body = []\n         self.has_docstring = False\n-        for expr in node.body:\n-            self.has_docstring = m.matches(node.body[0], DOCSTRING_NODE)\n+        parent_has_docstring = False\n+        if func_name in self.original_methods:\n+            parent_has_docstring = m.matches(self.original_methods[func_name].body.body[0], DOCSTRING_NODE)\n+        new_body = []\n+        has_super_call = False\n+        for idx, expr in enumerate(node.body):\n             if m.matches(\n                 expr,\n                 m.SimpleStatementLine(\n-                    body=[\n-                        m.Return(\n-                            value=m.Call(func=m.Attribute(value=m.Call(func=m.Name(\"super\")), attr=m.Name(func_name)))\n-                        )\n-                        | m.Expr(\n-                            value=m.Call(func=m.Attribute(value=m.Call(func=m.Name(\"super\")), attr=m.Name(func_name)))\n-                        )\n-                    ]\n+                    body=[m.Return(SUPER_CALL_NODE(func_name)) | m.Expr(SUPER_CALL_NODE(func_name))]\n                 ),\n             ):\n+                if idx != 0 and func_name == \"__init__\":\n+                    raise ValueError(f\"The call to super() in {self.class_name} should be at the top of the init\")\n                 new_body.extend(self.update_body(self.original_methods[func_name].body.body, node.body))\n-            else:\n+                has_super_call = True\n+            elif m.matches(expr, DOCSTRING_NODE):\n+                self.has_docstring = True\n+                if parent_has_docstring:  # actually here we ought to de-duplicate?\n+                    original_docstring = self.original_methods[func_name].body.body[0].body[0].value.value\n+                    updated_docstring = expr.body[0].value.value\n+                    merged_doc = merge_docstrings(original_docstring, updated_docstring)\n+                    new_node = [expr.with_changes(body=[cst.Expr(value=cst.SimpleString(value=merged_doc))])]\n+                else:\n+                    new_node = [expr]\n+                new_body.extend(new_node)\n+            elif not m.matches(expr, m.SimpleStatementLine(body=[m.Del()])) and not has_super_call:\n                 new_body.append(expr)\n+        if not self.has_docstring and parent_has_docstring:\n+            new_body = [self.original_methods[func_name].body.body[0]] + new_body\n         return node.with_changes(body=new_body)\n \n     def leave_FunctionDef(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CSTNode:\n@@ -330,14 +428,22 @@ def replace_call_to_super(class_finder: ClassFinder, updated_node: cst.ClassDef,\n                                                                             |     ```\n     \"\"\"\n     original_node = class_finder.classes[class_name]\n-    original_methods = {f.name.value if hasattr(f, \"name\") else f: f for f in original_node.body.body}\n-    updated_methods = {f.name.value if hasattr(f, \"name\") else f: f for f in updated_node.body.body}\n+    original_methods = {\n+        f.name.value if hasattr(f, \"name\") else class_finder.python_module.code_for_node(f): f\n+        for f in original_node.body.body\n+    }\n+    updated_methods = {\n+        f.name.value if hasattr(f, \"name\") else class_finder.python_module.code_for_node(f): f\n+        for f in updated_node.body.body\n+    }\n     end_meth = []\n \n+    assign_targets = {}\n+    docstring_node = []\n     # Iterate directly from node.body as there can be property/setters with same names which are overwritten when we use a dict\n     for func in original_node.body.body:\n-        name = func.name.value if hasattr(func, \"name\") else func\n-        if name in updated_methods and updated_methods[name] is not None:\n+        name = func.name.value if hasattr(func, \"name\") else class_finder.python_module.code_for_node(func)\n+        if m.matches(func, m.FunctionDef()) and name in updated_methods and updated_methods[name] is not None:\n             new_params = updated_methods[name].params\n             # Replace the method in the replacement class, preserving decorators\n             kwarg_name = getattr(updated_methods[name].params, \"star_kwarg\", None)\n@@ -348,22 +454,61 @@ def replace_call_to_super(class_finder: ClassFinder, updated_node: cst.ClassDef,\n                     params=list(parent_params.values()), star_kwarg=func.params.star_kwarg\n                 )\n             func = func.with_changes(body=updated_methods[name].body, params=new_params)\n-        end_meth.append(func)\n+        if m.matches(func, m.SimpleStatementLine(body=[m.Assign()])):\n+            target = class_finder.python_module.code_for_node(func.body[0].targets[0])\n+            assign_targets[target] = func\n+        elif m.matches(func, m.SimpleStatementLine(body=[m.AnnAssign()])):\n+            target = class_finder.python_module.code_for_node(func.body[0].target)\n+            assign_targets[target] = func\n+        elif m.matches(func, DOCSTRING_NODE):\n+            docstring_node = [func]\n+        else:\n+            end_meth.append(func)\n \n-    # Port new methods that are defined only in diff-file and append at the end\n-    for name, func in updated_methods.items():\n+    # Port new methods that are defined only in modular-file and append at the end\n+    for func in updated_node.body.body:\n+        name = func.name.value if hasattr(func, \"name\") else class_finder.python_module.code_for_node(func)\n+        if m.matches(func, DOCSTRING_NODE):  # This processes the docstring of the class!\n+            # Extract the original docstring\n+            updated_docstring = func.body[0].value.value\n+            original_docstring = docstring_node[0].body[0].value.value\n+            merged_doc = merge_docstrings(original_docstring, updated_docstring)\n+            # Update the docstring in the original function\n+            docstring_node = [\n+                docstring_node[0].with_changes(body=[cst.Expr(value=cst.SimpleString(value=merged_doc))])\n+            ]\n         if name not in original_methods and func is not None and isinstance(func, cst.FunctionDef):\n             end_meth.append(func)\n+        if m.matches(func, m.SimpleStatementLine(body=[m.Assign()])):\n+            # TODO we only use single assign might cause issues\n+            target = class_finder.python_module.code_for_node(func.body[0].targets[0])\n+            assign_targets[target] = func\n+        if m.matches(func, m.SimpleStatementLine(body=[m.AnnAssign()])):\n+            target = class_finder.python_module.code_for_node(func.body[0].target)\n+            assign_targets[target] = func\n+    end_meth = docstring_node + list(assign_targets.values()) + end_meth\n \n     result_node = original_node.with_changes(body=cst.IndentedBlock(body=end_meth))\n     temp_module = cst.Module(body=[result_node])\n     new_module = MetadataWrapper(temp_module)\n-    new_replacement_class = new_module.visit(SuperTransformer(temp_module, original_methods, updated_methods))\n+    new_replacement_class = new_module.visit(\n+        SuperTransformer(temp_module, original_methods, updated_methods, class_name)\n+    )\n     new_replacement_body = new_replacement_class.body[0].body  # get the indented block\n+\n     return original_node.with_changes(body=new_replacement_body)\n \n \n-class DiffConverterTransformer(CSTTransformer):\n+TYPE_TO_FILE_TYPE = {\n+    \"Config\": \"configuration\",\n+    \"Tokenizer\": \"tokenization\",\n+    \"Processor\": \"processor\",\n+    \"ImageProcessor\": \"image_processing\",\n+    \"FeatureExtractor\": \"feature_extractor\",\n+}\n+\n+\n+class ModularConverterTransformer(CSTTransformer):\n     METADATA_DEPENDENCIES = (ParentNodeProvider, ScopeProvider, PositionProvider)\n \n     def __init__(self, python_module, new_name, given_old_name=None, given_new_name=None):\n@@ -378,11 +523,21 @@ def __init__(self, python_module, new_name, given_old_name=None, given_new_name=\n         self.transformers_imports = {}      # maps the imports name like \"from transformers.models.xxx\" to the parsed AST module\n         self.imported_mapping = {}          # stores the name of the imported classes, with their source {\"LlamaModel\":\"transformers.model.llama.modeling_llama\"}\n         self.visited_module = {}            # modules visited like \"transformers.models.llama.modeling_llama\"\n-        self.new_body = {}                  # store the new body, all global scope nodes should be added here\n         self.inserted_deps = []             # nodes inserted via super dependency\n         self.all_imports = []               # just stores all of the imports\n+        self.all_safe_imports = []          # stores the import under simple statements\n         self.global_scope_index = 0\n         # fmt: on\n+        self.files = {  # mapping for different component bodies\n+            \"modeling\": {},\n+            \"configuration\": {},\n+            \"tokenization\": {},\n+            \"processing\": {},\n+            \"image_processing\": {},\n+            \"feature_extractor\": {},\n+        }\n+        self.match_patterns = \"|\".join(self.files.keys())\n+        self.all_functions = {}\n \n     def visit_ImportFrom(self, node: cst.ImportFrom) -> None:\n         \"\"\"When visiting imports from `transformers.models.xxx` we need to:\n@@ -393,52 +548,46 @@ def visit_ImportFrom(self, node: cst.ImportFrom) -> None:\n         import_statement = self.python_module.code_for_node(node.module)\n         if m.matches(node.module, m.Attribute()):\n             for imported_ in node.names:\n-                _import = re.search(r\"transformers\\.models\\..*\\.(modeling|configuration)_.*\", import_statement)\n+                _import = re.search(rf\"(transformers\\.models\\..|..)*\\.({self.match_patterns})_.*\", import_statement)\n                 if _import:\n                     source = _import.groups()[0]\n                     if source == \"modeling\" and \"Config\" in self.python_module.code_for_node(imported_):\n                         raise ValueError(\n                             f\"You are importing {self.python_module.code_for_node(imported_)} from the modeling file. Import from the `configuration_xxxx.py` file instead\"\n                         )\n                     if import_statement not in self.transformers_imports:\n+                        if \"models\" not in import_statement:\n+                            import_statement = \"models.\" + import_statement\n+                        if \"transformers\" not in import_statement:\n+                            import_statement = \"transformers.\" + import_statement\n                         source_code = get_module_source_from_name(import_statement)\n                         tree = cst.parse_module(source_code)\n                         self.transformers_imports[import_statement] = tree\n                     imported_class = self.python_module.code_for_node(imported_.name)\n                     self.imported_mapping[imported_class] = import_statement\n-\n-    def leave_FunctionDef(self, original_node, node):\n-        parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n-        if m.matches(parent_node, m.Module()):\n-            self.global_scope_index += 100\n-            self.new_body[node.name.value] = {\"insert_idx\": self.global_scope_index, \"node\": node}\n-        return node\n+        if m.matches(node.module, m.Name()):\n+            if \"transformers\" == import_statement:\n+                raise ValueError(\n+                    f\"You are importing from {import_statement} directly using global imports. Import from the correct local path\"\n+                )\n \n     def leave_SimpleStatementLine(self, original_node, updated_node):\n         parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n         if m.matches(parent_node, m.Module()):\n             if m.matches(updated_node, m.SimpleStatementLine(body=[m.Import()])):\n-                if parent_node not in self.all_imports:\n+                if updated_node not in self.all_imports:\n                     self.all_imports.append(updated_node)\n                 return updated_node\n             elif m.matches(updated_node, m.SimpleStatementLine(body=[m.ImportFrom()])):\n                 full_statement = self.python_module.code_for_node(updated_node.body[0].module)\n-                if re.search(r\"transformers\\.models\\..*\\.(modeling|configuration)_.*\", full_statement):\n+                if re.search(\n+                    rf\"(transformers\\.models\\..|..)*\\.({self.match_patterns})_.*\", full_statement\n+                ):  # OR MATCH ..llama.modeling_llama\n                     return cst.RemoveFromParent()\n-                if parent_node not in self.all_imports:\n+                if updated_node not in self.all_imports:\n                     self.all_imports.append(updated_node)\n                 return updated_node\n             self.global_scope_index += 100\n-            if m.matches(updated_node, m.SimpleStatementLine(body=[m.Assign()])):\n-                # TODO This only works for single target assigns!\n-                node_name = updated_node.body[0].targets[0].target.value\n-            else:\n-                node_name = self.python_module.code_for_node(updated_node.body[0])\n-            self.new_body[node_name] = {\n-                \"insert_idx\": self.global_scope_index,\n-                \"node\": updated_node,\n-            }\n-            self.config_body = [updated_node]\n         return updated_node\n \n     def leave_ClassDef(self, original_node, updated_node):\n@@ -454,6 +603,7 @@ def leave_ClassDef(self, original_node, updated_node):\n         \"\"\"\n         class_name = original_node.name.value\n         bases = [k.value.value for k in original_node.bases if k.value.value in self.imported_mapping]\n+        all_bases = [k.value.value for k in original_node.bases]\n         self.global_scope_index += 100\n         for super_class in bases:\n             if super_class not in self.imported_mapping:\n@@ -469,7 +619,7 @@ def leave_ClassDef(self, original_node, updated_node):\n                 raise ValueError(\n                     f\"Tried parsing the name of the imported package from {super_file_name}, could not extract the model name\"\n                 )\n-\n+            file_type = re.search(r\"models?\\.\\w*?\\.(\\w*?)_\", super_file_name).groups()[0]\n             visited_module = self.visited_module\n             if super_file_name not in visited_module:  # only extract classes once\n                 class_finder = find_classes_in_file(\n@@ -490,89 +640,117 @@ def leave_ClassDef(self, original_node, updated_node):\n \n             list_dependencies = sorted(list_dependencies.items(), key=lambda x: x[1], reverse=True)\n             start_insert_idx = self.global_scope_index\n+            file_to_update = self.files[file_type]\n+            is_empty_node = self.python_module.code_for_node(original_node.body) == \"pass\\n\"\n             for dependency, _ in list_dependencies:\n+                # we can write to the correct body, using the source of the parent class\n                 node = class_finder.global_nodes.get(dependency, None)\n-                if node is not None and \"Config\" not in class_name:\n-                    if dependency not in self.new_body:\n+                if node is not None:\n+                    if dependency not in file_to_update:\n                         start_insert_idx -= 1\n-                        self.new_body[dependency] = {\"insert_idx\": start_insert_idx, \"node\": node}\n+                        file_to_update[dependency] = {\"insert_idx\": start_insert_idx, \"node\": node}\n                     elif dependency not in self.inserted_deps:\n                         # make sure the node is written after its dependencies\n-                        start_insert_idx = self.new_body[dependency][\"insert_idx\"] - 1\n+                        start_insert_idx = file_to_update[dependency][\"insert_idx\"] - 1\n+                        if (\n+                            dependency in file_to_update.keys()\n+                            and dependency in class_finder.first_lvl_dependency_mapping[class_name]\n+                        ):\n+                            # If dependency is defined, but not used, raise error\n+                            calls = m.findall(original_node, m.Call(func=m.Name(dependency)))\n+                            if not calls and not is_empty_node and dependency not in all_bases:\n+                                raise ValueError(\n+                                    f\"\"\"You defined `{dependency}` in the modular_{self.model_name}.py, it should be used\n+                                    when you define `{class_name}`, as it is one of it's direct dependencies. Make sure\n+                                    you use it in the `__init__` function.\"\"\"\n+                                )\n                     self.inserted_deps.append(dependency)\n+\n             if len(list_dependencies) > 0:\n                 updated_node = replace_call_to_super(class_finder, updated_node, class_name)\n-        if \"Config\" in class_name:\n-            self.config_body += [updated_node]\n+            else:\n+                raise ValueError(\n+                    f\"Unable to find dependencies for {super_class} in {super_file_name}. Here are the dependencies found: {class_finder.class_dependency_mapping}. (The automatic renaming might have gone wrong!)\"\n+                )\n+\n+        # Now, if a class was defined without parents, we look for the name\n+        match_pattern = \"|\".join(TYPE_TO_FILE_TYPE.keys())\n+        match = re.search(rf\"({match_pattern})$\", class_name)\n+        if match:\n+            key = TYPE_TO_FILE_TYPE[match.group(1)]\n+            self.files[key][class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n         else:\n-            self.new_body[class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n+            self.files[\"modeling\"][class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n         return updated_node\n \n     def leave_If(self, original_node, node):\n         parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n         if m.matches(parent_node, m.Module()):\n             full_statement = self.python_module.code_for_node(original_node.test)\n             if re.search(r\"[\\s\\S]*is_.*available\", full_statement):\n-                self.all_imports.append(node)\n+                self.all_safe_imports.append(node)\n             elif full_statement not in self.new_body:\n                 self.new_body[node] = {\"insert_idx\": self.global_scope_index, \"node\": node}\n         return node\n \n     def leave_Module(self, original_node: cst.Assign, node):\n         imports = {self.python_module.code_for_node(k): k for k in self.all_imports}\n-        dependency_imports = {}\n-        config_imports = []\n-        for visiter in self.visited_module.values():\n-            dependency_imports.update({self.python_module.code_for_node(k): k for k in visiter.imports.values()})\n-\n-        # manually clean up if it's importing a config from configuration file (ruff doesn't do that)\n-        config_imports = []\n-        for i in list(dependency_imports.values()):\n-            if (\n-                hasattr(i.body[0], \"module\")\n-                and isinstance(i.body[0].module, cst.Name)\n-                and f\"configuration_{self.model_name}\" in i.body[0].module.value\n-            ):\n-                pass\n-            else:\n-                config_imports.append(i)\n-\n-        if hasattr(self, \"config_body\"):\n-            self.config_body = list(imports.values()) + config_imports + self.config_body\n-        dependency_imports.update(imports)\n-        new_body = list(dependency_imports.values())\n-        if len(self.new_body.keys()) > 0:\n-            new_body += [k[1][\"node\"] for k in sorted(self.new_body.items(), key=lambda x: x[1][\"insert_idx\"])]\n+        dependency_imports = {file_type: imports.copy() for file_type in self.files}\n+        for super_file_name, visiter in self.visited_module.items():\n+            file_type = re.search(r\"models?\\.\\w*?\\.(\\w*?)_\", super_file_name).groups()[0]\n+            dependency_imports[file_type].update(\n+                {self.python_module.code_for_node(k): k for k in visiter.imports.values()}\n+            )\n+\n+        for file, body in self.files.items():\n+            new_body = [k[1][\"node\"] for k in sorted(body.items(), key=lambda x: x[1][\"insert_idx\"])]\n+            if len(new_body) > 0:\n+                if file in dependency_imports.keys():\n+                    new_body = list(dependency_imports[file].values()) + new_body\n+                self.files[file] = cst.Module(body=[*new_body], header=node.header)\n+        return node\n+\n+\n+def convert_modular_file(modular_file, old_model_name=None, new_model_name=None, cst_transformers=None):\n+    pattern = re.search(r\"modular_(.*)(?=\\.py$)\", modular_file)\n+    output = {}\n+    if pattern is not None:\n+        model_name = pattern.groups()[0]\n+        # Parse the Python file\n+        with open(modular_file, \"r\") as file:\n+            code = file.read()\n+        module = cst.parse_module(code)\n+        wrapper = MetadataWrapper(module)\n+        if cst_transformers is None:\n+            cst_transformers = ModularConverterTransformer(module, model_name, old_model_name, new_model_name)\n+        wrapper.visit(cst_transformers)\n+        for file, node in cst_transformers.files.items():\n+            if node != {}:\n+                ruffed_code = run_ruff(AUTO_GENERATED_MESSAGE + node.code, True)\n+                formatted_code = run_ruff(ruffed_code, False)\n+                output[file] = [formatted_code, ruffed_code]\n+        return output\n+    else:\n+        print(f\"modular pattern not found in {modular_file}, exiting\")\n+        return {}\n+\n+\n+def save_modeling_file(modular_file, converted_file):\n+    for file_type in converted_file.keys():\n+        non_comment_lines = len(\n+            [line for line in converted_file[file_type][0].strip().split(\"\\n\") if not line.strip().startswith(\"#\")]\n+        )\n+        if len(converted_file[file_type][0].strip()) > 0 and non_comment_lines > 0:\n+            with open(modular_file.replace(\"modular_\", f\"{file_type}_\"), \"w\") as f:\n+                f.write(converted_file[file_type][0])\n         else:\n-            new_body = []\n-        return node.with_changes(body=[*new_body])\n-\n-\n-def convert_file(diff_file, old_model_name=None, new_model_name=None, cst_transformers=None):\n-    model_name = re.search(r\"diff_(.*)(?=\\.py$)\", diff_file).groups()[0]\n-    # Parse the Python file\n-    with open(diff_file, \"r\") as file:\n-        code = file.read()\n-    module = cst.parse_module(code)\n-    wrapper = MetadataWrapper(module)\n-    if cst_transformers is None:\n-        cst_transformers = DiffConverterTransformer(module, model_name, old_model_name, new_model_name)\n-    new_mod = wrapper.visit(cst_transformers)\n-    ruffed_code = run_ruff(new_mod.code, True)\n-    formatted_code = run_ruff(ruffed_code, False)\n-    if len(formatted_code.strip()) > 0:\n-        with open(diff_file.replace(\"diff_\", \"modeling_\"), \"w\") as f:\n-            f.write(AUTO_GENERATED_MESSAGE + formatted_code)\n-\n-    if hasattr(cst_transformers, \"config_body\"):\n-        config_module = cst.Module(body=[*cst_transformers.config_body], header=new_mod.header)\n-        with open(diff_file.replace(\"diff_\", \"configuration_\"), \"w\") as f:\n-            ruffed_code = run_ruff(config_module.code, True)\n-            formatted_code = run_ruff(ruffed_code, False)\n-            f.write(AUTO_GENERATED_MESSAGE + formatted_code)\n-\n-    # TODO optimize by re-using the class_finder\n-    return cst_transformers\n+            non_comment_lines = len(\n+                [line for line in converted_file[file_type][0].strip().split(\"\\n\") if not line.strip().startswith(\"#\")]\n+            )\n+            if len(converted_file[file_type][1].strip()) > 0 and non_comment_lines > 0:\n+                logger.warning(\"The modeling code contains errors, it's written without formatting\")\n+                with open(modular_file.replace(\"modular_\", f\"{file_type}_\"), \"w\") as f:\n+                    f.write(converted_file[file_type][1])\n \n \n if __name__ == \"__main__\":\n@@ -581,22 +759,24 @@ def convert_file(diff_file, old_model_name=None, new_model_name=None, cst_transf\n         \"--files_to_parse\",\n         default=[\"all\"],\n         nargs=\"+\",\n-        help=\"A list of `diff_xxxx` files that should be converted to single model file\",\n+        help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )\n     parser.add_argument(\n         \"--old_model_name\",\n         required=False,\n-        help=\"The name of the model from which the copying is done in CamelCase. If not provided is inferred from diff-file\",\n+        help=\"The name of the model from which the copying is done in CamelCase. If not provided is inferred from modular-file\",\n     )\n     parser.add_argument(\n         \"--new_model_name\",\n         required=False,\n-        help=\"The name of the new model being added in CamelCase. If not provided is inferred from diff-file\",\n+        help=\"The name of the new model being added in CamelCase. If not provided is inferred from modular-file\",\n     )\n     args = parser.parse_args()\n     if args.files_to_parse == [\"all\"]:\n-        args.files_to_parse = glob.glob(\"src/transformers/models/**/diff_*.py\", recursive=True)\n-    for file_name in args.files_to_parse:\n+        args.files_to_parse = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n+\n+    for file_name in find_priority_list(args.files_to_parse):\n         print(f\"Converting {file_name} to a single model single file format\")\n         module_path = file_name.replace(\"/\", \".\").replace(\".py\", \"\").replace(\"src.\", \"\")\n-        converter = convert_file(file_name, args.old_model_name, args.new_model_name)\n+        converted_files = convert_modular_file(file_name, args.old_model_name, args.new_model_name)\n+        converter = save_modeling_file(file_name, converted_files)",
            "previous_filename": "utils/diff_model_converter.py"
        }
    ],
    "stats": {
        "total": 7282,
        "additions": 6504,
        "deletions": 778
    }
}