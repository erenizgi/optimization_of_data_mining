{
    "author": "ArthurZucker",
    "message": "fix block mask typing (#36661)\n\n* fix block mask typing\n\n* updated\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* gemma\n\n* fix\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "2829013d2d00e63d75a1f6f7a3f003bc60cc69af",
    "files": [
        {
            "sha": "5c4a6435072ab76791aa4b7c76a54ab3110823a9",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 70,
            "deletions": 69,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=2829013d2d00e63d75a1f6f7a3f003bc60cc69af",
            "patch": "@@ -1,16 +1,14 @@\n-- title: Get started\n-  sections:\n+- sections:\n   - local: index\n     title: Transformers\n   - local: installation\n     title: Installation\n   - local: quicktour\n     title: Quickstart\n-- title: Base classes\n-  isExpanded: False\n+  title: Get started\n+- isExpanded: false\n   sections:\n-  - title: Models\n-    sections:\n+  - sections:\n     - local: models\n       title: Loading models\n     - local: custom_models\n@@ -31,8 +29,8 @@\n       title: The Transformer model family\n     - local: attention\n       title: Attention mechanisms\n-  - title: Preprocessors\n-    sections:\n+    title: Models\n+  - sections:\n     - local: fast_tokenizers\n       title: Tokenizers\n     - local: image_processors\n@@ -47,11 +45,11 @@\n       title: Summary of the tokenizers\n     - local: pad_truncation\n       title: Padding and truncation\n-- title: Inference\n-  isExpanded: False\n+    title: Preprocessors\n+  title: Base classes\n+- isExpanded: false\n   sections:\n-  - title: Pipeline API\n-    sections:\n+  - sections:\n     - local: pipeline_tutorial\n       title: Pipeline\n     - local: pipeline_gradio\n@@ -60,8 +58,8 @@\n       title: Web server inference\n     - local: add_new_pipeline\n       title: Adding a new pipeline\n-  - title: LLMs\n-    sections:\n+    title: Pipeline API\n+  - sections:\n     - local: llm_tutorial\n       title: Text generation\n     - local: generation_strategies\n@@ -82,8 +80,8 @@\n       title: Getting the most out of LLMs\n     - local: perplexity\n       title: Perplexity of fixed-length models\n-  - title: Chat with models\n-    sections:\n+    title: LLMs\n+  - sections:\n     - local: conversations\n       title: Chat basics\n     - local: chat_templating\n@@ -94,8 +92,8 @@\n       title: Template writing\n     - local: chat_extras\n       title: Tools and RAG\n-  - title: Optimization\n-    sections:\n+    title: Chat with models\n+  - sections:\n     - local: perf_torch_compile\n       title: torch.compile\n     - local: perf_infer_gpu_one\n@@ -106,15 +104,15 @@\n       title: CPU\n     - local: tf_xla\n       title: XLA\n+    title: Optimization\n   - local: agents\n     title: Agents\n   - local: tools\n     title: Tools\n-- title: Training\n-  isExpanded: False\n+  title: Inference\n+- isExpanded: false\n   sections:\n-  - title: Trainer API\n-    sections:\n+  - sections:\n     - local: trainer\n       title: Trainer\n     - local: training\n@@ -123,8 +121,8 @@\n       title: Optimizers\n     - local: hpo_train\n       title: Hyperparameter search\n-  - title: Distributed training\n-    sections:\n+    title: Trainer API\n+  - sections:\n     - local: gpu_selection\n       title: GPU selection\n     - local: accelerate\n@@ -139,8 +137,8 @@\n       title: Distributed CPUs\n     - local: perf_train_gpu_many\n       title: Parallelism methods\n-  - title: Hardware\n-    sections:\n+    title: Distributed training\n+  - sections:\n     - local: perf_train_gpu_one\n       title: GPU\n     - local: perf_train_cpu\n@@ -151,12 +149,13 @@\n       title: Apple Silicon\n     - local: perf_hardware\n       title: Build your own machine\n+    title: Hardware\n   - local: peft\n     title: PEFT\n   - local: model_memory_anatomy\n     title: Model training anatomy\n-- title: Quantization\n-  isExpanded: False\n+  title: Training\n+- isExpanded: false\n   sections:\n   - local: quantization/overview\n     title: Overview\n@@ -196,8 +195,8 @@\n     title: VPTQ\n   - local: quantization/contribute\n     title: Contribute\n-- title: Export to production\n-  isExpanded: False\n+  title: Quantization\n+- isExpanded: false\n   sections:\n   - local: serialization\n     title: ONNX\n@@ -207,13 +206,11 @@\n     title: ExecuTorch\n   - local: torchscript\n     title: TorchScript\n-- title: Resources\n-  isExpanded: False\n+  title: Export to production\n+- isExpanded: false\n   sections:\n-  - title: Task recipes\n-    sections:\n-    - title: Natural language processing\n-      sections:\n+  - sections:\n+    - sections:\n       - local: tasks/sequence_classification\n         title: Text classification\n       - local: tasks/token_classification\n@@ -230,14 +227,14 @@\n         title: Summarization\n       - local: tasks/multiple_choice\n         title: Multiple choice\n-    - title: Audio\n-      sections:\n+      title: Natural language processing\n+    - sections:\n       - local: tasks/audio_classification\n         title: Audio classification\n       - local: tasks/asr\n         title: Automatic speech recognition\n-    - title: Computer vision\n-      sections:\n+      title: Audio\n+    - sections:\n       - local: tasks/image_classification\n         title: Image classification\n       - local: tasks/semantic_segmentation\n@@ -262,8 +259,8 @@\n         title: Keypoint detection\n       - local: tasks/knowledge_distillation_for_image_classification\n         title: Knowledge Distillation for Computer Vision\n-    - title: Multimodal\n-      sections:\n+      title: Computer vision\n+    - sections:\n       - local: tasks/image_captioning\n         title: Image captioning\n       - local: tasks/document_question_answering\n@@ -278,6 +275,8 @@\n         title: Image-text-to-text\n       - local: tasks/video_text_to_text\n         title: Video-text-to-text\n+      title: Multimodal\n+    title: Task recipes\n   - local: run_scripts\n     title: Training scripts\n   - local: glossary\n@@ -290,20 +289,19 @@\n     title: Community resources\n   - local: troubleshooting\n     title: Troubleshoot\n-- title: Contribute\n-  isExpanded: False\n+  title: Resources\n+- isExpanded: false\n   sections:\n   - local: contributing\n     title: Contribute to Transformers\n   - local: testing\n     title: Transformers model tests\n   - local: pr_checks\n     title: Pull request checks\n-- title: API\n-  isExpanded: False\n+  title: Contribute\n+- isExpanded: false\n   sections:\n-  - title: Main classes\n-    sections:\n+  - sections:\n     - local: main_classes/agent\n       title: Agents and Tools\n     - local: model_doc/auto\n@@ -350,10 +348,9 @@\n       title: Feature Extractor\n     - local: main_classes/image_processor\n       title: Image Processor\n-  - title: Models\n-    sections:\n-    - title: Text models\n-      sections:\n+    title: Main classes\n+  - sections:\n+    - sections:\n       - local: model_doc/albert\n         title: ALBERT\n       - local: model_doc/bamba\n@@ -662,8 +659,8 @@\n         title: Zamba\n       - local: model_doc/zamba2\n         title: Zamba2\n-    - title: Vision models\n-      sections:\n+      title: Text models\n+    - sections:\n       - local: model_doc/beit\n         title: BEiT\n       - local: model_doc/bit\n@@ -790,8 +787,8 @@\n         title: YOLOS\n       - local: model_doc/zoedepth\n         title: ZoeDepth\n-    - title: Audio models\n-      sections:\n+      title: Vision models\n+    - sections:\n       - local: model_doc/audio-spectrogram-transformer\n         title: Audio Spectrogram Transformer\n       - local: model_doc/bark\n@@ -860,16 +857,16 @@\n         title: XLS-R\n       - local: model_doc/xlsr_wav2vec2\n         title: XLSR-Wav2Vec2\n-    - title: Video models\n-      sections:\n+      title: Audio models\n+    - sections:\n       - local: model_doc/timesformer\n         title: TimeSformer\n       - local: model_doc/videomae\n         title: VideoMAE\n       - local: model_doc/vivit\n         title: ViViT\n-    - title: Multimodal models\n-      sections:\n+      title: Video models\n+    - sections:\n       - local: model_doc/align\n         title: ALIGN\n       - local: model_doc/altclip\n@@ -908,6 +905,8 @@\n         title: Emu3\n       - local: model_doc/flava\n         title: FLAVA\n+      - local: model_doc/gemma3\n+        title: Gemma3\n       - local: model_doc/git\n         title: GIT\n       - local: model_doc/got_ocr2\n@@ -1012,14 +1011,14 @@\n         title: VisualBERT\n       - local: model_doc/xclip\n         title: X-CLIP\n-    - title: Reinforcement learning models\n-      sections:\n+      title: Multimodal models\n+    - sections:\n       - local: model_doc/decision_transformer\n         title: Decision Transformer\n       - local: model_doc/trajectory_transformer\n         title: Trajectory Transformer\n-    - title: Time series models\n-      sections:\n+      title: Reinforcement learning models\n+    - sections:\n       - local: model_doc/autoformer\n         title: Autoformer\n       - local: model_doc/informer\n@@ -1030,12 +1029,13 @@\n         title: PatchTST\n       - local: model_doc/time_series_transformer\n         title: Time Series Transformer\n-    - title: Graph models\n-      sections:\n+      title: Time series models\n+    - sections:\n       - local: model_doc/graphormer\n         title: Graphormer\n-  - title: Internal helpers\n-    sections:\n+      title: Graph models\n+    title: Models\n+  - sections:\n     - local: internal/modeling_utils\n       title: Custom Layers and Utilities\n     - local: internal/pipelines_utils\n@@ -1054,4 +1054,5 @@\n       title: General Utilities\n     - local: internal/time_series_utils\n       title: Utilities for Time Series\n-      \n\\ No newline at end of file\n+    title: Internal helpers\n+  title: API"
        },
        {
            "sha": "b0a054998c8a3121fae1cbe9196475b92cbc9a16",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=2829013d2d00e63d75a1f6f7a3f003bc60cc69af",
            "patch": "@@ -71,7 +71,7 @@ def __call__(self):\n         return self._compiled_flex_attention\n \n \n-def make_flex_block_causal_mask(attention_mask_2d: torch.Tensor) -> BlockMask:\n+def make_flex_block_causal_mask(attention_mask_2d: torch.Tensor) -> \"BlockMask\":\n     \"\"\"\n     Create a block causal document mask for a batch of sequences, both packed and unpacked.\n     Create Block causal logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.\n@@ -149,7 +149,7 @@ def flex_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Union[torch.Tensor, BlockMask],\n+    attention_mask: Union[torch.Tensor, \"BlockMask\"],\n     scaling: Optional[float] = None,\n     softcap: Optional[float] = None,\n     head_mask: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "c4cf20c060eed03b6a8aefa6d41003a5a6bed2d1",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=2829013d2d00e63d75a1f6f7a3f003bc60cc69af",
            "patch": "@@ -849,13 +849,13 @@ def _load_state_dict_into_meta_model(\n     is_quantized = hf_quantizer is not None\n \n     for serialized_param_name, empty_param in state_dict.items():\n-        if serialized_param_name not in expected_keys:\n-            continue\n-\n         # serialized_param_name is the raw, serialized name\n         # fixed_param_name is the model's equivalent\n         fixed_param_name, _ = model.rename_key(serialized_param_name)\n \n+        if fixed_param_name not in expected_keys:\n+            continue\n+\n         # we need to use serialized_param_name as file pointer is untouched\n         if shard_file.endswith(\".safetensors\"):\n             param = file_pointer.get_slice(serialized_param_name)"
        },
        {
            "sha": "500910404b04f4d4930911a54f1c3ad3c66a87c8",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2829013d2d00e63d75a1f6f7a3f003bc60cc69af/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=2829013d2d00e63d75a1f6f7a3f003bc60cc69af",
            "patch": "@@ -845,7 +845,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        }
    ],
    "stats": {
        "total": 151,
        "additions": 76,
        "deletions": 75
    }
}