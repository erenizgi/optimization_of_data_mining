{
    "author": "yonigozlan",
    "message": "Change Paligemma import logging to work with modular  (#34211)\n\n* change import logging\r\n\r\n* fix CI",
    "sha": "24bdc94da5d915afc8c766f8da8acdd509d808ff",
    "files": [
        {
            "sha": "a458c02a6feda7205429e9df130557a0f2689ec3",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 36,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/24bdc94da5d915afc8c766f8da8acdd509d808ff/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24bdc94da5d915afc8c766f8da8acdd509d808ff/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=24bdc94da5d915afc8c766f8da8acdd509d808ff",
            "patch": "@@ -25,7 +25,6 @@\n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -921,6 +920,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -1071,18 +1071,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1186,27 +1175,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1289,8 +1259,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "77103a4eabbaf0872c719d3de4bd060eaa13c6f3",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24bdc94da5d915afc8c766f8da8acdd509d808ff/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24bdc94da5d915afc8c766f8da8acdd509d808ff/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=24bdc94da5d915afc8c766f8da8acdd509d808ff",
            "patch": "@@ -16,7 +16,6 @@\n Processor class for PaliGemma.\n \"\"\"\n \n-import logging\n from typing import List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n@@ -34,9 +33,10 @@\n     PreTokenizedInput,\n     TextInput,\n )\n+from ...utils import logging\n \n \n-logger = logging.getLogger(__name__)\n+logger = logging.get_logger(__name__)\n \n IMAGE_TOKEN = \"<image>\"\n EXTRA_TOKENS = [f\"<loc{i:0>4}>\" for i in range(1024)] + [f\"<seg{i:0>3}>\" for i in range(128)]"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 7,
        "deletions": 38
    }
}