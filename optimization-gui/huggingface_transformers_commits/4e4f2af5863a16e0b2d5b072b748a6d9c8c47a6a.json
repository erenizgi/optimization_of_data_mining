{
    "author": "zheliuyu",
    "message": "Add conditional checks to _check_and_adjust_attn_implementation() (#41542)",
    "sha": "4e4f2af5863a16e0b2d5b072b748a6d9c8c47a6a",
    "files": [
        {
            "sha": "ab4dbed802bd159945ebb4a8228c5c4f74562d45",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e4f2af5863a16e0b2d5b072b748a6d9c8c47a6a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e4f2af5863a16e0b2d5b072b748a6d9c8c47a6a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4e4f2af5863a16e0b2d5b072b748a6d9c8c47a6a",
            "patch": "@@ -2499,6 +2499,7 @@ def _check_and_adjust_attn_implementation(\n             and self._supports_flash_attn\n             and not (is_flash_attn_2_available() or is_flash_attn_3_available())\n             and is_kernels_available()\n+            and not is_torch_npu_available()\n         ):\n             if attn_implementation.endswith(\"2\"):\n                 applicable_attn_implementation = \"kernels-community/flash-attn\""
        }
    ],
    "stats": {
        "total": 1,
        "additions": 1,
        "deletions": 0
    }
}