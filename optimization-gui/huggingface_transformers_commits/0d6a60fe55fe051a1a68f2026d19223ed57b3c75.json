{
    "author": "MinJu-Ha",
    "message": "ğŸŒ [i18n-KO] Translated `qwen2_vl.md` to Korean (#36750)\n\n* fix: manual edits\n\n* fix: resolve suggestions\n\n* Update toctree.yml",
    "sha": "0d6a60fe55fe051a1a68f2026d19223ed57b3c75",
    "files": [
        {
            "sha": "353d616b78b2ba9f24498d998ffd805227cb0bc4",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d6a60fe55fe051a1a68f2026d19223ed57b3c75/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d6a60fe55fe051a1a68f2026d19223ed57b3c75/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=0d6a60fe55fe051a1a68f2026d19223ed57b3c75",
            "patch": "@@ -720,6 +720,8 @@\n         title: (ë²ˆì—­ì¤‘) Perceiver\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Pix2Struct\n+      - local: model_doc/qwen2_vl\n+        title: Qwen2VL\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Segment Anything\n       - local: in_translation"
        },
        {
            "sha": "fb4ed27391e015918fa90b836b17ffa6a40369ea",
            "filename": "docs/source/ko/model_doc/qwen2_vl.md",
            "status": "added",
            "additions": 303,
            "deletions": 0,
            "changes": 303,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d6a60fe55fe051a1a68f2026d19223ed57b3c75/docs%2Fsource%2Fko%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d6a60fe55fe051a1a68f2026d19223ed57b3c75/docs%2Fsource%2Fko%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fqwen2_vl.md?ref=0d6a60fe55fe051a1a68f2026d19223ed57b3c75",
            "patch": "@@ -0,0 +1,303 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Qwen2-VL[[Qwen2-VL]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+</div>\n+\n+## Overview[[Overview]]\n+\n+[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/) ëª¨ë¸ì€ ì•Œë¦¬ë°”ë°” ë¦¬ì„œì¹˜ì˜ QweníŒ€ì—ì„œ ê°œë°œí•œ [Qwen-VL](https://arxiv.org/pdf/2308.12966) ëª¨ë¸ì˜ ì£¼ìš” ì—…ë°ì´íŠ¸ ë²„ì „ì…ë‹ˆë‹¤.\n+\n+ë¸”ë¡œê·¸ì˜ ìš”ì•½ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ì´ ë¸”ë¡œê·¸ëŠ” ì§€ë‚œ ëª‡ ë…„ê°„ Qwen-VLì—ì„œ ì¤‘ëŒ€í•œ ê°œì„ ì„ ê±°ì³ ë°œì „ëœ Qwen2-VL ëª¨ë¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì¤‘ìš” ê°œì„  ì‚¬í•­ì€ í–¥ìƒëœ ì´ë¯¸ì§€ ì´í•´, ê³ ê¸‰ ë¹„ë””ì˜¤ ì´í•´, í†µí•© ì‹œê° ì—ì´ì „íŠ¸ ê¸°ëŠ¥, í™•ì¥ëœ ë‹¤ì–¸ì–´ ì§€ì›ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” Naive Dynamic Resolution ì§€ì›ì„ í†µí•´ ì„ì˜ì˜ ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”ë˜ì—ˆìœ¼ë©°, ë©€í‹°ëª¨ë‹¬ íšŒì „ ìœ„ì¹˜ ì„ë² ë”©(M-ROPE)ì„ í™œìš©í•˜ì—¬ 1D í…ìŠ¤íŠ¸ì™€ ë‹¤ì°¨ì› ì‹œê° ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ì€ ì‹œê° ê´€ë ¨ ì‘ì—…ì—ì„œ GPT-4oì™€ Claude 3.5 Sonnet ê°™ì€ ì„ ë„ì ì¸ AI ì‹œìŠ¤í…œê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, í…ìŠ¤íŠ¸ ëŠ¥ë ¥ì—ì„œëŠ” ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì¤‘ ìƒìœ„ê¶Œì— ë­í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œì „ì€ Qwen2-VLì„ ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë° ì¶”ë¡  ëŠ¥ë ¥ì´ í•„ìš”í•œ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì¬ë‹¤ëŠ¥í•œ ë„êµ¬ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/qwen2_vl_architecture.jpeg\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> Qwen2-VL êµ¬ì¡°. ì¶œì²˜: <a href=\"https://qwenlm.github.io/blog/qwen2-vl/\">ë¸”ë¡œê·¸ ê²Œì‹œê¸€</a> </small>\n+\n+ì´ ëª¨ë¸ì€ [simonJJJ](https://huggingface.co/simonJJJ)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© ì˜ˆì‹œ[[Usage example]]\n+\n+### ë‹¨ì¼ ë¯¸ë””ì–´ ì¶”ë¡ [[Single Media inference]]\n+\n+ì´ ëª¨ë¸ì€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ëª¨ë‘ ì¸í’‹ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ì¶”ë¡ ì„ ìœ„í•œ ì˜ˆì œ ì½”ë“œì…ë‹ˆë‹¤.\n+\n+```python\n+\n+import torch\n+from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n+\n+# ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜ì—ì„œ ëª¨ë¸ì„ ë°˜ ì •ë°€ë„(half-precision)ë¡œ ë¡œë“œ\n+model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n+\n+\n+conversation = [\n+    {\n+        \"role\":\"user\",\n+        \"content\":[\n+            {\n+                \"type\":\"image\",\n+                \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n+            },\n+            {\n+                \"type\":\"text\",\n+                \"text\":\"Describe this image.\"\n+            }\n+        ]\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n+# ì¶”ë¡ : ì•„ì›ƒí’‹ ìƒì„±\n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+print(output_text)\n+\n+\n+\n+# ë¹„ë””ì˜¤\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n+            {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    video_fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n+\n+# ì¶”ë¡ : ì•„ì›ƒí’‹ ìƒì„±\n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+print(output_text)\n+```\n+\n+### ë°°ì¹˜ í˜¼í•© ë¯¸ë””ì–´ ì¶”ë¡ [[Batch Mixed Media Inference]]\n+\n+ì´ ëª¨ë¸ì€ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ í˜¼í•©í•˜ì—¬ ë°°ì¹˜ ì…ë ¥ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ì˜ˆì œì…ë‹ˆë‹¤.\n+\n+```python\n+\n+# ì²«ë²ˆì§¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ëŒ€í™”\n+conversation1 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"path\": \"/path/to/image1.jpg\"},\n+            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n+        ]\n+    }\n+]\n+\n+# ë‘ ê°œì˜ ì´ë¯¸ì§€ì— ëŒ€í•œ ëŒ€í™”\n+conversation2 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"path\": \"/path/to/image2.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n+        ]\n+    }\n+]\n+\n+# ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì´ë£¨ì–´ì§„ ëŒ€í™”\n+conversation3 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": \"who are you?\"\n+    }\n+]\n+\n+\n+# í˜¼í•©ëœ ë¯¸ë””ì–´ë¡œ ì´ë£¨ì–´ì§„ ëŒ€í™”\n+conversation4 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image4.jpg\"},\n+            {\"type\": \"video\", \"path\": \"/path/to/video.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What are the common elements in these medias?\"},\n+        ],\n+    }\n+]\n+\n+conversations = [conversation1, conversation2, conversation3, conversation4]\n+# ë°°ì¹˜ ì¶”ë¡ ì„ ìœ„í•œ ì¤€ë¹„\n+ipnuts = processor.apply_chat_template(\n+    conversations,\n+    video_fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n+\n+# ë°°ì¹˜ ì¶”ë¡ \n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+print(output_text)\n+```\n+\n+### ì‚¬ìš© íŒ[[Usage Tips]]\n+\n+#### ì´ë¯¸ì§€ í•´ìƒë„ íŠ¸ë ˆì´ë“œì˜¤í”„[[Image Resolution trade-off]]\n+\n+ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì…ë ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. ë””í´íŠ¸ë¡œ ì…ë ¥ì— ëŒ€í•´ ë„¤ì´í‹°ë¸Œ(native) í•´ìƒë„ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë” ë†’ì€ í•´ìƒë„ë¥¼ ì ìš©í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì´ëŠ” ë” ë§ì€ ì—°ì‚° ë¹„ìš©ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ìµœì ì˜ ì„¤ì •ì„ ìœ„í•´ ìµœì†Œ ë° ìµœëŒ€ í”½ì…€ ìˆ˜ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```python\n+min_pixels = 224*224\n+max_pixels = 2048*2048\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n+```\n+\n+ì œí•œëœ GPU RAMì˜ ê²½ìš°, ë‹¤ìŒê³¼ ê°™ì´ í•´ìƒë„ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```python\n+min_pixels = 256*28*28\n+max_pixels = 1024*28*28 \n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n+```\n+ì´ë ‡ê²Œ í•˜ë©´ ê° ì´ë¯¸ì§€ê°€ 256~1024ê°œì˜ í† í°ìœ¼ë¡œ ì¸ì½”ë”©ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ 28ì€ ëª¨ë¸ì´ 14 í¬ê¸°ì˜ íŒ¨ì¹˜(patch)ì™€ 2ì˜ ì‹œê°„ íŒ¨ì¹˜(temporal patch size)ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‚˜ì˜¨ ê°’ì…ë‹ˆë‹¤ (14 Ã— 2 = 28).\n+\n+\n+#### ë‹¤ì¤‘ ì´ë¯¸ì§€ ì¸í’‹[[Multiple Image Inputs]]\n+\n+ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì½˜í…ì¸ ëŠ” ëŒ€í™”ì— ì§ì ‘ í¬í•¨ë©ë‹ˆë‹¤. ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ë•ŒëŠ” ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ì— ë¼ë²¨ì„ ì¶”ê°€í•˜ë©´ ì°¸ì¡°í•˜ê¸°ê°€ ë” ì‰¬ì›Œì§‘ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ë‹¤ìŒ ì„¤ì •ì„ í†µí•´ ì´ ë™ì‘ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```python\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"}, \n+            {\"type\": \"text\", \"text\": \"Hello, how are you?\"}\n+        ]\n+    },\n+    {\n+        \"role\": \"assistant\",\n+        \"content\": \"I'm doing well, thank you for asking. How can I assist you today?\"\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"Can you describe these images and video?\"}, \n+            {\"type\": \"image\"}, \n+            {\"type\": \"image\"}, \n+            {\"type\": \"video\"}, \n+            {\"type\": \"text\", \"text\": \"These are from my vacation.\"}\n+        ]\n+    },\n+    {\n+        \"role\": \"assistant\",\n+        \"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\"\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\"\n+    }\n+]\n+\n+# ë””í´íŠ¸:\n+prompt_without_id = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+# ì˜ˆìƒ ì•„ì›ƒí’‹: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n+\n+\n+# id ì¶”ê°€\n+prompt_with_id = processor.apply_chat_template(conversation, add_generation_prompt=True, add_vision_id=True)\n+# ì˜ˆìƒ ì•„ì›ƒí’‹: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n+\n+```\n+\n+#### ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•œ Flash-Attention 2[[Flash-Attention 2 to speed up generation]]\n+\n+ì²«ë²ˆì§¸ë¡œ, Flash Attention 2ì˜ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:\n+\n+```bash\n+pip install -U flash-attn --no-build-isolation\n+```\n+\n+ë˜í•œ, Flash-Attention 2ë¥¼ ì§€ì›í•˜ëŠ” í•˜ë“œì›¨ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œì¸ [flash attention repository](https://github.com/Dao-AILab/flash-attention)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. FlashAttention-2ëŠ” ëª¨ë¸ì´ `torch.float16` ë˜ëŠ” `torch.bfloat16` í˜•ì‹ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+Flash Attention-2ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ `attn_implementation=\"flash_attention_2\"` ì˜µì…˜ì„ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤:\n+\n+```python\n+from transformers import Qwen2VLForConditionalGeneration\n+\n+model = Qwen2VLForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen2-VL-7B-Instruct\", \n+    torch_dtype=torch.bfloat16, \n+    attn_implementation=\"flash_attention_2\",\n+)\n+```\n+\n+## Qwen2VLConfig\n+\n+[[autodoc]] Qwen2VLConfig\n+\n+## Qwen2VLImageProcessor\n+\n+[[autodoc]] Qwen2VLImageProcessor\n+    - preprocess\n+\n+## Qwen2VLImageProcessorFast\n+\n+[[autodoc]] Qwen2VLImageProcessorFast\n+    - preprocess\n+\n+## Qwen2VLProcessor\n+\n+[[autodoc]] Qwen2VLProcessor\n+\n+## Qwen2VLModel\n+\n+[[autodoc]] Qwen2VLModel\n+    - forward\n+\n+## Qwen2VLForConditionalGeneration\n+\n+[[autodoc]] Qwen2VLForConditionalGeneration\n+    - forward"
        }
    ],
    "stats": {
        "total": 305,
        "additions": 305,
        "deletions": 0
    }
}