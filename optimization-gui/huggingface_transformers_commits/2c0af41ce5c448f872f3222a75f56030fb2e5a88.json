{
    "author": "ebezzam",
    "message": "Fix bad markdown links (#39819)\n\nFix bad markdown links.",
    "sha": "2c0af41ce5c448f872f3222a75f56030fb2e5a88",
    "files": [
        {
            "sha": "9ddf0b59c900b3e56b7c0122d1c8867c4fbbc33e",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -13,7 +13,7 @@\n \n ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³Ù†Ø³ØªØ¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„ØªÙØ­Ø³ÙÙ‘Ù† Ù…Ù† ÙƒÙØ§Ø¡Ø© Ù†Ø´Ø± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©:\n \n-1. Ø³Ù†ØªÙ†Ø§ÙˆÙ„ ØªÙ‚Ù†ÙŠØ© \"Ø¯Ù‚Ø© Ø£Ù‚Ù„\" Ø§Ù„ØªÙŠ Ø£Ø«Ø¨ØªØª Ø§Ù„Ø£Ø¨Ø­Ø§Ø« ÙØ¹Ø§Ù„ÙŠØªÙ‡Ø§ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ù…Ø²Ø§ÙŠØ§ Ø­Ø³Ø§Ø¨ÙŠØ© Ø¯ÙˆÙ† Ø§Ù„ØªØ£Ø«ÙŠØ± Ø¨Ø´ÙƒÙ„ Ù…Ù„Ø­ÙˆØ¸ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø¯Ù‚Ø© Ø±Ù‚Ù…ÙŠØ© Ø£Ù‚Ù„ [8 Ø¨Øª Ùˆ4 Ø¨Øª](/main_classes/quantization.md).\n+1. Ø³Ù†ØªÙ†Ø§ÙˆÙ„ ØªÙ‚Ù†ÙŠØ© \"Ø¯Ù‚Ø© Ø£Ù‚Ù„\" Ø§Ù„ØªÙŠ Ø£Ø«Ø¨ØªØª Ø§Ù„Ø£Ø¨Ø­Ø§Ø« ÙØ¹Ø§Ù„ÙŠØªÙ‡Ø§ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ù…Ø²Ø§ÙŠØ§ Ø­Ø³Ø§Ø¨ÙŠØ© Ø¯ÙˆÙ† Ø§Ù„ØªØ£Ø«ÙŠØ± Ø¨Ø´ÙƒÙ„ Ù…Ù„Ø­ÙˆØ¸ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø¯Ù‚Ø© Ø±Ù‚Ù…ÙŠØ© Ø£Ù‚Ù„ [8 Ø¨Øª Ùˆ4 Ø¨Øª](/main_classes/quantization).\n \n 2.  **Ø§Flash Attention:** Ø¥Ù† Flash Attention ÙˆÙ‡ÙŠ Ù†Ø³Ø®Ø© Ù…ÙØ¹Ø¯ÙÙ‘Ù„Ø© Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙˆÙØ± ÙÙ‚Ø· Ù†Ù‡Ø¬Ù‹Ø§ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙˆÙ„ÙƒÙ†Ù‡Ø§ ØªØ­Ù‚Ù‚ Ø£ÙŠØ¶Ù‹Ø§ ÙƒÙØ§Ø¡Ø© Ù…ØªØ²Ø§ÙŠØ¯Ø© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ø°Ø§ÙƒØ±Ø© GPU.\n "
        },
        {
            "sha": "4d416ef8fb407319ccd283fde8900b280a2dbaa1",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -27,7 +27,7 @@ This guide shows you how to quickly start chatting with Transformers from the co\n \n ## chat CLI\n \n-After you've [installed Transformers](./installation.md), chat with a model directly from the command line as shown below. It launches an interactive session with a model, with a few base commands listed at the start of the session.\n+After you've [installed Transformers](./installation), chat with a model directly from the command line as shown below. It launches an interactive session with a model, with a few base commands listed at the start of the session.\n \n ```bash\n transformers chat Qwen/Qwen2.5-0.5B-Instruct\n@@ -158,4 +158,4 @@ The easiest solution for improving generation speed is to either quantize a mode\n You can also try techniques like [speculative decoding](./generation_strategies#speculative-decoding), where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token per `forward` pass. This significantly alleviates the bandwidth bottleneck and improves generation speed.\n \n > [!TIP]\n-> Parameters may not be active for every generated token in MoE models such as [Mixtral](./model_doc/mixtral), [Qwen2MoE](./model_doc/qwen2_moe.md), and [DBRX](./model_doc/dbrx). As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because parameters become activated with each new speculated token.\n+> Parameters may not be active for every generated token in MoE models such as [Mixtral](./model_doc/mixtral), [Qwen2MoE](./model_doc/qwen2_moe), and [DBRX](./model_doc/dbrx). As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because parameters become activated with each new speculated token."
        },
        {
            "sha": "1f6e166d8847cf7c1469473e76104a89488466bf",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -148,9 +148,9 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n | Option name | Type | Simplified description |\n |---|---|---|\n | `max_new_tokens` | `int` | Controls the maximum generation length. Be sure to define it, as it usually defaults to a small value. |\n-| `do_sample` | `bool` | Defines whether generation will sample the next token (`True`), or is greedy instead (`False`). Most use cases should set this flag to `True`. Check [this guide](./generation_strategies.md) for more information. |\n+| `do_sample` | `bool` | Defines whether generation will sample the next token (`True`), or is greedy instead (`False`). Most use cases should set this flag to `True`. Check [this guide](./generation_strategies) for more information. |\n | `temperature` | `float` | How unpredictable the next selected token will be. High values (`>0.8`) are good for creative tasks, low values (e.g. `<0.4`) for tasks that require \"thinking\". Requires `do_sample=True`. |\n-| `num_beams` | `int` | When set to `>1`, activates the beam search algorithm. Beam search is good on input-grounded tasks. Check [this guide](./generation_strategies.md) for more information. |\n+| `num_beams` | `int` | When set to `>1`, activates the beam search algorithm. Beam search is good on input-grounded tasks. Check [this guide](./generation_strategies) for more information. |\n | `repetition_penalty` | `float` | Set it to `>1.0` if you're seeing the model repeat itself often. Larger values apply a larger penalty. |\n | `eos_token_id` | `list[int]` | The token(s) that will cause generation to stop. The default value is usually good, but you can specify a different token. |\n "
        },
        {
            "sha": "5f48adb5b8984a1876ba326df51480ce86039947",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -23,7 +23,7 @@ The crux of these challenges lies in augmenting the computational and memory cap\n \n In this guide, we will go over the effective techniques for efficient LLM deployment:\n \n-1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.\n+1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization) can achieve computational advantages without a considerable decline in model performance.\n \n 2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\n "
        },
        {
            "sha": "e9c4db5ef0dad026ab6eb144840b0349e4a286e9",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -95,7 +95,7 @@ images = [\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-The example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to int4.\n \n ```python\n import requests"
        },
        {
            "sha": "654a6129a3d1127d86507618b40bfb402ab2d6a1",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -99,7 +99,7 @@ images = [\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-The example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to int4.\n \n ```python\n import requests"
        },
        {
            "sha": "0abd7a73a511f8b86a61f3748d3a80799d56b638",
            "filename": "docs/source/en/model_doc/csm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -21,7 +21,7 @@ rendered properly in your Markdown viewer.\n The Conversational Speech Model (CSM) is the first open-source contextual text-to-speech model [released by Sesame](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice). It is designed to generate natural-sounding speech with or without conversational context. This context typically consists of multi-turn dialogue between speakers, represented as sequences of text and corresponding spoken audio.\n \n **Model Architecture:**\n-CSM is composed of two LLaMA-style auto-regressive transformer decoders: a backbone decoder that predicts the first codebook token and a depth decoder that generates the remaining tokens. It uses the pretrained codec model [Mimi](./mimi.md), introduced by Kyutai, to encode speech into discrete codebook tokens and decode them back into audio.\n+CSM is composed of two LLaMA-style auto-regressive transformer decoders: a backbone decoder that predicts the first codebook token and a depth decoder that generates the remaining tokens. It uses the pretrained codec model [Mimi](./mimi), introduced by Kyutai, to encode speech into discrete codebook tokens and decode them back into audio.\n \n The original csm-1b checkpoint is available under the [Sesame](https://huggingface.co/sesame/csm-1b) organization on Hugging Face.\n "
        },
        {
            "sha": "07a241efe9f0955cee5791c3c92f41e73eaab9f6",
            "filename": "docs/source/en/model_doc/dia.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -26,14 +26,14 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-Dia is an opensource text-to-speech (TTS) model (1.6B parameters) developed by [Nari Labs](https://huggingface.co/nari-labs).\n-It can generate highly realistic dialogue from transcript including nonverbal communications such as laughter and coughing.\n+Dia is an open-source text-to-speech (TTS) model (1.6B parameters) developed by [Nari Labs](https://huggingface.co/nari-labs).\n+It can generate highly realistic dialogue from transcript including non-verbal communications such as laughter and coughing.\n Furthermore, emotion and tone control is also possible via audio conditioning (voice cloning).\n \n **Model Architecture:**\n Dia is an encoder-decoder transformer based on the original transformer architecture. However, some more modern features such as\n rotational positional embeddings (RoPE) are also included. For its text portion (encoder), a byte tokenizer is utilized while\n-for the audio portion (decoder), a pretrained codec model [DAC](./dac.md) is used - DAC encodes speech into discrete codebook\n+for the audio portion (decoder), a pretrained codec model [DAC](./dac) is used - DAC encodes speech into discrete codebook\n tokens and decodes them back into audio.\n \n ## Usage Tips"
        },
        {
            "sha": "4d076c95d40e142b18af2f7ca597582019e61fe0",
            "filename": "docs/source/en/model_doc/ernie.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ERNIE (Enhanced Representation through kNowledge IntEgration) is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking.\n \n-Other ERNIE models released by baidu can be found at [Ernie 4.5](./ernie4_5.md), and [Ernie 4.5 MoE](./ernie4_5_moe.md).\n+Other ERNIE models released by baidu can be found at [Ernie 4.5](./ernie4_5), and [Ernie 4.5 MoE](./ernie4_5_moe).\n \n > [!TIP]\n > This model was contributed by [nghuyong](https://huggingface.co/nghuyong), and the official code can be found in [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) (in PaddlePaddle)."
        },
        {
            "sha": "c9f4c043569c510ba7dd083f514a70833eb2fae9",
            "filename": "docs/source/en/model_doc/ernie4_5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -29,9 +29,9 @@ rendered properly in your Markdown viewer.\n \n The Ernie 4.5 model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\n This family of models contains multiple different architectures and model sizes. This model in specific targets the base text\n-model without mixture of experts (moe) with 0.3B parameters in total. It uses the standard [Llama](./llama.md) at its core.\n+model without mixture of experts (moe) with 0.3B parameters in total. It uses the standard [Llama](./llama) at its core.\n \n-Other models from the family can be found at [Ernie 4.5 Moe](./ernie4_5_moe.md).\n+Other models from the family can be found at [Ernie 4.5 Moe](./ernie4_5_moe).\n \n <div class=\"flex justify-center\">\n     <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>"
        },
        {
            "sha": "f16cfb1f92e8f6fcd9fa96f21823f6526509c361",
            "filename": "docs/source/en/model_doc/ernie4_5_moe.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -30,10 +30,10 @@ rendered properly in your Markdown viewer.\n The Ernie 4.5 Moe model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\n This family of models contains multiple different architectures and model sizes. This model in specific targets the base text\n model with mixture of experts (moe) - one with 21B total, 3B active parameters and another one with 300B total, 47B active parameters.\n-It uses the standard [Llama](./llama.md) at its core combined with a specialized MoE based on [Mixtral](./mixtral.md) with additional shared\n+It uses the standard [Llama](./llama) at its core combined with a specialized MoE based on [Mixtral](./mixtral) with additional shared\n experts.\n \n-Other models from the family can be found at [Ernie 4.5](./ernie4_5.md).\n+Other models from the family can be found at [Ernie 4.5](./ernie4_5).\n \n <div class=\"flex justify-center\">\n     <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>"
        },
        {
            "sha": "803940b6f2073294eb899158b32d3ad8c538b34d",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -30,7 +30,7 @@ Gemma3n is a multimodal model with pretrained and instruction-tuned variants, av\n large portions of the language model architecture are shared with prior Gemma releases, there are many new additions in\n this model, including [Alternating Updates][altup] (AltUp), [Learned Augmented Residual Layer][laurel] (LAuReL),\n [MatFormer][matformer], Per-Layer Embeddings (PLE), [Activation Sparsity with Statistical Top-k][spark-transformer], and KV cache sharing. The language model uses\n-a similar attention pattern to [Gemma 3](./gemma3.md) with alternating 4 local sliding window self-attention layers for\n+a similar attention pattern to [Gemma 3](./gemma3) with alternating 4 local sliding window self-attention layers for\n every global self-attention layer with a maximum context length of 32k tokens. Gemma 3n introduces\n [MobileNet v5][mobilenetv5] as the vision encoder, using a default resolution of 768x768 pixels, and adds a newly\n trained audio encoder based on the [Universal Speech Model][usm] (USM) architecture."
        },
        {
            "sha": "58e0dd0ecb5ad2edf0d02fb6856991cecca77776",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -169,9 +169,9 @@ model = Idefics2ForConditionalGeneration.from_pretrained(\n \n ## Shrinking down Idefics2 using quantization\n \n-As the Idefics2 model has 8 billion parameters, that would require about 16GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization.md). If the model is quantized to 4 bits (or half a byte per parameter), that requires only about 3.5GB of RAM.\n+As the Idefics2 model has 8 billion parameters, that would require about 16GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization). If the model is quantized to 4 bits (or half a byte per parameter), that requires only about 3.5GB of RAM.\n \n-Quantizing a model is as simple as passing a `quantization_config` to the model. One can change the code snippet above with the changes below. We'll leverage the BitsAndyBytes quantization (but refer to [this page](../quantization.md) for other quantization methods):\n+Quantizing a model is as simple as passing a `quantization_config` to the model. One can change the code snippet above with the changes below. We'll leverage the BitsAndyBytes quantization (but refer to [this page](../quantization) for other quantization methods):\n \n ```diff\n + from transformers import BitsAndBytesConfig\n@@ -193,7 +193,7 @@ model = Idefics2ForConditionalGeneration.from_pretrained(\n \n A list of official Hugging Face and community (indicated by ğŸŒ) resources to help you get started with Idefics2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n \n-- A notebook on how to fine-tune Idefics2 on a custom dataset using the [Trainer](../main_classes/trainer.md) can be found [here](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing). It supports both full fine-tuning as well as (quantized) LoRa.\n+- A notebook on how to fine-tune Idefics2 on a custom dataset using the [Trainer](../main_classes/trainer) can be found [here](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing). It supports both full fine-tuning as well as (quantized) LoRa.\n - A script regarding how to fine-tune Idefics2 using the TRL library can be found [here](https://gist.github.com/edbeeching/228652fc6c2b29a1641be5a5778223cb).\n - Demo notebook regarding fine-tuning Idefics2 for JSON extraction use cases can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Idefics2). ğŸŒ\n "
        },
        {
            "sha": "da4697fae875e7067a68e47a727f33de0358c2fc",
            "filename": "docs/source/en/model_doc/mimi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -30,7 +30,7 @@ The abstract from the paper is the following:\n \n *We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaningâ€” such as emotion or non-speech soundsâ€” is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this â€œInner Monologueâ€ method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.* \n \n-Its architecture is based on [Encodec](model_doc/encodec) with several major differences:\n+Its architecture is based on [Encodec](./encodec) with several major differences:\n * it uses a much lower frame-rate.\n * it uses additional transformers for encoding and decoding for better latent contextualization\n * it uses a different quantization scheme: one codebook is dedicated to semantic projection."
        },
        {
            "sha": "258f3ff30382567e75a68bef8ede9fefb42fd93a",
            "filename": "docs/source/en/model_doc/minimax.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -115,9 +115,9 @@ The Flash Attention-2 model uses also a more memory efficient cache slicing mech\n \n ## Shrinking down MiniMax using quantization\n \n-As the MiniMax model has 456 billion parameters, that would require about 912GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization.md). If the model is quantized to 4 bits (or half a byte per parameter), about 228 GB of RAM is required.\n+As the MiniMax model has 456 billion parameters, that would require about 912GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization). If the model is quantized to 4 bits (or half a byte per parameter), about 228 GB of RAM is required.\n \n-Quantizing a model is as simple as passing a `quantization_config` to the model. Below, we'll leverage the bitsandbytes quantization library (but refer to [this page](../quantization.md) for alternative quantization methods):\n+Quantizing a model is as simple as passing a `quantization_config` to the model. Below, we'll leverage the bitsandbytes quantization library (but refer to [this page](../quantization) for alternative quantization methods):\n \n ```python\n >>> import torch"
        },
        {
            "sha": "73172f3168013af11dc392a5d7ed14141ec583ee",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -146,9 +146,9 @@ The Flash Attention-2 model uses also a more memory efficient cache slicing mech\n \n ## Shrinking down Mixtral using quantization\n \n-As the Mixtral model has 45 billion parameters, that would require about 90GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization.md). If the model is quantized to 4 bits (or half a byte per parameter), a single A100 with 40GB of RAM is enough to fit the entire model, as in that case only about 27 GB of RAM is required.\n+As the Mixtral model has 45 billion parameters, that would require about 90GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization). If the model is quantized to 4 bits (or half a byte per parameter), a single A100 with 40GB of RAM is enough to fit the entire model, as in that case only about 27 GB of RAM is required.\n \n-Quantizing a model is as simple as passing a `quantization_config` to the model. Below, we'll leverage the bitsandbytes quantization library (but refer to [this page](../quantization.md) for alternative quantization methods):\n+Quantizing a model is as simple as passing a `quantization_config` to the model. Below, we'll leverage the bitsandbytes quantization library (but refer to [this page](../quantization) for alternative quantization methods):\n \n ```python\n >>> import torch"
        },
        {
            "sha": "15573330a056fefef9c759db1a451096c7488ddd",
            "filename": "docs/source/en/model_doc/patchtsmixer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -38,7 +38,7 @@ This model was contributed by [ajati](https://huggingface.co/ajati), [vijaye12](\n \n ## Usage example\n \n-The code snippet below shows how to randomly initialize a PatchTSMixer model. The model is compatible with the [Trainer API](../trainer.md).\n+The code snippet below shows how to randomly initialize a PatchTSMixer model. The model is compatible with the [Trainer API](../trainer).\n \n ```python\n "
        },
        {
            "sha": "b112c92d53bd3d2052d45760af73346596e192c1",
            "filename": "docs/source/en/model_doc/zamba.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -69,11 +69,11 @@ print(tokenizer.decode(outputs[0]))\n ## Model card\n \n The model cards can be found at:\n-* [Zamba-7B](MODEL_CARD_ZAMBA-7B-v1.md)\n+* [Zamba-7B](https://huggingface.co/Zyphra/Zamba-7B-v1)\n \n \n ## Issues\n-For issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/zyphra/zamba-7b)\n+For issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/Zyphra/Zamba-7B-v1/discussions)\n \n \n ## License"
        },
        {
            "sha": "3a5871d01a2b8f47230768d0e9ead22b5eb38ecf",
            "filename": "docs/source/en/tasks/keypoint_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_detection.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -25,7 +25,7 @@ Keypoint detection identifies and locates specific points of interest within an\n \n In this guide, we will show how to extract keypoints from images.\n \n-For this tutorial, we will use [SuperPoint](./model_doc/superpoint.md), a foundation model for keypoint detection.\n+For this tutorial, we will use [SuperPoint](./model_doc/superpoint), a foundation model for keypoint detection.\n \n ```python\n from transformers import AutoImageProcessor, SuperPointForKeypointDetection"
        },
        {
            "sha": "a7aafdf1954771ce015c8fa622026feb278d08e3",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n Video-text-to-text models, also known as video language models or vision language models with video input, are language models that take a video input. These models can tackle various tasks, from video question answering to video captioning. \n \n-These models have nearly the same architecture as [image-text-to-text](../image_text_to_text.md) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos. Moreover, video-text-to-text models are often trained with all vision modalities. Each example might have videos, multiple videos, images and multiple images. Some of these models can also take interleaved inputs. For example, you can refer to a specific video inside a string of text by adding a video token in text like \"What is happening in this video? `<video>`\". \n+These models have nearly the same architecture as [image-text-to-text](../image_text_to_text) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos. Moreover, video-text-to-text models are often trained with all vision modalities. Each example might have videos, multiple videos, images and multiple images. Some of these models can also take interleaved inputs. For example, you can refer to a specific video inside a string of text by adding a video token in text like \"What is happening in this video? `<video>`\". \n \n In this guide, we provide a brief overview of video LMs and show how to use them with Transformers for inference.\n "
        },
        {
            "sha": "a93ef3d364409737638d382b371a5b0d11f5a3e4",
            "filename": "docs/source/ja/generation_strategies.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fgeneration_strategies.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -307,7 +307,7 @@ culture, and they allow us to design the'\n \n ã‚¢ã‚·ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€`assistant_model` å¼•æ•°ã‚’ãƒ¢ãƒ‡ãƒ«ã§è¨­å®šã—ã¾ã™ã€‚\n \n-ã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€ã•ã¾ã–ã¾ãªãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’å¯èƒ½ã«ã™ã‚‹ä¸»è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«é«˜åº¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¯ [`generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã«å­˜åœ¨ã—ã€[`generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã®å‹•ä½œã‚’ã•ã‚‰ã«åˆ¶å¾¡ã§ãã¾ã™ã€‚ä½¿ç”¨å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å®Œå…¨ãªãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã¯ã€[APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](./main_classes/text_generation.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n+ã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€ã•ã¾ã–ã¾ãªãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’å¯èƒ½ã«ã™ã‚‹ä¸»è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«é«˜åº¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¯ [`generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã«å­˜åœ¨ã—ã€[`generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã®å‹•ä½œã‚’ã•ã‚‰ã«åˆ¶å¾¡ã§ãã¾ã™ã€‚ä½¿ç”¨å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å®Œå…¨ãªãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã¯ã€[APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](./main_classes/text_generation) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n \n \n ```python"
        },
        {
            "sha": "4584f4815017b2d7f2ad131c4ff0551a468d982f",
            "filename": "docs/source/ja/model_doc/bart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -111,7 +111,7 @@ BART ã‚’å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n - [`TFBartForConditionalGeneration`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)ã€‚\n - [`FlaxBartForConditionalGeneration`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization) ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n - [è¦ç´„](https://huggingface.co/course/chapter7/5?fw=pt#summarization) ğŸ¤— ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ã‚³ãƒ¼ã‚¹ã®ç« ã€‚\n-- [è¦ç´„ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/summarization.md)\n+- [è¦ç´„ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/summarization)\n \n <PipelineTag pipeline=\"fill-mask\"/>\n "
        },
        {
            "sha": "f45fea5b2280b416d85ac7abeef1f08991cd3e08",
            "filename": "docs/source/ko/generation_strategies.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fgeneration_strategies.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -289,7 +289,7 @@ time.\"\\n\\nHe added: \"I am very proud of the work I have been able to do in the l\n culture, and they allow us to design the'\n ```\n \n-ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì£¼ìš” ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. [`generate`] ë©”ì„œë“œì— ëŒ€í•œ ê³ ê¸‰ ë§¤ê°œë³€ìˆ˜ê°€ ì¡´ì¬í•˜ë¯€ë¡œ [`generate`] ë©”ì„œë“œì˜ ë™ì‘ì„ ë”ìš± ì„¸ë¶€ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ì „ì²´ ëª©ë¡ì€ [API ë¬¸ì„œ](./main_classes/text_generation.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì£¼ìš” ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. [`generate`] ë©”ì„œë“œì— ëŒ€í•œ ê³ ê¸‰ ë§¤ê°œë³€ìˆ˜ê°€ ì¡´ì¬í•˜ë¯€ë¡œ [`generate`] ë©”ì„œë“œì˜ ë™ì‘ì„ ë”ìš± ì„¸ë¶€ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ì „ì²´ ëª©ë¡ì€ [API ë¬¸ì„œ](./main_classes/text_generation)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n \n ### ì¶”ë¡  ë””ì½”ë”©(Speculative Decoding)[[speculative-decoding]]\n "
        },
        {
            "sha": "91fdeff0445a59d3fcfd683754f2f01eabb2284a",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -21,7 +21,7 @@ GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), [Llama](https://hugg\n \n ì´ ê°€ì´ë“œì—ì„œëŠ” íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë°°í¬ë¥¼ ìœ„í•œ íš¨ê³¼ì ì¸ ê¸°ë²•ë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. \n \n-1.  **ë‚®ì€ ì •ë°€ë„:** ì—°êµ¬ì— ë”°ë¥´ë©´, [8ë¹„íŠ¸ì™€ 4ë¹„íŠ¸](./main_classes/quantization.md)ì™€ ê°™ì´ ë‚®ì€ ìˆ˜ì¹˜ ì •ë°€ë„ë¡œ ì‘ë™í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì˜ í° ì €í•˜ ì—†ì´ ê³„ì‚°ìƒì˜ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+1.  **ë‚®ì€ ì •ë°€ë„:** ì—°êµ¬ì— ë”°ë¥´ë©´, [8ë¹„íŠ¸ì™€ 4ë¹„íŠ¸](./main_classes/quantization)ì™€ ê°™ì´ ë‚®ì€ ìˆ˜ì¹˜ ì •ë°€ë„ë¡œ ì‘ë™í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì˜ í° ì €í•˜ ì—†ì´ ê³„ì‚°ìƒì˜ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n 2.  **í”Œë˜ì‹œ ì–´í…ì…˜:** í”Œë˜ì‹œ ì–´í…ì…˜ì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì¼ ë¿ë§Œ ì•„ë‹ˆë¼ ìµœì í™”ëœ GPU ë©”ëª¨ë¦¬ í™œìš©ì„ í†µí•´ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì–´í…ì…˜ ì•Œê³ ë¦¬ì¦˜ì˜ ë³€í˜•ì…ë‹ˆë‹¤.\n "
        },
        {
            "sha": "06ea9a396d0ea967829830190475a0ee2164bae4",
            "filename": "docs/source/ko/model_doc/mistral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -136,9 +136,9 @@ pip install -U flash-attn --no-build-isolation\n \n ## ì–‘ìí™”ë¡œ ë¯¸ìŠ¤íŠ¸ë„ í¬ê¸° ì¤„ì´ê¸°[[shrinking-down-mistral-using-quantization]]\n \n-ë¯¸ìŠ¤íŠ¸ë„ ëª¨ë¸ì€ 70ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆì–´, ì ˆë°˜ì˜ ì •ë°€ë„(float16)ë¡œ ì•½ 14GBì˜ GPU RAMì´ í•„ìš”í•©ë‹ˆë‹¤. ê° íŒŒë¼ë¯¸í„°ê°€ 2ë°”ì´íŠ¸ë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ [ì–‘ìí™”](../quantization.md)ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ 4ë¹„íŠ¸(ì¦‰, íŒŒë¼ë¯¸í„°ë‹¹ ë°˜ ë°”ì´íŠ¸)ë¡œ ì–‘ìí™”í•˜ë©´ ì•½ 3.5GBì˜ RAMë§Œ í•„ìš”í•©ë‹ˆë‹¤.\n+ë¯¸ìŠ¤íŠ¸ë„ ëª¨ë¸ì€ 70ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆì–´, ì ˆë°˜ì˜ ì •ë°€ë„(float16)ë¡œ ì•½ 14GBì˜ GPU RAMì´ í•„ìš”í•©ë‹ˆë‹¤. ê° íŒŒë¼ë¯¸í„°ê°€ 2ë°”ì´íŠ¸ë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ [ì–‘ìí™”](../quantization)ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ 4ë¹„íŠ¸(ì¦‰, íŒŒë¼ë¯¸í„°ë‹¹ ë°˜ ë°”ì´íŠ¸)ë¡œ ì–‘ìí™”í•˜ë©´ ì•½ 3.5GBì˜ RAMë§Œ í•„ìš”í•©ë‹ˆë‹¤.\n \n-ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ê²ƒì€ `quantization_config`ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒë§Œí¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œëŠ” BitsAndBytes ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì–‘ìí™” ë°©ë²•ì€ [ì´ í˜ì´ì§€](../quantization.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:\n+ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ê²ƒì€ `quantization_config`ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒë§Œí¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œëŠ” BitsAndBytes ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì–‘ìí™” ë°©ë²•ì€ [ì´ í˜ì´ì§€](../quantization)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:\n \n ```python\n >>> import torch"
        },
        {
            "sha": "7af873b9ee04efbdfa66aed768ba3595ff518c64",
            "filename": "docs/source/ko/model_doc/patchtsmixer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fmodel_doc%2Fpatchtsmixer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fko%2Fmodel_doc%2Fpatchtsmixer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fpatchtsmixer.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -35,7 +35,7 @@ PatchTSMixerëŠ” MLP-Mixer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²½ëŸ‰ ì‹œê³„ì—´ ëª¨\n ## ì‚¬ìš© ì˜ˆ[[usage-example]]\n \n ì•„ë˜ì˜ ì½”ë“œ ìŠ¤ë‹ˆí«ì€ PatchTSMixer ëª¨ë¸ì„ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. \n-PatchTSMixer ëª¨ë¸ì€ [Trainer API](../trainer.md)ì™€ í˜¸í™˜ë©ë‹ˆë‹¤.\n+PatchTSMixer ëª¨ë¸ì€ [Trainer API](../trainer)ì™€ í˜¸í™˜ë©ë‹ˆë‹¤.\n \n ```python\n "
        },
        {
            "sha": "156b7119439a8d8e8afc359aca8dcc7f03a42076",
            "filename": "docs/source/zh/index.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fzh%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c0af41ce5c448f872f3222a75f56030fb2e5a88/docs%2Fsource%2Fzh%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Findex.md?ref=2c0af41ce5c448f872f3222a75f56030fb2e5a88",
            "patch": "@@ -57,8 +57,8 @@ rendered properly in your Markdown viewer.\n \n |                                  æ¨¡å‹                                   | PyTorch æ”¯æŒ | TensorFlow æ”¯æŒ | Flax æ”¯æŒ |\n |:------------------------------------------------------------------------:|:---------------:|:------------------:|:------------:|\n-|                        [ALBERT](../en/model_doc/albert.md)                        |       âœ…        |         âœ…         |      âœ…      |\n-|                         [ALIGN](../en/model_doc/align.md)                         |       âœ…        |         âŒ         |      âŒ      |\n+|                        [ALBERT](../en/model_doc/albert)                        |       âœ…        |         âœ…         |      âœ…      |\n+|                         [ALIGN](../en/model_doc/align)                         |       âœ…        |         âŒ         |      âŒ      |\n |                       [AltCLIP](../en/model_doc/altclip)                       |       âœ…        |         âŒ         |      âŒ      |\n | [Audio Spectrogram Transformer](../en/model_doc/audio-spectrogram-transformer) |       âœ…        |         âŒ         |      âŒ      |\n |                    [Autoformer](../en/model_doc/autoformer)                    |       âœ…        |         âŒ         |      âŒ      |"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 40,
        "deletions": 40
    }
}