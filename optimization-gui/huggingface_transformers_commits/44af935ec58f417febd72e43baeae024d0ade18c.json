{
    "author": "xinpengzz",
    "message": "Refine the code of Universal Assisted Generation (#34823)\n\n* removed the useless attritbutes\n\n* add configs for window size\n\n* fixed the wrong kwargs\n\n* added docstring",
    "sha": "44af935ec58f417febd72e43baeae024d0ade18c",
    "files": [
        {
            "sha": "7cab88a4bc2e6561a03ddd72a437b12fadb9f4e4",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/44af935ec58f417febd72e43baeae024d0ade18c/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44af935ec58f417febd72e43baeae024d0ade18c/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=44af935ec58f417febd72e43baeae024d0ade18c",
            "patch": "@@ -310,10 +310,9 @@ def __init__(\n \n         self.target_tokenizer = target_tokenizer\n         self.assistant_tokenizer = assistant_tokenizer\n-        self.prev_tokens = None\n         self.prev_assistant_ids = None\n-        self.target_lookbehind = 10\n-        self.assistant_lookbehind = 10\n+        self.target_lookbehind = assistant_model.generation_config.target_lookbehind\n+        self.assistant_lookbehind = assistant_model.generation_config.assistant_lookbehind\n \n     @staticmethod\n     def _get_longest_diag_dict(input_matrix, nonzero_idx):\n@@ -450,9 +449,9 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n         # Since re-encoding the tokens may result in tokenization discrepancies, we use 2 look behind values\n         # (one for each conversion) which mark where to start looking for the overlap between the\n         # source and target encodings, to ensure the new tokens include the correct prompt suffix.\n-        if self.prev_tokens is not None and self.prev_target_ids.shape[1] > self.target_lookbehind:\n+        if self.prev_assistant_ids is not None and input_ids.shape[1] > self.target_lookbehind:\n             # input_ids contains all target prompt input ids and some new target input ids\n-            start_index_in_target_window = self.prev_target_ids.shape[1] - self.target_lookbehind\n+            start_index_in_target_window = input_ids.shape[1] - self.target_lookbehind\n \n             new_assistant_ids = self.convert_source_tokens_to_target_tokens(\n                 input_ids[:, start_index_in_target_window:], **convert_kwargs\n@@ -485,7 +484,6 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n \n         else:\n             assistant_input_ids = self.convert_source_tokens_to_target_tokens(input_ids, **convert_kwargs)\n-            self.prev_target_ids = input_ids\n \n         self.prev_assistant_ids = assistant_input_ids\n         new_cur_len = assistant_input_ids.shape[-1]\n@@ -520,6 +518,8 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n \n         num_prev_assistant = self.prev_assistant_ids.shape[1]\n         start_assistant_look_index = num_prev_assistant - self.assistant_lookbehind\n+        if start_assistant_look_index < 0:\n+            start_assistant_look_index = 0\n \n         new_target_ids_from_window = self.convert_source_tokens_to_target_tokens(\n             assistant_output.sequences[:, start_assistant_look_index:],\n@@ -543,14 +543,11 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n             # edge case: in case of no intersection between prompt and new_target_ids\n             new_target_ids = torch.cat([new_target_ids, new_target_ids_from_window], dim=-1)\n \n-        self.prev_target_ids = input_ids\n-\n         if hasattr(self.generation_config, \"max_length\"):\n             new_target_ids = new_target_ids[:, : self.generation_config.max_length]\n \n         # 3. Update variables for the next round of candidate generation\n         self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n-        self.prev_tokens = assistant_output.sequences\n \n         # 4. Prepare variables for output\n         if input_ids.shape[1] >= new_target_ids.shape[1]:"
        },
        {
            "sha": "cbc445308ad5412ee0c495d2502078b53cfee7b2",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/44af935ec58f417febd72e43baeae024d0ade18c/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44af935ec58f417febd72e43baeae024d0ade18c/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=44af935ec58f417febd72e43baeae024d0ade18c",
            "patch": "@@ -360,6 +360,14 @@ class GenerationConfig(PushToHubMixin):\n         assistant_early_exit(`int`, *optional*):\n             If set to a positive integer, early exit of the model will be used as an assistant. Can only be used with\n             models that support early exit (i.e. models where logits from intermediate layers can be interpreted by the LM head).\n+        assistant_lookbehind(`int`, *optional*, defaults to 10):\n+            If set to a positive integer, the re-encodeing process will additionally consider the last `assistant_lookbehind` assistant tokens\n+            to correctly align tokens. Can only be used with different tokenizers in speculative decoding.\n+            See this [blog](https://huggingface.co/blog/universal_assisted_generation) for more details.\n+        target_lookbehind(`int`, *optional*, defaults to 10):\n+            If set to a positive integer, the re-encodeing process will additionally consider the last `target_lookbehind` target tokens\n+            to correctly align tokens. Can only be used with different tokenizers in speculative decoding.\n+            See this [blog](https://huggingface.co/blog/universal_assisted_generation) for more details.\n \n         > Wild card\n \n@@ -460,6 +468,9 @@ def __init__(self, **kwargs):\n         self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)\n         self.max_matching_ngram_size = kwargs.pop(\"max_matching_ngram_size\", None)\n         self.assistant_early_exit = kwargs.pop(\"assistant_early_exit\", None)\n+        ## assistant generation for different tokenizers, the windows size for assistant/target model\n+        self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", 10)\n+        self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n \n         # Wild card\n         self.generation_kwargs = kwargs.pop(\"generation_kwargs\", {})"
        },
        {
            "sha": "e403a528a8c8b77ac367eb56636e3b9e91986822",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44af935ec58f417febd72e43baeae024d0ade18c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44af935ec58f417febd72e43baeae024d0ade18c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=44af935ec58f417febd72e43baeae024d0ade18c",
            "patch": "@@ -3812,7 +3812,7 @@ def test_speculative_decoding_equals_regular_decoding(self):\n             do_sample=False,\n             max_new_tokens=max_new_tokens_item,\n             assistant_model=draft_model,\n-            target_tokenizer=target_tokenizer,\n+            tokenizer=target_tokenizer,\n             assistant_tokenizer=assistant_tokenizer,\n         )\n "
        }
    ],
    "stats": {
        "total": 28,
        "additions": 18,
        "deletions": 10
    }
}