{
    "author": "dvrogozh",
    "message": "tests: revert change of torch_require_multi_gpu to be device agnostic (#35721)\n\n* tests: revert change of torch_require_multi_gpu to be device agnostic\n\nThe 11c27dd33 modified `torch_require_multi_gpu()` to be device agnostic\ninstead of being CUDA specific. This broke some tests which are rightfully\nCUDA specific, such as:\n\n* `tests/trainer/test_trainer_distributed.py::TestTrainerDistributed`\n\nIn the current Transformers tests architecture `require_torch_multi_accelerator()`\nshould be used to mark multi-GPU tests agnostic to device.\n\nThis change addresses the issue introduced by 11c27dd33 and reverts\nmodification of `torch_require_multi_gpu()`.\n\nFixes: 11c27dd33 (\"Enable BNB multi-backend support (#31098)\")\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\n\n* fix bug: modification of frozen set\n\n---------\n\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\nCo-authored-by: Titus von Koeller <9048635+Titus-von-Koeller@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "b4b9da6d9b3fef860889bb843bcf10e8d5e2646a",
    "files": [
        {
            "sha": "61e9f8d8db5d6e6eb6b32c2584fd91ddae52036b",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=b4b9da6d9b3fef860889bb843bcf10e8d5e2646a",
            "patch": "@@ -486,7 +486,7 @@ def _validate_bnb_multi_backend_availability(raise_exception):\n     import bitsandbytes as bnb\n \n     bnb_supported_devices = getattr(bnb, \"supported_torch_devices\", set())\n-    available_devices = get_available_devices()\n+    available_devices = set(get_available_devices())\n \n     if available_devices == {\"cpu\"} and not is_ipex_available():\n         from importlib.util import find_spec"
        },
        {
            "sha": "bb0b3d3b2f864e6e79a623669b3f34786d185c64",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=b4b9da6d9b3fef860889bb843bcf10e8d5e2646a",
            "patch": "@@ -238,17 +238,6 @@ def parse_int_from_env(key, default=None):\n _run_third_party_device_tests = parse_flag_from_env(\"RUN_THIRD_PARTY_DEVICE_TESTS\", default=False)\n \n \n-def get_device_count():\n-    import torch\n-\n-    if is_torch_xpu_available():\n-        num_devices = torch.xpu.device_count()\n-    else:\n-        num_devices = torch.cuda.device_count()\n-\n-    return num_devices\n-\n-\n def is_staging_test(test_case):\n     \"\"\"\n     Decorator marking a test as a staging test.\n@@ -756,17 +745,17 @@ def require_spacy(test_case):\n \n def require_torch_multi_gpu(test_case):\n     \"\"\"\n-    Decorator marking a test that requires a multi-GPU setup (in PyTorch). These tests are skipped on a machine without\n-    multiple GPUs.\n+    Decorator marking a test that requires a multi-GPU CUDA setup (in PyTorch). These tests are skipped on a machine without\n+    multiple CUDA GPUs.\n \n     To run *only* the multi_gpu tests, assuming all test names contain multi_gpu: $ pytest -sv ./tests -k \"multi_gpu\"\n     \"\"\"\n     if not is_torch_available():\n         return unittest.skip(reason=\"test requires PyTorch\")(test_case)\n \n-    device_count = get_device_count()\n+    import torch\n \n-    return unittest.skipUnless(device_count > 1, \"test requires multiple GPUs\")(test_case)\n+    return unittest.skipUnless(torch.cuda.device_count() > 1, \"test requires multiple CUDA GPUs\")(test_case)\n \n \n def require_torch_multi_accelerator(test_case):"
        },
        {
            "sha": "f8888accd7d3229d059851cfcf88f8dba5608a79",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=b4b9da6d9b3fef860889bb843bcf10e8d5e2646a",
            "patch": "@@ -39,7 +39,7 @@\n     require_bitsandbytes,\n     require_torch,\n     require_torch_gpu_if_bnb_not_multi_backend_enabled,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -517,7 +517,7 @@ def test_pipeline(self):\n         self.assertIn(pipeline_output[0][\"generated_text\"], self.EXPECTED_OUTPUTS)\n \n \n-@require_torch_multi_gpu\n+@require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n class Bnb4bitTestMultiGpu(Base4bitTest):\n     def setUp(self):"
        },
        {
            "sha": "c4025ce93b25a62a5f185129627255897fbf84e2",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4b9da6d9b3fef860889bb843bcf10e8d5e2646a/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=b4b9da6d9b3fef860889bb843bcf10e8d5e2646a",
            "patch": "@@ -39,7 +39,7 @@\n     require_bitsandbytes,\n     require_torch,\n     require_torch_gpu_if_bnb_not_multi_backend_enabled,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -671,7 +671,7 @@ def test_pipeline(self):\n         self.assertIn(pipeline_output[0][\"generated_text\"], self.EXPECTED_OUTPUTS)\n \n \n-@require_torch_multi_gpu\n+@require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n class MixedInt8TestMultiGpu(BaseMixedInt8Test):\n     def setUp(self):\n@@ -700,7 +700,7 @@ def test_multi_gpu_loading(self):\n         self.assertIn(self.tokenizer.decode(output_parallel[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n \n-@require_torch_multi_gpu\n+@require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n class MixedInt8TestCpuGpu(BaseMixedInt8Test):\n     def setUp(self):"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 10,
        "deletions": 21
    }
}