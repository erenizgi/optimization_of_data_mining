{
    "author": "MekkCyber",
    "message": "[Quantization] Remove dequant fp8 config (#42596)\n\n* fix\n\n* style\n\n* up",
    "sha": "7266f50bc8c3f071fd2fddb5eeb87280a029eabc",
    "files": [
        {
            "sha": "75f49e6c9da1b031855ce2f857db142338d53635",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7266f50bc8c3f071fd2fddb5eeb87280a029eabc/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7266f50bc8c3f071fd2fddb5eeb87280a029eabc/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=7266f50bc8c3f071fd2fddb5eeb87280a029eabc",
            "patch": "@@ -178,6 +178,10 @@ def _process_model_before_weight_loading(\n \n         model.config.quantization_config = self.quantization_config\n \n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        if self.pre_quantized and self.quantization_config.dequantize:\n+            self.remove_quantization_config(model)\n+\n     def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n         from ..integrations import FP8Linear\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}