{
    "author": "guangy10",
    "message": "Unbreak optimum-executorch (#38646)\n\n* Unbreak optimum-executorch\n\n* use static cache if has layer_types but no sliding_window\n\n* revert view on kv_arange\n\n---------\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
    "files": [
        {
            "sha": "a16114fe9539144045f3800c55de9c131a96b393",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -56,13 +56,15 @@ def __init__(\n         if not hasattr(model.config, \"use_cache\") or model.config.use_cache is False:\n             raise ValueError(\"The model must have caching enabled to be performant.\")\n \n-        if not hasattr(model.config, \"layer_types\"):\n-            # If `layer_types` is not specified explicitly in the config, there is only 1 type of layers, so\n-            # export will use `StaticCache` by default.\n-            logging.info(\"Using `StaticCache` for export as `layer_types` is not specified in the config.\")\n-            self.model = TorchExportableModuleWithStaticCache(model)\n-        else:\n+        if hasattr(model.config, \"layer_types\") and getattr(model.config, \"sliding_window\", None) is not None:\n             self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n+        else:\n+            # If `layer_types` is not specified explicitly in the config or `sliding_window` is null,\n+            # there is only 1 type of layers, so export will use `StaticCache` by default.\n+            logging.info(\n+                \"Using `StaticCache` for export as `layer_types` is not specified or `sliding_window` is `null` in the config.\"\n+            )\n+            self.model = TorchExportableModuleWithStaticCache(model)\n \n     def forward(\n         self,\n@@ -400,12 +402,6 @@ def __init__(\n         if not self.model.config.use_cache:\n             raise AssertionError(\"Model must have caching enabled\")\n \n-        if (\n-            not hasattr(self.model.config, \"cache_implementation\")\n-            or self.model.config.cache_implementation != \"hybrid\"\n-        ):\n-            raise AssertionError(\"Model must use 'hybrid' cache implementation\")\n-\n         # Initialize the HybridCache\n         self.cache = HybridCache(\n             config=self.model.config,"
        },
        {
            "sha": "3f28fcd7044982564e9b7332a1174f12d10b4f60",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -378,7 +378,6 @@ def test_export_static_cache(self):\n \n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", pad_token=\"</s>\", padding_side=\"right\")\n@@ -424,7 +423,10 @@ def test_export_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, eager_generated_text)\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model)\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export()\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "0f06ed3cea5689d28097715e4784ebb1967bb191",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -313,7 +313,6 @@ def test_export_static_cache(self):\n \n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\", pad_token=\"</s>\", padding_side=\"right\")\n@@ -363,7 +362,10 @@ def test_export_static_cache(self):\n         max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model)\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export()\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "743dd0e86f8ac7898804b18f267aaaf01e035b19",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -306,7 +306,6 @@ def test_export_static_cache(self):\n \n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         llama_models = {\n@@ -352,7 +351,10 @@ def test_export_static_cache(self):\n             max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n             # Static Cache + export\n-            exported_program = convert_and_export_with_cache(model)\n+            from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+            exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+            exported_program = exportable_module.export()\n             ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n                 exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n             )"
        },
        {
            "sha": "4e94d23101754600b696eb6b256d570549e0bc9d",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -334,7 +334,6 @@ def test_export_static_cache(self):\n \n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         olmo_model = \"allenai/OLMo-1B-hf\"\n@@ -382,7 +381,10 @@ def test_export_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, eager_generated_text)\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model)\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export()\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "1f76a22bffb2ea0f87d0f720a44a80fc2cc1563d",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -347,7 +347,6 @@ def test_export_static_cache(self):\n         from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         model_id = \"microsoft/Phi-4-mini-instruct\"\n@@ -399,7 +398,10 @@ def test_export_static_cache(self):\n         max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model)\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export()\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "0f846a6a5e3c5500b7fe6f1de9cae9f3db167a4f",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -31,7 +31,6 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils.import_utils import is_torch_greater_or_equal\n \n \n if is_torch_available():\n@@ -246,7 +245,6 @@ def test_export_static_cache(self):\n \n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         qwen_model = \"Qwen/Qwen2-0.5B\"\n@@ -287,8 +285,13 @@ def test_export_static_cache(self):\n         max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n-        strict = is_torch_greater_or_equal(\"2.7.0\")  # Due to https://github.com/pytorch/pytorch/issues/150994\n-        exported_program = convert_and_export_with_cache(model, strict=strict)\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        strict = version.parse(torch.__version__) != version.parse(\n+            \"2.7.0\"\n+        )  # Due to https://github.com/pytorch/pytorch/issues/150994\n+        exported_program = exportable_module.export(strict=strict)\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "02841dcb6b4f97ca3b2a909e7ababf083d67909f",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -31,7 +31,6 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils.import_utils import is_torch_greater_or_equal\n \n \n if is_torch_available():\n@@ -240,13 +239,12 @@ def test_export_static_cache(self):\n \n         from transformers.integrations.executorch import (\n             TorchExportableModuleWithStaticCache,\n-            convert_and_export_with_cache,\n         )\n \n         qwen_model = \"Qwen/Qwen3-0.6B-Base\"\n \n         tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n-        if is_torch_greater_or_equal(\"2.7.0\"):\n+        if version.parse(torch.__version__) == version.parse(\"2.7.0\"):\n             strict = False  # Due to https://github.com/pytorch/pytorch/issues/150994\n             EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated.\"]\n         else:\n@@ -285,7 +283,10 @@ def test_export_static_cache(self):\n         max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model, strict=strict)\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export(strict=strict)\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "150bd9e1b2bbb3da5a599aa042e1053808c3b948",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 9,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f00b325f8140c4964e3e81e6af0e53f5b9a2592/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=7f00b325f8140c4964e3e81e6af0e53f5b9a2592",
            "patch": "@@ -15,6 +15,7 @@\n import copy\n import unittest\n \n+from packaging import version\n from parameterized import parameterized\n \n from transformers import set_seed\n@@ -680,15 +681,27 @@ def test_static_cache_exportability(self):\n         self.assertEqual(n_static_key_caches, model.config.num_hidden_layers)\n         self.assertEqual(n_static_value_caches, model.config.num_hidden_layers)\n \n-        # Export with dynamic shapes using Dim.AUTO\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        input_ids = tokenizer(\"Here's everything I know\", return_tensors=\"pt\").input_ids\n-        dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.AUTO}, \"cache_position\": None}\n+        # Export with dynamic shapes\n+        input_ids = torch.zeros((1, 3), dtype=torch.long)\n+        cache_position = torch.tensor([0, 1, 2], dtype=torch.long)\n+        dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.DYNAMIC}, \"cache_position\": {0: torch.export.Dim.DYNAMIC}}\n+        strict = version.parse(torch.__version__) != version.parse(\"2.7.0\")\n         exported_program = convert_and_export_with_cache(\n             model,\n             example_input_ids=input_ids,\n+            example_cache_position=cache_position,\n+            dynamic_shapes=dynamic_shapes,\n+            strict=strict,\n+        )\n+\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=input_ids,\n+            cache_position=cache_position,\n             dynamic_shapes=dynamic_shapes,\n-            strict=False,\n+            strict=strict,\n         )\n \n     def test_hybrid_cache_exportability(self):\n@@ -727,13 +740,15 @@ def test_hybrid_cache_exportability(self):\n         self.assertEqual(n_g_value_caches, model.config.num_hidden_layers)\n \n         # Export with dynamic shapes using Dim.AUTO\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        input_ids = tokenizer(\"Here's everything I know\", return_tensors=\"pt\").input_ids\n-        dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.AUTO}, \"cache_position\": None}\n+        input_ids = torch.zeros((1, 3), dtype=torch.long)\n+        cache_position = torch.tensor([0, 1, 2], dtype=torch.long)\n+        dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.DYNAMIC}, \"cache_position\": {0: torch.export.Dim.DYNAMIC}}\n+        strict = version.parse(torch.__version__) != version.parse(\"2.7.0\")\n         exported_program = exportable_module.export(\n             input_ids=input_ids,\n+            cache_position=cache_position,\n             dynamic_shapes=dynamic_shapes,\n-            strict=False,\n+            strict=strict,\n         )\n \n "
        }
    ],
    "stats": {
        "total": 103,
        "additions": 64,
        "deletions": 39
    }
}