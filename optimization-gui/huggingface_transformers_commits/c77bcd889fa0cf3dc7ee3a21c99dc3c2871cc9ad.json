{
    "author": "ydshieh",
    "message": "Fix `qwen3_moe` tests (#38865)\n\n* try 1\n\n* try 2\n\n* try 3\n\n* try 4\n\n* try 5\n\n* try 6\n\n* try 7\n\n* try 8\n\n* try 9\n\n* try 10\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad",
    "files": [
        {
            "sha": "4503a6b1acbe9ffc8f067852270461773f7b3b42",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad",
            "patch": "@@ -338,7 +338,7 @@ def setUpClass(cls):\n \n     @classmethod\n     def tearDownClass(cls):\n-        del cls.model_checkpoint\n+        del cls.model\n         cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):"
        },
        {
            "sha": "3dd30de34a24f26cfde6ddd62b482e4b7f63198f",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 43,
            "deletions": 64,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad",
            "patch": "@@ -13,18 +13,19 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Qwen3MoE model.\"\"\"\n \n-import gc\n import unittest\n \n import pytest\n \n from transformers import AutoTokenizer, Qwen3MoeConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    backend_empty_cache,\n+    cleanup,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n+    require_torch_large_accelerator,\n+    require_torch_multi_accelerator,\n     require_torch_sdpa,\n     slow,\n     torch_device,\n@@ -143,45 +144,61 @@ def test_load_balancing_loss(self):\n         self.assertNotAlmostEqual(include_padding_result.aux_loss.item(), result.aux_loss.item())\n \n \n+# Run on runners with larger accelerators (for example A10 instead of T4) with a lot of CPU RAM (e.g. g5-12xlarge)\n+@require_torch_multi_accelerator\n+@require_torch_large_accelerator\n @require_torch\n class Qwen3MoeIntegrationTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = Qwen3MoeForCausalLM.from_pretrained(\n+                \"Qwen/Qwen3-30B-A3B-Base\", device_map=\"auto\", load_in_4bit=True\n+            )\n+\n+        return cls.model\n+\n     @slow\n     def test_model_15b_a2b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = Qwen3MoeForCausalLM.from_pretrained(\"Qwen/Qwen3-15B-A2B-Base\", device_map=\"auto\")\n+        model = self.get_model()\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n+\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-1.1184, 1.1356, 9.2112, 8.0254, 5.1663, 7.9287, 8.9245, 10.0671]])\n+        EXPECTED_MEAN = torch.tensor([[0.3244, 0.4406, 9.0972, 7.3597, 4.9985, 8.0314, 8.2148, 9.2134]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+\n         # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([7.5938, 2.6094, 4.0312, 4.0938, 2.5156, 2.7812, 2.9688, 1.5547, 1.3984, 2.2344, 3.0156, 3.1562, 1.1953, 3.2500, 1.0938, 8.4375, 9.5625, 9.0625, 7.5625, 7.5625, 7.9062, 7.2188, 7.0312, 6.9375, 8.0625, 1.7266, 0.9141, 3.7969, 5.3438, 3.9844])  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor([6.8984, 4.8633, 4.7734, 4.5898, 2.5664, 2.9902, 4.8828, 5.9414, 4.6250, 3.0840, 5.1602, 6.0117, 4.9453, 5.3008, 3.3145, 11.3906, 12.8359, 12.4844, 11.2891, 11.0547, 11.0391, 10.3359, 10.3438, 10.2578, 10.7969, 5.9688, 3.7676, 5.5938, 5.3633, 5.8203])  # fmt: skip\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @slow\n     def test_model_15b_a2b_generation(self):\n-        EXPECTED_TEXT_COMPLETION = (\n-            \"\"\"To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer the sl\"\"\"\n-        )\n+        EXPECTED_TEXT_COMPLETION = \"To be or not to be: the role of the cell cycle in the regulation of apoptosis.\\nThe cell cycle is a highly\"\n         prompt = \"To be or not to\"\n-        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-15B-A2B-Base\", use_fast=False)\n-        model = Qwen3MoeForCausalLM.from_pretrained(\"Qwen/Qwen3-15B-A2B-Base\", device_map=\"auto\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-30B-A3B-Base\", use_fast=False)\n+        model = self.get_model()\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n         generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @require_bitsandbytes\n     @slow\n     @require_flash_attn\n@@ -191,7 +208,7 @@ def test_model_15b_a2b_long_prompt(self):\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n         model = Qwen3MoeForCausalLM.from_pretrained(\n-            \"Qwen/Qwen3-15B-A2B-Base\",\n+            \"Qwen/Qwen3-30B-A3B-Base\",\n             device_map=\"auto\",\n             load_in_4bit=True,\n             attn_implementation=\"flash_attention_2\",\n@@ -200,50 +217,20 @@ def test_model_15b_a2b_long_prompt(self):\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n-        # Assisted generation\n-        assistant_model = model\n-        assistant_model.generation_config.num_assistant_tokens = 2\n-        assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"\n-        generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n-        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n-\n-        del assistant_model\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @slow\n     @require_torch_sdpa\n     def test_model_15b_a2b_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n-        model = Qwen3MoeForCausalLM.from_pretrained(\n-            \"Qwen/Qwen3-15B-A2B-Base\",\n-            device_map=\"auto\",\n-            attn_implementation=\"sdpa\",\n-        )\n+        model = self.get_model()\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n-        # Assisted generation\n-        assistant_model = model\n-        assistant_model.generation_config.num_assistant_tokens = 2\n-        assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"\n-        generated_ids = assistant_model.generate(input_ids, max_new_tokens=4, temperature=0)\n-        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n-\n-        del assistant_model\n-\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n-        EXPECTED_TEXT_COMPLETION = (\n-            \"\"\"To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer the sl\"\"\"\n-        )\n+        EXPECTED_TEXT_COMPLETION = \"To be or not to be: the role of the cell cycle in the regulation of apoptosis.\\nThe cell cycle is a highly\"\n         prompt = \"To be or not to\"\n-        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-15B-A2B-Base\", use_fast=False)\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-30B-A3B-Base\", use_fast=False)\n \n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n@@ -255,16 +242,12 @@ def test_model_15b_a2b_long_prompt_sdpa(self):\n     @slow\n     def test_speculative_generation(self):\n         EXPECTED_TEXT_COMPLETION = (\n-            \"To be or not to be, that is the question: whether 'tis nobler in the mind to suffer the sl\"\n+            \"To be or not to be: the role of the liver in the pathogenesis of obesity and type 2 diabetes.\\nThe\"\n         )\n         prompt = \"To be or not to\"\n-        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-15B-A2B-Base\", use_fast=False)\n-        model = Qwen3MoeForCausalLM.from_pretrained(\n-            \"Qwen/Qwen3-15B-A2B-Base\", device_map=\"auto\", torch_dtype=torch.float16\n-        )\n-        assistant_model = Qwen3MoeForCausalLM.from_pretrained(\n-            \"Qwen/Qwen3-15B-A2B-Base\", device_map=\"auto\", torch_dtype=torch.float16\n-        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-30B-A3B-Base\", use_fast=False)\n+        model = self.get_model()\n+        assistant_model = model\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n@@ -274,7 +257,3 @@ def test_speculative_generation(self):\n         )\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n-\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()"
        }
    ],
    "stats": {
        "total": 109,
        "additions": 44,
        "deletions": 65
    }
}