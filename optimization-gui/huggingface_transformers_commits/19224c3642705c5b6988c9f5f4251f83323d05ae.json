{
    "author": "DePasqualeOrg",
    "message": "fix: \"check out\" as verb (#38678)\n\n\"check out\" as verb",
    "sha": "19224c3642705c5b6988c9f5f4251f83323d05ae",
    "files": [
        {
            "sha": "0d76c2bbe33a4756de2d4a1213cbc4117c3ebaa8",
            "filename": "docs/source/en/internal/import_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -38,7 +38,7 @@ However, no method can be called on that object:\n ```python\n >>> DetrImageProcessorFast.from_pretrained()\n ImportError: \n-DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Checkout the instructions on the\n+DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Check out the instructions on the\n installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n Please note that you may need to restart your runtime after installation.\n ```"
        },
        {
            "sha": "7f307eaf7073cc20397fb4ea51ecfcc6eadd42c7",
            "filename": "examples/flax/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Frun_qa.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -546,7 +546,7 @@ def main():\n     # region Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n         raise ValueError(\n-            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n+            \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\"\n         )"
        },
        {
            "sha": "863217d304f0d199aaa4f8b0e402fa7fdd474e5e",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -36,7 +36,7 @@ class MyNewModelConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "f9954a3c7a07a12ee44edac7cb97bfa661d22a1c",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -34,7 +34,7 @@ class NewModelConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "b673a699f96e7376e43462c95572654827c51e3f",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -357,7 +357,7 @@ def main():\n     # Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n         raise ValueError(\n-            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n+            \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\"\n         )"
        },
        {
            "sha": "6b32267c9241d55ab1a88b8d19994f84273301c8",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -399,7 +399,7 @@ def get_label_list(labels):\n     # Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n         raise ValueError(\n-            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n+            \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\"\n         )"
        },
        {
            "sha": "5979e351d0af99a97c1c2eb3f0de4446ffd99200",
            "filename": "examples/tensorflow/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -378,7 +378,7 @@ def main():\n     # region Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n         raise ValueError(\n-            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n+            \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\"\n         )"
        },
        {
            "sha": "761dd2f722acaa8fea61926da1d6a71c419aac3a",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -49,7 +49,7 @@ class AriaTextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "c80351cd9a81572e035743ac4122395ec75a4743",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -120,7 +120,7 @@ class AriaTextConfig(LlamaConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "488f414444bbd99f313f542bce26388fc43c998c",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -53,7 +53,7 @@ class BambaConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "87177bab98264bcc4df2707d70cb2efd2a502e5a",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -48,7 +48,7 @@ class BitNetConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"relu2\"`):"
        },
        {
            "sha": "7760f55b7c13396ec8d0be280041528870629dc3",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -125,7 +125,7 @@ class ChameleonConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "dbfa8232313770fb6a9b410a0a0ffeb0c1f78be4",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -446,7 +446,7 @@ def main():\n         \"--model_size\",\n         choices=[\"7B\", \"30B\"],\n         help=\"\"\n-        \" models correspond to the finetuned versions, and are specific to the Chameleon official release. For more details on Chameleon, checkout the original repo: https://github.com/facebookresearch/chameleon\",\n+        \" models correspond to the finetuned versions, and are specific to the Chameleon official release. For more details on Chameleon, check out the original repo: https://github.com/facebookresearch/chameleon\",\n     )\n     parser.add_argument(\n         \"--output_dir\","
        },
        {
            "sha": "3e257448bc27747be2aed668f9780414045b63cf",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -56,7 +56,7 @@ class CohereConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "8da5a81c09c63c3b7f817e957cefd03bbb3c7a70",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -52,7 +52,7 @@ class Cohere2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "7153b1e5473a29f1e26a0748e04b1b016c4fea88",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -74,7 +74,7 @@ class Cohere2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "56a60cda24e5eb6f9f8f7434d43c800da0f8a439",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -54,7 +54,7 @@ class CsmDepthDecoderConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -235,7 +235,7 @@ class CsmConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf).\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the backbone model Transformer decoder."
        },
        {
            "sha": "4372cad67f148101e7487c3df1c8dbd23aed4a4f",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -52,7 +52,7 @@ class DeepseekV3Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         n_shared_experts (`int`, *optional*, defaults to 1):"
        },
        {
            "sha": "3e0b918e909da486ff93385b17e9b185af0fbfff",
            "filename": "src/transformers/models/diffllama/configuration_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -48,7 +48,7 @@ class DiffLlamaConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "509150df8d90e432f21d5973702f19f4165ffc20",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -138,7 +138,7 @@ class Emu3TextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "94ca7f848d7ec47d2c254b1769240d3075eb7dd1",
            "filename": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -50,7 +50,7 @@ class FalconH1Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "d8f26d38450b8ad7d6688f96a653f0891cf07503",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -47,7 +47,7 @@ class GemmaConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "494e2c7187ef5e56ec014bc9a39499469a1310f7",
            "filename": "src/transformers/models/gemma/convert_gemma_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -151,7 +151,7 @@ def main():\n         \"--model_size\",\n         default=\"7B\",\n         choices=[\"2B\", \"7B\", \"tokenizer_only\"],\n-        help=\"'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b\",\n+        help=\"'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b\",\n     )\n     parser.add_argument(\n         \"--output_dir\","
        },
        {
            "sha": "b10bd51f0cfed9af6e867d74051a36ece6efbf80",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -74,7 +74,7 @@ class GemmaConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "c1390bf205ca038dafc3cbc81ef300c640b0d12e",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -47,7 +47,7 @@ class Gemma2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "1a3a4a92f3a3f3a5b8807162154540c711e6b3ff",
            "filename": "src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -184,7 +184,7 @@ def main():\n         \"--model_size\",\n         default=\"9B\",\n         choices=[\"9B\", \"27B\", \"tokenizer_only\"],\n-        help=\"'f' models correspond to the finetuned versions, and are specific to the Gemma22 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b\",\n+        help=\"'f' models correspond to the finetuned versions, and are specific to the Gemma22 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b\",\n     )\n     parser.add_argument(\n         \"--output_dir\","
        },
        {
            "sha": "f35fdefac6dc922e33a8c39d244caf93bd06b411",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -71,7 +71,7 @@ class Gemma2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "d6935cfef7c4fb3e0dc5974398cce6bea696c2e1",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -55,7 +55,7 @@ class Gemma3TextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "3c1f5b0d5fdb4d6e5c923580e14c57a8f75320a6",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -82,7 +82,7 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "94b7eb528fde8313932f2933db28f134b60e2bad",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -42,7 +42,7 @@ class GlmConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position."
        },
        {
            "sha": "7ed2fbf0ee2023bef9236a8e9f845a326f3cc74e",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -42,7 +42,7 @@ class Glm4Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position."
        },
        {
            "sha": "c7f324026046170be0cb794b9dd962af547e8129",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -54,7 +54,7 @@ class GraniteConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "cb1c30da5ab47d10ea928f69e985d4a768805153",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -54,7 +54,7 @@ class GraniteMoeConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "cd38fc8f9e85afbe24b8ba685343d28d2dc1c81c",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -49,7 +49,7 @@ class GraniteMoeHybridConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "dc71da6a5f53d1e09b2fdff0c371b92d36ef0613",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -54,7 +54,7 @@ class GraniteMoeSharedConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "fe3a5d95d1252640fafe8a8f9661cee1d2e65e72",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -42,7 +42,7 @@ class HeliumConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 128):"
        },
        {
            "sha": "5980a28e4a0d517009ee12531b179b8e821ca341",
            "filename": "src/transformers/models/jamba/configuration_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -55,7 +55,7 @@ class JambaConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "1bca8282701782a6147e87626ed6fe070104c4f1",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -51,7 +51,7 @@ class LlamaConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "e8282ef7438d978748386f48e77e4bcf0b1ff3b9",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -528,7 +528,7 @@ def main():\n     parser.add_argument(\n         \"--model_size\",\n         default=None,\n-        help=\"'f' Deprecated in favor of `num_shards`: models correspond to the finetuned versions, and are specific to the Llama2 official release. For more details on Llama2, checkout the original repo: https://huggingface.co/meta-llama\",\n+        help=\"'f' Deprecated in favor of `num_shards`: models correspond to the finetuned versions, and are specific to the Llama2 official release. For more details on Llama2, check out the original repo: https://huggingface.co/meta-llama\",\n     )\n     parser.add_argument(\n         \"--output_dir\","
        },
        {
            "sha": "4d5a1b0078498c06072a9077a00829478881d7d4",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -95,7 +95,7 @@ class MimiConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n             The attention head dimension."
        },
        {
            "sha": "b8ec562de09a93b062ba6c4b6af430ac8f1a31f2",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -51,7 +51,7 @@ class MiniMaxConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n             The attention head dimension."
        },
        {
            "sha": "0028dcbfb6c0dd856cf30802e004c869a0a7db68",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -76,7 +76,7 @@ class MiniMaxConfig(MixtralConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n             The attention head dimension."
        },
        {
            "sha": "5a3cac5225f26d8a0c1bec5f4ceb807926cb41a9",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -51,7 +51,7 @@ class MistralConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n             The attention head dimension."
        },
        {
            "sha": "ef2d870c1b88b2bd46e931c6485b651f95129b99",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -51,7 +51,7 @@ class MixtralConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n             The attention head dimension."
        },
        {
            "sha": "f6df5901feff99635204d305063be5729817959d",
            "filename": "src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -227,7 +227,7 @@ def main():\n     parser.add_argument(\n         \"--model_size\",\n         choices=[\"7B\"],\n-        help=\"'f' models correspond to the finetuned versions, and are specific to the Mixtral official release. For more details on Mixtral, checkout the original repo: https://huggingface.co/mistral-ai\",\n+        help=\"'f' models correspond to the finetuned versions, and are specific to the Mixtral official release. For more details on Mixtral, check out the original repo: https://huggingface.co/mistral-ai\",\n         default=\"7B\",\n     )\n     parser.add_argument(\"--output_dir\", help=\"Location to write HF model\", required=True)"
        },
        {
            "sha": "dba0d973af824c50879ee01acf4fd1a6d7761764",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -53,15 +53,15 @@ class MoonshineConfig(PretrainedConfig):\n             `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         decoder_num_key_value_heads (`int`, *optional*):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `decoder_num_attention_heads`.\n         pad_head_dim_to_multiple_of (`int`, *optional*):"
        },
        {
            "sha": "f99de20eb02c17b188aeda4b07b7db6bcceb49bd",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -75,15 +75,15 @@ class MoonshineConfig(PretrainedConfig):\n             `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         decoder_num_key_value_heads (`int`, *optional*):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `decoder_num_attention_heads`.\n         pad_head_dim_to_multiple_of (`int`, *optional*):"
        },
        {
            "sha": "02b82ee5ed59119bff9bb330476a74fdefd0b2d2",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -47,7 +47,7 @@ class MoshiDepthConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.\n         audio_vocab_size (`int`, *optional*, defaults to 2048):\n             Vocabulary size of the audio part of model. Defines the number of different tokens that can be\n@@ -171,7 +171,7 @@ class MoshiConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.\n         audio_vocab_size (`int`, *optional*):\n             Vocabulary size of the audio part of model. Defines the number of different tokens that can be"
        },
        {
            "sha": "e52d6d14484c7c28ad2264c664427bb7492d66c1",
            "filename": "src/transformers/models/nemotron/configuration_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -52,7 +52,7 @@ class NemotronConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"relu2\"`):"
        },
        {
            "sha": "2c2da0146356e12ef8d837c2525377926dcbabb8",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -53,7 +53,7 @@ class OlmoConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "96ed3993ed4bac50e117330ae80910918e61f047",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -35,7 +35,7 @@ class Olmo2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "22d716f85593b9b14a431df4983f95713063f273",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -49,7 +49,7 @@ class Olmo2Config(OlmoConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "79976597375e5d968d96afe99e3ffcc66a8276f3",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -42,7 +42,7 @@ class OlmoeConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "bd6eb48003c88393f4143ca876fae211892b1a63",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -50,7 +50,7 @@ class PhiConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         resid_pdrop (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "4b91cbcd14714d9d4cafdb0819efe10bb142f9ec",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -49,7 +49,7 @@ class Phi3Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         resid_pdrop (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "fee669feb83e401061d80d7b515a80b1818dd149",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -268,7 +268,7 @@ class Phi4MultimodalConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         resid_pdrop (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "76925919eb2fe8db24c15c572ebb560ad185cf00",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -304,7 +304,7 @@ class Phi4MultimodalConfig(Phi3Config):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         resid_pdrop (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "f30ed7435cae9ce49cd8bdc9401cc5a03e166f8e",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -48,7 +48,7 @@ class PhimoeConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "d8171da3cf7fc40900a277a9bf74cc96c165b02d",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -50,7 +50,7 @@ class Qwen2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "6367355994c5833f5fc16f69194c3b3c448291a9",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -238,7 +238,7 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n@@ -584,7 +584,7 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "bed63636cd7482a449a954455bd4086b3464319e",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -277,7 +277,7 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n@@ -623,7 +623,7 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "01f0599b083aab58ec09a3346773e520dd64746a",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -94,7 +94,7 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "bb7839fa442d0a43deb8bcfdf475143e88a16ad0",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -49,7 +49,7 @@ class Qwen2MoeConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "a63e32be849967a89380e6a7a77f65c35962b932",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -83,7 +83,7 @@ class Qwen2VLTextConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "0af5d1f70f34eb5b284254d33a876f6a1d1b1adb",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -50,7 +50,7 @@ class Qwen3Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         head_dim (`int`, *optional*, defaults to 128):\n             The attention head dimension."
        },
        {
            "sha": "e00297109246f0353e3b1d0323bc6f1570b9d4d0",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -49,7 +49,7 @@ class Qwen3MoeConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "a31c9c3b7ad4c4862bb0ca9bf0e09388f4368360",
            "filename": "src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -167,7 +167,7 @@ def main():\n         \"--model_size\",\n         default=\"2B\",\n         choices=[\"2B\", \"7B\", \"tokenizer_only\"],\n-        help=\"'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b\",\n+        help=\"'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b\",\n     )\n     parser.add_argument(\n         \"--output_dir\","
        },
        {
            "sha": "a30f9510c43ec94a4e3b601443041312a1d4bd90",
            "filename": "src/transformers/models/stablelm/configuration_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -51,7 +51,7 @@ class StableLmConfig(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "b58e8ddf1a203f7263d72c3c0d89b84849a2493c",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -50,7 +50,7 @@ class Starcoder2Config(PretrainedConfig):\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "e51d0e4ef421907c7fc5cda4574daa8c01956d6b",
            "filename": "src/transformers/models/zamba/configuration_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -59,7 +59,7 @@ class ZambaConfig(PretrainedConfig):\n             `num_key_value_heads=None`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf).\n         n_mamba_heads (`int`, *optional*, defaults to 2):\n             Number of mamba heads for each mamba layer."
        },
        {
            "sha": "1392eab1b3cd8ef7d2855f33c2c814b739546760",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -79,7 +79,7 @@ class Zamba2Config(PretrainedConfig):\n             `num_key_value_heads=None`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n+            by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf).\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities."
        },
        {
            "sha": "4df34d22a7d5c99af2205d01cd87d24d3afd4206",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -34,7 +34,7 @@ class BitNetHfQuantizer(HfQuantizer):\n     1.58-bit quantization from BitNet quantization method:\n     Before loading: it converts the linear layers into BitLinear layers during loading.\n \n-    Checkout the paper introducing this method : https://arxiv.org/pdf/2402.17764\n+    Check out the paper introducing this method : https://arxiv.org/pdf/2402.17764\n     \"\"\"\n \n     requires_parameters_quantization = False"
        },
        {
            "sha": "f11045912958bf7c536a0fe37ddf4ef62da919a5",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -90,7 +90,7 @@ def is_offline_mode():\n default_cache_path = constants.default_cache_path\n \n # Determine default cache directory. Lots of legacy environment variables to ensure backward compatibility.\n-# The best way to set the cache path is with the environment variable HF_HOME. For more details, checkout this\n+# The best way to set the cache path is with the environment variable HF_HOME. For more details, check out this\n # documentation page: https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables.\n #\n # In code, use `HF_HUB_CACHE` as the default cache path. This variable is set by the library and is guaranteed\n@@ -542,7 +542,7 @@ def cached_files(\n             elif _raise_exceptions_for_missing_entries:\n                 raise OSError(\n                     f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the\"\n-                    f\" cached files.\\nCheckout your internet connection or see how to run the library in offline mode at\"\n+                    f\" cached files.\\nCheck your internet connection or see how to run the library in offline mode at\"\n                     \" 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\n                 ) from e\n         # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated"
        },
        {
            "sha": "0d412406d9f86a625fe53e96e1a30ef9a477674c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -1492,39 +1492,39 @@ def check_torch_load_is_safe():\n \n # docstyle-ignore\n SENTENCEPIECE_IMPORT_ERROR = \"\"\"\n-{0} requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n that match your environment. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n \n # docstyle-ignore\n PROTOBUF_IMPORT_ERROR = \"\"\"\n-{0} requires the protobuf library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the protobuf library but it was not found in your environment. Check out the instructions on the\n installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\n that match your environment. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n \n # docstyle-ignore\n FAISS_IMPORT_ERROR = \"\"\"\n-{0} requires the faiss library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the faiss library but it was not found in your environment. Check out the instructions on the\n installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\n that match your environment. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n \n # docstyle-ignore\n PYTORCH_IMPORT_ERROR = \"\"\"\n-{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the PyTorch library but it was not found in your environment. Check out the instructions on the\n installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n \n # docstyle-ignore\n TORCHVISION_IMPORT_ERROR = \"\"\"\n-{0} requires the Torchvision library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the Torchvision library but it was not found in your environment. Check out the instructions on the\n installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n@@ -1576,30 +1576,30 @@ def check_torch_load_is_safe():\n \n # docstyle-ignore\n TENSORFLOW_IMPORT_ERROR = \"\"\"\n-{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the TensorFlow library but it was not found in your environment. Check out the instructions on the\n installation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n \n # docstyle-ignore\n DETECTRON2_IMPORT_ERROR = \"\"\"\n-{0} requires the detectron2 library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the detectron2 library but it was not found in your environment. Check out the instructions on the\n installation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones\n that match your environment. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n \n # docstyle-ignore\n FLAX_IMPORT_ERROR = \"\"\"\n-{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the FLAX library but it was not found in your environment. Check out the instructions on the\n installation page: https://github.com/google/flax and follow the ones that match your environment.\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n # docstyle-ignore\n FTFY_IMPORT_ERROR = \"\"\"\n-{0} requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n+{0} requires the ftfy library but it was not found in your environment. Check out the instructions on the\n installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n that match your environment. Please note that you may need to restart your runtime after installation.\n \"\"\""
        },
        {
            "sha": "7f60be589b8fd6eb62111171bfd5711023287450",
            "filename": "templates/adding_a_new_model/README.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19224c3642705c5b6988c9f5f4251f83323d05ae/templates%2Fadding_a_new_model%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/19224c3642705c5b6988c9f5f4251f83323d05ae/templates%2Fadding_a_new_model%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_model%2FREADME.md?ref=19224c3642705c5b6988c9f5f4251f83323d05ae",
            "patch": "@@ -19,5 +19,5 @@ limitations under the License.\n This page has been updated in light of the removal of the `add_new_model` script in favor of the more complete \n `add_new_model_like` script.\n \n-We recommend you checkout the documentation of [How to add a model](https://huggingface.co/docs/transformers/main/en/add_new_model)\n-in the Hugging Face Transformers documentation for complete and up-to-date instructions.\n+We recommend you check out the documentation on [how to add a model](https://huggingface.co/docs/transformers/main/en/add_new_model) \n+for complete and up-to-date instructions."
        }
    ],
    "stats": {
        "total": 182,
        "additions": 91,
        "deletions": 91
    }
}