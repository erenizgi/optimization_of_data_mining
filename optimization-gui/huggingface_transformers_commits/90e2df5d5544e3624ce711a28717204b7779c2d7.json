{
    "author": "Ssukriti",
    "message": "fix: loss computation after embeddings resize - mllama (#36840)\n\n* move loss to generation class\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n* code cleanup\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n* test for resize and loss computation\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n* fix tests\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n* fix:test for resize and loss\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n* fix resize embedding mllama test\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n* review changes\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>\n\n---------\n\nSigned-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>",
    "sha": "90e2df5d5544e3624ce711a28717204b7779c2d7",
    "files": [
        {
            "sha": "0818d90a57baead10617d95d2a84d802a2cdebbb",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 19,
            "deletions": 2,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/90e2df5d5544e3624ce711a28717204b7779c2d7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90e2df5d5544e3624ce711a28717204b7779c2d7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=90e2df5d5544e3624ce711a28717204b7779c2d7",
            "patch": "@@ -2056,6 +2056,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2157,15 +2158,31 @@ def forward(\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             inputs_embeds=inputs_embeds,\n-            labels=labels,\n             output_hidden_states=output_hidden_states,\n             output_attentions=output_attentions,\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **loss_kwargs,\n         )\n \n-        return outputs\n+        # Temporary fix to calculate the loss in main class, as the model's vocab size may be resized\n+        loss = None\n+        logits = outputs[0]\n+\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config.get_text_config().vocab_size, **loss_kwargs)\n+\n+        if not return_dict:\n+            return (loss,) + outputs if loss is not None else outputs\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=outputs.logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n \n     def prepare_inputs_for_generation(\n         self,"
        },
        {
            "sha": "ae28bd6697e895eae1b8285a5ed3bc1937766b7f",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/90e2df5d5544e3624ce711a28717204b7779c2d7/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90e2df5d5544e3624ce711a28717204b7779c2d7/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=90e2df5d5544e3624ce711a28717204b7779c2d7",
            "patch": "@@ -321,6 +321,24 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             torch.testing.assert_close(out_embeds, out_ids)\n \n+    def test_resize_embeddings_results_in_successful_loss(self):\n+        # resizing embeddings should result in successful loss computation\n+        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model_vocab_size = config.get_text_config().vocab_size\n+            inputs = self._prepare_for_class(inputs, model_class, return_labels=True)\n+            # Resize embeddings and call forward\n+            model.resize_token_embeddings(model_vocab_size + 10)\n+            output = model(\n+                input_ids=inputs[\"input_ids\"],\n+                attention_mask=inputs[\"attention_mask\"],\n+                labels=inputs[\"labels\"],\n+                return_dict=True,\n+            )\n+            self.assertTrue(\"loss\" in output)\n+\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 37,
        "deletions": 2
    }
}