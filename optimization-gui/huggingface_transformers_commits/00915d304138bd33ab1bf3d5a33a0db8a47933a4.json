{
    "author": "YenFuLin",
    "message": "Fix chameleon's TypeError because inputs_embeds may None (#36673)\n\n* fix chameleon TypeError when inputs_embeds is None\n\n* reformat\n\n* hotfix",
    "sha": "00915d304138bd33ab1bf3d5a33a0db8a47933a4",
    "files": [
        {
            "sha": "bb09eb26cc636ce77add2e15c177256e1154cc5b",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/00915d304138bd33ab1bf3d5a33a0db8a47933a4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00915d304138bd33ab1bf3d5a33a0db8a47933a4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=00915d304138bd33ab1bf3d5a33a0db8a47933a4",
            "patch": "@@ -1289,6 +1289,9 @@ def forward(\n                 \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n             )\n \n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n         if pixel_values is not None:\n             image_tokens = self.get_image_tokens(pixel_values)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n@@ -1301,9 +1304,6 @@ def forward(\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n-\n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n             past_key_values = DynamicCache()"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}