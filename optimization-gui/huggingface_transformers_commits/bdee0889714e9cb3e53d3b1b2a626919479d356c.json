{
    "author": "vasqu",
    "message": "[`Attn Masks`] Lift bidirectional mask restriction on eager (#42325)\n\n* remove restriction\n\n* fix\n\n* add test and refactor tests\n\n* style\n\n* add docstring",
    "sha": "bdee0889714e9cb3e53d3b1b2a626919479d356c",
    "files": [
        {
            "sha": "311467e4c7818b770f1c412806b719a112ab7f58",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdee0889714e9cb3e53d3b1b2a626919479d356c/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdee0889714e9cb3e53d3b1b2a626919479d356c/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=bdee0889714e9cb3e53d3b1b2a626919479d356c",
            "patch": "@@ -340,9 +340,6 @@ def sdpa_mask(\n         allow_is_causal_skip (`bool`, optional):\n             Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in\n             `torch.sdpa` instead. Default to `True`.\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n         allow_is_bidirectional_skip (`bool`, optional):\n             Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n             i.e. full attention without any padding. Default to `False`.\n@@ -480,6 +477,7 @@ def eager_mask(\n     mask_function: Callable = causal_mask_function,\n     attention_mask: Optional[torch.Tensor] = None,\n     dtype: torch.dtype = torch.float32,\n+    allow_is_bidirectional_skip: bool = False,\n     use_vmap: bool = False,\n     **kwargs,\n ) -> torch.Tensor:\n@@ -503,13 +501,15 @@ def eager_mask(\n             The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n         dtype (`torch.dtype`, optional):\n             The dtype to use for the mask. By default, `torch.float32`.\n+        allow_is_bidirectional_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n+            i.e. full attention without any padding. Default to `False`.\n         use_vmap (`bool`, optional):\n             Whether to use `vmap` during the mask construction or not. Allows powerful custom patterns that may not be\n             index-based (for the cost of speed performance). By default `False`.\n     \"\"\"\n     # The masks for eager attention are simply boolean mask from sdpa, casted to 0 and -inf\n     _ = kwargs.pop(\"allow_is_causal_skip\", None)\n-    _ = kwargs.pop(\"allow_is_bidirectional_skip\", None)\n     _ = kwargs.pop(\"allow_torch_fix\", None)\n     mask = sdpa_mask(\n         batch_size=batch_size,\n@@ -519,14 +519,16 @@ def eager_mask(\n         mask_function=mask_function,\n         attention_mask=attention_mask,\n         allow_is_causal_skip=False,\n-        allow_is_bidirectional_skip=False,\n+        allow_is_bidirectional_skip=allow_is_bidirectional_skip,\n         allow_torch_fix=False,\n         use_vmap=use_vmap,\n         **kwargs,\n     )\n-    min_dtype = torch.finfo(dtype).min\n-    # we need 0s where the tokens should be taken into account, and -inf otherwise (mask is already of boolean type)\n-    mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)\n+    # only bidirectional masks can be skipped, otherwise we convert bool -> float\n+    if mask is not None:\n+        min_dtype = torch.finfo(dtype).min\n+        # we need 0s where the tokens should be taken into account, and -inf otherwise (mask is already of boolean type)\n+        mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)\n     return mask\n \n "
        },
        {
            "sha": "3b20ddeb2302792cd8317f33b66f633ee2e75ba2",
            "filename": "tests/utils/test_masking_utils.py",
            "status": "modified",
            "additions": 39,
            "deletions": 20,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdee0889714e9cb3e53d3b1b2a626919479d356c/tests%2Futils%2Ftest_masking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdee0889714e9cb3e53d3b1b2a626919479d356c/tests%2Futils%2Ftest_masking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_masking_utils.py?ref=bdee0889714e9cb3e53d3b1b2a626919479d356c",
            "patch": "@@ -18,7 +18,6 @@\n     cleanup,\n     is_torch_available,\n     require_torch,\n-    require_torch_accelerator,\n     torch_device,\n )\n \n@@ -262,30 +261,25 @@ def test_chunked_mask_with_left_padding_decoding(self):\n \n         self.assertTrue((chunked_attention_mask == EXPECTED_CHUNKED_MASK).all())\n \n-    @require_torch_accelerator\n-    def test_bidirectional_mask_cudagraphs(self):\n-        \"\"\"\n-        Checks whether the bidirectional mask creation is compatible with cuda graphs, i.e. we do not into any error\n-        during this test.\n-        \"\"\"\n-\n+    @staticmethod\n+    def _run_bidirectional_mask(mask_fn, attn_implementation):\n         def run_mask_creation(mask_fn, config, input_embeds, encoder_mask, cross_mask, encoder_hidden_states):\n-            _ = mask_fn(\n+            encoder_attn_mask = mask_fn(\n                 config=config,\n                 input_embeds=input_embeds,\n                 attention_mask=encoder_mask,\n             )\n-\n-            _ = mask_fn(\n+            cross_attn_mask = mask_fn(\n                 config=config,\n                 input_embeds=input_embeds,\n                 attention_mask=cross_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n             )\n+            return encoder_attn_mask, cross_attn_mask\n \n         # We use llama but could be also bert/bart --> we only need the `_attn_implementation` here\n         config = LlamaConfig()\n-        config._attn_implementation = \"sdpa\"\n+        config._attn_implementation = attn_implementation\n \n         # Meta data\n         batch_size = 2\n@@ -298,19 +292,17 @@ def run_mask_creation(mask_fn, config, input_embeds, encoder_mask, cross_mask, e\n         encoder_mask = torch.ones_like(input_embeds)[..., 0]\n         cross_mask = torch.ones_like(encoder_hidden_states)[..., 0]\n \n-        mask_creation_function = torch.compile(create_bidirectional_mask, mode=\"reduce-overhead\")\n-\n         # Case 1: Full mask\n-        run_mask_creation(\n-            mask_fn=mask_creation_function,\n+        full_mask_encoder_1, full_mask_cross_1 = run_mask_creation(\n+            mask_fn=mask_fn,\n             config=config,\n             input_embeds=input_embeds,\n             encoder_mask=encoder_mask,\n             cross_mask=cross_mask,\n             encoder_hidden_states=encoder_hidden_states,\n         )\n-        run_mask_creation(\n-            mask_fn=mask_creation_function,\n+        full_mask_encoder_2, full_mask_cross_2 = run_mask_creation(\n+            mask_fn=mask_fn,\n             config=config,\n             input_embeds=input_embeds,\n             encoder_mask=None,\n@@ -322,11 +314,38 @@ def run_mask_creation(mask_fn, config, input_embeds, encoder_mask, cross_mask, e\n         cross_mask[:, -1] = 0\n         encoder_mask[:, -1] = 0\n \n-        run_mask_creation(\n-            mask_fn=mask_creation_function,\n+        padded_mask_encoder, padded_mask_cross = run_mask_creation(\n+            mask_fn=mask_fn,\n             config=config,\n             input_embeds=input_embeds,\n             encoder_mask=encoder_mask,\n             cross_mask=cross_mask,\n             encoder_hidden_states=encoder_hidden_states,\n         )\n+\n+        full_masks = (full_mask_encoder_1, full_mask_encoder_2), (full_mask_cross_1, full_mask_cross_2)\n+        padded_masks = (padded_mask_encoder, padded_mask_cross)\n+        return full_masks, padded_masks\n+\n+    def test_bidirectional_mask_cudagraphs(self):\n+        \"\"\"\n+        Checks whether the bidirectional mask creation is compatible with cuda graphs, i.e. we do not into any error\n+        during this test.\n+        \"\"\"\n+        mask_creation_function = torch.compile(create_bidirectional_mask, mode=\"reduce-overhead\")\n+        self._run_bidirectional_mask(mask_fn=mask_creation_function, attn_implementation=\"sdpa\")\n+\n+    def test_bidirectional_mask_skip_eager(self):\n+        \"\"\"\n+        Checks whether the bidirectional mask creation can skip the mask creation if we have a full mask.\n+        \"\"\"\n+        full_masks, padded_mask = self._run_bidirectional_mask(\n+            mask_fn=create_bidirectional_mask, attn_implementation=\"eager\"\n+        )\n+\n+        for alternative_masks in full_masks:\n+            self.assertTrue(alternative_masks[0] is None)\n+            self.assertTrue(alternative_masks[1] is None)\n+\n+        self.assertTrue(padded_mask[0] is not None)\n+        self.assertTrue(padded_mask[1] is not None)"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 49,
        "deletions": 28
    }
}