{
    "author": "bastrob",
    "message": "owlvit/2 dynamic input resolution (#34764)\n\n* owlvit/2 dynamic input resolution.\r\n\r\n* adapt box grid to patch_dim_h patch_dim_w\r\n\r\n* fix ci\r\n\r\n* clarify variable naming\r\n\r\n* clarify variable naming..\r\n\r\n* compute box_bias dynamically inside box_predictor\r\n\r\n* change style part of code\r\n\r\n* [run-slow] owlvit, owlv2",
    "sha": "8f38f58f3de5a35f9b8505e9b48985dce5470985",
    "files": [
        {
            "sha": "7b631a77fcdda3e8ba46f1cf6f3da9f2b084bf04",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 146,
            "deletions": 36,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f38f58f3de5a35f9b8505e9b48985dce5470985/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=8f38f58f3de5a35f9b8505e9b48985dce5470985",
            "patch": "@@ -33,6 +33,7 @@\n     is_vision_available,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_owlv2 import Owlv2Config, Owlv2TextConfig, Owlv2VisionConfig\n \n@@ -274,6 +275,7 @@ def to_tuple(self) -> Tuple[Any]:\n class Owlv2VisionEmbeddings(nn.Module):\n     def __init__(self, config: Owlv2VisionConfig):\n         super().__init__()\n+        self.patch_size = config.patch_size\n         self.config = config\n         self.embed_dim = config.hidden_size\n         self.class_embedding = nn.Parameter(torch.randn(config.hidden_size))\n@@ -291,15 +293,59 @@ def __init__(self, config: Owlv2VisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.interpolate_pos_encoding\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embedding(self.position_ids)\n+\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n         patch_embeds = self.patch_embedding(pixel_values)  # shape = [batch_size, num_channels, height, width]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n-\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -610,6 +656,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -635,6 +683,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_base_image_embeds (`bool`, *optional*):\n             Whether or not to return the base image embeddings.\n         return_dict (`bool`, *optional*):\n@@ -657,6 +707,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the last hidden state. See `text_model_last_hidden_state` and\n             `vision_model_last_hidden_state` under returned tensors for more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -673,6 +725,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -914,6 +968,7 @@ def forward(\n         pixel_values: torch.FloatTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -929,7 +984,7 @@ def forward(\n         expected_input_dtype = self.embeddings.patch_embedding.weight.dtype\n         pixel_values = pixel_values.to(expected_input_dtype)\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layernorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -976,6 +1031,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1002,6 +1058,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1084,6 +1141,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1115,6 +1173,7 @@ def get_image_features(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1133,6 +1192,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_base_image_embeds: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Owlv2Output]:\n@@ -1165,6 +1225,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1295,21 +1356,23 @@ def __init__(self, config: Owlv2Config):\n \n         self.layer_norm = nn.LayerNorm(config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps)\n         self.sigmoid = nn.Sigmoid()\n-\n-        self.sqrt_num_patches = config.vision_config.image_size // config.vision_config.patch_size\n-        self.box_bias = self.compute_box_bias(self.sqrt_num_patches)\n+        self.config = config\n+        self.num_patches_height = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        self.num_patches_width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        self.box_bias = self.compute_box_bias(self.num_patches_height, self.num_patches_width)\n \n     @staticmethod\n     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.normalize_grid_corner_coordinates\n-    def normalize_grid_corner_coordinates(num_patches: int) -> torch.Tensor:\n+    def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width: int) -> torch.Tensor:\n         # Create grid coordinates using torch\n-        x_coordinates = torch.arange(1, num_patches + 1, dtype=torch.float32)\n-        y_coordinates = torch.arange(1, num_patches + 1, dtype=torch.float32)\n+        x_coordinates = torch.arange(1, num_patches_width + 1, dtype=torch.float32)\n+        y_coordinates = torch.arange(1, num_patches_height + 1, dtype=torch.float32)\n         xx, yy = torch.meshgrid(x_coordinates, y_coordinates, indexing=\"xy\")\n \n-        # Stack the coordinates and divide by num_patches\n+        # Stack the coordinates and divide by their respective patch counts\n         box_coordinates = torch.stack((xx, yy), dim=-1)\n-        box_coordinates /= num_patches\n+        box_coordinates[..., 0] /= num_patches_width\n+        box_coordinates[..., 1] /= num_patches_height\n \n         # Flatten (h, w, 2) -> (h*w, 2)\n         box_coordinates = box_coordinates.view(-1, 2)\n@@ -1332,18 +1395,22 @@ def objectness_predictor(self, image_features: torch.FloatTensor) -> torch.Float\n \n     @lru_cache(maxsize=2)\n     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.compute_box_bias\n-    def compute_box_bias(self, num_patches: int, feature_map: Optional[torch.FloatTensor] = None) -> torch.Tensor:\n+    def compute_box_bias(\n+        self, num_patches_height: int, num_patches_width: int, feature_map: Optional[torch.FloatTensor] = None\n+    ) -> torch.Tensor:\n         if feature_map is not None:\n             raise ValueError(\"feature_map has been deprecated as an input. Please pass in num_patches instead\")\n         # The box center is biased to its position on the feature grid\n-        box_coordinates = self.normalize_grid_corner_coordinates(num_patches)\n+        box_coordinates = self.normalize_grid_corner_coordinates(num_patches_height, num_patches_width)\n         box_coordinates = torch.clip(box_coordinates, 0.0, 1.0)\n \n         # Unnormalize xy\n         box_coord_bias = torch.log(box_coordinates + 1e-4) - torch.log1p(-box_coordinates + 1e-4)\n \n         # The box size is biased to the patch size\n-        box_size = torch.full_like(box_coord_bias, 1.0 / num_patches)\n+        box_size = torch.full_like(box_coord_bias, 1.0)\n+        box_size[..., 0] /= num_patches_width\n+        box_size[..., 1] /= num_patches_height\n         box_size_bias = torch.log(box_size + 1e-4) - torch.log1p(-box_size + 1e-4)\n \n         # Compute box bias\n@@ -1355,13 +1422,16 @@ def box_predictor(\n         self,\n         image_feats: torch.FloatTensor,\n         feature_map: torch.FloatTensor,\n+        interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         \"\"\"\n         Args:\n             image_feats:\n                 Features extracted from the image, returned by the `image_text_embedder` method.\n             feature_map:\n                 A spatial re-arrangement of image_features, also returned by the `image_text_embedder` method.\n+            interpolate_pos_encoding:\n+                Whether to interpolate the pre-trained position encodings.\n         Returns:\n             pred_boxes:\n                 List of predicted boxes (cxcywh normalized to 0, 1) nested within a dictionary.\n@@ -1370,7 +1440,13 @@ def box_predictor(\n         pred_boxes = self.box_head(image_feats)\n \n         # Compute the location of each token on the grid and use it to compute a bias for the bbox prediction\n-        box_bias = self.box_bias.to(feature_map.device)\n+        if interpolate_pos_encoding:\n+            _, num_patches_height, num_patches_width, _ = feature_map.shape\n+            box_bias = self.compute_box_bias(num_patches_height, num_patches_width)\n+        else:\n+            box_bias = self.box_bias\n+\n+        box_bias = box_bias.to(feature_map.device)\n         pred_boxes += box_bias\n         pred_boxes = self.sigmoid(pred_boxes)\n         return pred_boxes\n@@ -1403,6 +1479,7 @@ def image_text_embedder(\n         attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n     ) -> Tuple[torch.FloatTensor]:\n         # Encode text and image\n         outputs = self.owlv2(\n@@ -1411,9 +1488,18 @@ def image_text_embedder(\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=True,\n         )\n \n+        if interpolate_pos_encoding:\n+            _, _, height, width = pixel_values.shape\n+            num_patches_height = height // self.config.vision_config.patch_size\n+            num_patches_width = width // self.config.vision_config.patch_size\n+        else:\n+            num_patches_height = self.num_patches_height\n+            num_patches_width = self.num_patches_width\n+\n         # Get image embeddings\n         last_hidden_state = outputs.vision_model_output[0]\n         image_embeds = self.owlv2.vision_model.post_layernorm(last_hidden_state)\n@@ -1425,11 +1511,11 @@ def image_text_embedder(\n         image_embeds = image_embeds[:, 1:, :] * class_token_out\n         image_embeds = self.layer_norm(image_embeds)\n \n-        # Resize to [batch_size, num_patches, num_patches, hidden_size]\n+        # Resize to [batch_size, num_patches_height, num_patches_width, hidden_size]\n         new_size = (\n             image_embeds.shape[0],\n-            self.sqrt_num_patches,\n-            self.sqrt_num_patches,\n+            num_patches_height,\n+            num_patches_width,\n             image_embeds.shape[-1],\n         )\n         image_embeds = image_embeds.reshape(new_size)\n@@ -1443,9 +1529,20 @@ def image_embedder(\n         pixel_values: torch.FloatTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n     ) -> Tuple[torch.FloatTensor]:\n         # Get Owlv2Model vision embeddings (same as CLIP)\n-        vision_outputs = self.owlv2.vision_model(pixel_values=pixel_values, return_dict=True)\n+        vision_outputs = self.owlv2.vision_model(\n+            pixel_values=pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, return_dict=True\n+        )\n+\n+        if interpolate_pos_encoding:\n+            _, _, height, width = pixel_values.shape\n+            num_patches_height = height // self.config.vision_config.patch_size\n+            num_patches_width = width // self.config.vision_config.patch_size\n+        else:\n+            num_patches_height = self.num_patches_height\n+            num_patches_width = self.num_patches_width\n \n         # Apply post_layernorm to last_hidden_state, return non-projected output\n         last_hidden_state = vision_outputs[0]\n@@ -1458,11 +1555,11 @@ def image_embedder(\n         image_embeds = image_embeds[:, 1:, :] * class_token_out\n         image_embeds = self.layer_norm(image_embeds)\n \n-        # Resize to [batch_size, num_patches, num_patches, hidden_size]\n+        # Resize to [batch_size, num_patches_height, num_patches_width, hidden_size]\n         new_size = (\n             image_embeds.shape[0],\n-            self.sqrt_num_patches,\n-            self.sqrt_num_patches,\n+            num_patches_height,\n+            num_patches_width,\n             image_embeds.shape[-1],\n         )\n         image_embeds = image_embeds.reshape(new_size)\n@@ -1471,10 +1568,13 @@ def image_embedder(\n \n     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.embed_image_query\n     def embed_image_query(\n-        self, query_image_features: torch.FloatTensor, query_feature_map: torch.FloatTensor\n+        self,\n+        query_image_features: torch.FloatTensor,\n+        query_feature_map: torch.FloatTensor,\n+        interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         _, class_embeds = self.class_predictor(query_image_features)\n-        pred_boxes = self.box_predictor(query_image_features, query_feature_map)\n+        pred_boxes = self.box_predictor(query_image_features, query_feature_map, interpolate_pos_encoding)\n         pred_boxes_as_corners = center_to_corners_format(pred_boxes)\n \n         # Loop over query images\n@@ -1519,6 +1619,7 @@ def image_guided_detection(\n         query_pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Owlv2ImageGuidedObjectDetectionOutput:\n         r\"\"\"\n@@ -1576,26 +1677,33 @@ def image_guided_detection(\n         return_dict = return_dict if return_dict is not None else self.config.return_dict\n \n         # Compute feature maps for the input and query images\n-        query_feature_map = self.image_embedder(pixel_values=query_pixel_values)[0]\n+        query_feature_map = self.image_embedder(\n+            pixel_values=query_pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n+        )[0]\n         feature_map, vision_outputs = self.image_embedder(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n-        batch_size, num_patches, num_patches, hidden_dim = feature_map.shape\n-        image_feats = torch.reshape(feature_map, (batch_size, num_patches * num_patches, hidden_dim))\n+        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape\n+        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))\n \n-        batch_size, num_patches, num_patches, hidden_dim = query_feature_map.shape\n-        query_image_feats = torch.reshape(query_feature_map, (batch_size, num_patches * num_patches, hidden_dim))\n+        batch_size, num_patches_height, num_patches_width, hidden_dim = query_feature_map.shape\n+        query_image_feats = torch.reshape(\n+            query_feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim)\n+        )\n         # Get top class embedding and best box index for each query image in batch\n-        query_embeds, best_box_indices, query_pred_boxes = self.embed_image_query(query_image_feats, query_feature_map)\n+        query_embeds, best_box_indices, query_pred_boxes = self.embed_image_query(\n+            query_image_feats, query_feature_map, interpolate_pos_encoding\n+        )\n \n         # Predict object classes [batch_size, num_patches, num_queries+1]\n         (pred_logits, class_embeds) = self.class_predictor(image_feats=image_feats, query_embeds=query_embeds)\n \n         # Predict object boxes\n-        target_pred_boxes = self.box_predictor(image_feats, feature_map)\n+        target_pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)\n \n         if not return_dict:\n             output = (\n@@ -1630,6 +1738,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Owlv2ObjectDetectionOutput:\n         r\"\"\"\n@@ -1683,14 +1792,15 @@ def forward(\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n         # Text and vision model outputs\n         text_outputs = outputs.text_model_output\n         vision_outputs = outputs.vision_model_output\n \n-        batch_size, num_patches, num_patches, hidden_dim = feature_map.shape\n-        image_feats = torch.reshape(feature_map, (batch_size, num_patches * num_patches, hidden_dim))\n+        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape\n+        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))\n \n         # Reshape from [batch_size * max_text_queries, hidden_dim] -> [batch_size, max_text_queries, hidden_dim]\n         max_text_queries = input_ids.shape[0] // batch_size\n@@ -1707,7 +1817,7 @@ def forward(\n         objectness_logits = self.objectness_predictor(image_feats)\n \n         # Predict object boxes\n-        pred_boxes = self.box_predictor(image_feats, feature_map)\n+        pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)\n \n         if not return_dict:\n             output = ("
        },
        {
            "sha": "570d154a554c03c6a9d6f13008e3d55264f9bf1b",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 143,
            "deletions": 37,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f38f58f3de5a35f9b8505e9b48985dce5470985/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=8f38f58f3de5a35f9b8505e9b48985dce5470985",
            "patch": "@@ -33,6 +33,7 @@\n     is_vision_available,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_owlvit import OwlViTConfig, OwlViTTextConfig, OwlViTVisionConfig\n \n@@ -268,6 +269,7 @@ def to_tuple(self) -> Tuple[Any]:\n class OwlViTVisionEmbeddings(nn.Module):\n     def __init__(self, config: OwlViTVisionConfig):\n         super().__init__()\n+        self.patch_size = config.patch_size\n         self.config = config\n         self.embed_dim = config.hidden_size\n         self.class_embedding = nn.Parameter(torch.randn(config.hidden_size))\n@@ -285,15 +287,55 @@ def __init__(self, config: OwlViTVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.interpolate_pos_encoding\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embedding(self.position_ids)\n+\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n         patch_embeds = self.patch_embedding(pixel_values)  # shape = [batch_size, num_channels, height, width]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n-\n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n-\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -601,6 +643,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -626,6 +670,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -646,6 +692,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the last hidden state. See `text_model_last_hidden_state` and\n             `vision_model_last_hidden_state` under returned tensors for more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -662,6 +710,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -899,6 +949,7 @@ def forward(\n         pixel_values: torch.FloatTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -914,7 +965,7 @@ def forward(\n         expected_input_dtype = self.embeddings.patch_embedding.weight.dtype\n         pixel_values = pixel_values.to(expected_input_dtype)\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layernorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -960,6 +1011,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -986,6 +1038,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1067,6 +1120,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1098,6 +1152,7 @@ def get_image_features(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1116,6 +1171,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_base_image_embeds: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, OwlViTOutput]:\n@@ -1148,6 +1204,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1275,39 +1332,45 @@ def __init__(self, config: OwlViTConfig):\n \n         self.layer_norm = nn.LayerNorm(config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps)\n         self.sigmoid = nn.Sigmoid()\n-\n-        self.sqrt_num_patches = config.vision_config.image_size // config.vision_config.patch_size\n-        self.box_bias = self.compute_box_bias(self.sqrt_num_patches)\n+        self.config = config\n+        self.num_patches_height = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        self.num_patches_width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        self.box_bias = self.compute_box_bias(self.num_patches_height, self.num_patches_width)\n \n     @staticmethod\n-    def normalize_grid_corner_coordinates(num_patches: int) -> torch.Tensor:\n+    def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width: int) -> torch.Tensor:\n         # Create grid coordinates using torch\n-        x_coordinates = torch.arange(1, num_patches + 1, dtype=torch.float32)\n-        y_coordinates = torch.arange(1, num_patches + 1, dtype=torch.float32)\n+        x_coordinates = torch.arange(1, num_patches_width + 1, dtype=torch.float32)\n+        y_coordinates = torch.arange(1, num_patches_height + 1, dtype=torch.float32)\n         xx, yy = torch.meshgrid(x_coordinates, y_coordinates, indexing=\"xy\")\n \n-        # Stack the coordinates and divide by num_patches\n+        # Stack the coordinates and divide by their respective patch counts\n         box_coordinates = torch.stack((xx, yy), dim=-1)\n-        box_coordinates /= num_patches\n+        box_coordinates[..., 0] /= num_patches_width\n+        box_coordinates[..., 1] /= num_patches_height\n \n         # Flatten (h, w, 2) -> (h*w, 2)\n         box_coordinates = box_coordinates.view(-1, 2)\n \n         return box_coordinates\n \n     @lru_cache(maxsize=2)\n-    def compute_box_bias(self, num_patches: int, feature_map: Optional[torch.FloatTensor] = None) -> torch.Tensor:\n+    def compute_box_bias(\n+        self, num_patches_height: int, num_patches_width: int, feature_map: Optional[torch.FloatTensor] = None\n+    ) -> torch.Tensor:\n         if feature_map is not None:\n             raise ValueError(\"feature_map has been deprecated as an input. Please pass in num_patches instead\")\n         # The box center is biased to its position on the feature grid\n-        box_coordinates = self.normalize_grid_corner_coordinates(num_patches)\n+        box_coordinates = self.normalize_grid_corner_coordinates(num_patches_height, num_patches_width)\n         box_coordinates = torch.clip(box_coordinates, 0.0, 1.0)\n \n         # Unnormalize xy\n         box_coord_bias = torch.log(box_coordinates + 1e-4) - torch.log1p(-box_coordinates + 1e-4)\n \n         # The box size is biased to the patch size\n-        box_size = torch.full_like(box_coord_bias, 1.0 / num_patches)\n+        box_size = torch.full_like(box_coord_bias, 1.0)\n+        box_size[..., 0] /= num_patches_width\n+        box_size[..., 1] /= num_patches_height\n         box_size_bias = torch.log(box_size + 1e-4) - torch.log1p(-box_size + 1e-4)\n \n         # Compute box bias\n@@ -1318,13 +1381,16 @@ def box_predictor(\n         self,\n         image_feats: torch.FloatTensor,\n         feature_map: torch.FloatTensor,\n+        interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         \"\"\"\n         Args:\n             image_feats:\n                 Features extracted from the image, returned by the `image_text_embedder` method.\n             feature_map:\n                 A spatial re-arrangement of image_features, also returned by the `image_text_embedder` method.\n+            interpolate_pos_encoding:\n+                Whether to interpolate the pre-trained position encodings.\n         Returns:\n             pred_boxes:\n                 List of predicted boxes (cxcywh normalized to 0, 1) nested within a dictionary.\n@@ -1333,7 +1399,13 @@ def box_predictor(\n         pred_boxes = self.box_head(image_feats)\n \n         # Compute the location of each token on the grid and use it to compute a bias for the bbox prediction\n-        box_bias = self.box_bias.to(feature_map.device)\n+        if interpolate_pos_encoding:\n+            _, num_patches_height, num_patches_width, _ = feature_map.shape\n+            box_bias = self.compute_box_bias(num_patches_height, num_patches_width)\n+        else:\n+            box_bias = self.box_bias\n+\n+        box_bias = box_bias.to(feature_map.device)\n         pred_boxes += box_bias\n         pred_boxes = self.sigmoid(pred_boxes)\n         return pred_boxes\n@@ -1364,6 +1436,7 @@ def image_text_embedder(\n         attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n     ) -> Tuple[torch.FloatTensor]:\n         # Encode text and image\n         outputs = self.owlvit(\n@@ -1372,9 +1445,18 @@ def image_text_embedder(\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=True,\n         )\n \n+        if interpolate_pos_encoding:\n+            _, _, height, width = pixel_values.shape\n+            num_patches_height = height // self.config.vision_config.patch_size\n+            num_patches_width = width // self.config.vision_config.patch_size\n+        else:\n+            num_patches_height = self.num_patches_height\n+            num_patches_width = self.num_patches_width\n+\n         # Get image embeddings\n         last_hidden_state = outputs.vision_model_output[0]\n         image_embeds = self.owlvit.vision_model.post_layernorm(last_hidden_state)\n@@ -1386,11 +1468,11 @@ def image_text_embedder(\n         image_embeds = image_embeds[:, 1:, :] * class_token_out\n         image_embeds = self.layer_norm(image_embeds)\n \n-        # Resize to [batch_size, num_patches, num_patches, hidden_size]\n+        # Resize to [batch_size, num_patches_height, num_patches_width, hidden_size]\n         new_size = (\n             image_embeds.shape[0],\n-            self.sqrt_num_patches,\n-            self.sqrt_num_patches,\n+            num_patches_height,\n+            num_patches_width,\n             image_embeds.shape[-1],\n         )\n         image_embeds = image_embeds.reshape(new_size)\n@@ -1403,9 +1485,20 @@ def image_embedder(\n         pixel_values: torch.FloatTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n     ) -> Tuple[torch.FloatTensor]:\n         # Get OwlViTModel vision embeddings (same as CLIP)\n-        vision_outputs = self.owlvit.vision_model(pixel_values=pixel_values, return_dict=True)\n+        vision_outputs = self.owlvit.vision_model(\n+            pixel_values=pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, return_dict=True\n+        )\n+\n+        if interpolate_pos_encoding:\n+            _, _, height, width = pixel_values.shape\n+            num_patches_height = height // self.config.vision_config.patch_size\n+            num_patches_width = width // self.config.vision_config.patch_size\n+        else:\n+            num_patches_height = self.num_patches_height\n+            num_patches_width = self.num_patches_width\n \n         # Apply post_layernorm to last_hidden_state, return non-projected output\n         last_hidden_state = vision_outputs[0]\n@@ -1418,22 +1511,25 @@ def image_embedder(\n         image_embeds = image_embeds[:, 1:, :] * class_token_out\n         image_embeds = self.layer_norm(image_embeds)\n \n-        # Resize to [batch_size, num_patches, num_patches, hidden_size]\n+        # Resize to [batch_size, num_patches_height, num_patches_width, hidden_size]\n         new_size = (\n             image_embeds.shape[0],\n-            self.sqrt_num_patches,\n-            self.sqrt_num_patches,\n+            num_patches_height,\n+            num_patches_width,\n             image_embeds.shape[-1],\n         )\n         image_embeds = image_embeds.reshape(new_size)\n \n         return (image_embeds, vision_outputs)\n \n     def embed_image_query(\n-        self, query_image_features: torch.FloatTensor, query_feature_map: torch.FloatTensor\n+        self,\n+        query_image_features: torch.FloatTensor,\n+        query_feature_map: torch.FloatTensor,\n+        interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         _, class_embeds = self.class_predictor(query_image_features)\n-        pred_boxes = self.box_predictor(query_image_features, query_feature_map)\n+        pred_boxes = self.box_predictor(query_image_features, query_feature_map, interpolate_pos_encoding)\n         pred_boxes_as_corners = center_to_corners_format(pred_boxes)\n \n         # Loop over query images\n@@ -1478,6 +1574,7 @@ def image_guided_detection(\n         query_pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> OwlViTImageGuidedObjectDetectionOutput:\n         r\"\"\"\n@@ -1520,26 +1617,33 @@ def image_guided_detection(\n         return_dict = return_dict if return_dict is not None else self.config.return_dict\n \n         # Compute feature maps for the input and query images\n-        query_feature_map = self.image_embedder(pixel_values=query_pixel_values)[0]\n+        query_feature_map = self.image_embedder(\n+            pixel_values=query_pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n+        )[0]\n         feature_map, vision_outputs = self.image_embedder(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n-        batch_size, num_patches, num_patches, hidden_dim = feature_map.shape\n-        image_feats = torch.reshape(feature_map, (batch_size, num_patches * num_patches, hidden_dim))\n+        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape\n+        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))\n \n-        batch_size, num_patches, num_patches, hidden_dim = query_feature_map.shape\n-        query_image_feats = torch.reshape(query_feature_map, (batch_size, num_patches * num_patches, hidden_dim))\n+        batch_size, num_patches_height, num_patches_width, hidden_dim = query_feature_map.shape\n+        query_image_feats = torch.reshape(\n+            query_feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim)\n+        )\n         # Get top class embedding and best box index for each query image in batch\n-        query_embeds, best_box_indices, query_pred_boxes = self.embed_image_query(query_image_feats, query_feature_map)\n+        query_embeds, best_box_indices, query_pred_boxes = self.embed_image_query(\n+            query_image_feats, query_feature_map, interpolate_pos_encoding\n+        )\n \n         # Predict object classes [batch_size, num_patches, num_queries+1]\n         (pred_logits, class_embeds) = self.class_predictor(image_feats=image_feats, query_embeds=query_embeds)\n \n         # Predict object boxes\n-        target_pred_boxes = self.box_predictor(image_feats, feature_map)\n+        target_pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)\n \n         if not return_dict:\n             output = (\n@@ -1574,6 +1678,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> OwlViTObjectDetectionOutput:\n         r\"\"\"\n@@ -1625,14 +1730,15 @@ def forward(\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n         # Text and vision model outputs\n         text_outputs = outputs.text_model_output\n         vision_outputs = outputs.vision_model_output\n \n-        batch_size, num_patches, num_patches, hidden_dim = feature_map.shape\n-        image_feats = torch.reshape(feature_map, (batch_size, num_patches * num_patches, hidden_dim))\n+        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape\n+        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))\n \n         # Reshape from [batch_size * max_text_queries, hidden_dim] -> [batch_size, max_text_queries, hidden_dim]\n         max_text_queries = input_ids.shape[0] // batch_size\n@@ -1646,7 +1752,7 @@ def forward(\n         (pred_logits, class_embeds) = self.class_predictor(image_feats, query_embeds, query_mask)\n \n         # Predict object boxes\n-        pred_boxes = self.box_predictor(image_feats, feature_map)\n+        pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)\n \n         if not return_dict:\n             output = ("
        },
        {
            "sha": "b35f58e99a04022bada00431ad4e568588c9a4ec",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f38f58f3de5a35f9b8505e9b48985dce5470985/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=8f38f58f3de5a35f9b8505e9b48985dce5470985",
            "patch": "@@ -828,6 +828,144 @@ def test_inference(self):\n         expected_logits = torch.tensor([[-6.2229, -8.2601]], device=torch_device)\n         self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n \n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        model_name = \"google/owlv2-base-patch16\"\n+        model = Owlv2Model.from_pretrained(model_name).to(torch_device)\n+        processor = OwlViTProcessor.from_pretrained(model_name)\n+        processor.image_processor.size = {\"height\": 1024, \"width\": 1024}\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            images=image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        self.assertEqual(\n+            outputs.logits_per_image.shape,\n+            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n+        )\n+        self.assertEqual(\n+            outputs.logits_per_text.shape,\n+            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n+        )\n+        expected_logits = torch.tensor([[-6.2520, -8.2970]], device=torch_device)\n+        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        expected_shape = torch.Size((1, 4097, 768))\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        # Owlv2ForObjectDetection part.\n+        model = Owlv2ForObjectDetection.from_pretrained(model_name).to(torch_device)\n+        processor.image_processor.size = {\"height\": 1024, \"width\": 1024}\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        num_queries = int((inputs.pixel_values.shape[-1] / model.config.vision_config.patch_size) ** 2)\n+        self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+        expected_slice_boxes = torch.tensor(\n+            [[0.2407, 0.0553, 0.4636], [0.1082, 0.0494, 0.1861], [0.2459, 0.0527, 0.4398]]\n+        ).to(torch_device)\n+        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+\n+        model = Owlv2ForObjectDetection.from_pretrained(model_name).to(torch_device)\n+        query_image = prepare_img()\n+        inputs = processor(\n+            images=image,\n+            query_images=query_image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model.image_guided_detection(**inputs, interpolate_pos_encoding=True)\n+\n+        # No need to check the logits, we just check inference runs fine.\n+        num_queries = int((inputs.pixel_values.shape[-1] / model.config.vision_config.patch_size) ** 2)\n+        self.assertEqual(outputs.target_pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n+        # Deactivate interpolate_pos_encoding on same model, and use default image size.\n+        # Verify the dynamic change caused by the activation/deactivation of interpolate_pos_encoding of variables: self.sqrt_num_patches, self.box_bias from (OwlViTForObjectDetection).\n+        processor = OwlViTProcessor.from_pretrained(model_name)\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            images=image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=False)\n+\n+        num_queries = int((inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size) ** 2)\n+        self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n+        expected_default_box_bias = torch.tensor(\n+            [\n+                [-4.0717, -4.0717, -4.0717, -4.0717],\n+                [-3.3644, -4.0717, -4.0717, -4.0717],\n+                [-2.9425, -4.0717, -4.0717, -4.0717],\n+            ]\n+        )\n+\n+        self.assertTrue(torch.allclose(model.box_bias[:3, :4], expected_default_box_bias, atol=1e-4))\n+\n+        # Interpolate with any resolution size.\n+        processor.image_processor.size = {\"height\": 1264, \"width\": 1024}\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            images=image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        num_queries = int(\n+            (inputs.pixel_values.shape[-2] // model.config.vision_config.patch_size)\n+            * (inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size)\n+        )\n+        self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+        expected_slice_boxes = torch.tensor(\n+            [[0.2438, 0.0945, 0.4675], [0.1361, 0.0431, 0.2406], [0.2465, 0.0428, 0.4429]]\n+        ).to(torch_device)\n+        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+\n+        query_image = prepare_img()\n+        inputs = processor(\n+            images=image,\n+            query_images=query_image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model.image_guided_detection(**inputs, interpolate_pos_encoding=True)\n+\n+        # No need to check the logits, we just check inference runs fine.\n+        num_queries = int(\n+            (inputs.pixel_values.shape[-2] // model.config.vision_config.patch_size)\n+            * (inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size)\n+        )\n+        self.assertEqual(outputs.target_pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n     @slow\n     def test_inference_object_detection(self):\n         model_name = \"google/owlv2-base-patch16\""
        },
        {
            "sha": "545fee0c4fe3afdadeee6beabb1e892da5d13888",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f38f58f3de5a35f9b8505e9b48985dce5470985/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=8f38f58f3de5a35f9b8505e9b48985dce5470985",
            "patch": "@@ -821,6 +821,144 @@ def test_inference(self):\n         expected_logits = torch.tensor([[3.4613, 0.9403]], device=torch_device)\n         self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n \n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        model_name = \"google/owlvit-base-patch32\"\n+        model = OwlViTModel.from_pretrained(model_name).to(torch_device)\n+        processor = OwlViTProcessor.from_pretrained(model_name)\n+        processor.image_processor.size = {\"height\": 800, \"width\": 800}\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            images=image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        self.assertEqual(\n+            outputs.logits_per_image.shape,\n+            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n+        )\n+        self.assertEqual(\n+            outputs.logits_per_text.shape,\n+            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n+        )\n+        expected_logits = torch.tensor([[3.6278, 0.8861]], device=torch_device)\n+        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+\n+        expected_shape = torch.Size((1, 626, 768))\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        # OwlViTForObjectDetection part.\n+        model = OwlViTForObjectDetection.from_pretrained(model_name).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        num_queries = int((inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size) ** 2)\n+        self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n+        expected_slice_boxes = torch.tensor(\n+            [[0.0680, 0.0422, 0.1347], [0.2071, 0.0450, 0.4146], [0.2000, 0.0418, 0.3476]]\n+        ).to(torch_device)\n+        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+\n+        model = OwlViTForObjectDetection.from_pretrained(model_name).to(torch_device)\n+        query_image = prepare_img()\n+        inputs = processor(\n+            images=image,\n+            query_images=query_image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model.image_guided_detection(**inputs, interpolate_pos_encoding=True)\n+\n+        # No need to check the logits, we just check inference runs fine.\n+        num_queries = int((inputs.pixel_values.shape[-1] / model.config.vision_config.patch_size) ** 2)\n+        self.assertEqual(outputs.target_pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n+        # Deactivate interpolate_pos_encoding on same model, and use default image size.\n+        # Verify the dynamic change caused by the activation/deactivation of interpolate_pos_encoding of variables: (self.sqrt_num_patch_h, self.sqrt_num_patch_w), self.box_bias from (OwlViTForObjectDetection).\n+        processor = OwlViTProcessor.from_pretrained(model_name)\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            images=image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=False)\n+\n+        num_queries = int((inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size) ** 2)\n+        self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n+        expected_default_box_bias = torch.tensor(\n+            [\n+                [-3.1332, -3.1332, -3.1332, -3.1332],\n+                [-2.3968, -3.1332, -3.1332, -3.1332],\n+                [-1.9452, -3.1332, -3.1332, -3.1332],\n+            ]\n+        )\n+        self.assertTrue(torch.allclose(model.box_bias[:3, :4], expected_default_box_bias, atol=1e-4))\n+\n+        # Interpolate with any resolution size.\n+        processor.image_processor.size = {\"height\": 1264, \"width\": 1024}\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            images=image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        num_queries = int(\n+            (inputs.pixel_values.shape[-2] // model.config.vision_config.patch_size)\n+            * (inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size)\n+        )\n+        self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+        expected_slice_boxes = torch.tensor(\n+            [[0.0499, 0.0301, 0.0983], [0.2244, 0.0365, 0.4663], [0.1387, 0.0314, 0.1859]]\n+        ).to(torch_device)\n+        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+\n+        query_image = prepare_img()\n+        inputs = processor(\n+            images=image,\n+            query_images=query_image,\n+            max_length=16,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model.image_guided_detection(**inputs, interpolate_pos_encoding=True)\n+\n+        # No need to check the logits, we just check inference runs fine.\n+        num_queries = int(\n+            (inputs.pixel_values.shape[-2] // model.config.vision_config.patch_size)\n+            * (inputs.pixel_values.shape[-1] // model.config.vision_config.patch_size)\n+        )\n+        self.assertEqual(outputs.target_pred_boxes.shape, torch.Size((1, num_queries, 4)))\n+\n     @slow\n     def test_inference_object_detection(self):\n         model_name = \"google/owlvit-base-patch32\""
        }
    ],
    "stats": {
        "total": 638,
        "additions": 565,
        "deletions": 73
    }
}