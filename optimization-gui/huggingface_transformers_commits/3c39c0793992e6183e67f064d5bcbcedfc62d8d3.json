{
    "author": "MekkCyber",
    "message": "Remove triton mlp kernel, not compiling for some models (#37449)\n\n* remove mlp for now\n\n* disable on docker",
    "sha": "3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
    "files": [
        {
            "sha": "72853d4ca4d6387f6a23797712656457a9dd377a",
            "filename": "docker/transformers-all-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-all-latest-gpu%2FDockerfile?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -14,6 +14,8 @@ ARG PYTORCH='2.6.0'\n ARG INTEL_TORCH_EXT='2.3.0'\n # Example: `cu102`, `cu113`, etc.\n ARG CUDA='cu121'\n+# Disable kernel mapping for now until all tests pass\n+ENV DISABLE_KERNEL_MAPPING=1\n \n RUN apt update\n RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg git-lfs"
        },
        {
            "sha": "fdb825cad3706f72991fdca1a33198c6455c6413",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -228,7 +228,6 @@ def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tens\n         return out\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class AriaSharedExpertsMLP(nn.Module):\n     \"\"\"\n     Shared Expert MLP for shared experts."
        },
        {
            "sha": "8fd2483bcd608782aa85e8fe1ea3fc1438f6c9cd",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -882,7 +882,6 @@ def forward(\n         return self.torch_forward(hidden_states, cache_params, cache_position, attention_mask)\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class BambaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "8cbb7128c73467e49aacca9c67cf562a6c22bf0c",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -36,7 +36,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -118,7 +117,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class CohereMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "18a3a50ac157fee0a7dfd8db9984c61cd83b236e",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -28,7 +28,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -268,7 +267,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Cohere2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "e7fecb4be6a9a984940979d5e938b18891f35c66",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -74,7 +74,6 @@\n _CONFIG_FOR_DOC = \"DiffLlamaConfig\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class DiffLlamaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "fcc55b67d153773158144cfeeb4c586a118d8fd8",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -84,7 +84,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Emu3MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "40497433284a4910d56724097e0bf9688cb4346f",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -27,7 +27,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -85,7 +84,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class GemmaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "144a94ef33e9c9a7646001ddd19f6f0640c5e9dc",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -28,7 +28,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -78,7 +77,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Gemma2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "0988e2692aa430a1ec9e6bbd8c645bc2e4c58d99",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -31,7 +31,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -107,7 +106,6 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Gemma3MLP(nn.Module):\n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__()"
        },
        {
            "sha": "80d3ad696dc05911fc301608e3a898e08a7d10ef",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -228,7 +228,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class GraniteMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "d565af9e27f166b76f27f479d748ca81719f3adf",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -29,7 +29,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -118,7 +117,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class HeliumMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "d36fb1b6a47e9d0bef48d9cbe231a533d257ed1b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -160,7 +160,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class LlamaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "7f88b8d8570c0d70346c60d6d27c99dfb965e28d",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -45,7 +45,6 @@\n _CONFIG_FOR_DOC = \"MistralConfig\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class MistralMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "5b6ca9f4b3560fe78726efc06d61d5b0d5c5ce7e",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -14,7 +14,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -58,7 +57,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         )\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class OlmoMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "4046dc582673ba6187f368b2b71e364b8e4e9303",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -218,7 +218,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Olmo2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "7b62632bd8e4f2142f6f90c6ca3b89186eb106d4",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -45,7 +45,6 @@\n _CONFIG_FOR_DOC = \"Qwen2Config\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Qwen2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "15773b4516aec0f7ab15cbe87d1f05ca7fad0425",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c39c0793992e6183e67f064d5bcbcedfc62d8d3/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=3c39c0793992e6183e67f064d5bcbcedfc62d8d3",
            "patch": "@@ -81,7 +81,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-@use_kernel_forward_from_hub(\"MLP\")\n class Qwen3MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 2,
        "deletions": 24
    }
}