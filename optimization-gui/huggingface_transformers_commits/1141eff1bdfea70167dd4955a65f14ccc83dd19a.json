{
    "author": "VladOS95-cyber",
    "message": "Add Pytorch Tensor Parallel support for Mistral (#34927)\n\nadd base tp support",
    "sha": "1141eff1bdfea70167dd4955a65f14ccc83dd19a",
    "files": [
        {
            "sha": "c4b874f2701743e185874653b8cf4c81f3b7b0c5",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1141eff1bdfea70167dd4955a65f14ccc83dd19a/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1141eff1bdfea70167dd4955a65f14ccc83dd19a/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=1141eff1bdfea70167dd4955a65f14ccc83dd19a",
            "patch": "@@ -97,6 +97,16 @@ class MistralConfig(PretrainedConfig):\n \n     model_type = \"mistral\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `MistralModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "9c82b06813c4f13d4e62359256198ad43b38da69",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1141eff1bdfea70167dd4955a65f14ccc83dd19a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1141eff1bdfea70167dd4955a65f14ccc83dd19a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=1141eff1bdfea70167dd4955a65f14ccc83dd19a",
            "patch": "@@ -227,9 +227,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -983,6 +983,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class MistralForCausalLM(MistralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 14,
        "deletions": 3
    }
}