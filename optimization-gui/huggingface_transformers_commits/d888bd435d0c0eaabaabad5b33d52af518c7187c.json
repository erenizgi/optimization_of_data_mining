{
    "author": "cyyever",
    "message": "Fix typos (#40511)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "d888bd435d0c0eaabaabad5b33d52af518c7187c",
    "files": [
        {
            "sha": "09fc2f23f8e0bc6b16184c6c412ed94ac80da0b1",
            "filename": "src/transformers/generation/continuous_batching/classes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -191,7 +191,7 @@ def __repr__(self):\n             f\"query_length={len(self.prompt_ids)}\",\n             f\"remaining_tokens={len(self.remaining_prompt_ids)}\",\n             f\"kv_length={self.position_offset}\",\n-            f\"full_prompt_lenght={len(self.full_prompt_ids)}\",\n+            f\"full_prompt_length={len(self.full_prompt_ids)}\",\n             f\"allocated_blocks={self.allocated_blocks}\",\n             f\"generated_tokens={self.static_outputs}\",\n         ]"
        },
        {
            "sha": "62bacb72a44eba73ab9ddfb8d65d8de6606141c7",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -981,7 +981,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "0800d7ad463f20c3d489bf372826d99ddc7441e1",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -246,7 +246,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "b8c8c0e16b2447ac02555ef7aa95624173d5ff44",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1457,7 +1457,7 @@ def get_qformer_features(\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()(\n@@ -1947,7 +1947,7 @@ def get_image_features(\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()("
        },
        {
            "sha": "c0c6b560ef16599be19ad436e6bd19f51b8f64af",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -897,7 +897,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "b67749c2f42dec2a54fbd5c93d147dcd1ad16e1c",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -195,7 +195,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "2b0a287f21a433a3a64734f3e25c8ae5ddb90192",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -530,7 +530,7 @@ def post_process_panoptic_sample(\n         masks (`torch.Tensor`):\n             The predicted segmentation masks for this sample.\n         boxes (`torch.Tensor`):\n-            The prediced bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n+            The predicted bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n             width, height)` and values between `[0, 1]`, relative to the size the image (disregarding padding).\n         processed_size (`tuple[int, int]`):\n             The processed size of the image `(height, width)`, as returned by the preprocessing step i.e. the size"
        },
        {
            "sha": "e745c80e76c46f1e11b621c8a11ae895c9d2f74d",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -181,7 +181,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "02eb3e4f9c79520190a600c493aff0a596244220",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -285,7 +285,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "c069a872ca6c26638949788925c26cf422255b6b",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -528,7 +528,7 @@ def post_process_panoptic_sample(\n         masks (`torch.Tensor`):\n             The predicted segmentation masks for this sample.\n         boxes (`torch.Tensor`):\n-            The prediced bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n+            The predicted bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n             width, height)` and values between `[0, 1]`, relative to the size the image (disregarding padding).\n         processed_size (`tuple[int, int]`):\n             The processed size of the image `(height, width)`, as returned by the preprocessing step i.e. the size"
        },
        {
            "sha": "fe4d85304ab47e0fe7b3d10fd1e5bb993c2f2a9e",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -517,7 +517,7 @@ def post_process_panoptic_sample(\n         masks (`torch.Tensor`):\n             The predicted segmentation masks for this sample.\n         boxes (`torch.Tensor`):\n-            The prediced bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n+            The predicted bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n             width, height)` and values between `[0, 1]`, relative to the size the image (disregarding padding).\n         processed_size (`tuple[int, int]`):\n             The processed size of the image `(height, width)`, as returned by the preprocessing step i.e. the size"
        },
        {
            "sha": "d50f85283fb0e2fc78943126cb61b017355dfed8",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1386,7 +1386,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "7f5ee236ef06a5b653d0bbbe985725b2971a6c83",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -971,7 +971,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "57a00843e32efc5e3c8460d1b901c01d60065481",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -685,7 +685,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "25e13813f349e878f7033290c36ff3cef9daa45a",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -151,7 +151,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "83a1283b56e2670936d5306f6277764a76b498e0",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -801,7 +801,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "737d288f82b5dcc71456d872e5f621fe634995c9",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1974,7 +1974,7 @@ def get_placeholder_mask(\n         audio_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "eec712fc1f56d36d37ab5d6c34e6db2ad70ba3c8",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -2280,7 +2280,7 @@ def get_placeholder_mask(\n         audio_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "0f2d574193fc674390e147aadcbed31f3b836730",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1157,7 +1157,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "f1bdc171320441e821272d6622355c1907bf020d",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1154,7 +1154,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "1511ca34833b97d8977fff97a97d54815195673e",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1273,7 +1273,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "c7da9129910fde906128a55bae7c87873fce94c6",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -573,7 +573,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "736a95247dfb3682e723233ef348944c059ebca4",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -383,7 +383,7 @@ def __init__(\n     ):\n         super().__init__(vocab_file, pattern=None)\n \n-        # TODO 1st donwload the vocabfile!!!\n+        # TODO 1st download the vocabfile!!!\n         tokenizer = tiktoken.get_encoding(vocab_file)\n         self.additional_special_tokens = {}\n         # Complete list of Harmony special tokens as per o200k_harmony spec"
        },
        {
            "sha": "4f16a5387a05f3d5e1cc5b147d5d957a23661713",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -537,7 +537,7 @@ def post_process_panoptic_sample(\n         masks (`torch.Tensor`):\n             The predicted segmentation masks for this sample.\n         boxes (`torch.Tensor`):\n-            The prediced bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n+            The predicted bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n             width, height)` and values between `[0, 1]`, relative to the size the image (disregarding padding).\n         processed_size (`tuple[int, int]`):\n             The processed size of the image `(height, width)`, as returned by the preprocessing step i.e. the size"
        },
        {
            "sha": "c01e082ab1691afefeef4b434954e557ca6d0bb5",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1197,7 +1197,7 @@ def _preprocess_accelerate(self):\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()(\n@@ -1472,7 +1472,7 @@ def get_image_features(\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()("
        },
        {
            "sha": "b554e69af9e8efdaf495fa83647752d4ac327ea9",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1193,7 +1193,7 @@ def _preprocess_accelerate(self):\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()(\n@@ -1450,7 +1450,7 @@ def get_image_features(\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()("
        },
        {
            "sha": "ff30263700cf34502ec4b79da8e85eec356d9868",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -374,7 +374,7 @@ def get_image_features(\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`.\n         \"\"\"\n         if input_ids is None:\n             special_image_mask = inputs_embeds == self.get_input_embeddings()("
        },
        {
            "sha": "1bd47c73622404ca642444c1e2c8ea8df0cc1e4a",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -636,7 +636,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "c29c98596658315a6964d42954e9342e38f886de",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1050,7 +1050,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "d959b7e7b04d4276df4bafb094dd4cc6f4f201dc",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -910,7 +910,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "38eb3ce8eb820c45a7ede73ec6de29c14e9b4a8a",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1205,7 +1205,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "13c760e196c853ff721ed83dbb93403b8ab8a26a",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -222,7 +222,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "e4a05d80f08a7581ff14ec7198f7b0fbb50100f1",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -430,7 +430,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "4c658fec0134c36776e5e7518b8618d1215332d5",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -484,7 +484,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "653dfbb4ad481dba2cbb9ae12da959e409fc6ac1",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -405,7 +405,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "f506d01320aa906d4e4dc40652e875b263fe227e",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -459,7 +459,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "b14f8217c71d40161ab9644729be420a18b22d9d",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -314,10 +314,10 @@ def _get_output_length(self, input_length: torch.LongTensor) -> torch.LongTensor\n         input_length = input_length + padding_left + padding_right\n \n         # conv\n-        output_lenght = (\n+        output_length = (\n             input_length + 2 * self.conv.padding[0] - self.conv.dilation[0] * (self.conv.kernel_size[0] - 1) - 1\n         ) // self.conv.stride[0] + 1\n-        return output_lenght\n+        return output_length\n \n     def forward(self, hidden_states, padding_cache=None):\n         extra_padding = self._get_extra_padding_for_conv1d(hidden_states)"
        },
        {
            "sha": "8b3ebea43e8f2ddf224eb6ebbfec2c7e9dce8dde",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -266,7 +266,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "42b66aa185c892a45d16cc6d3980fcbef33b0636",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -532,7 +532,7 @@ def forward(\n             input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n                 Float values of the raw speech waveform. Raw speech waveform can be\n                 obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n                 the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n                 and conversion into a tensor of type `torch.FloatTensor`.\n@@ -904,7 +904,7 @@ def forward(\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n             obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n@@ -1026,7 +1026,7 @@ def forward(\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n             obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`."
        },
        {
            "sha": "12b2ee647bb795cc899adf7a2df301438d3d9260",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -564,7 +564,7 @@ def forward(\n             input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n                 Float values of the raw speech waveform. Raw speech waveform can be\n                 obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n                 the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n                 and conversion into a tensor of type `torch.FloatTensor`.\n@@ -739,7 +739,7 @@ def forward(\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n             obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n@@ -845,7 +845,7 @@ def forward(\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n             obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`."
        },
        {
            "sha": "7239412084c8ef0bbd0879833362085f07b284bd",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -629,7 +629,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "204693b429d660ce64457a58f61491f80a42bc63",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -252,7 +252,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "3032cc7e195eae2ad6e54e2fb772a299aceae3c1",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -215,7 +215,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "9affbd3ec5dc1a6361a452bcc69d26b42f58ec8f",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -176,7 +176,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "fec83f1b18c3d3f75bc5990c99c93a405c1156d2",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1761,7 +1761,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "775896876aa8c5ff3fe899d17f507467f666f6ea",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -2210,7 +2210,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "6f050d6d1ac6f715c86c7f2744d8dc30b9ec8840",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1188,7 +1188,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "a7e458ac9b100a791e5e2f535a77bbba1a9bcbd3",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1126,7 +1126,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "e48e21c82fcd9fe9ce72922ec0c9bf8fd3a785e7",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1892,7 +1892,7 @@ def _single_frame_forward(\n             Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n             much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n             that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n             In the order (`x1`, `y1`, `x2`, `y2`):\n \n             - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "00494e6558b609c57f047d6ba07d36d975896e51",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -1603,7 +1603,7 @@ def _single_frame_forward(\n             Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n             much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n             that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n             In the order (`x1`, `y1`, `x2`, `y2`):\n \n             - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "f6f71b30a1ba35c86ea42ab6a22c2b08a83871f1",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -608,7 +608,7 @@ def forward(\n             input_features (`torch.LongTensor` of shape `(batch_size, sequence_length, feature_size)`):\n                 Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be\n                 obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n                 the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the fbank features,\n                 padding and conversion into a tensor of type `torch.FloatTensor`. See"
        },
        {
            "sha": "402c005b7be78e5368c787bd1cebea0e25a27c94",
            "filename": "src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -849,7 +849,7 @@ def call(\n             input_features (`tf.Tensor` of shape `(batch_size, sequence_length, feature_size)`):\n                 Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be\n                 obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n             the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the fbank features,\n                 padding and conversion into a tensor of floats. See [`~Speech2TextFeatureExtractor.__call__`]"
        },
        {
            "sha": "8257ca9d8eaa432a876e6689afcd6c9b0295347b",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -137,7 +137,7 @@ def preprocess(\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                 - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of typ, input_data_format=input_data_formate\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of typ, input_data_format=input_data_format\n                   `tf.Tensor`.\n                 - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n                 - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`."
        },
        {
            "sha": "422083fc914c28e9444e4d6f523e4b1bbb156705",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -716,7 +716,7 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        # As we want to pass `past_key_values=None` explicitly everwhere, we need to pop them from kwargs if present\n+        # As we want to pass `past_key_values=None` explicitly everywhere, we need to pop them from kwargs if present\n         kwargs.pop(\"past_key_values\", None)\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "ef35bb1609e14fe6970d6daf4de56de62c752edc",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -573,7 +573,7 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        # As we want to pass `past_key_values=None` explicitly everwhere, we need to pop them from kwargs if present\n+        # As we want to pass `past_key_values=None` explicitly everywhere, we need to pop them from kwargs if present\n         kwargs.pop(\"past_key_values\", None)\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "f2a967144b5344b20d9fb71c6374f93f889b8355",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -290,7 +290,7 @@ def get_placeholder_mask(\n         video_features: torch.FloatTensor = None,\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "f435e8b75fc0ab5105415263b7edcfe6b2d23edd",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -190,7 +190,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:"
        },
        {
            "sha": "15ef5d48a32d6d2f67c5dd7d5107683dd6261181",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -556,7 +556,7 @@ def from_vision_text_pretrained(\n     ... )\n \n     >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-    >>> image_processor = AutoImageProcesor.from_pretrained(\"google/vit-base-patch16-224\")\n+    >>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n     >>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n     >>> model = FlaxVisionTextDualEncoderModel.from_vision_text_pretrained(\n     ...     \"google/vit-base-patch16-224\", \"google-bert/bert-base-uncased\""
        },
        {
            "sha": "ab28526ba198dd818d1c098bd634004fdbb3e44c",
            "filename": "src/transformers/models/voxtral/convert_voxtral_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -205,7 +205,7 @@ def write_model(\n     # we need to add embed positions as they are not in the state dict\n     with torch.no_grad(), torch.device(\"cuda\"):\n         # TODO: @eustlb, we are here creating on GPU\n-        # vllm initalizes on device, while we save in state dict\n+        # vllm initializes on device, while we save in state dict\n         embed_positions_weight = sinusoids(config.audio_config.max_source_positions, config.audio_config.hidden_size)\n     converted_state_dict[\"audio_tower.embed_positions.weight\"] = embed_positions_weight.cpu()\n "
        },
        {
            "sha": "c768db3c3070438347eaa63571e657e078fc3824",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -730,7 +730,7 @@ def call(\n             input_features (`tf.Tensor` of shape `(batch_size, feature_size, sequence_length)`):\n                 Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be\n                 obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n                 the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the fbank features,\n                 padding and conversion into a tensor of type `tf.Tensor`. See [`~WhisperFeatureExtractor.__call__`]"
        },
        {
            "sha": "a64dcc95374b4347204eccedeb616d0e3c1dda3e",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -644,7 +644,7 @@ def forward(\n             input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n                 Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n                 obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or\n                 the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n                 and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]"
        },
        {
            "sha": "037bf3ed73d48c9e55a2d7ef364927822ed899ef",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d888bd435d0c0eaabaabad5b33d52af518c7187c/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=d888bd435d0c0eaabaabad5b33d52af518c7187c",
            "patch": "@@ -247,8 +247,8 @@ def post_init(self):\n             raise ValueError(\"group_size must be greater than 0 or equal to -1\")\n \n     def get_loading_attributes(self):\n-        loading_attibutes_dict = {\"backend\": self.backend}\n-        return loading_attibutes_dict\n+        loading_attributes_dict = {\"backend\": self.backend}\n+        return loading_attributes_dict\n \n     def to_dict(self):\n         config_dict = super().to_dict()\n@@ -449,7 +449,7 @@ class BitsAndBytesConfig(QuantizationConfigMixin):\n             This flag is used for nested quantization where the quantization constants from the first quantization are\n             quantized again.\n         bnb_4bit_quant_storage (`torch.dtype` or str, *optional*, defaults to `torch.uint8`):\n-            This sets the storage type to pack the quanitzed 4-bit prarams.\n+            This sets the storage type to pack the quantized 4-bit params.\n         kwargs (`dict[str, Any]`, *optional*):\n             Additional parameters from which to initialize the configuration object.\n     \"\"\"\n@@ -759,16 +759,16 @@ def __init__(\n         self.post_init()\n \n     def get_loading_attributes(self):\n-        attibutes_dict = copy.deepcopy(self.__dict__)\n-        loading_attibutes = [\n+        attributes_dict = copy.deepcopy(self.__dict__)\n+        loading_attributes = [\n             \"use_exllama\",\n             \"exllama_config\",\n             \"use_cuda_fp16\",\n             \"max_input_length\",\n             \"backend\",\n         ]\n-        loading_attibutes_dict = {i: j for i, j in attibutes_dict.items() if i in loading_attibutes}\n-        return loading_attibutes_dict\n+        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n+        return loading_attributes_dict\n \n     def post_init(self):\n         r\"\"\"\n@@ -1045,10 +1045,10 @@ def post_init(self):\n                     )\n \n     def get_loading_attributes(self):\n-        attibutes_dict = copy.deepcopy(self.__dict__)\n-        loading_attibutes = [\"version\", \"do_fuse\", \"modules_to_fuse\", \"fuse_max_seq_len\", \"exllama_config\"]\n-        loading_attibutes_dict = {i: j for i, j in attibutes_dict.items() if i in loading_attibutes}\n-        return loading_attibutes_dict\n+        attributes_dict = copy.deepcopy(self.__dict__)\n+        loading_attributes = [\"version\", \"do_fuse\", \"modules_to_fuse\", \"fuse_max_seq_len\", \"exllama_config\"]\n+        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n+        return loading_attributes_dict\n \n \n @dataclass\n@@ -1489,10 +1489,10 @@ def __init__(\n         self.modules_to_not_convert = modules_to_not_convert\n \n     def get_loading_attributes(self):\n-        attibutes_dict = copy.deepcopy(self.__dict__)\n-        loading_attibutes = [\"activation_scale_ub\"]\n-        loading_attibutes_dict = {i: j for i, j in attibutes_dict.items() if i in loading_attibutes}\n-        return loading_attibutes_dict\n+        attributes_dict = copy.deepcopy(self.__dict__)\n+        loading_attributes = [\"activation_scale_ub\"]\n+        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n+        return loading_attributes_dict\n \n \n @dataclass\n@@ -1828,8 +1828,8 @@ def to_dict(self):\n     @classmethod\n     def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n         \"\"\"Create configuration from a dictionary.\"\"\"\n-        ao_verison = cls._get_ao_version()\n-        assert ao_verison > version.parse(\"0.9.0\"), \"TorchAoConfig requires torchao > 0.9.0 for construction from dict\"\n+        ao_version = cls._get_ao_version()\n+        assert ao_version > version.parse(\"0.9.0\"), \"TorchAoConfig requires torchao > 0.9.0 for construction from dict\"\n         config_dict = config_dict.copy()\n         quant_type = config_dict.pop(\"quant_type\")\n "
        }
    ],
    "stats": {
        "total": 172,
        "additions": 86,
        "deletions": 86
    }
}