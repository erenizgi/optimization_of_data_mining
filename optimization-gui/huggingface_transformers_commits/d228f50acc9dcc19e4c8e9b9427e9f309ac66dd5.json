{
    "author": "MekkCyber",
    "message": "Fixing gated repo issues (#37463)\n\nusing unsloth model",
    "sha": "d228f50acc9dcc19e4c8e9b9427e9f309ac66dd5",
    "files": [
        {
            "sha": "e5bf504d81da697262663fc246cbd32addd46557",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d228f50acc9dcc19e4c8e9b9427e9f309ac66dd5/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d228f50acc9dcc19e4c8e9b9427e9f309ac66dd5/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=d228f50acc9dcc19e4c8e9b9427e9f309ac66dd5",
            "patch": "@@ -19,7 +19,6 @@\n     is_torch_available,\n     require_accelerate,\n     require_quark,\n-    require_read_token,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -44,7 +43,7 @@ def test_commmon_args(self):\n @require_quark\n @require_torch_gpu\n class QuarkTest(unittest.TestCase):\n-    reference_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n+    reference_model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n     quantized_model_name = \"amd/Llama-3.1-8B-Instruct-w-int8-a-int8-sym-test\"\n \n     input_text = \"Today I am in Paris and\"\n@@ -76,13 +75,11 @@ def setUpClass(cls):\n             device_map=cls.device_map,\n         )\n \n-    @require_read_token\n     def test_memory_footprint(self):\n         mem_quantized = self.quantized_model.get_memory_footprint()\n \n         self.assertTrue(self.mem_fp16 / mem_quantized > self.EXPECTED_RELATIVE_DIFFERENCE)\n \n-    @require_read_token\n     def test_device_and_dtype_assignment(self):\n         r\"\"\"\n         Test whether trying to cast (or assigning a device to) a model after quantization will throw an error.\n@@ -96,7 +93,6 @@ def test_device_and_dtype_assignment(self):\n             # Tries with a `dtype``\n             self.quantized_model.to(torch.float16)\n \n-    @require_read_token\n     def test_original_dtype(self):\n         r\"\"\"\n         A simple test to check if the model succesfully stores the original dtype\n@@ -107,7 +103,6 @@ def test_original_dtype(self):\n \n         self.assertTrue(isinstance(self.quantized_model.model.layers[0].mlp.gate_proj, QParamsLinear))\n \n-    @require_read_token\n     def check_inference_correctness(self, model):\n         r\"\"\"\n         Test the generation quality of the quantized model and see that we are matching the expected output.\n@@ -131,7 +126,6 @@ def check_inference_correctness(self, model):\n         # Get the generation\n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n-    @require_read_token\n     def test_generate_quality(self):\n         \"\"\"\n         Simple test to check the quality of the model by comparing the generated tokens with the expected tokens"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 1,
        "deletions": 7
    }
}