{
    "author": "gante",
    "message": "Generate: Deprecate returning legacy cache by default; Handle `use_cache=False` (#32863)",
    "sha": "a26de151390f5cb029b2e39231c00ad4303b4347",
    "files": [
        {
            "sha": "160a8a7eae2dba45e4847ebc405c934942d6ebcc",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 58,
            "deletions": 51,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/a26de151390f5cb029b2e39231c00ad4303b4347/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a26de151390f5cb029b2e39231c00ad4303b4347/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=a26de151390f5cb029b2e39231c00ad4303b4347",
            "patch": "@@ -130,9 +130,29 @@ class GenerationConfig(PushToHubMixin):\n             [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n         penalty_alpha (`float`, *optional*):\n             The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n+        dola_layers (`str` or `List[int]`, *optional*):\n+            The layers to use for DoLa decoding. If `None`, DoLa decoding is not used. If a string, it must\n+            be one of \"low\" or \"high\", which means using the lower part or higher part of the model layers, respectively.\n+            \"low\" means the first half of the layers up to the first 20 layers, and \"high\" means the last half of the\n+            layers up to the last 20 layers.\n+            If a list of integers, it must contain the indices of the layers to use for candidate premature layers in DoLa.\n+            The 0-th layer is the word embedding layer of the model. Set to `'low'` to improve long-answer reasoning tasks,\n+            `'high'` to improve short-answer tasks. Check the [documentation](https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md)\n+            or [the paper](https://arxiv.org/abs/2309.03883) for more details.\n+\n+        > Parameters that control the cache\n+\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n             speed up decoding.\n+        cache_implementation (`str`, *optional*, default to `None`):\n+            Cache class that should be used when generating.\n+        cache_config (`CacheConfig` or `dict`, *optional*, default to `None`):\n+            Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n+            it will be converted to its repsective `CacheConfig` internally.\n+            Otherwise can be passed as a `CacheConfig` class matching the indicated `cache_implementation`.\n+        return_legacy_cache (`bool`, *optional*, default to `True`):\n+            Whether to return the legacy or new format of the cache when `DynamicCache` is used by default.\n \n         > Parameters for manipulation of the model output logits\n \n@@ -307,29 +327,6 @@ class GenerationConfig(PushToHubMixin):\n         max_matching_ngram_size (`int`, *optional*, default to `None`):\n             The maximum ngram size to be considered for matching in the prompt. Default to 2 if not provided.\n \n-        > Generation parameters exclusive to [DoLa decoding](https://arxiv.org/abs/2309.03883)\n-\n-        dola_layers (`str` or `List[int]`, *optional*):\n-            The layers to use for DoLa decoding. If `None`, DoLa decoding is not used. If a string, it must\n-            be one of \"low\" or \"high\", which means using the lower part or higher part of the model layers, respectively.\n-            \"low\" means the first half of the layers up to the first 20 layers, and \"high\" means the last half of the\n-            layers up to the last 20 layers.\n-            If a list of integers, it must contain the indices of the layers to use for candidate premature layers in DoLa.\n-            The 0-th layer is the word embedding layer of the model. Set to `'low'` to improve long-answer reasoning tasks,\n-            `'high'` to improve short-answer tasks. Check the [documentation](https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md)\n-            or [the paper](https://arxiv.org/abs/2309.03883) for more details.\n-\n-        > Parameters specific to the caching mechanism:\n-\n-        cache_implementation (`str`, *optional*, default to `None`):\n-            Cache class that should be used when generating.\n-        cache_config (`CacheConfig` or `dict`, *optional*, default to `None`):\n-            Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n-            it will be converted to its repsective `CacheConfig` internally.\n-            Otherwise can be passed as a `CacheConfig` class matching the indicated `cache_implementation`.\n-        return_legacy_cache (`bool`, *optional*, default to `True`):\n-            Whether to return the legacy or new format of the cache when `DynamicCache` is used by default.\n-\n         > Wild card\n \n         generation_kwargs:\n@@ -352,7 +349,19 @@ def __init__(self, **kwargs):\n         self.num_beams = kwargs.pop(\"num_beams\", 1)\n         self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n         self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n+        self.dola_layers = kwargs.pop(\"dola_layers\", None)\n+\n+        # Parameters that control the cache\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n+        self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n+        self.cache_config = kwargs.pop(\"cache_config\", None)\n+        if self.cache_implementation is not None and self.cache_implementation in NEEDS_CACHE_CONFIG:\n+            cache_config_class = NEEDS_CACHE_CONFIG[self.cache_implementation]\n+            if self.cache_config is None:\n+                self.cache_config = cache_config_class()\n+            elif isinstance(self.cache_config, dict):\n+                self.cache_config = cache_config_class.from_dict(self.cache_config)\n+        self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n \n         # Parameters for manipulation of the model output logits\n         self.temperature = kwargs.pop(\"temperature\", 1.0)\n@@ -411,20 +420,6 @@ def __init__(self, **kwargs):\n         self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 5)\n         self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"heuristic\")\n \n-        # DoLa generation\n-        self.dola_layers = kwargs.pop(\"dola_layers\", None)\n-\n-        # Cache implementation\n-        self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n-        self.cache_config = kwargs.pop(\"cache_config\", None)\n-        if self.cache_implementation is not None and self.cache_implementation in NEEDS_CACHE_CONFIG:\n-            cache_config_class = NEEDS_CACHE_CONFIG[self.cache_implementation]\n-            if self.cache_config is None:\n-                self.cache_config = cache_config_class()\n-            elif isinstance(self.cache_config, dict):\n-                self.cache_config = cache_config_class.from_dict(self.cache_config)\n-        self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", True)\n-\n         # Prompt lookup decoding\n         self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)\n         self.max_matching_ngram_size = kwargs.pop(\"max_matching_ngram_size\", None)\n@@ -544,8 +539,9 @@ def validate(self, is_init=False):\n             raise ValueError(f\"`max_new_tokens` must be greater than 0, but is {self.max_new_tokens}.\")\n         if self.pad_token_id is not None and self.pad_token_id < 0:\n             warnings.warn(\n-                f\"`pad_token_id` should be positive but got {self.pad_token_id}. This will cause errors when batch generating, if there is padding. \"\n-                \"Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\"\n+                f\"`pad_token_id` should be positive but got {self.pad_token_id}. This will cause errors when batch \"\n+                \"generating, if there is padding. Please set `pad_token_id` explicitly as \"\n+                \"`model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\"\n             )\n \n         # Validation of attribute relations:\n@@ -675,6 +671,14 @@ def validate(self, is_init=False):\n                         group_error_prefix\n                         + \"`diversity_penalty` should be greater than `0.0`, otherwise your groups will be identical.\"\n                     )\n+            # DoLa generation\n+            if self.dola_layers is not None and (self.repetition_penalty is None or self.repetition_penalty < 1.2):\n+                warnings.warn(\n+                    \"`dola_layers` is set to trigger DoLa decoding, but `repetition_penalty` is set to a value of \"\n+                    f\"{self.repetition_penalty}, which could induce unwanted repetition. The recommended value for \"\n+                    \"DoLa decoding is `repetition_penalty>=1.2`.\",\n+                    UserWarning,\n+                )\n \n         # 4. check `num_return_sequences`\n         if self.num_return_sequences != 1:\n@@ -690,7 +694,7 @@ def validate(self, is_init=False):\n                     f\"({self.num_beams}).\"\n                 )\n \n-        # 5. check `cache_config`\n+        # 5. check cache-related arguments\n         if self.cache_config is not None:\n             cache_class = NEEDS_CACHE_CONFIG.get(self.cache_implementation)\n             if cache_class is None:\n@@ -702,6 +706,20 @@ def validate(self, is_init=False):\n             if not isinstance(self.cache_config, cache_class):\n                 self.cache_config = cache_class.from_dict(self.cache_config)\n             self.cache_config.validate()\n+        if self.use_cache is False:\n+            # In this case, all cache-related arguments should be unset. However, since `use_cache=False` is often used\n+            # passed to `generate` directly to hot-fix cache issues, let's raise a warning instead of an error\n+            # (otherwise a user might need to overwrite several parameters).\n+            no_cache_warning = (\n+                \"You have set `use_cache` to `False`, but {cache_arg} is set to {cache_arg_value}. {cache_arg} will \"\n+                \"have no effect.\"\n+            )\n+            for arg_name in (\"cache_implementation\", \"cache_config\", \"return_legacy_cache\"):\n+                if getattr(self, arg_name) is not None:\n+                    logger.warning_once(\n+                        no_cache_warning.format(cache_arg=arg_name, cache_arg_value=getattr(self, arg_name)),\n+                        UserWarning,\n+                    )\n \n         # 6.  check watermarking arguments\n         if self.watermarking_config is not None:\n@@ -727,17 +745,6 @@ def validate(self, is_init=False):\n                     \"`generate()` (or a pipeline) directly.\"\n                 )\n \n-        # 6. if dola_layers is set, check if repetition_penalty is set to >= 1.2\n-        if self.dola_layers is not None and (self.repetition_penalty is None or self.repetition_penalty < 1.2):\n-            dola_decoding_wrong_parameter_msg = (\n-                \"`dola_layers` is set to trigger DoLa decoding, but `repetition_penalty` is set to a value of {repetition_penalty}, \"\n-                \"which could induce unwanted repetition. The recommended value for DoLa decoding is `repetition_penalty>=1.2`.\"\n-            )\n-            warnings.warn(\n-                dola_decoding_wrong_parameter_msg.format(repetition_penalty=self.repetition_penalty),\n-                UserWarning,\n-            )\n-\n     def save_pretrained(\n         self,\n         save_directory: Union[str, os.PathLike],"
        },
        {
            "sha": "a9ebdcdd477549b60753f78d3a150253b3274e8f",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 192,
            "deletions": 157,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/a26de151390f5cb029b2e39231c00ad4303b4347/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a26de151390f5cb029b2e39231c00ad4303b4347/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=a26de151390f5cb029b2e39231c00ad4303b4347",
            "patch": "@@ -136,27 +136,23 @@ class GenerateDecoderOnlyOutput(ModelOutput):\n         sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n             The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n             if all batches finished early due to the `eos_token_id`.\n-        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n             Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n             at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n             each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n             Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n             at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n             each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n-        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n-            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n-            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n+        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+            Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n+            the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n     sequences: torch.LongTensor = None\n@@ -176,36 +172,32 @@ class GenerateEncoderDecoderOutput(ModelOutput):\n         sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n             The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n             if all batches finished early due to the `eos_token_id`.\n-        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n             Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n             at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n             each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n             Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n             at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n             each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n             Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n             sequence_length, sequence_length)`.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n             shape `(batch_size, sequence_length, hidden_size)`.\n-        decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n-        cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n-        decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n         past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n-            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n-            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n+            Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n+            the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n     sequences: torch.LongTensor = None\n@@ -228,33 +220,29 @@ class GenerateBeamDecoderOnlyOutput(ModelOutput):\n         sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n             The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n             if all batches finished early due to the `eos_token_id`.\n-        sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True`):\n             Final beam scores of the generated `sequences`.\n-        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n             Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n             of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n             Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n             with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n-        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n             Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n             at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n             each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True`):\n             Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n             `(batch_size*num_return_sequences, sequence_length)`.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n-        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n-            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n-            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n+        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+            Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n+            the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n     sequences: torch.LongTensor = None\n@@ -276,43 +264,39 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n         sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n             The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n             if all batches finished early due to the `eos_token_id`.\n-        sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True`):\n             Final beam scores of the generated `sequences`.\n-        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n             Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n             of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n             Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n             with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n-        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n             Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n             at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n             each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n+        beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True`):\n             Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n             `(batch_size*num_return_sequences, sequence_length)`.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n             Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n             sequence_length, sequence_length)`.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n             shape `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.\n-        decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, num_heads, generated_length,\n             sequence_length)`.\n-        cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n+        cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n-        decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n-            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n-            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n+        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+            Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n+            the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n     sequences: torch.LongTensor = None\n@@ -328,6 +312,7 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n     past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None\n \n \n+# TODO (joao): remove the equivalent classes and typing shortcuts below in v5\n # Equivalent classes (kept for retrocompatibility purposes)\n GreedySearchDecoderOnlyOutput = GenerateDecoderOnlyOutput\n ContrastiveSearchDecoderOnlyOutput = GenerateDecoderOnlyOutput\n@@ -1501,6 +1486,121 @@ def _supports_default_dynamic_cache(self) -> bool:\n         \"\"\"\n         return self._supports_cache_class and \"jamba\" not in self.__class__.__name__.lower()\n \n+    def _prepare_cache_for_generation(\n+        self,\n+        generation_config: GenerationConfig,\n+        model_kwargs: Dict,\n+        assistant_model: \"PreTrainedModel\",\n+        batch_size: int,\n+        device: torch.device,\n+    ) -> bool:\n+        \"\"\"\n+        Prepares the cache for generation (if applicable), given `generate`'s paramaterization. If a cache is\n+        instantiated, writes it to `model_kwargs`, under the name expected by the model.\n+        \"\"\"\n+\n+        cache_name = \"past_key_values\" if \"mamba\" not in self.__class__.__name__.lower() else \"cache_params\"\n+        requires_cross_attention_cache = (\n+            self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n+        )\n+\n+        # Quick escape route 1: if the user specifies a cache, we only need to:\n+        # a) check for conflicting `generate` arguments\n+        # b) convert to the new cache format (if the user passes a legacy cache and model supports it)\n+        user_defined_cache = model_kwargs.get(cache_name)\n+        if user_defined_cache is not None:\n+            if generation_config.cache_implementation is not None:\n+                raise ValueError(\n+                    f\"Passing both `cache_implementation` (used to initialize certain caches) and `{cache_name}` (a \"\n+                    \"Cache object) is unsupported. Please use only one of the two.\"\n+                )\n+            if isinstance(user_defined_cache, tuple) and self._supports_default_dynamic_cache():\n+                model_kwargs[cache_name] = (\n+                    DynamicCache.from_legacy_cache(user_defined_cache)\n+                    if not requires_cross_attention_cache\n+                    else EncoderDecoderCache.from_legacy_cache(user_defined_cache)\n+                )\n+            return\n+\n+        # Quick escape route 2: if the user specifies no cache is to be used. (conflicting arguments are handled in\n+        # `generation_config.validate()`)\n+        if generation_config.use_cache is False:\n+            return\n+\n+        # Quick escape route 3: model that only supports legacy caches = nothing to prepare\n+        if not self._supports_default_dynamic_cache():\n+            if generation_config.cache_implementation is not None:\n+                warnings.warn(\n+                    \"This model does not support `Cache` instances, it only supports the legacy cache format (tuple \"\n+                    f\"of tuples). `cache_implementation` (set to {generation_config.cache_implementation}) will be \"\n+                    \"ignored.\",\n+                    UserWarning,\n+                )\n+            return\n+\n+        # Otherwise we NEED to prepare a cache, based on `generation_config.cache_implementation`\n+\n+        # TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,\n+        # which is only supported in dynamic caches atm\n+        if assistant_model is not None and generation_config.cache_implementation is not None:\n+            logger.warning_once(\n+                \"An assistant model is provided, using a dynamic cache instead of a cache of type=\"\n+                f\"'{generation_config.cache_implementation}'.\"\n+            )\n+            generation_config.cache_implementation = None\n+\n+        if generation_config.cache_implementation is not None:\n+            if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n+                if generation_config.cache_implementation == \"static\" and not self._supports_static_cache:\n+                    raise ValueError(\n+                        \"This model does not support `cache_implementation='static'`. Please check the following \"\n+                        \"issue: https://github.com/huggingface/transformers/issues/28981\"\n+                    )\n+                model_kwargs[cache_name] = self._get_cache(\n+                    cache_implementation=generation_config.cache_implementation,\n+                    batch_size=generation_config.num_beams * generation_config.num_return_sequences * batch_size,\n+                    max_cache_len=generation_config.max_length,\n+                    device=device,\n+                    model_kwargs=model_kwargs,\n+                )\n+            elif generation_config.cache_implementation == \"quantized\":\n+                if not self._supports_quantized_cache:\n+                    raise ValueError(\n+                        \"This model does not support the quantized cache. If you want your model to support quantized \"\n+                        \"cache, please open an issue and tag @zucchini-nlp.\"\n+                    )\n+\n+                cache_config = (\n+                    generation_config.cache_config\n+                    if generation_config.cache_config is not None\n+                    else QuantizedCacheConfig()\n+                )\n+                cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]\n+\n+                if cache_config.backend == \"quanto\" and not is_quanto_available():\n+                    raise ImportError(\n+                        \"You need to install `quanto` in order to use KV cache quantization with quanto backend. \"\n+                        \"Please install it via  with `pip install quanto`\"\n+                    )\n+                elif cache_config.backend == \"HQQ\" and not is_hqq_available():\n+                    raise ImportError(\n+                        \"You need to install `HQQ` in order to use KV cache quantization with HQQ backend. \"\n+                        \"Please install it via  with `pip install hqq`\"\n+                    )\n+\n+                model_kwargs[cache_name] = cache_class(cache_config)\n+            elif generation_config.cache_implementation == \"offloaded\":\n+                model_kwargs[cache_name] = OffloadedCache()\n+\n+        # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n+        # keeps copying the cache thus using much more memory\n+        else:\n+            model_kwargs[cache_name] = (\n+                DynamicCache()\n+                if not requires_cross_attention_cache\n+                else EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            )\n+\n     def _prepare_special_tokens(\n         self,\n         generation_config: GenerationConfig,\n@@ -1776,104 +1876,18 @@ def generate(\n             inputs_tensor=inputs_tensor,\n             input_ids_length=input_ids_length,\n         )\n-\n-        use_dynamic_cache_by_default = False\n-        if \"mamba\" in self.__class__.__name__.lower():\n-            cache_name = \"cache_params\"\n-        else:\n-            cache_name = \"past_key_values\"\n-\n-        # TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,\n-        # which is only supported in dynamic caches atm\n-        if (\n-            assistant_model is not None\n-            and generation_config.cache_implementation is not None\n-            and self._supports_default_dynamic_cache()\n-        ):\n-            logger.warning_once(\n-                \"An assistant model is provided, using a dynamic cache instead of a cache of type=\"\n-                f\"'{generation_config.cache_implementation}'.\"\n-            )\n-            generation_config.cache_implementation = None\n-\n-        if (model_kwargs.get(cache_name) is not None) and is_torchdynamo_compiling():\n-            raise ValueError(\n-                \"Passing `past_key_values` is not supported when compiling `model.generate` with torch.compile -- you \"\n-                \"may get incorrect outputs. Please compile `model.forward` only or use the `cache_implementation` \"\n-                \"input argument.\"\n-            )\n-        if generation_config.cache_implementation is not None and (model_kwargs.get(cache_name) is not None):\n-            raise ValueError(\n-                f\"Passing both `cache_implementation` (used to initialize certain caches) and `{cache_name}` (a \"\n-                \"Cache object) is unsupported. Please use only one of the two.\"\n-            )\n-        elif generation_config.cache_implementation is not None:\n-            if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n-                if generation_config.cache_implementation == \"static\" and not self._supports_static_cache:\n-                    raise ValueError(\n-                        \"This model does not support `cache_implementation='static'`. Please check the following \"\n-                        \"issue: https://github.com/huggingface/transformers/issues/28981\"\n-                    )\n-                model_kwargs[cache_name] = self._get_cache(\n-                    cache_implementation=generation_config.cache_implementation,\n-                    batch_size=generation_config.num_beams * generation_config.num_return_sequences * batch_size,\n-                    max_cache_len=generation_config.max_length,\n-                    device=device,\n-                    model_kwargs=model_kwargs,\n-                )\n-            elif generation_config.cache_implementation == \"quantized\":\n-                if not self._supports_quantized_cache:\n-                    raise ValueError(\n-                        \"This model does not support the quantized cache. If you want your model to support quantized \"\n-                        \"cache, please open an issue.\"\n-                    )\n-\n-                cache_config = (\n-                    generation_config.cache_config\n-                    if generation_config.cache_config is not None\n-                    else QuantizedCacheConfig()\n-                )\n-                cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]\n-\n-                if cache_config.backend == \"quanto\" and not is_quanto_available():\n-                    raise ImportError(\n-                        \"You need to install `quanto` in order to use KV cache quantization with quanto backend. \"\n-                        \"Please install it via  with `pip install quanto`\"\n-                    )\n-                elif cache_config.backend == \"HQQ\" and not is_hqq_available():\n-                    raise ImportError(\n-                        \"You need to install `HQQ` in order to use KV cache quantization with HQQ backend. \"\n-                        \"Please install it via  with `pip install hqq`\"\n-                    )\n-\n-                model_kwargs[cache_name] = cache_class(cache_config)\n-            elif generation_config.cache_implementation == \"offloaded\":\n-                model_kwargs[cache_name] = OffloadedCache()\n-        # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n-        # keeps copying the cache thus using much more memory\n-        elif generation_config.cache_implementation is None and self._supports_default_dynamic_cache():\n-            past = model_kwargs.get(cache_name, None)\n-            requires_cross_attention_cache = (\n-                self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n-            )\n-            if past is None:\n-                model_kwargs[cache_name] = (\n-                    DynamicCache()\n-                    if not requires_cross_attention_cache\n-                    else EncoderDecoderCache(DynamicCache(), DynamicCache())\n-                )\n-                use_dynamic_cache_by_default = True\n-            elif isinstance(past, tuple):\n-                model_kwargs[cache_name] = (\n-                    DynamicCache.from_legacy_cache(past)\n-                    if not requires_cross_attention_cache\n-                    else EncoderDecoderCache.from_legacy_cache(past)\n-                )\n-                use_dynamic_cache_by_default = True\n-\n         self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n \n-        # 7. determine generation mode\n+        # 7. Prepare the cache.\n+        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n+        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n+        # - `max_length`, prepared above, is used to determine the maximum cache length\n+        # TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\n+        cache_name = \"past_key_values\" if \"mamba\" not in self.__class__.__name__.lower() else \"cache_params\"\n+        user_defined_cache = model_kwargs.get(cache_name)\n+        self._prepare_cache_for_generation(generation_config, model_kwargs, assistant_model, batch_size, device)\n+\n+        # 8. determine generation mode\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n \n         if streamer is not None and (generation_config.num_beams > 1):\n@@ -1892,7 +1906,7 @@ def generate(\n                 UserWarning,\n             )\n \n-        # 8. prepare distribution pre_processing samplers\n+        # 9. prepare logits processors and stopping criteria\n         prepared_logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_length,\n@@ -1904,8 +1918,6 @@ def generate(\n             negative_prompt_ids=negative_prompt_ids,\n             negative_prompt_attention_mask=negative_prompt_attention_mask,\n         )\n-\n-        # 9. prepare stopping criteria\n         prepared_stopping_criteria = self._get_stopping_criteria(\n             generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n         )\n@@ -2138,11 +2150,34 @@ def typeerror():\n                 **model_kwargs,\n             )\n \n-        # Convert to legacy cache if needed\n-        if use_dynamic_cache_by_default and generation_config.return_legacy_cache:\n-            if isinstance(result, ModelOutput) and hasattr(result, \"past_key_values\"):\n-                if isinstance(result.past_key_values, (DynamicCache, EncoderDecoderCache)):\n-                    result.past_key_values = result.past_key_values.to_legacy_cache()\n+        # Convert to legacy cache format if requested\n+        if (\n+            generation_config.return_legacy_cache is not False  # Should check for `True` after v4.47\n+            and not is_torchdynamo_compiling()\n+            and hasattr(result, \"past_key_values\")\n+            and hasattr(result.past_key_values, \"to_legacy_cache\")\n+            and result.past_key_values.to_legacy_cache is not None\n+        ):\n+            # handle BC (convert by default if he user hasn't passed a cache AND the cache is of the default type)\n+            should_convert_cache = generation_config.return_legacy_cache\n+            is_user_defined_cache = user_defined_cache is not None\n+            is_default_cache_type = (\n+                type(result.past_key_values) == DynamicCache  # noqa E721\n+                or (\n+                    isinstance(result.past_key_values, EncoderDecoderCache)\n+                    and type(result.past_key_values.self_attention_cache) == DynamicCache  # noqa E721\n+                    and type(result.past_key_values.cross_attention_cache) == DynamicCache  # noqa E721\n+                )\n+            )\n+            if not is_user_defined_cache and is_default_cache_type:\n+                logger.warning_once(\n+                    \"From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` \"\n+                    \"instance instead by default (as opposed to the legacy tuple of tuples format). If you want to \"\n+                    \"keep returning the legacy format, please set `return_legacy_cache=True`.\"\n+                )\n+                should_convert_cache = True\n+            if should_convert_cache:\n+                result.past_key_values = result.past_key_values.to_legacy_cache()\n         return result\n \n     def _has_unfinished_sequences("
        },
        {
            "sha": "7dec1f26e1a39009cc5063e4cd1ee8e404d7ba7a",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a26de151390f5cb029b2e39231c00ad4303b4347/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a26de151390f5cb029b2e39231c00ad4303b4347/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=a26de151390f5cb029b2e39231c00ad4303b4347",
            "patch": "@@ -768,7 +768,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.head = new_embeddings\n \n-    def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=None, **kwargs):\n+    def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=None, use_cache=None, **kwargs):\n         # only last token for inputs_ids if the state is passed along.\n         if state is not None:\n             input_ids = input_ids[:, -1].unsqueeze(-1)\n@@ -780,6 +780,7 @@ def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=Non\n             model_inputs = {\"input_ids\": input_ids}\n \n         model_inputs[\"state\"] = state\n+        model_inputs[\"use_cache\"] = use_cache\n         return model_inputs\n \n     @add_start_docstrings_to_model_forward(RWKV_INPUTS_DOCSTRING)"
        },
        {
            "sha": "ae52f6c67404697216025ab34d6d82875c8f0be5",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 59,
            "deletions": 47,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/a26de151390f5cb029b2e39231c00ad4303b4347/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a26de151390f5cb029b2e39231c00ad4303b4347/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a26de151390f5cb029b2e39231c00ad4303b4347",
            "patch": "@@ -194,6 +194,7 @@ def _greedy_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n@@ -207,6 +208,7 @@ def _greedy_generate(\n             output_scores=output_scores,\n             output_logits=output_logits,\n             return_dict_in_generate=return_dict_in_generate,\n+            use_cache=use_cache,\n             **logits_processor_kwargs,\n             **model_kwargs,\n         )\n@@ -224,6 +226,7 @@ def _sample_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         torch.manual_seed(0)\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True)\n@@ -239,6 +242,7 @@ def _sample_generate(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict_in_generate=return_dict_in_generate,\n+            use_cache=use_cache,\n             **logits_processor_kwargs,\n             **model_kwargs,\n         )\n@@ -256,6 +260,7 @@ def _beam_search_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n@@ -268,6 +273,7 @@ def _beam_search_generate(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict_in_generate=return_dict_in_generate,\n+            use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n@@ -286,6 +292,7 @@ def _beam_sample_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         torch.manual_seed(0)\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True)\n@@ -299,6 +306,7 @@ def _beam_sample_generate(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict_in_generate=return_dict_in_generate,\n+            use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n@@ -317,6 +325,7 @@ def _group_beam_search_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n@@ -329,6 +338,7 @@ def _group_beam_search_generate(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict_in_generate=return_dict_in_generate,\n+            use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n@@ -348,6 +358,7 @@ def _constrained_beam_search_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n@@ -361,6 +372,7 @@ def _constrained_beam_search_generate(\n             output_hidden_states=output_hidden_states,\n             return_dict_in_generate=return_dict_in_generate,\n             constraints=constraints,\n+            use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n@@ -378,6 +390,7 @@ def _contrastive_generate(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict_in_generate=False,\n+        use_cache=True,\n     ):\n         contrastive_search_kwargs = {\n             \"penalty_alpha\": 0.6,\n@@ -396,6 +409,7 @@ def _contrastive_generate(\n             output_scores=output_scores,\n             output_logits=output_logits,\n             return_dict_in_generate=return_dict_in_generate,\n+            use_cache=use_cache,\n             **logits_processor_kwargs,\n             **model_kwargs,\n             **contrastive_search_kwargs,\n@@ -419,7 +433,6 @@ def test_greedy_generate_dict_outputs(self):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n \n-            config.use_cache = False\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n                 model=model,\n@@ -430,6 +443,7 @@ def test_greedy_generate_dict_outputs(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=False,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -454,7 +468,6 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"rwkv\"]):\n                 self.skipTest(reason=\"Won't fix: model with non-standard dictionary output shapes\")\n \n-            config.use_cache = True\n             config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n@@ -466,6 +479,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=True,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -495,7 +509,6 @@ def test_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n \n-            config.use_cache = False\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(\n                 model=model,\n@@ -507,6 +520,7 @@ def test_sample_generate_dict_output(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=False,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -545,9 +559,6 @@ def test_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n \n-            # disable cache\n-            config.use_cache = False\n-\n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_search_generate(\n@@ -560,6 +571,7 @@ def test_beam_search_generate_dict_output(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=False,\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n@@ -589,7 +601,6 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n \n-            config.use_cache = True\n             config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._beam_search_generate(\n@@ -602,6 +613,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=True,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -676,9 +688,6 @@ def test_beam_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n \n-            # disable cache\n-            config.use_cache = False\n-\n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n \n@@ -692,6 +701,7 @@ def test_beam_sample_generate_dict_output(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=False,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -764,7 +774,6 @@ def test_group_beam_search_generate(self):\n     def test_group_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = False\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_diverse_beam_kwargs()\n@@ -778,6 +787,7 @@ def test_group_beam_search_generate_dict_output(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=False,\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n@@ -857,9 +867,6 @@ def test_constrained_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n \n-            # disable cache\n-            config.use_cache = False\n-\n             model = model_class(config).to(torch_device).eval()\n \n             # Sample constraints\n@@ -882,6 +889,7 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=False,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -913,13 +921,12 @@ def test_contrastive_generate(self):\n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n-            config.use_cache = True\n             config.is_decoder = True\n \n             # test old generation output for backwards compatibility\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._contrastive_generate(\n-                model=model, input_ids=input_ids, attention_mask=attention_mask\n+                model=model, input_ids=input_ids, attention_mask=attention_mask, use_cache=True\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n@@ -940,7 +947,6 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n-            config.use_cache = True\n             config.is_decoder = True\n \n             model = model_class(config).to(torch_device).eval()\n@@ -953,6 +959,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n+                use_cache=True,\n             )\n \n             if model.config.is_encoder_decoder:\n@@ -978,7 +985,6 @@ def test_contrastive_generate_low_memory(self):\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n \n-            config.use_cache = True\n             config.is_decoder = True\n \n             # test output equality of low versus high memory\n@@ -991,6 +997,7 @@ def test_contrastive_generate_low_memory(self):\n                 low_memory=True,\n                 max_new_tokens=self.max_new_tokens,\n                 attention_mask=attention_mask,\n+                use_cache=True,\n             )\n \n             high_output = model.generate(\n@@ -1000,6 +1007,7 @@ def test_contrastive_generate_low_memory(self):\n                 low_memory=False,\n                 max_new_tokens=self.max_new_tokens,\n                 attention_mask=attention_mask,\n+                use_cache=True,\n             )\n             self.assertListEqual(low_output.tolist(), high_output.tolist())\n \n@@ -1031,10 +1039,17 @@ def test_beam_search_low_memory(self):\n             # test output equality of low versus high memory\n             model = model_class(config).to(torch_device).eval()\n \n-            low_output = model.generate(input_ids, max_new_tokens=8, num_beams=5, early_stopping=True, low_memory=True)\n+            low_output = model.generate(\n+                input_ids, max_new_tokens=8, num_beams=5, early_stopping=True, low_memory=True, use_cache=True\n+            )\n \n             high_output = model.generate(\n-                input_ids, max_new_tokens=8, num_beams=5, early_stopping=True, low_memory=False\n+                input_ids,\n+                max_new_tokens=8,\n+                num_beams=5,\n+                early_stopping=True,\n+                low_memory=False,\n+                use_cache=True,\n             )\n             self.assertListEqual(low_output.tolist(), high_output.tolist())\n \n@@ -1079,7 +1094,6 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n \n-            config.use_cache = True\n             config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n             # Sets assisted generation arguments such that:\n@@ -1098,6 +1112,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 \"output_hidden_states\": True,\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n+                \"use_cache\": True,\n             }\n             output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n \n@@ -1150,7 +1165,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n \n-            config.use_cache = True\n             config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n             # Sets assisted generation arguments such that:\n@@ -1169,6 +1183,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                 \"output_hidden_states\": True,\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n+                \"use_cache\": True,\n             }\n \n             output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n@@ -1196,12 +1211,6 @@ def test_dola_decoding_sample(self):\n             # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n \n-            # Some models don't support the cache and returning past_key_values\n-            if not hasattr(config, \"use_cache\"):\n-                config.use_cache = False\n-            else:\n-                config.use_cache = True\n-\n             # Encoder-decoder models are not supported\n             if config.is_encoder_decoder:\n                 self.skipTest(\"DoLa is not supported for encoder-decoder models\")\n@@ -1224,11 +1233,12 @@ def test_dola_decoding_sample(self):\n                 \"output_hidden_states\": True,\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n+                \"use_cache\": hasattr(config, \"use_cache\"),  # Some models don't support the cache\n             }\n             generation_kwargs.update({\"dola_layers\": \"low\"})\n             model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n             output_dola = model.generate(input_ids, **model_kwargs, **generation_kwargs)\n-            self._check_outputs(output_dola, input_ids, model.config, use_cache=config.use_cache)\n+            self._check_outputs(output_dola, input_ids, model.config, use_cache=hasattr(config, \"use_cache\"))\n \n     def test_assisted_decoding_sample(self):\n         # In this test we don't check assisted vs non-assisted output -- seeded assisted decoding with sample will not\n@@ -1261,7 +1271,6 @@ def test_assisted_decoding_sample(self):\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n \n-            config.use_cache = True\n             config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n             # Sets assisted generation arguments such that:\n@@ -1284,6 +1293,7 @@ def test_assisted_decoding_sample(self):\n                 \"output_hidden_states\": True,\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n+                \"use_cache\": True,\n             }\n             output_assisted = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n \n@@ -1566,14 +1576,14 @@ def test_generate_continue_from_past_key_values(self):\n             # 3. ignore `token_type_ids` for simplicity\n             # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n             #    active by default on some models\n-            config.use_cache = True\n             if \"token_type_ids\" in inputs:\n                 del inputs[\"token_type_ids\"]\n \n             model = model_class(config).to(torch_device)\n             model.eval()\n             model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n             model.generation_config.forced_eos_token_id = None\n+            model.generation_config.use_cache = True\n \n             # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n             outputs = model(**inputs)\n@@ -1631,7 +1641,6 @@ def test_new_cache_format(self, num_beams, do_sample):\n                 self.skipTest(reason=\"This model does not support the new cache format\")\n \n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = True\n \n             model = model_class(config).to(torch_device).eval()\n             generation_kwargs = {\n@@ -1640,6 +1649,7 @@ def test_new_cache_format(self, num_beams, do_sample):\n                 \"num_beams\": num_beams,\n                 \"num_return_sequences\": num_beams,\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+                \"use_cache\": True,\n             }\n \n             # Sets seed before calling `generate` for the case with do_sample=True\n@@ -1701,7 +1711,6 @@ def test_generate_with_static_cache(self):\n             if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n-            config.use_cache = True\n             config.is_decoder = True\n             batch_size, seq_length = input_ids.shape\n             max_new_tokens = 20\n@@ -1712,6 +1721,7 @@ def test_generate_with_static_cache(self):\n                 \"max_new_tokens\": max_new_tokens,\n                 \"cache_implementation\": \"static\",\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+                \"use_cache\": True,\n             }\n \n             max_cache_len = seq_length + max_new_tokens\n@@ -1740,7 +1750,6 @@ def test_generate_with_quant_cache(self):\n                 self.skipTest(reason=\"This model does not support the quantized cache format\")\n \n             config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = True\n             config.is_decoder = True\n \n             model = model_class(config).to(torch_device).eval()\n@@ -1750,6 +1759,7 @@ def test_generate_with_quant_cache(self):\n                 # careful with group size, should be divisor of model's hidden size\n                 \"cache_config\": {\"backend\": \"quanto\", \"nbits\": 2, \"q_group_size\": 8, \"residual_length\": 128},\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+                \"use_cache\": True,\n             }\n \n             results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n@@ -1890,22 +1900,24 @@ def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_\n \n         # Past Key Value States -- a few notes here:\n         # 1. Its inner sequence length is with respect to the inputs of the latest forward pass, hence the \"-1\"\n-        # 2. Some old models still return `output.past_key_values` even without `use_cache=True`\n-        # 3. TODO (joao): A few models have different formats/types, skipping those until the cache refactor is\n-        # complete\n-        models_without_standard_cache = (\"ctrl\", \"fsmt\", \"gptbigcode\", \"mega\", \"reformer\", \"jamba\", \"mamba\")\n+        # 2. We ignore models that have unique cache structures (e.g. mamba) or are in need of refatoring to match the\n+        #    standard cache format (e.g.gptbigcode )\n+        models_without_standard_cache = (\"ctrl\", \"fsmt\", \"gptbigcode\", \"mega\", \"reformer\", \"jamba\", \"mamba\", \"xlnet\")\n         has_standard_cache = not any(\n             model_name in config.__class__.__name__.lower() for model_name in models_without_standard_cache\n         )\n-        if use_cache and has_standard_cache:\n-            past_key_values = output.past_key_values\n-            past_sequence_length = output.sequences.shape[-1] - 1\n-            self._check_past_key_values_for_generate(\n-                num_sequences_in_output,\n-                past_key_values,\n-                seq_length=past_sequence_length,\n-                config=config,\n-            )\n+        if has_standard_cache:\n+            if use_cache:\n+                past_key_values = output.past_key_values\n+                past_sequence_length = output.sequences.shape[-1] - 1\n+                self._check_past_key_values_for_generate(\n+                    num_sequences_in_output,\n+                    past_key_values,\n+                    seq_length=past_sequence_length,\n+                    config=config,\n+                )\n+            elif use_cache is False:\n+                self.assertTrue(output.past_key_values is None)\n \n     def _check_scores(self, batch_size, scores, length, config):\n         expected_shape = (batch_size, config.vocab_size)"
        }
    ],
    "stats": {
        "total": 567,
        "additions": 311,
        "deletions": 256
    }
}