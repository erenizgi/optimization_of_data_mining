{
    "author": "Ryoo72",
    "message": "layernorm_decay_fix (#35927)\n\n* layernorm_decay_fix\r\n\r\n* W293 fix\r\n\r\n* ruff format fix\r\n\r\n* black format\r\n\r\n* ruff format\r\n\r\n* erase last layer\r\n\r\n* add test_get_parameter_names_rmsnorm\r\n\r\n* rmsnorm fix",
    "sha": "b1954fd64abf392a60e3e007f03471db0f3cf4db",
    "files": [
        {
            "sha": "0c21fd92b834035dac28381381c97619a151c39f",
            "filename": "docs/source/en/perf_train_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1954fd64abf392a60e3e007f03471db0f3cf4db/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1954fd64abf392a60e3e007f03471db0f3cf4db/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md?ref=b1954fd64abf392a60e3e007f03471db0f3cf4db",
            "patch": "@@ -298,8 +298,7 @@ from transformers.trainer_pt_utils import get_parameter_names\n \n training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n \n-decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n-decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n+decay_parameters = get_parameter_names(model, [nn.LayerNorm], [\"bias\", \"layernorm\", \"rmsnorm\"])\n optimizer_grouped_parameters = [\n     {\n         \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],"
        },
        {
            "sha": "c45737370a5133b2b8769354a8fb54e77976f7ad",
            "filename": "docs/source/ja/perf_train_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1954fd64abf392a60e3e007f03471db0f3cf4db/docs%2Fsource%2Fja%2Fperf_train_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1954fd64abf392a60e3e007f03471db0f3cf4db/docs%2Fsource%2Fja%2Fperf_train_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_train_gpu_one.md?ref=b1954fd64abf392a60e3e007f03471db0f3cf4db",
            "patch": "@@ -237,8 +237,7 @@ from transformers.trainer_pt_utils import get_parameter_names\n \n training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n \n-decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n-decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n+decay_parameters = get_parameter_names(model, [nn.LayerNorm], [\"bias\", \"layernorm\", \"rmsnorm\"])\n optimizer_grouped_parameters = [\n     {\n         \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],"
        },
        {
            "sha": "e95bc185e48cce44be2f6fb499e5595c6ceb40fa",
            "filename": "examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1954fd64abf392a60e3e007f03471db0f3cf4db/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1954fd64abf392a60e3e007f03471db0f3cf4db/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py?ref=b1954fd64abf392a60e3e007f03471db0f3cf4db",
            "patch": "@@ -680,8 +680,7 @@ def compute_metrics(pred):\n     # Instantiate custom data collator\n     data_collator = DataCollatorCTCWithPadding(processor=processor)\n \n-    decay_parameters = get_parameter_names(model, [torch.nn.LayerNorm])\n-    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n+    decay_parameters = get_parameter_names(model, [torch.nn.LayerNorm], [\"bias\", \"layernorm\", \"rmsnorm\"])\n     optimizer_grouped_parameters = [\n         {\n             \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],"
        },
        {
            "sha": "74129a7e5c7fc41463f88865a7bf9c4c9ccf7717",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1954fd64abf392a60e3e007f03471db0f3cf4db/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1954fd64abf392a60e3e007f03471db0f3cf4db/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=b1954fd64abf392a60e3e007f03471db0f3cf4db",
            "patch": "@@ -1177,13 +1177,13 @@ def create_optimizer_and_scheduler(self, num_training_steps: int):\n \n     def get_decay_parameter_names(self, model) -> List[str]:\n         \"\"\"\n-        Get all parameter names that weight decay will be applied to\n+        Get all parameter names that weight decay will be applied to.\n \n-        Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still\n-        apply to those modules since this function only filter out instance of nn.LayerNorm\n+        This function filters out parameters in two ways:\n+        1. By layer type (instances of layers specified in ALL_LAYERNORM_LAYERS)\n+        2. By parameter name patterns (containing 'bias', 'layernorm', or 'rmsnorm')\n         \"\"\"\n-        decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)\n-        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n+        decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS, [\"bias\", \"layernorm\", \"rmsnorm\"])\n         return decay_parameters\n \n     def create_optimizer(self):"
        },
        {
            "sha": "10e6678728f6517fe9d14314663df66c84197573",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1954fd64abf392a60e3e007f03471db0f3cf4db/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1954fd64abf392a60e3e007f03471db0f3cf4db/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=b1954fd64abf392a60e3e007f03471db0f3cf4db",
            "patch": "@@ -1120,19 +1120,25 @@ def numel(p):\n     return sum(numel(p) for p in model.parameters() if not trainable_only or p.requires_grad)\n \n \n-def get_parameter_names(model, forbidden_layer_types):\n+def get_parameter_names(model, forbidden_layer_types, forbidden_layer_names=None):\n     \"\"\"\n     Returns the names of the model parameters that are not inside a forbidden layer.\n     \"\"\"\n+    if forbidden_layer_names is None:\n+        forbidden_layer_names = []\n     result = []\n     for name, child in model.named_children():\n+        child_params = get_parameter_names(child, forbidden_layer_types, forbidden_layer_names)\n         result += [\n             f\"{name}.{n}\"\n-            for n in get_parameter_names(child, forbidden_layer_types)\n+            for n in child_params\n             if not isinstance(child, tuple(forbidden_layer_types))\n+            and not any(forbidden in f\"{name}.{n}\".lower() for forbidden in forbidden_layer_names)\n         ]\n-    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n-    result += list(model._parameters.keys())\n+    # Add model specific parameters that are not in any child\n+    result += [\n+        k for k in model._parameters.keys() if not any(forbidden in k.lower() for forbidden in forbidden_layer_names)\n+    ]\n     return result\n \n "
        },
        {
            "sha": "12c750c6ca52663e7953bb9e5fab123fa54f805f",
            "filename": "tests/trainer/test_trainer_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1954fd64abf392a60e3e007f03471db0f3cf4db/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1954fd64abf392a60e3e007f03471db0f3cf4db/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_utils.py?ref=b1954fd64abf392a60e3e007f03471db0f3cf4db",
            "patch": "@@ -244,6 +244,33 @@ def test_get_parameter_names(self):\n         )\n         # fmt: on\n \n+    def test_get_parameter_names_rmsnorm(self):\n+        class RMSNorm(nn.Module):\n+            def __init__(self, hidden_size):\n+                super().__init__()\n+                self.weight = nn.Parameter(torch.ones(hidden_size))\n+                self.bias = nn.Parameter(torch.zeros(hidden_size))\n+\n+        class ModelWithRMSNorm(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.linear = nn.Linear(128, 128)\n+                self.rmsnorm = RMSNorm(128)\n+                self.bias = nn.Parameter(torch.zeros(128))\n+\n+        model = ModelWithRMSNorm()\n+        # Test both type-based and name-based filtering\n+        decay_parameters = get_parameter_names(model, [], [\"bias\", \"rmsnorm\"])\n+\n+        # Parameters that should be in weight decay\n+        self.assertIn(\"linear.weight\", decay_parameters)\n+\n+        # Parameters that should NOT be in weight decay\n+        self.assertNotIn(\"linear.bias\", decay_parameters)\n+        self.assertNotIn(\"rmsnorm.weight\", decay_parameters)\n+        self.assertNotIn(\"rmsnorm.bias\", decay_parameters)\n+        self.assertNotIn(\"bias\", decay_parameters)\n+\n     def test_distributed_sampler_with_loop(self):\n         batch_size = 16\n         for length in [23, 64, 123]:"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 45,
        "deletions": 15
    }
}