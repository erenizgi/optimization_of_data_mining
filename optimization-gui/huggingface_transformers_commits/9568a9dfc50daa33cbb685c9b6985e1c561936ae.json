{
    "author": "faaany",
    "message": "[docs] add XPU besides CUDA, MPS etc. (#34777)\n\nadd XPU",
    "sha": "9568a9dfc50daa33cbb685c9b6985e1c561936ae",
    "files": [
        {
            "sha": "f5bba54a6e6bc74b5ff7d133830e2e19ce77209f",
            "filename": "docs/source/en/quantization/quanto.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9568a9dfc50daa33cbb685c9b6985e1c561936ae/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9568a9dfc50daa33cbb685c9b6985e1c561936ae/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md?ref=9568a9dfc50daa33cbb685c9b6985e1c561936ae",
            "patch": "@@ -28,7 +28,7 @@ Try Quanto + transformers with this [notebook](https://colab.research.google.com\n - weights quantization (`float8`,`int8`,`int4`,`int2`)\n - activation quantization (`float8`,`int8`)\n - modality agnostic (e.g CV,LLM)\n-- device agnostic (e.g CUDA,MPS,CPU)\n+- device agnostic (e.g CUDA,XPU,MPS,CPU)\n - compatibility with `torch.compile`\n - easy to add custom kernel for specific device\n - supports quantization aware training"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}