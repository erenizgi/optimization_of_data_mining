{
    "author": "ssum21",
    "message": "[i18n-KO] Translated `big_bird.md` to Korean (#40445)\n\n* docs: ko: BigBird.md\n\n* feat: nmt draft\n\n* fix: manual edits",
    "sha": "8739fc05c463dfdf5729a69891cebfb7e52a6d60",
    "files": [
        {
            "sha": "0c3a6be7cbdfaf7d1c98e79fe048f3d3a8f41c6b",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8739fc05c463dfdf5729a69891cebfb7e52a6d60/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/8739fc05c463dfdf5729a69891cebfb7e52a6d60/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=8739fc05c463dfdf5729a69891cebfb7e52a6d60",
            "patch": "@@ -461,7 +461,7 @@\n         title: BertJapanese\n       - local: model_doc/bertweet\n         title: BERTweet\n-      - local: in_translation\n+      - local: model_doc/big_bird\n         title: BigBird\n       - local: in_translation\n         title: BigBirdPegasus"
        },
        {
            "sha": "ae4f14ec3579d6406167842176bf0afc111dde9c",
            "filename": "docs/source/ko/model_doc/big_bird.md",
            "status": "added",
            "additions": 158,
            "deletions": 0,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/8739fc05c463dfdf5729a69891cebfb7e52a6d60/docs%2Fsource%2Fko%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8739fc05c463dfdf5729a69891cebfb7e52a6d60/docs%2Fsource%2Fko%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbig_bird.md?ref=8739fc05c463dfdf5729a69891cebfb7e52a6d60",
            "patch": "@@ -0,0 +1,158 @@\n+<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*이 모델은 2020-07-28에 출시되었으며 2021-03-30에 Hugging Face Transformers에 추가되었습니다.*\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\" >\n+    </div>\n+</div>\n+\n+# BigBird[[bigbird]]\n+\n+[BigBird](https://huggingface.co/papers/2007.14062)는 [BERT](./bert)의 512토큰과 달리 최대 4096토큰까지의 시퀀스 길이를 처리하도록 설계된 트랜스포머 모델입니다. 기존 트랜스포머들은 시퀀스 길이가 늘어날수록 어텐션 계산 비용이 급격히 증가하여 긴 입력 처리에 어려움을 겪습니다. BigBird는 희소 어텐션 메커니즘으로 이 문제를 해결하는데, 모든 토큰을 동시에 살펴보는 대신 로컬 어텐션, 랜덤 어텐션, 그리고 몇 개의 전역 토큰을 조합하여 전체 입력을 효율적으로 처리합니다. 이런 방식을 통해 계산 효율성을 유지하면서도 시퀀스 전체를 충분히 이해할 수 있게 됩니다. 따라서 BigBird는 질의응답, 요약, 유전체학 응용처럼 긴 문서를 다루는 작업에 특히 우수한 성능을 보입니다.\n+\n+모든 원본 BigBird 체크포인트는 [Google](https://huggingface.co/google?search_models=bigbird) 조직에서 찾아볼 수 있습니다.\n+\n+> [!TIP]\n+> 오른쪽 사이드바의 BigBird 모델들을 클릭하여 다양한 언어 작업에 BigBird를 적용하는 더 많은 예시를 확인해보세요.\n+\n+아래 예시는 [`Pipeline`], [`AutoModel`], 그리고 명령줄에서 `[MASK]` 토큰을 예측하는 방법을 보여줍니다.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"fill-mask\",\n+    model=\"google/bigbird-roberta-base\",\n+    dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"google/bigbird-roberta-base\",\n+)\n+model = AutoModelForMaskedLM.from_pretrained(\n+    \"google/bigbird-roberta-base\",\n+    dtype=torch.float16,\n+    device_map=\"auto\",\n+)\n+inputs = tokenizer(\"Plants create [MASK] through a process known as photosynthesis.\", return_tensors=\"pt\").to(model.device)\n+\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+    predictions = outputs.logits\n+\n+masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n+predicted_token_id = predictions[0, masked_index].argmax(dim=-1)\n+predicted_token = tokenizer.decode(predicted_token_id)\n+\n+print(f\"The predicted token is: {predicted_token}\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+!echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model google/bigbird-roberta-base --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## 참고사항[[notes]]\n+\n+- BigBird는 절대 위치 임베딩을 사용하므로 입력을 오른쪽에 패딩해야 합니다.\n+- BigBird는 `original_full`과 `block_sparse` 어텐션을 지원합니다. 입력 시퀀스 길이가 1024 미만인 경우에는 희소 패턴의 이점이 크지 않으므로 `original_full` 사용을 권장합니다.\n+- 현재 구현은 3블록 윈도우 크기와 2개의 전역 블록을 사용하며, ITC 구현만 지원하고 `num_random_blocks=0`은 지원하지 않습니다.\n+- 시퀀스 길이는 블록 크기로 나누어떨어져야 합니다.\n+\n+## 리소스[[resources]]\n+\n+- BigBird 어텐션 메커니즘의 자세한 작동 원리는 [BigBird](https://huggingface.co/blog/big-bird) 블로그 포스트를 참고하세요.\n+\n+## BigBirdConfig[[bigbirdconfig]]\n+\n+[[autodoc]] BigBirdConfig\n+\n+## BigBirdTokenizer[[bigbirdtokenizer]]\n+\n+[[autodoc]] BigBirdTokenizer\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - save_vocabulary\n+\n+## BigBirdTokenizerFast[[bigbirdtokenizerfast]]\n+\n+[[autodoc]] BigBirdTokenizerFast\n+\n+## BigBird 특정 출력[[bigbird-specific-outputs]]\n+\n+[[autodoc]] models.big_bird.modeling_big_bird.BigBirdForPreTrainingOutput\n+\n+## BigBirdModel[[bigbirdmodel]]\n+\n+[[autodoc]] BigBirdModel\n+    - forward\n+\n+## BigBirdForPreTraining[[bigbirdforpretraining]]\n+\n+[[autodoc]] BigBirdForPreTraining\n+    - forward\n+\n+## BigBirdForCausalLM[[bigbirdforcausallm]]\n+\n+[[autodoc]] BigBirdForCausalLM\n+    - forward\n+\n+## BigBirdForMaskedLM[[bigbirdformaskedlm]]\n+\n+[[autodoc]] BigBirdForMaskedLM\n+    - forward\n+\n+## BigBirdForSequenceClassification[[bigbirdforsequenceclassification]]\n+\n+[[autodoc]] BigBirdForSequenceClassification\n+    - forward\n+\n+## BigBirdForMultipleChoice[[bigbirdformultiplechoice]]\n+\n+[[autodoc]] BigBirdForMultipleChoice\n+    - forward\n+\n+## BigBirdForTokenClassification[[bigbirdfortokenclassification]]\n+\n+[[autodoc]] BigBirdForTokenClassification\n+    - forward\n+\n+## BigBirdForQuestionAnswering[[bigbirdforquestionanswering]]\n+\n+[[autodoc]] BigBirdForQuestionAnswering\n+    - forward\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 160,
        "additions": 159,
        "deletions": 1
    }
}