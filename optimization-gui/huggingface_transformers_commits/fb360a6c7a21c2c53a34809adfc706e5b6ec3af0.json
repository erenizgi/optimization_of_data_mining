{
    "author": "Rocketknight1",
    "message": "BatchFeature.to() supports non-tensor keys (#33918)\n\n* Fix issue in oneformer preprocessing\r\n\r\n* [run slow] oneformer\r\n\r\n* [run_slow] oneformer\r\n\r\n* Make the same fixes in DQA and object detection pipelines\r\n\r\n* Fix BatchFeature.to() instead\r\n\r\n* Revert pipeline-specific changes\r\n\r\n* Add the same check in Pixtral's methods\r\n\r\n* Add the same check in BatchEncoding\r\n\r\n* make sure torch is imported",
    "sha": "fb360a6c7a21c2c53a34809adfc706e5b6ec3af0",
    "files": [
        {
            "sha": "f3cde8180c1bd4a2061a3af1ada1ddd295ba66cc",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=fb360a6c7a21c2c53a34809adfc706e5b6ec3af0",
            "patch": "@@ -237,10 +237,10 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n         # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n         for k, v in self.items():\n             # check if v is a floating point\n-            if torch.is_floating_point(v):\n+            if isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n                 # cast and send to device\n                 new_data[k] = v.to(*args, **kwargs)\n-            elif device is not None:\n+            elif isinstance(v, torch.Tensor) and device is not None:\n                 new_data[k] = v.to(device=device)\n             else:\n                 new_data[k] = v"
        },
        {
            "sha": "a75704fc3dbac8fb96547bcc4e2f84ff7d314fb0",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=fb360a6c7a21c2c53a34809adfc706e5b6ec3af0",
            "patch": "@@ -86,10 +86,10 @@ def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n                 new_data[k] = [\n                     element.to(*args, **kwargs) for sample in v for element in sample if is_torch_tensor(element)\n                 ]\n-            elif torch.is_floating_point(v):\n+            elif isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n                 # cast and send to device\n                 new_data[k] = v.to(*args, **kwargs)\n-            elif device is not None:\n+            elif isinstance(v, torch.Tensor) and device is not None:\n                 new_data[k] = v.to(device=device)\n             else:\n                 new_data[k] = v"
        },
        {
            "sha": "70d28fb7b79c93997c8870e61a00b06df1574423",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=fb360a6c7a21c2c53a34809adfc706e5b6ec3af0",
            "patch": "@@ -90,10 +90,10 @@ def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n                 new_data[k] = [\n                     element.to(*args, **kwargs) for sample in v for element in sample if is_torch_tensor(element)\n                 ]\n-            elif torch.is_floating_point(v):\n+            elif isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n                 # cast and send to device\n                 new_data[k] = v.to(*args, **kwargs)\n-            elif device is not None:\n+            elif isinstance(v, torch.Tensor) and device is not None:\n                 new_data[k] = v.to(device=device)\n             else:\n                 new_data[k] = v"
        },
        {
            "sha": "438ef1c8a4a5e25c4ad5268f301455fd9aef7636",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb360a6c7a21c2c53a34809adfc706e5b6ec3af0/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=fb360a6c7a21c2c53a34809adfc706e5b6ec3af0",
            "patch": "@@ -809,12 +809,13 @@ def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":\n             [`BatchEncoding`]: The same instance after modification.\n         \"\"\"\n         requires_backends(self, [\"torch\"])\n+        import torch\n \n         # This check catches things like APEX blindly calling \"to\" on all inputs to a module\n         # Otherwise it passes the casts down and casts the LongTensor containing the token idxs\n         # into a HalfTensor\n         if isinstance(device, str) or is_torch_device(device) or isinstance(device, int):\n-            self.data = {k: v.to(device=device) for k, v in self.data.items() if v is not None}\n+            self.data = {k: v.to(device=device) for k, v in self.data.items() if isinstance(v, torch.Tensor)}\n         else:\n             logger.warning(f\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\")\n         return self"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 8,
        "deletions": 7
    }
}