{
    "author": "guangy10",
    "message": "Generate using exported model and enable gemma2-2b in ExecuTorch (#33707)\n\n* Generate using exported model and enable gemma2-2b in ExecuTorch\r\n\r\n* [run_slow] gemma, gemma2\r\n\r\n* truncate expected output message\r\n\r\n* Bump required torch version to support gemma2 export\r\n\r\n* [run_slow] gemma, gemma2\r\n\r\n---------\r\n\r\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4",
    "files": [
        {
            "sha": "c0adff386f6312c0df1bc34ad9bcca9b1f4b32db",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4",
            "patch": "@@ -114,6 +114,56 @@ def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n         )\n         return outs.logits\n \n+    @staticmethod\n+    def generate(\n+        exported_program: torch.export.ExportedProgram, prompt_token_ids: torch.Tensor, max_new_tokens: int\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Generate a sequence of tokens using an exported program.\n+\n+        This util function is designed to test exported models by simulating the generation process.\n+        It processes the input prompt tokens sequentially (no parallel prefill).\n+        This generate function is not intended to replace the original `generate` method, and the support\n+        for leveraging the original `generate` is potentially planed!\n+\n+        Args:\n+            exported_program (`torch.export.ExportedProgram`): The exported program generated via `torch.export`.\n+            prompt_token_ids (`torch.Tensor`): Tensor representing the input prompt token IDs.\n+            max_new_tokens (`int`): Maximum number of new tokens to generate. Note that the total generation\n+                length is limited by both `max_new_tokens` and the model's cache size.\n+\n+        Returns:\n+            torch.Tensor: A tensor containing the generated sequence of token IDs, including the original prompt tokens.\n+        \"\"\"\n+        prompt_token_len = prompt_token_ids.shape[-1]\n+        max_generation_length = prompt_token_len + max_new_tokens\n+        for buffer_name, buffer in exported_program.named_buffers():\n+            if buffer_name.startswith(\"static_cache.key_cache\"):\n+                max_cache_len = buffer.shape[2]\n+                max_generation_length = min(max_generation_length, max_cache_len)\n+                break\n+\n+        response_tokens = []\n+        for input_pos in range(min(max_generation_length, prompt_token_len)):\n+            result = exported_program.module().forward(\n+                input_ids=prompt_token_ids[:, input_pos : input_pos + 1],\n+                cache_position=torch.tensor([input_pos], dtype=torch.long),\n+            )\n+            response_tokens.append(prompt_token_ids[0][input_pos].item())\n+\n+        current_token = torch.argmax(result[:, -1, :], dim=-1).item()\n+        response_tokens.append(current_token)\n+\n+        while len(response_tokens) < max_generation_length:\n+            result = exported_program.module().forward(\n+                input_ids=torch.tensor([[current_token]], dtype=torch.long),\n+                cache_position=torch.tensor([len(response_tokens)], dtype=torch.long),\n+            )\n+            current_token = torch.argmax(result[:, -1, :], dim=-1).item()\n+            response_tokens.append(current_token)\n+\n+        return torch.tensor([response_tokens], dtype=torch.long)\n+\n \n def convert_and_export_with_cache(\n     model: PreTrainedModel,"
        },
        {
            "sha": "a888bdcd3bc7be031133c9c053e74b1452c895eb",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4",
            "patch": "@@ -21,6 +21,7 @@\n from packaging import version\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, GemmaConfig, is_torch_available\n+from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     is_flaky,\n     require_bitsandbytes,\n@@ -841,6 +842,67 @@ def test_compile_static_cache(self):\n         static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n \n+    @slow\n+    @require_read_token\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.3.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", pad_token=\"</s>\", padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have looked on the internet and I have found\",\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = GemmaForCausalLM.from_pretrained(\n+            \"google/gemma-2b\",\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompts = [\"Hello I am doing\"]\n+        prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + eager\n+        eager_generated_ids = model.generate(\n+            **prompt_tokens, max_new_tokens=max_new_tokens, do_sample=False, cache_implementation=cache_implementation\n+        )\n+        eager_generated_text = tokenizer.batch_decode(eager_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, eager_generated_text)\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n+\n     def test_model_2b_bf16_dola(self):\n         model_id = \"google/gemma-2b\"\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2"
        },
        {
            "sha": "8f9a918dca00826d831c7be98b3e57b682182645",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=7d97cca8dde8b1d5a21a4df56c340cfe3cdd9fb4",
            "patch": "@@ -16,10 +16,12 @@\n \n import unittest\n \n+from packaging import version\n from parameterized import parameterized\n from pytest import mark\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, HybridCache, is_torch_available, pipeline\n+from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     require_flash_attn,\n     require_read_token,\n@@ -306,3 +308,57 @@ def test_model_9b_flash_attn(self):\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    @slow\n+    @require_read_token\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.5.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.5 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\", pad_token=\"</s>\", padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\",\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"google/gemma-2-2b\",\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompts = [\"Hello I am doing\"]\n+        prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        }
    ],
    "stats": {
        "total": 168,
        "additions": 168,
        "deletions": 0
    }
}