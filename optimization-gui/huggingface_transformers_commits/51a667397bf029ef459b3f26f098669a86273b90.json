{
    "author": "a4lg",
    "message": "Override Transformers defaults by GGUF defaults (#42770)\n\n* Override Transformers defaults by GGUF defaults\n\nIn some models, GGUF uses default or fixed values different from this\nlibrary.  To integrate GGUF-based models without additional configuration,\nwe need some kind of compatibility layer.\n\nThis commit provides additional mapping to provide GGUF-specific default\nvalues to initialize parameters in this library.\n\nCurrently, only fixed \"norm_topk_prob\" value of Qwen3 MoE (True) is\ndefined because (a) it differs from the default value of this library\n(False) and (b) if this parameter is incorrectly set, it results in\nalmost completely garbled output.\n\nSigned-off-by: Tsukasa OI <floss_llm@irq.a4lg.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Tsukasa OI <floss_llm@irq.a4lg.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "51a667397bf029ef459b3f26f098669a86273b90",
    "files": [
        {
            "sha": "7c7ec898c718c6187ab8f1e68580852b297a8a38",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/51a667397bf029ef459b3f26f098669a86273b90/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51a667397bf029ef459b3f26f098669a86273b90/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=51a667397bf029ef459b3f26f098669a86273b90",
            "patch": "@@ -54,6 +54,7 @@\n     \"finegrained_fp8\": [\"FP8Linear\", \"replace_with_fp8_linear\"],\n     \"fsdp\": [\"is_fsdp_enabled\", \"is_fsdp_managed_module\"],\n     \"ggml\": [\n+        \"GGUF_CONFIG_DEFAULTS_MAPPING\",\n         \"GGUF_CONFIG_MAPPING\",\n         \"GGUF_TOKENIZER_MAPPING\",\n         \"_gguf_parse_value\",\n@@ -201,6 +202,7 @@\n     from .finegrained_fp8 import FP8Linear, replace_with_fp8_linear\n     from .fsdp import is_fsdp_enabled, is_fsdp_managed_module\n     from .ggml import (\n+        GGUF_CONFIG_DEFAULTS_MAPPING,\n         GGUF_CONFIG_MAPPING,\n         GGUF_TOKENIZER_MAPPING,\n         _gguf_parse_value,"
        },
        {
            "sha": "a79acea0cb2faf47b361a1a799a9744091cd3578",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/51a667397bf029ef459b3f26f098669a86273b90/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51a667397bf029ef459b3f26f098669a86273b90/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=51a667397bf029ef459b3f26f098669a86273b90",
            "patch": "@@ -313,6 +313,16 @@\n     },\n }\n \n+# We only need to set here the parameters that default to different values between transformers and llamacpp.\n+GGUF_CONFIG_DEFAULTS_MAPPING = {\n+    \"qwen3_moe\": {\n+        # NOTE: Qwen3MoeConfig defaults to false but llama.cpp needs this to be true.\n+        # See: https://github.com/ggml-org/llama.cpp/blob/17f7f4baad8b3a716ee139da7bb56ae984e8c0fa/src/models/qwen3moe.cpp#L85-L96\n+        #      (the parameter right after LLM_FFN_SILU corresponds to norm_topk_prob)\n+        \"norm_topk_prob\": True,\n+    },\n+}\n+\n \n def _gguf_parse_value(_value, data_type):\n     if not isinstance(data_type, list):"
        },
        {
            "sha": "f3ac6d8afb19acd6d7801b032c502d8c181e0a9e",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/51a667397bf029ef459b3f26f098669a86273b90/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51a667397bf029ef459b3f26f098669a86273b90/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=51a667397bf029ef459b3f26f098669a86273b90",
            "patch": "@@ -20,6 +20,7 @@\n from tqdm.auto import tqdm\n \n from .integrations import (\n+    GGUF_CONFIG_DEFAULTS_MAPPING,\n     GGUF_CONFIG_MAPPING,\n     GGUF_TOKENIZER_MAPPING,\n     _gguf_parse_value,\n@@ -437,6 +438,13 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n         all(\"output.weight\" != tensor.name for tensor in reader.tensors) or architecture in exceptions\n     )\n \n+    # Set GGUF-specific default values\n+    config_defaults = GGUF_CONFIG_DEFAULTS_MAPPING.get(\n+        updated_architecture, GGUF_CONFIG_DEFAULTS_MAPPING.get(architecture) or {}\n+    )\n+    for key, value in config_defaults.items():\n+        parsed_parameters[\"config\"].setdefault(key, value)\n+\n     # List all key-value pairs in a columnized format\n     for gguf_key, field in reader.fields.items():\n         gguf_key = gguf_key.replace(architecture, updated_architecture)"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 20,
        "deletions": 0
    }
}