{
    "author": "SunMarc",
    "message": "fix async client for transformers chat (#41255)\n\n* fix-client\n\n* fix",
    "sha": "2f3e26669202d885fd8bfef5cc27019c355ef916",
    "files": [
        {
            "sha": "6ddf90164ba71814111ee3f9e0716e598e154553",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 38,
            "deletions": 40,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f3e26669202d885fd8bfef5cc27019c355ef916/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f3e26669202d885fd8bfef5cc27019c355ef916/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=2f3e26669202d885fd8bfef5cc27019c355ef916",
            "patch": "@@ -687,7 +687,6 @@ async def _inner_run(self):\n \n         model = self.args.model_name_or_path + \"@\" + self.args.model_revision\n         host = \"http://localhost\" if self.args.host == \"localhost\" else self.args.host\n-        client = AsyncInferenceClient(f\"{host}:{self.args.port}\")\n \n         args = self.args\n         if args.examples_path is None:\n@@ -710,48 +709,47 @@ async def _inner_run(self):\n \n         # Starts the session with a minimal help message at the top, so that a user doesn't get stuck\n         interface.print_help(minimal=True)\n-        while True:\n-            try:\n-                user_input = interface.input()\n-\n-                # User commands\n-                if user_input.startswith(\"!\"):\n-                    # `!exit` is special, it breaks the loop\n-                    if user_input == \"!exit\":\n-                        break\n-                    else:\n-                        chat, valid_command, generation_config, model_kwargs = self.handle_non_exit_user_commands(\n-                            user_input=user_input,\n-                            args=args,\n-                            interface=interface,\n-                            examples=examples,\n-                            generation_config=generation_config,\n-                            model_kwargs=model_kwargs,\n-                            chat=chat,\n-                        )\n-                    # `!example` sends a user message to the model\n-                    if not valid_command or not user_input.startswith(\"!example\"):\n-                        continue\n-                else:\n-                    chat.append({\"role\": \"user\", \"content\": user_input})\n-\n-                stream = client.chat_completion(\n-                    chat,\n-                    stream=True,\n-                    extra_body={\n-                        \"generation_config\": generation_config.to_json_string(),\n-                        \"model\": model,\n-                    },\n-                )\n \n-                model_output = await interface.stream_output(stream)\n+        async with AsyncInferenceClient(f\"{host}:{self.args.port}\") as client:\n+            while True:\n+                try:\n+                    user_input = interface.input()\n+\n+                    # User commands\n+                    if user_input.startswith(\"!\"):\n+                        # `!exit` is special, it breaks the loop\n+                        if user_input == \"!exit\":\n+                            break\n+                        else:\n+                            chat, valid_command, generation_config, model_kwargs = self.handle_non_exit_user_commands(\n+                                user_input=user_input,\n+                                args=args,\n+                                interface=interface,\n+                                examples=examples,\n+                                generation_config=generation_config,\n+                                model_kwargs=model_kwargs,\n+                                chat=chat,\n+                            )\n+                        # `!example` sends a user message to the model\n+                        if not valid_command or not user_input.startswith(\"!example\"):\n+                            continue\n+                    else:\n+                        chat.append({\"role\": \"user\", \"content\": user_input})\n+\n+                    stream = client.chat_completion(\n+                        chat,\n+                        stream=True,\n+                        extra_body={\n+                            \"generation_config\": generation_config.to_json_string(),\n+                            \"model\": model,\n+                        },\n+                    )\n \n-                chat.append({\"role\": \"assistant\", \"content\": model_output})\n+                    model_output = await interface.stream_output(stream)\n \n-            except KeyboardInterrupt:\n-                break\n-            finally:\n-                await client.close()\n+                    chat.append({\"role\": \"assistant\", \"content\": model_output})\n+                except KeyboardInterrupt:\n+                    break\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 38,
        "deletions": 40
    }
}