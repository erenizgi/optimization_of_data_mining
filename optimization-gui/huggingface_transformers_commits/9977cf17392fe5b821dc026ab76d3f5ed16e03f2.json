{
    "author": "vasqu",
    "message": "[`Flash Attention`] Fix flash attention integration (#40002)\n\n* fix flash attention\n\n* i got a stroke reading that comment\n\n* change dropout kwarg back to before\n\n* rename _fa3... as it's used for multiple variants and should work as fallback instead\n\n* simplify imports and support kwargs for fa\n\n* style\n\n* fix comments order\n\n* small fix\n\n* skip kernels test (causes cuda illegal memories w/o cleanup), fix fa test in general esp for models like bart\n\n* style\n\n* allow fullgraph by preloading on init\n\n* make globals \"private\"\n\n* ci pls be happy\n\n* change skip conditions based on backend flag (indicating missing mask interface)\n\n* move globals support to a function to prepare kwargs\n\n* style\n\n* generalize supported kwargs\n\n* small change to doc\n\n* fix\n\n* add comments\n\n* style\n\n* revert prep during generate\n\n* style\n\n* revert weird style changes\n\n* add fa kwarg prep during generate with fixes back\n\n* how did this even happen\n\n* how\n\n* add comment",
    "sha": "9977cf17392fe5b821dc026ab76d3f5ed16e03f2",
    "files": [
        {
            "sha": "b83d5a9733985d163c4e7de48afb04265bdc7580",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9977cf17392fe5b821dc026ab76d3f5ed16e03f2",
            "patch": "@@ -678,9 +678,10 @@ def prepare_inputs_for_generation(\n         if encoder_attention_mask is not None:\n             model_inputs[\"attention_mask\"] = encoder_attention_mask\n \n+        # 7. Prepare kwargs for flash attention to avoid recomputations\n         if \"flash\" in self.config._attn_implementation and self._supports_attention_backend:\n-            cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k = prepare_fa_kwargs_from_position_ids(\n-                position_ids, is_packed_sequence=False\n+            (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = prepare_fa_kwargs_from_position_ids(\n+                model_inputs[\"position_ids\"], is_packed_sequence=False\n             )\n             model_inputs.update(\n                 cu_seq_lens_q=cu_seq_lens_q.to(self.device),\n@@ -689,12 +690,12 @@ def prepare_inputs_for_generation(\n                 max_length_k=max_length_k,\n             )\n \n-        # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        # 8. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():\n             if key not in model_inputs:\n                 model_inputs[key] = value\n \n-        # 8. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n+        # 9. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n         model_inputs.pop(\"labels\", None)\n         return model_inputs\n "
        },
        {
            "sha": "716a3481a82a689728944d646ee0216b04ff758f",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "modified",
            "additions": 6,
            "deletions": 126,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=9977cf17392fe5b821dc026ab76d3f5ed16e03f2",
            "patch": "@@ -10,20 +10,16 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import math\n import os\n \n import torch\n-import torch.nn.functional as F\n \n from ..utils.import_utils import is_torch_npu_available\n \n \n if is_torch_npu_available():\n-    import math\n-\n-    import torch_npu\n-    from einops import rearrange, repeat\n-    from torch_npu import npu_rotary_mul\n+    from torch_npu import npu_fusion_attention, npu_rotary_mul\n \n \n # FlashAttention2 is supported on Ascend NPU with down-right aligned causal mask by default.\n@@ -52,117 +48,6 @@ def is_npu_fa2_top_left_aligned_causal_mask():\n     return SPARSE_MODE == TOP_LEFT_ALIGNED_CAUSAL_MASK_MODE if is_torch_npu_available() else False\n \n \n-# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n-class IndexFirstAxis(torch.autograd.Function):\n-    @staticmethod\n-    def forward(ctx, input, indices):\n-        ctx.save_for_backward(indices)\n-        assert input.ndim >= 2\n-        ctx.first_axis_dim, other_shape = input.shape[0], input.shape[1:]\n-        second_dim = other_shape.numel()\n-        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n-        # return input[indices]\n-        return torch.gather(\n-            rearrange(input, \"b ... -> b (...)\"), 0, repeat(indices, \"z -> z d\", d=second_dim)\n-        ).reshape(-1, *other_shape)\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        (indices,) = ctx.saved_tensors\n-        assert grad_output.ndim >= 2\n-        other_shape = grad_output.shape[1:]\n-        grad_output = rearrange(grad_output, \"b ... -> b (...)\")\n-        grad_input = torch.zeros(\n-            [ctx.first_axis_dim, grad_output.shape[1]],\n-            device=grad_output.device,\n-            dtype=grad_output.dtype,\n-        )\n-        # TD [2022-03-04] For some reason torch.scatter is a bit faster than indexing.\n-        # grad_input[indices] = grad_output\n-        grad_input.scatter_(0, repeat(indices, \"z -> z d\", d=grad_output.shape[1]), grad_output)\n-        return grad_input.reshape(ctx.first_axis_dim, *other_shape), None\n-\n-\n-index_first_axis = IndexFirstAxis.apply\n-\n-\n-# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n-class IndexPutFirstAxis(torch.autograd.Function):\n-    @staticmethod\n-    def forward(ctx, values, indices, first_axis_dim):\n-        ctx.save_for_backward(indices)\n-        assert indices.ndim == 1\n-        assert values.ndim >= 2\n-        output = torch.zeros(first_axis_dim, *values.shape[1:], device=values.device, dtype=values.dtype)\n-        # TD [2022-03-04] For some reason torch.scatter is a bit faster than indexing.\n-        output[indices] = values\n-        # output.scatter_(0, repeat(indices, 'z -> z d', d=values.shape[1]), values)\n-        return output\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        (indices,) = ctx.saved_tensors\n-        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n-        grad_values = grad_output[indices]\n-        # grad_values = torch.gather(grad_output, 0, repeat(indices, 'z -> z d', d=grad_output.shape[1]))\n-        return grad_values, None, None\n-\n-\n-index_put_first_axis = IndexPutFirstAxis.apply\n-\n-\n-# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n-def pad_input(hidden_states, indices, batch, seqlen):\n-    \"\"\"\n-    Arguments:\n-        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n-        indices: (total_nnz), the indices that represent the non-masked tokens of the original padded input sequence.\n-        batch: int, batch size for the padded sequence.\n-        seqlen: int, maximum sequence length for the padded sequence.\n-    Return:\n-        hidden_states: (batch, seqlen, ...)\n-    \"\"\"\n-    # dim = hidden_states.shape[-1]\n-    # output = torch.zeros((batch * seqlen), dim, device=hidden_states.device, dtype=hidden_states.dtype)\n-    # output[indices] = hidden_states\n-    output = index_put_first_axis(hidden_states, indices, batch * seqlen)\n-    return rearrange(output, \"(b s) ... -> b s ...\", b=batch)\n-\n-\n-# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n-def unpad_input(hidden_states, attention_mask, unused_mask=None):\n-    \"\"\"\n-    Arguments:\n-        hidden_states: (batch, seqlen, ...)\n-        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n-        unused_mask: (batch, seqlen), bool / int, 1 means the element is allocated but unused.\n-    Return:\n-        hidden_states: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask + unused_mask.\n-        indices: (total_nnz), the indices of masked tokens from the flattened input sequence.\n-        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n-        max_seqlen_in_batch: int\n-        seqused: (batch), returns the number of tokens selected in attention_mask + unused_mask.\n-    \"\"\"\n-    all_masks = (attention_mask + unused_mask) if unused_mask is not None else attention_mask\n-    seqlens_in_batch = all_masks.sum(dim=-1, dtype=torch.int32)\n-    used_seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n-    indices = torch.nonzero(all_masks.flatten(), as_tuple=False).flatten()\n-    max_seqlen_in_batch = seqlens_in_batch.max().item()\n-    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n-    # TD [2022-03-04] We don't want to index with a bool mask, because Pytorch will expand the\n-    # bool mask, then call nonzero to get the indices, then index with those. The indices is @dim\n-    # times larger than it needs to be, wasting memory. It's faster and more memory-efficient to\n-    # index with integer indices. Moreover, torch's index is a bit slower than it needs to be,\n-    # so we write custom forward and backward to make it a bit faster.\n-    return (\n-        index_first_axis(rearrange(hidden_states, \"b s ... -> (b s) ...\"), indices),\n-        indices,\n-        cu_seqlens,\n-        max_seqlen_in_batch,\n-        used_seqlens_in_batch,\n-    )\n-\n-\n def npu_flash_attn_func(\n     q,\n     k,\n@@ -179,11 +64,11 @@ def npu_flash_attn_func(\n \n     if not causal:\n         head_num = q.shape[2]\n-        output = torch_npu.npu_fusion_attention(q, k, v, head_num, \"BSND\", keep_prob=keep_prob, scale=softmax_scale)[0]\n+        output = npu_fusion_attention(q, k, v, head_num, \"BSND\", keep_prob=keep_prob, scale=softmax_scale)[0]\n     else:\n         attn_mask_npu = get_attn_mask_npu(q.device)\n         head_num = q.shape[2]\n-        output = torch_npu.npu_fusion_attention(\n+        output = npu_fusion_attention(\n             q,\n             k,\n             v,\n@@ -218,7 +103,7 @@ def npu_flash_attn_varlen_func(\n \n     if not causal:\n         head_num = q.shape[1]\n-        output = torch_npu.npu_fusion_attention(\n+        output = npu_fusion_attention(\n             q,\n             k,\n             v,\n@@ -234,7 +119,7 @@ def npu_flash_attn_varlen_func(\n     else:\n         attn_mask_npu = get_attn_mask_npu(q.device)\n         head_num = q.shape[1]\n-        output = torch_npu.npu_fusion_attention(\n+        output = npu_fusion_attention(\n             q,\n             k,\n             v,\n@@ -267,8 +152,3 @@ def npu_apply_rotary_emb(x, cos, sin, **kwargs):\n         sin = sin.unsqueeze(0).unsqueeze(2)\n \n     return npu_rotary_mul(x, cos, sin)\n-\n-\n-def get_npu_flash_attn_funcs():\n-    # return flash attention related functions used for Ascend NPU in order\n-    return npu_flash_attn_func, npu_flash_attn_varlen_func, pad_input, unpad_input, False"
        },
        {
            "sha": "0d89060768298bc964b2dff8b3ddae3b38169e38",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 352,
            "deletions": 167,
            "changes": 519,
            "blob_url": "https://github.com/huggingface/transformers/blob/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=9977cf17392fe5b821dc026ab76d3f5ed16e03f2",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+# Copyright 2025 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -14,17 +14,15 @@\n import inspect\n import os\n import warnings\n+from functools import partial\n from typing import Optional, TypedDict\n \n import torch\n import torch.nn.functional as F\n \n-from transformers.utils.import_utils import is_kernels_available\n-\n from .utils import (\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n-    is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n     is_torch_npu_available,\n     logging,\n@@ -34,18 +32,135 @@\n logger = logging.get_logger(__name__)\n \n \n-def _index_first_axis(tensor: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n-    reshaped = tensor.contiguous().reshape(-1, *tensor.shape[2:])\n-    return reshaped[indices]\n+# TODO Deprecate when all models have the attention interface\n+def flash_attn_supports_top_left_mask():\n+    if is_flash_attn_3_available():\n+        return False\n+    if is_flash_attn_2_available():\n+        return not is_flash_attn_greater_or_equal_2_10()\n+\n+    from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n+\n+    return is_npu_fa2_top_left_aligned_causal_mask()\n+\n+\n+# TODO Deprecate when all models have the attention interface\n+def is_flash_attn_available():\n+    return is_flash_attn_3_available() or is_flash_attn_2_available() or is_torch_npu_available()\n+\n+\n+# `globals()` is not compatible with dynamo, hence we have do define them in global scope ourselves\n+_flash_fn = None\n+_flash_varlen_fn = None\n+_pad_fn = None\n+_unpad_fn = None\n+\n+# function that processes kwargs, generalized to handle any supported kwarg within the function\n+_process_flash_kwargs_fn = None\n+# exceptions where hf API doesn't match the original flash attention API\n+_hf_api_to_flash_mapping = {\n+    \"dropout\": \"dropout_p\",\n+    \"sliding_window\": \"window_size\",\n+}\n+\n+\n+def _lazy_imports(implementation: Optional[str]):\n+    \"\"\"\n+    Lazy loads the respective flash attention implementations.\n+\n+    Return:\n+        flash_attn_func: The base flash attention function.\n+        flash_attn_varlen_func: The flash attention function supporting variable sequence lengths,\n+                                e.g. for padding-free training.\n+        pad_input: The function to pad inputs into one sequence and returning the respective kwargs.\n+        unpad_input: The function to unpad outputs based on the kwargs (from pad_input).\n+    \"\"\"\n+    is_fa2 = is_flash_attn_2_available()\n+    is_fa3 = is_flash_attn_3_available()\n+    if implementation == \"flash_attention_2\" or (implementation is None and is_fa2 and not is_fa3):\n+        from flash_attn import flash_attn_func, flash_attn_varlen_func\n+        from flash_attn.bert_padding import pad_input, unpad_input\n+    else:\n+        pad_input, unpad_input = _pad_input, _unpad_input\n+        if implementation == \"flash_attention_3\" or (implementation is None and is_fa3):\n+            from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n+        elif is_torch_npu_available():\n+            from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_func\n+            from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_varlen_func\n+        # Kernels fallback\n+        else:\n+            flash_attn_func = getattr(implementation, \"flash_attn_func\", None)\n+            flash_attn_varlen_func = getattr(implementation, \"flash_attn_varlen_func\", None)\n+            if flash_attn_varlen_func is None or flash_attn_func is None:\n+                raise ValueError(\n+                    f\"Could not find the currently requested flash attention implementation at `{implementation}`.\"\n+                    f\"Make sure that you request a valid kernel from the hub, e.g. `kernels-community/flash-attn`.\"\n+                )\n+\n+    return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input\n+\n+\n+def _lazy_define_process_function(flash_function):\n+    \"\"\"\n+    Depending on the version and kernel some features are not supported. Due to limitations in\n+    `torch.compile`, we opt to statically type which (optional) kwarg parameters are supported\n+    within `_process_flash_attention_kwargs`.\n+\n+    NOTE: While all supported kwargs are marked as `True`, everything else is marked as `False`.\n+          This might be confusing for kwargs that we use in any case, e.g. `is_causal`.\n+    \"\"\"\n+    global _process_flash_kwargs_fn, _hf_api_to_flash_mapping\n+\n+    flash_parameters = inspect.signature(flash_function).parameters\n+    process_parameters = inspect.signature(_process_flash_attention_kwargs).parameters\n+\n+    supports_mapping = {}\n+    for param in process_parameters:\n+        fa_param = _hf_api_to_flash_mapping.get(param, param)\n+        supports_mapping[fa_param] = fa_param in flash_parameters\n+\n+    return partial(_process_flash_attention_kwargs, supports_mapping=supports_mapping)\n+\n+\n+def lazy_import_flash_attention(implementation: Optional[str]):\n+    \"\"\"\n+    Lazy loading flash attention and returning the respective functions + flags back\n+\n+    NOTE: For fullgraph, this needs to be called before compile while no fullgraph can\n+          can work without preloading. See `_check_and_adjust_attn_implementation` in `modeling_utils`.\n+    \"\"\"\n+    global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn\n+    if any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):\n+        _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn = _lazy_imports(implementation)\n+\n+    global _process_flash_kwargs_fn\n+    if _process_flash_kwargs_fn is None:\n+        _process_flash_kwargs_fn = _lazy_define_process_function(_flash_varlen_fn)\n \n+    return (_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn), _process_flash_kwargs_fn\n \n-def _fa3_unpad_input(hidden_states, attention_mask, unused_mask=None):\n+\n+def _index_first_axis(tensor, indices):\n+    \"\"\"\n+    A local implementation of the PyTorch indexing operation `tensor[indices]` on the first axis,\n+    after flattening the first two dimensions of the tensor. This is functionally equivalent to\n+    FA2's `index_first_axis` and replaces the need to import it.\n     \"\"\"\n-    FA3-compatible unpad_input function.\n+    # The input tensor is expected to be of shape (batch, seq_len, ...). We flatten the first\n+    # two dimensions to get (total_tokens, ...) before indexing.\n+    reshaped_tensor = tensor.reshape(-1, *tensor.shape[2:])\n+    return reshaped_tensor[indices]\n+\n+\n+def _unpad_input(hidden_states, attention_mask, unused_mask=None):\n+    \"\"\"\n+    unpad_input function for flash attention variants that do not have them within their pkg themselves, e.g. fa3.\n+\n     Arguments:\n         hidden_states: (batch, seqlen, ...)\n         attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n         unused_mask: (batch, seqlen), bool / int, 1 means the element is allocated but unused.\n+\n     Return:\n         hidden_states: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask + unused_mask.\n         indices: (total_nnz), the indices of masked tokens from the flattened input sequence.\n@@ -69,14 +184,16 @@ def _fa3_unpad_input(hidden_states, attention_mask, unused_mask=None):\n     )\n \n \n-def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n+def _pad_input(hidden_states, indices, batch, seqlen):\n     \"\"\"\n-    FA3-compatible pad_input function.\n+    pad_input function for flash attention variants that do not have them within their pkg themselves, e.g. fa3.\n+\n     Arguments:\n         hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n         indices: (total_nnz), the indices that represent the non-masked tokens of the original padded input sequence.\n         batch: int, batch size for the padded sequence.\n         seqlen: int, maximum sequence length for the padded sequence.\n+\n     Return:\n         hidden_states: (batch, seqlen, ...)\n     \"\"\"\n@@ -89,9 +206,11 @@ def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n def _get_unpad_data(attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, int]:\n     \"\"\"\n     Retrieves indexing data required to repad unpadded (ragged) tensors.\n+\n     Arguments:\n         attention_mask (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n+\n     Return:\n         indices (`torch.Tensor`):\n             The indices of non-masked tokens from the flattened input sequence.\n@@ -125,6 +244,7 @@ def _upad_input(\n     Unpads query, key, and values tensors, using a single dimension for all tokens even though they belong to different batches.\n     This function is used instead of `flash_attn.bert_padding.unpad_input` in order to avoid the recomputation of the same intermediary\n     tensors for query, key, value tensors.\n+\n     Arguments:\n         query_layer (`torch.Tensor`):\n             Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n@@ -138,6 +258,7 @@ def _upad_input(\n             Target length.\n         unpad_input_func:\n             The function to use for unpadding the input tensors.\n+\n     Return:\n         query_layer (`torch.Tensor`):\n             Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n@@ -193,13 +314,15 @@ def _upad_input(\n def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool = True):\n     \"\"\"\n     This function returns all the necessary kwargs to call `flash_attn_varlen_func`\n-    extracted from position_ids.The `position_ids` can be either packed sequence or\n-    the usual padded position ids, for example in inference time..\n+    extracted from position_ids. The `position_ids` can be either packed sequence or\n+    the usual padded position ids, for example in inference time.\n+\n     Arguments:\n         position_ids (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n         is_packed_sequence (`bool`, *optional*, defaults to `True`):\n             Whether the input position ids are a packed sequence or not.\n+\n     Return:\n         (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n             The cumulative sequence lengths for the target (query) and source (key, value), used to index into\n@@ -212,19 +335,21 @@ def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool =\n     # In that case the position ids will not always start with `0` and we need a better way to infer\n     # cumulative seq lengths.\n     if not is_packed_sequence:\n-        tensor_kws = {\"dtype\": torch.int32, \"device\": position_ids.device}\n-        last_position_ids = position_ids[:, -1]\n+        tensor_kwargs = {\"dtype\": torch.int32, \"device\": position_ids.device}\n \n+        last_position_ids = position_ids[:, -1]\n+        q_len = (\n+            torch.ones(position_ids.size(0), **tensor_kwargs)\n+            if position_ids.shape[-1] == 1\n+            else last_position_ids.add(1)\n+        )\n+        cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kwargs), q_len.cumsum(0).to(torch.int32)], 0)\n         cu_seq_lens_k = torch.cat(\n-            [torch.zeros(1, **tensor_kws), last_position_ids.cumsum(0).add(1).to(torch.int32)], 0\n+            [torch.zeros(1, **tensor_kwargs), last_position_ids.add(1).cumsum(0).to(torch.int32)], 0\n         )\n-        max_length_k = int(last_position_ids.max()) + 1\n \n-        q_len = (\n-            torch.ones(position_ids.size(0), **tensor_kws) if position_ids.shape[-1] == 1 else last_position_ids.add(1)\n-        )\n-        cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kws), q_len.cumsum(0).to(torch.int32)], 0)\n         max_length_q = int(q_len.max())\n+        max_length_k = int(last_position_ids.max()) + 1\n     else:\n         position_ids = position_ids.flatten()\n         indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n@@ -237,16 +362,18 @@ def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool =\n         )\n         cu_seq_lens_k = cu_seq_lens_q\n \n+        # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n+        # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n+        # for some models (e.g. qwen2-vl).\n+        max_length_q = cu_seq_lens_q.diff().max()\n         # NOTE: With torch compile, this will cause a graph break if you don't set\n         # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n         # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n         # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n         # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n-        # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n-        # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n-        # for some models (e.g. qwen2-vl).\n-        max_length_q = cu_seq_lens_q.diff().max().item()\n+        max_length_q = max_length_q.item()\n         max_length_k = max_length_q\n+\n     return (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k)\n \n \n@@ -256,6 +383,7 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n     All three query, key, value states will be flattened.\n     Cumulative lengths of each examples in the batch will be extracted from position_ids.\n     NOTE: ideally cumulative lengths should be prepared at the data collator stage\n+\n     Arguments:\n         query (`torch.Tensor`):\n             Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n@@ -267,6 +395,7 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n         query_length (`int`):\n             Sequence length of the input queries.\n+\n     Return:\n         query (`torch.Tensor`):\n             Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n@@ -275,121 +404,156 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n         value (`torch.Tensor`):\n             Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n         (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n-            The cumulative sequence lengths for the target (query) and source (key, value), used to index into\n-            ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n+            The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n         (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n-            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query,\n-            `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n+            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n     kv_length = key.shape[1]\n+    is_packed_sequence = query_length == kv_length\n+\n     query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n     key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n     value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n-    is_packed_sequence = query_length == kv_length\n \n-    cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k = prepare_fa_kwargs_from_position_ids(\n+    (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = prepare_fa_kwargs_from_position_ids(\n         position_ids, is_packed_sequence=is_packed_sequence\n     )\n+\n     return (query, key, value, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k))\n \n \n def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n     warnings.warn(\n-        \"prepare_fa2_from_position_ids is deprecated, use _prepare_from_posids\",\n+        \"The function `_prepare_flash_attention_from_position_ids` in `transformers.modeling_flash_attention_utils` is deprecated and will be removed in a future version. Please use `_prepare_from_posids` instead.\",\n         FutureWarning,\n     )\n     return _prepare_from_posids(query, key, value, position_ids)\n \n \n-def fa_peft_integration_check(q, k, v, target_dtype: Optional[torch.dtype] = None):\n+def _is_packed_sequence(position_ids, batch_size):\n+    \"\"\"\n+    Check the position ids whether packed sequences are indicated or not\n+        1. Position ids exist\n+        2. Flattened sequences only are supported\n+        3. Compile-friendly `not (torch.diff(position_ids, dim=-1) >= 0).all()`, i.e. we have multiple increasing sequences\n+    \"\"\"\n+    if position_ids is None:\n+        return False\n+\n+    increasing_position_sequences = (\n+        torch.arange(position_ids.shape[1], device=position_ids.device) + position_ids.min()\n+    )\n+    return batch_size == 1 and (increasing_position_sequences - position_ids).abs().sum().bool()\n+\n+\n+def fa_peft_integration_check(\n+    q: torch.Tensor,\n+    k: torch.Tensor,\n+    v: torch.Tensor,\n+    target_dtype: Optional[torch.dtype] = None,\n+):\n+    \"\"\"\n+    PEFT usually casts the layer norms in float32 for training stability reasons\n+    therefore the input hidden states gets silently casted in float32. Hence, we need\n+    cast them back in float16 / bfloat16 just to be sure everything works as expected.\n+    This might slowdown training & inference so it is recommended to not cast the LayerNorms!\n+    \"\"\"\n     if target_dtype and q.dtype == torch.float32:\n         logger.warning_once(f\"Casting fp32 inputs back to {target_dtype} for flash-attn compatibility.\")\n         q, k, v = q.to(target_dtype), k.to(target_dtype), v.to(target_dtype)\n     return q, k, v\n \n \n-def _lazy_imports(impl: Optional[str]):\n-    # returns funcs and pad/unpad based on impl\n-    is_fa2 = is_flash_attn_2_available()\n-    is_fa3 = is_flash_attn_3_available()\n-    if impl == \"flash_attention_2\" or (impl is None and is_fa2 and not is_fa3):\n-        try:\n-            from flash_attn import flash_attn_func, flash_attn_varlen_func\n-            from flash_attn.bert_padding import pad_input, unpad_input\n-\n-            return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input, False\n-\n-        except ImportError as e:\n-            if not globals().get(\"use_remote_fa2\", None):\n-                use_remote_fa2 = (\n-                    input(\n-                        \"Unable to import the official flash attention, do you want to try to use `kernels-community/flash-attn` (trust remote code) Yes or No? \"\n-                    )\n-                    .strip()\n-                    .lower()\n-                )\n-                globals()[\"use_remote_fa2\"] = use_remote_fa2 in {\"yes\", \"y\", \"1\"}\n-            if globals()[\"use_remote_fa2\"]:\n-                if not is_kernels_available():\n-                    raise ImportError(\"You need to install kernels: `pip install kernels`\")\n-                from kernels import get_kernel\n-\n-                impl = get_kernel(\"kernels-community/flash-attn\")\n-                pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input\n-                return (\n-                    getattr(impl, \"flash_attn_func\", None),\n-                    getattr(impl, \"flash_attn_varlen_func\"),\n-                    pad_input,\n-                    unpad_input,\n-                    True,\n-                )\n-\n-            else:\n-                raise ImportError(\n-                    \"Failed to import flash attention 2, please install it or use another implementation.\"\n-                ) from e\n-    elif is_torch_npu_available():\n-        # get flash attention related functions from `.integrations.npu_flash_attention` module for Ascend NPU\n-        from .integrations.npu_flash_attention import get_npu_flash_attn_funcs\n-\n-        return get_npu_flash_attn_funcs()\n-    elif impl == \"flash_attention_3\" or (impl is None and is_fa3):\n-        from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n-\n-        pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input\n-        return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input, True\n-    else:\n-        pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input\n-        return (\n-            getattr(impl, \"flash_attn_func\", None),\n-            getattr(impl, \"flash_attn_varlen_func\"),\n-            pad_input,\n-            unpad_input,\n-            True,\n-        )\n+class FlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for Flash Attention with Compile.\n+\n+    Attributes:\n+        cumulative_seqlens_q (`torch.LongTensor`, *optional*)\n+            Gets cumulative sequence length for query state.\n+        cumulative_seqlens_k (`torch.LongTensor`, *optional*)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`, *optional*):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`, *optional*):\n+            Maximum sequence length for key state.\n+    \"\"\"\n \n+    cumulative_seqlens_q: Optional[torch.LongTensor]\n+    cumulative_seqlens_k: Optional[torch.LongTensor]\n+    max_length_q: Optional[int]\n+    max_length_k: Optional[int]\n \n-_flash_supports_window = None\n \n+def _process_flash_attention_kwargs(\n+    query_length: int,\n+    key_length: int,\n+    is_causal: bool,\n+    dropout: float = 0.0,\n+    softmax_scale: Optional[float] = None,\n+    sliding_window: Optional[int] = None,\n+    use_top_left_mask: bool = False,\n+    softcap: Optional[float] = None,\n+    deterministic: Optional[bool] = None,\n+    s_aux: Optional[torch.Tensor] = None,\n+    supports_mapping: Optional[dict[str, bool]] = None,\n+    **kwargs,\n+):\n+    \"\"\"\n+    Returns a set of kwargs that are passed down to the according flash attention function based on\n+    requested features and whether it is supported - depends on the version and kernel implementation\n+    which is dynamically configued at `lazy_import_flash_attention`. The (un)supported features can be\n+    inspected in `supports_mapping`, see `_lazy_define_process_function` for more details.\n \n-def is_flash_attn_available():\n-    return is_flash_attn_3_available() or is_flash_attn_2_available() or is_torch_npu_available()\n+    Args:\n+        query_length (`int`):\n+            Length of the query states\n+        key_length (`int`):\n+            Length of the key states\n+        is_causal (`bool`):\n+            Whether we perform causal (decoder) attention or full attention.\n+        dropout (`float`):\n+            Attention dropout.\n+        softmax_scale (`float`, *optional*):\n+            The scaling of QK^T before applying softmax. Default to `1 / sqrt(head_dim)`.\n+        sliding_window (`int`, *optional*):\n+            The size of the sliding window, i.e. we look at a max of `sliding_window` tokens back.\n+        use_top_left_mask (`bool`):\n+            Deprecated behavior of older versions of flash attention requiring different masking.\n+        softcap (`float`, *optional*):\n+            Softcap for the attention logits, used e.g. in gemma2.\n+        deterministic (`bool`, *optional*):\n+            Determines if the deterministic option introduced in flash_attn>=2.4.1 is enabled.\n+        s_aux (`torch.Tensor`, *optional*):\n+            Attention sink auxiliary that adds a `bias` to the attention calculation via an additional head.\n+    Return:\n+        flash_kwargs (`dict`):\n+            A dict of kwargs that are requested and supported.\n+    \"\"\"\n+    flash_kwargs = {\n+        \"causal\": is_causal and not (use_top_left_mask and query_length == 1),\n+        \"softmax_scale\": softmax_scale,\n+    }\n \n+    if supports_mapping[\"dropout_p\"]:\n+        flash_kwargs[\"dropout_p\"] = dropout\n \n-def flash_attn_supports_top_left_mask():\n-    if is_flash_attn_3_available():\n-        return False\n-    if is_flash_attn_2_available():\n-        return not is_flash_attn_greater_or_equal_2_10()\n+    if supports_mapping[\"window_size\"] and sliding_window is not None and key_length > sliding_window:\n+        flash_kwargs[\"window_size\"] = (sliding_window, sliding_window)\n \n-    from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n+    if supports_mapping[\"deterministic\"]:\n+        flash_kwargs[\"deterministic\"] = (\n+            deterministic if deterministic is not None else os.getenv(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n+        )\n \n-    return is_npu_fa2_top_left_aligned_causal_mask()\n+    if supports_mapping[\"softcap\"] and softcap is not None:\n+        flash_kwargs[\"softcap\"] = softcap\n \n+    # Only within kernel implementation atm\n+    if supports_mapping[\"s_aux\"] and s_aux is not None:\n+        flash_kwargs[\"s_aux\"] = s_aux\n \n-class FlashAttentionKwargs(TypedDict, total=False):\n-    cumulative_seqlens_q: Optional[torch.LongTensor]\n-    cumulative_seqlens_k: Optional[torch.LongTensor]\n+    return flash_kwargs\n \n \n def _flash_attention_forward(\n@@ -414,100 +578,121 @@ def _flash_attention_forward(\n     implementation: Optional[str] = None,\n     **kwargs,\n ):\n-    if not all(k in globals() for k in (\"_flash_fn\", \"_flash_varlen_fn\", \"_pad_fn\", \"_unpad_fn\", \"_is_fa3\")):\n-        flash_fn, flash_varlen_fn, pad_fn, unpad_fn, is_fa3 = _lazy_imports(implementation)\n-        globals()[\"_flash_fn\"] = flash_fn\n-        globals()[\"_flash_varlen_fn\"] = flash_varlen_fn\n-        globals()[\"_pad_fn\"] = pad_fn\n-        globals()[\"_unpad_fn\"] = unpad_fn\n-        globals()[\"_is_fa3\"] = is_fa3\n-        flash_supports_window = \"window_size\" in inspect.signature(flash_varlen_fn).parameters\n-        globals()[\"_flash_supports_window\"] = flash_supports_window\n-    else:\n-        flash_fn = globals()[\"_flash_fn\"]\n-        flash_varlen_fn = globals()[\"_flash_varlen_fn\"]\n-        pad_fn = globals()[\"_pad_fn\"]\n-        unpad_fn = globals()[\"_unpad_fn\"]\n-        is_fa3 = globals()[\"_is_fa3\"]\n-        flash_supports_window = globals()[\"_flash_supports_window\"]\n-\n-    causal = is_causal and not (use_top_left_mask and query_length == 1)\n-    use_sw = (\n-        (_flash_supports_window or flash_supports_window) and sliding_window and key_states.shape[1] > sliding_window\n+    \"\"\"\n+    Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n+    first unpad the input, then computes the attention scores and pad the final attention scores.\n+\n+    (Optional) kwargs are described further in `_process_flash_attention_kwargs` and `FlashAttentionKwargs`.\n+\n+    Args:\n+        query_states (`torch.Tensor`):\n+            Input query states to be passed to Flash Attention API\n+        key_states (`torch.Tensor`):\n+            Input key states to be passed to Flash Attention API\n+        value_states (`torch.Tensor`):\n+            Input value states to be passed to Flash Attention API\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n+            position of padding tokens and 1 for the position of non-padding tokens.\n+        implementation (`str`, *optional*):\n+            The attention implementation to use. If None, will default to the one based on the environment.\n+    \"\"\"\n+    (flash_fn, flash_varlen_fn, pad_fn, unpad_fn), process_flash_kwargs_fn = lazy_import_flash_attention(\n+        implementation\n     )\n-    flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sw else {}\n-    if not is_fa3:\n-        flash_kwargs[\"dropout_p\"] = dropout\n-    if is_flash_attn_greater_or_equal(\"2.4.1\"):\n-        det = deterministic if deterministic is not None else os.getenv(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n-        flash_kwargs[\"deterministic\"] = det\n-    if softcap is not None:\n-        flash_kwargs[\"softcap\"] = softcap\n-    if \"s_aux\" in kwargs:\n-        flash_kwargs[\"s_aux\"] = kwargs.get(\"s_aux\")\n+\n+    # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op\n     query_states, key_states, value_states = fa_peft_integration_check(\n         query_states, key_states, value_states, target_dtype\n     )\n-    use_mask = position_ids is not None or all(\n-        k is not None for k in [cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k]\n+\n+    # Extract the flash attention kwargs that have been requested (and are supported by the implementation)\n+    flash_kwargs = process_flash_kwargs_fn(\n+        query_length=query_length,\n+        key_length=key_states.size(1),\n+        is_causal=is_causal,\n+        dropout=dropout,\n+        softmax_scale=softmax_scale,\n+        sliding_window=sliding_window,\n+        use_top_left_mask=use_top_left_mask,\n+        softcap=softcap,\n+        deterministic=deterministic,\n+        **kwargs,\n+    )\n+\n+    # We will use `flash_varlen_fn` to prevent cross-example attention and also allow padding free approach under two cases:\n+    # Case 1. If position ids is provided and the position ids indicate packed sequences, see `_is_packed_sequence`.\n+    # Case 2. Some models pass directly pre-computed `cu_seqlens` so we don't need to infer it from position ids. It is safe to\n+    # use `flash_varlen_fn` knowing we already have all necessary the kwargs.\n+    #\n+    # NOTE: it is user's responsibility to take care of flattenning `position_ids` if that's needed by the model.\n+    # See #39121 for more information.\n+    is_fa_with_position_ids = _is_packed_sequence(position_ids, batch_size=query_states.size(0))\n+    is_fa_with_varlen_kwargs = all(\n+        kwarg is not None for kwarg in (cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k)\n     )\n+\n+    # Contains at least one padding token in the sequence\n     if attention_mask is not None:\n-        q, k, v, idx, (cu_q, cu_k), (mq, mk) = _upad_input(\n+        q, k, v, indices_q, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _upad_input(\n             query_states, key_states, value_states, attention_mask, query_length, unpad_fn\n         )\n-        # TODO for now this is required to work with https://huggingface.co/kernels-community/metal-flash-sdpa/blob/main/torch-ext/metal_flash_sdpa/__init__.p\n+\n+        # TODO for now this is required to work with\n+        # https://huggingface.co/kernels-community/metal-flash-sdpa/blob/main/torch-ext/metal_flash_sdpa/__init__.py\n         if \"mps\" in str(q.device):\n-            cu_k = cu_k.clone()\n+            cu_seq_lens_k = cu_seq_lens_k.clone()\n+\n         out_unpad = flash_varlen_fn(\n             q,\n             k,\n             v,\n-            cu_seqlens_q=cu_q.to(torch.int32),\n-            cu_seqlens_k=cu_k.to(torch.int32),\n-            max_seqlen_q=mq,\n-            max_seqlen_k=mk,\n-            softmax_scale=softmax_scale,\n-            causal=causal,\n+            cu_seqlens_q=cu_seq_lens_q,\n+            cu_seqlens_k=cu_seq_lens_k,\n+            max_seqlen_q=max_length_q,\n+            max_seqlen_k=max_length_k,\n             **flash_kwargs,\n         )\n         if isinstance(out_unpad, tuple):\n             out_unpad = out_unpad[0]\n-        out = pad_fn(out_unpad, idx, query_states.shape[0], query_length)\n-    elif use_mask:\n+\n+        out = pad_fn(out_unpad, indices_q, query_states.size(0), query_length)\n+\n+    # Padding free, i.e. sequences flattened into one total sequence\n+    elif is_fa_with_varlen_kwargs or is_fa_with_position_ids:\n         if cu_seq_lens_q is None or cu_seq_lens_k is None:\n-            if position_ids is None:\n-                raise ValueError(\n-                    \"Position ids should be passed if the attention mask is not passed and the cu_seq-lens are not passed.\"\n-                )\n-            q, k, v, (cu_q, cu_k), (mq, mk) = _prepare_from_posids(\n+            q, k, v, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _prepare_from_posids(\n                 query_states, key_states, value_states, position_ids, query_length=query_length\n             )\n         else:\n             q = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n             k = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n             v = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n-            mq, mk = max_length_q, max_length_k\n-            cu_q, cu_k = cu_seq_lens_q, cu_seq_lens_k\n+\n+        # TODO for now this is required to work with\n+        # https://huggingface.co/kernels-community/metal-flash-sdpa/blob/main/torch-ext/metal_flash_sdpa/__init__.py\n         if \"mps\" in str(q.device):\n-            cu_k = cu_k.clone()\n+            cu_seq_lens_k = cu_seq_lens_k.clone()\n+\n         out = flash_varlen_fn(\n             q,\n             k,\n             v,\n-            cu_seqlens_q=cu_q.to(torch.int32),\n-            cu_seqlens_k=cu_k.to(torch.int32),\n-            max_seqlen_q=mq,\n-            max_seqlen_k=mk,\n-            softmax_scale=softmax_scale,\n-            causal=causal,\n+            cu_seqlens_q=cu_seq_lens_q,\n+            cu_seqlens_k=cu_seq_lens_k,\n+            max_seqlen_q=max_length_q,\n+            max_seqlen_k=max_length_k,\n             **flash_kwargs,\n         )\n         if isinstance(out, tuple):\n             out = out[0]\n-        out = out.view(query_states.shape[0], -1, out.size(-2), out.size(-1))\n+\n+        out = out.view(query_states.size(0), -1, out.size(-2), out.size(-1))\n+\n+    # No padding\n     else:\n-        out = flash_fn(\n-            query_states, key_states, value_states, softmax_scale=softmax_scale, causal=causal, **flash_kwargs\n-        )\n+        out = flash_fn(query_states, key_states, value_states, **flash_kwargs)\n+        if isinstance(out, tuple):\n+            out = out[0]\n \n-    return out[0] if isinstance(out, tuple) else out\n+    return out"
        },
        {
            "sha": "b8a7d6a44024f4ef15e78cfe3ca1cd4513815a8c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9977cf17392fe5b821dc026ab76d3f5ed16e03f2",
            "patch": "@@ -74,6 +74,7 @@\n )\n from .loss.loss_utils import LOSS_MAPPING\n from .masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+from .modeling_flash_attention_utils import lazy_import_flash_attention\n from .pytorch_utils import (  # noqa: F401\n     Conv1D,\n     apply_chunking_to_forward,\n@@ -2126,7 +2127,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     _pp_plan = None\n \n     # This flag signal that the model can be used as an efficient backend in TGI and vLLM\n-    # In practice, it means that they support attention interface functions, fully pass the kwargs\n+    # In practice, it means that they support attention (mask) interface functions, fully pass the kwargs\n     # through all modules up to the Attention layer, can slice logits with Tensor, and have a default TP plan\n     _supports_attention_backend = False\n     _can_record_outputs = None\n@@ -2748,6 +2749,7 @@ def _check_and_adjust_attn_implementation(\n                     if attention_wrapper is None:\n                         attention_wrapper = flash_attention_forward\n                     kernel_function = partial(attention_wrapper, implementation=kernel)\n+                    lazy_import_flash_attention(kernel)\n                 elif kernel_name is not None:\n                     kernel_function = getattr(kernel, kernel_name)\n                 ALL_ATTENTION_FUNCTIONS.register(attn_implementation, kernel_function)\n@@ -2763,7 +2765,13 @@ def _check_and_adjust_attn_implementation(\n                 attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n             return attn_implementation\n         else:\n-            return self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)\n+            attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)\n+\n+            # preload flash attention here to allow compile with fullgraph\n+            if applicable_attn_implementation.startswith(\"flash_attention\"):\n+                lazy_import_flash_attention(applicable_attn_implementation)\n+\n+            return attn_implementation\n \n     def get_correct_attn_implementation(self, _requested_attention: str, is_init_check: bool = False) -> str:\n         requested_attention = \"sdpa\" if _requested_attention is None else _requested_attention"
        },
        {
            "sha": "b7ca0e2d9b4240d209cd80ca3803481f4c4d5dce",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 153,
            "deletions": 125,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9977cf17392fe5b821dc026ab76d3f5ed16e03f2/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=9977cf17392fe5b821dc026ab76d3f5ed16e03f2",
            "patch": "@@ -3483,92 +3483,107 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n         for model_class in self.all_model_classes:\n             if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n+            # Custom kernel which needs the mask interface to be properly usable on these models\n+            if not model_class._supports_attention_backend and not attn_implementation.startswith(\"flash_attention\"):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            config.head_dim = 64  # fa2 does not always support arbitrary headim\n-            model = model_class(config)\n-\n-            model.to(torch_device)\n-            model.to(torch.bfloat16)\n-            dummy_input = inputs_dict[model.main_input_name][:1]\n-            if dummy_input.dtype in [torch.float32, torch.float16]:\n-                dummy_input = dummy_input.to(torch.bfloat16)\n-\n-            dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n \n-            if dummy_attention_mask is not None:\n-                dummy_attention_mask = dummy_attention_mask[:1]\n-                if padding_side == \"left\":\n-                    dummy_attention_mask[:, 1:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-                else:\n-                    dummy_attention_mask[:, :-1] = 1\n-                    dummy_attention_mask[:, -1:] = 0\n-            if model.config.is_encoder_decoder:\n-                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n+            # flash attention variants does not always support arbitrary headim\n+            config = self._prepare_config_headdim(config, 16)\n \n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                model.set_attn_implementation(attn_implementation)\n-                outputs_fa = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-            else:\n-                outputs = model(dummy_input, output_hidden_states=True)\n-                model.set_attn_implementation(attn_implementation)\n-                outputs_fa = model(dummy_input, output_hidden_states=True)\n+            # TODO it is unclear why saving and reloading with dtype works while\n+            # casting with `.to(dtype=..., device=...)` does not.\n+            # Discovered on tests with `Bart` models.\n+            model = model_class(config)\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n \n-            model.set_attn_implementation(\"sdpa\")\n-            logits = (\n-                outputs.hidden_states[-1] if not model.config.is_encoder_decoder else outputs.decoder_hidden_states[-1]\n-            )\n-            logits_fa = (\n-                outputs_fa.hidden_states[-1]\n-                if not model.config.is_encoder_decoder\n-                else outputs_fa.decoder_hidden_states[-1]\n-            )\n+                dummy_input = inputs_dict[model.main_input_name][:1]\n+                if dummy_input.dtype in [torch.float32, torch.float16]:\n+                    dummy_input = dummy_input.to(torch.bfloat16)\n \n-            assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n \n-            if model.config.is_encoder_decoder:\n-                other_inputs = {\n-                    \"decoder_input_ids\": decoder_input_ids,\n-                    \"decoder_attention_mask\": dummy_attention_mask,\n-                    \"output_hidden_states\": True,\n-                }\n                 if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n+                    dummy_attention_mask = dummy_attention_mask[:1]\n+                    if padding_side == \"left\":\n+                        dummy_attention_mask[:, 1:] = 1\n+                        dummy_attention_mask[:, :1] = 0\n+                    else:\n+                        dummy_attention_mask[:, :-1] = 1\n+                        dummy_attention_mask[:, -1:] = 0\n+                if model.config.is_encoder_decoder:\n+                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n \n-                outputs = model(dummy_input, **other_inputs)\n-                model.set_attn_implementation(attn_implementation)\n-                outputs_fa = model(dummy_input, **other_inputs)\n-            else:\n-                other_inputs = {\n-                    \"output_hidden_states\": True,\n-                }\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n+                    outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                    model.set_attn_implementation(attn_implementation)\n+                    outputs_fa = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                else:\n+                    outputs = model(dummy_input, output_hidden_states=True)\n+                    model.set_attn_implementation(attn_implementation)\n+                    outputs_fa = model(dummy_input, output_hidden_states=True)\n+\n+                model.set_attn_implementation(\"sdpa\")\n+                logits = (\n+                    outputs.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs.decoder_hidden_states[-1]\n+                )\n+                logits_fa = (\n+                    outputs_fa.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs_fa.decoder_hidden_states[-1]\n+                )\n \n-                outputs = model(dummy_input, **other_inputs)\n-                model.set_attn_implementation(attn_implementation)\n-                outputs_fa = model(dummy_input, **other_inputs)\n+                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n \n-            model.set_attn_implementation(\"sdpa\")\n-            logits = (\n-                outputs.hidden_states[-1] if not model.config.is_encoder_decoder else outputs.decoder_hidden_states[-1]\n-            )\n-            logits_fa = (\n-                outputs_fa.hidden_states[-1]\n-                if not model.config.is_encoder_decoder\n-                else outputs_fa.decoder_hidden_states[-1]\n-            )\n+                if model.config.is_encoder_decoder:\n+                    other_inputs = {\n+                        \"decoder_input_ids\": decoder_input_ids,\n+                        \"decoder_attention_mask\": dummy_attention_mask,\n+                        \"output_hidden_states\": True,\n+                    }\n+                    if dummy_attention_mask is not None:\n+                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n \n-            if padding_side == \"left\":\n-                assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n+                    outputs = model(dummy_input, **other_inputs)\n+                    model.set_attn_implementation(attn_implementation)\n+                    outputs_fa = model(dummy_input, **other_inputs)\n+                else:\n+                    other_inputs = {\n+                        \"output_hidden_states\": True,\n+                    }\n+                    if dummy_attention_mask is not None:\n+                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                    outputs = model(dummy_input, **other_inputs)\n+                    model.set_attn_implementation(attn_implementation)\n+                    outputs_fa = model(dummy_input, **other_inputs)\n+\n+                model.set_attn_implementation(\"sdpa\")\n+                logits = (\n+                    outputs.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs.decoder_hidden_states[-1]\n+                )\n+                logits_fa = (\n+                    outputs_fa.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs_fa.decoder_hidden_states[-1]\n+                )\n \n-                # check with inference + dropout\n-                model.train()\n-                model.set_attn_implementation(attn_implementation)\n-                _ = model(dummy_input, **other_inputs)\n-            else:\n-                assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n+                if padding_side == \"left\":\n+                    assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n+\n+                    # check with inference + dropout\n+                    model.train()\n+                    model.set_attn_implementation(attn_implementation)\n+                    _ = model(dummy_input, **other_inputs)\n+                else:\n+                    assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n     @require_kernels\n     @require_torch_gpu\n@@ -4698,6 +4713,70 @@ def recursively_check(eager_outputs, exported_outputs):\n                 is_tested = recursively_check(eager_outputs, exported_outputs)\n                 self.assertTrue(is_tested, msg=f\"No outputs were compared for {model_class.__name__}\")\n \n+    @staticmethod\n+    def _prepare_config_headdim(config, requested_dim):\n+        \"\"\"\n+        This method allows to update the head dim for all model types including\n+        composite models and models that do not support head dim by themselves.\n+\n+        Why? A lot of kernels including flex attention rely on triton for compilation.\n+        However, triton cannot handle hidden dimensions of less than 16 for example.\n+        (There are many more examples especially now that the `kernels` library is\n+        supported)\n+        \"\"\"\n+\n+        def update_config_headdim(config, requested_dim):\n+            # Flex Attention cannot use dropout\n+            if hasattr(config, \"attention_dropout\"):\n+                config.attention_dropout = 0\n+            if hasattr(config, \"attention_probs_dropout_prob\"):\n+                config.attention_probs_dropout_prob = 0\n+\n+            # Update the head dim and try to update hidden size as well if present in config\n+            # NOTE: some models may have none if the values in sub-config, thus we check for `Noneness`\n+            head_dim = None\n+            if hasattr(config, \"head_dim\") and config.head_dim is not None:\n+                head_dim = config.head_dim\n+                config.head_dim = max(requested_dim, config.head_dim)\n+\n+            cross_head_dim = None\n+            if hasattr(config, \"cross_head_dim\") and config.cross_head_dim is not None:\n+                cross_head_dim = config.cross_head_dim\n+                config.cross_head_dim = max(requested_dim, config.cross_head_dim)\n+\n+            if (\n+                getattr(config, \"hidden_size\", None) is not None\n+                and getattr(config, \"num_attention_heads\", None) is not None\n+            ):\n+                head_dim = head_dim if head_dim is not None else config.hidden_size // config.num_attention_heads\n+                config.hidden_size *= max(requested_dim // head_dim, 1)\n+\n+            if (\n+                getattr(config, \"decoder_hidden_size\", None) is not None\n+                and getattr(config, \"decoder_num_attention_heads\", None) is not None\n+            ):\n+                decoder_head_dim = config.decoder_hidden_size // config.decoder_num_attention_heads\n+                config.decoder_hidden_size *= max(requested_dim // decoder_head_dim, 1)\n+\n+            if (\n+                getattr(config, \"cross_hidden_size\", None) is not None\n+                and getattr(config, \"cross_num_attention_heads\", None) is not None\n+            ):\n+                cross_head_dim = (\n+                    cross_head_dim\n+                    if cross_head_dim is not None\n+                    else config.cross_hidden_size // config.cross_num_attention_heads\n+                )\n+                config.cross_hidden_size *= max(requested_dim // cross_head_dim, 1)\n+\n+        # Update config values\n+        update_config_headdim(config, requested_dim)\n+        for key in config.sub_configs:\n+            sub_config = getattr(config, key)\n+            update_config_headdim(sub_config, requested_dim)\n+\n+        return config\n+\n     @require_torch_gpu\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:\n@@ -4711,59 +4790,8 @@ def test_flex_attention_with_grads(self):\n             ):\n                 self.skipTest(reason=\"At least some parts of this model do not support flex attention\")\n \n-            def update_config_for_flex(config):\n-                # Flex Attention cannot use dropout\n-                if hasattr(config, \"attention_dropout\"):\n-                    config.attention_dropout = 0\n-                if hasattr(config, \"attention_probs_dropout_prob\"):\n-                    config.attention_probs_dropout_prob = 0\n-\n-                # Flex attention relies on triton on compilation\n-                # However, triton cannot handle hidden dimensions of less than 16\n-                # --> forcing at least a hidden dim of 16\n-\n-                # Update the head dim and try to update hidden size as well if present in config\n-                # NOTE: some models may have none if the values in sub-config, thus we check for `Noneness`\n-                head_dim = None\n-                if hasattr(config, \"head_dim\") and config.head_dim is not None:\n-                    head_dim = config.head_dim\n-                    config.head_dim = max(16, config.head_dim)\n-\n-                cross_head_dim = None\n-                if hasattr(config, \"cross_head_dim\") and config.cross_head_dim is not None:\n-                    cross_head_dim = config.cross_head_dim\n-                    config.cross_head_dim = max(16, config.cross_head_dim)\n-\n-                if (\n-                    getattr(config, \"hidden_size\", None) is not None\n-                    and getattr(config, \"num_attention_heads\", None) is not None\n-                ):\n-                    head_dim = head_dim if head_dim is not None else config.hidden_size // config.num_attention_heads\n-                    config.hidden_size *= max(16 // head_dim, 1)\n-\n-                if (\n-                    getattr(config, \"decoder_hidden_size\", None) is not None\n-                    and getattr(config, \"decoder_num_attention_heads\", None) is not None\n-                ):\n-                    decoder_head_dim = config.decoder_hidden_size // config.decoder_num_attention_heads\n-                    config.decoder_hidden_size *= max(16 // decoder_head_dim, 1)\n-\n-                if (\n-                    getattr(config, \"cross_hidden_size\", None) is not None\n-                    and getattr(config, \"cross_num_attention_heads\", None) is not None\n-                ):\n-                    cross_head_dim = (\n-                        cross_head_dim\n-                        if cross_head_dim is not None\n-                        else config.cross_hidden_size // config.cross_num_attention_heads\n-                    )\n-                    config.cross_hidden_size *= max(16 // cross_head_dim, 1)\n-\n             # Set default attention to flex and update config values\n-            update_config_for_flex(config)\n-            for key in config.sub_configs:\n-                sub_config = getattr(config, key)\n-                update_config_for_flex(sub_config)\n+            config = self._prepare_config_headdim(config, 16)  # specific to triton\n \n             if model_class._can_set_attn_implementation():\n                 model = model_class(config).to(device=torch_device)"
        }
    ],
    "stats": {
        "total": 950,
        "additions": 526,
        "deletions": 424
    }
}