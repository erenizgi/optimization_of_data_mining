{
    "author": "sahil-kabir",
    "message": "Model card for NLLB (#40074)\n\n* initializing branch and draft PR\n\n* updated model card .md file\n\n* minor\n\n* minor\n\n* Update docs/source/en/model_doc/nllb.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* resolving comments + adding visuals\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\r\n\r\nsuggestion\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/nllb.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* NllbTokenizerFast and NllbTokenizer added\n\n* endline\n\n* minor\n\n* Update nllb.md\n\n---------\n\nCo-authored-by: Sahil Kabir <sahilkabir@Sahils-MacBook-Pro.local>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "a7eabf1ddebe6140655d42a680def17e8dac53db",
    "files": [
        {
            "sha": "d4ffe509890b9736e1bf46e26e543c43ee3d6ba7",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 100,
            "deletions": 157,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7eabf1ddebe6140655d42a680def17e8dac53db/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7eabf1ddebe6140655d42a680def17e8dac53db/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=a7eabf1ddebe6140655d42a680def17e8dac53db",
            "patch": "@@ -13,136 +13,140 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on 2022-07-11 and added to Hugging Face Transformers on 2022-07-18.*\n-\n-# NLLB\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Updated tokenizer behavior \n+*This model was released on 2022-07-11 and added to Hugging Face Transformers on 2022-07-18.*\n \n-**DISCLAIMER:** The default behaviour for the tokenizer was fixed and thus changed in April 2023.\n-The previous version adds `[self.eos_token_id, self.cur_lang_code]` at the end of the token sequence for both target and source tokenization. This is wrong as the NLLB paper mentions (page 48, 6.1.1. Model Architecture) :\n \n-*Note that we prefix the source sequence with the source language, as opposed to the target\n-language as previously done in several works (Arivazhagan et al., 2019; Johnson et al.,\n-2017). This is primarily because we prioritize optimizing zero-shot performance of our\n-model on any pair of 200 languages at a minor cost to supervised performance.*\n+# NLLB\n \n-Previous behaviour:\n+[NLLB: No Language Left Behind](https://huggingface.co/papers/2207.04672) is a multilingual translation model. It's trained on data using data mining techniques tailored for low-resource languages and supports over 200 languages. NLLB features a conditional compute architecture using a Sparsely Gated Mixture of Experts.\n \n-```python\n->>> from transformers import NllbTokenizer\n \n->>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n->>> tokenizer(\"How was your day?\").input_ids\n-[13374, 1398, 4260, 4039, 248130, 2, 256047]\n+You can find all the original NLLB checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=nllb) organization.\n \n->>> # 2: '</s>'\n->>> # 256047 : 'eng_Latn'\n-```\n-New behaviour\n+> [!TIP]\n+> This model was contributed by [Lysandre](https://huggingface.co/lysandre).  \n+> Click on the NLLB models in the right sidebar for more examples of how to apply NLLB to different translation tasks.\n \n-```python\n->>> from transformers import NllbTokenizer\n+The example below demonstrates how to translate text with [`Pipeline`] or the [`AutoModel`] class.\n \n->>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n->>> tokenizer(\"How was your day?\").input_ids\n-[256047, 13374, 1398, 4260, 4039, 248130, 2]\n- ```\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-Enabling the old behaviour can be done as follows:\n ```python\n->>> from transformers import NllbTokenizer\n+import torch\n+from transformers import pipeline\n \n->>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", legacy_behaviour=True)\n+pipeline = pipeline(task=\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\", torch_dtype=torch.float16, device=0)\n+pipeline(\"UN Chief says there is no military solution in Syria\")\n ```\n \n-For more details, feel free to check the linked [PR](https://github.com/huggingface/transformers/pull/22313) and [Issue](https://github.com/huggingface/transformers/issues/19943).\n-\n-## Overview\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://huggingface.co/papers/2207.04672) by Marta R. Costa-jussà, James Cross, Onur Çelebi,\n-Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\n-Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\n-Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\n-Safiyyah Saleem, Holger Schwenk, and Jeff Wang.\n+```python\n+from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n-The abstract of the paper is the following:\n+tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", attn_implementaiton=\"sdpa\")\n \n-*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.\n-However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the\n-200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by\n-first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed\n-at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of\n-Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training\n-improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using\n-a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.\n-Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*\n+article = \"UN Chief says there is no military solution in Syria\"\n+inputs = tokenizer(article, return_tensors=\"pt\")\n \n-This implementation contains the dense models available on release.\n+translated_tokens = model.generate(\n+    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30\n+)\n+print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n+```\n \n-**The sparse model NLLB-MoE (Mixture of Expert) is now available! More details [here](nllb-moe)**\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n \n-This model was contributed by [Lysandre](https://huggingface.co/lysandre). The authors' code can be found [here](https://github.com/facebookresearch/fairseq/tree/nllb).\n+```bash\n+echo -e \"UN Chief says there is no military solution in Syria\" | transformers run --task \"translation_en_to_fr\" --model facebook/nllb-200-distilled-600M --device 0\n+```\n \n-## Generating with NLLB\n+</hfoption>\n+</hfoptions>\n \n-While generating the target text set the `forced_bos_token_id` to the target language id. The following\n-example shows how to translate English to French using the *facebook/nllb-200-distilled-600M* model.\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-Note that we're using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\n-for the list of all BCP-47 in the Flores 200 dataset.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to 8-bits.\n \n ```python\n->>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n-\n->>> article = \"UN Chief says there is no military solution in Syria\"\n->>> inputs = tokenizer(article, return_tensors=\"pt\")\n-\n->>> translated_tokens = model.generate(\n-...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30\n-... )\n->>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n-Le chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\n+from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n+\n+bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", quantization_config=bnb_config)\n+tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")\n+\n+article = \"UN Chief says there is no military solution in Syria\"\n+inputs = tokenizer(article, return_tensors=\"pt\").to(\"cuda\")\n+translated_tokens = model.generate(\n+    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30,\n+)\n+print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n ```\n \n-### Generating from any other language than English\n-\n-English (`eng_Latn`) is set as the default language from which to translate. In order to specify that you'd like to translate from a different language,\n-you should specify the BCP-47 code in the `src_lang` keyword argument of the tokenizer initialization.\n-\n-See example below for a translation from romanian to german:\n+Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/attention_visualizer.py#L139) to better understand what tokens the model can and cannot attend to.\n \n-```py\n->>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\n-...     \"facebook/nllb-200-distilled-600M\", token=True, src_lang=\"ron_Latn\"\n-... )\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", token=True)\n-\n->>> article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n->>> inputs = tokenizer(article, return_tensors=\"pt\")\n+```python\n+from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n \n->>> translated_tokens = model.generate(\n-...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n-... )\n->>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n-UN-Chef sagt, es gibt keine militärische Lösung in Syrien\n+visualizer = AttentionMaskVisualizer(\"facebook/nllb-200-distilled-600M\")\n+visualizer(\"UN Chief says there is no military solution in Syria\")\n ```\n \n-## Resources\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/NLLB-Attn-Mask.png\"/>\n+</div>\n \n-- [Translation task guide](../tasks/translation)\n-- [Summarization task guide](../tasks/summarization)\n+## Notes\n+\n+- The tokenizer was updated in April 2023 to prefix the source sequence with the source language rather than the target language. This prioritizes zero-shot performance at a minor cost to supervised performance.\n+\n+   ```python\n+   >>> from transformers import NllbTokenizer\n+\n+   >>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n+   >>> tokenizer(\"How was your day?\").input_ids\n+   [256047, 13374, 1398, 4260, 4039, 248130, 2]\n+   ```\n+   \n+   To revert to the legacy behavior, use the code example below.\n+   \n+   ```python\n+   >>> from transformers import NllbTokenizer\n+\n+   >>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", legacy_behaviour=True)\n+   ```\n+   \n+ - For non-English languages, specify the language's [BCP-47](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200) code with the `src_lang` keyword as shown below.\n+ \n+ - See example below for a translation from Romanian to German.\n+    ```python\n+    >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n+\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n+    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n+\n+    >>> article = \"UN Chief says there is no military solution in Syria\"\n+    >>> inputs = tokenizer(article, return_tensors=\"pt\")\n+\n+    >>> translated_tokens = model.generate(\n+    ...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30\n+    ... )\n+    >>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n+    Le chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\n+    ```\n \n ## NllbTokenizer\n \n@@ -152,64 +156,3 @@ UN-Chef sagt, es gibt keine militärische Lösung in Syrien\n ## NllbTokenizerFast\n \n [[autodoc]] NllbTokenizerFast\n-\n-## Using Flash Attention 2\n-\n-Flash Attention 2 is a faster, optimized version of the attention scores computation which relies on `cuda` kernels.\n-\n-### Installation \n-\n-First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n-\n-Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n-\n-```bash\n-pip install -U flash-attn --no-build-isolation\n-```\n-\n-### Usage\n-\n-To load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). You can use either `torch.float16` or `torch.bfloat16` precision.\n-\n-```python\n->>> import torch\n->>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n-\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\").eval()\n->>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n-\n->>> article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n->>> inputs = tokenizer(article, return_tensors=\"pt\").to(\"cuda\")\n-\n->>> translated_tokens = model.generate(\n-...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n-... )\n->>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n-\"UN-Chef sagt, es gibt keine militärische Lösung in Syrien\"\n-```\n-\n-### Expected speedups\n-\n-Below is an expected speedup diagram that compares pure inference time between the native implementation and the Flash Attention 2.\n-\n-<div style=\"text-align: center\">\n-<img src=\"https://huggingface.co/datasets/visheratin/documentation-images/resolve/main/nllb-speedup.webp\">\n-</div>\n-\n-## Using Scaled Dot Product Attention (SDPA)\n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n-or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n-page for more information.\n-\n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n-`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n-\n-```python\n-from transformers import AutoModelForSeq2SeqLM\n-model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n-...\n-```\n-\n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 257,
        "additions": 100,
        "deletions": 157
    }
}