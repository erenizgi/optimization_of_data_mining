{
    "author": "jerryzh168",
    "message": "Add autoquant support for torchao quantizer (#35503)\n\n* Add autoquant support for torchao quantizer\n\nSummary:\natt, also verified that autoquantized model can be saved and loaded:\n\nsave: https://gist.github.com/jerryzh168/01d367aaf44dbbbfd4068a4a10a00061\nload: https://gist.github.com/jerryzh168/d5c6c401b2abdf18e0b6771341f1525c\n\nTest Plan:\ntested locally with above script\nmodel uploaded to https://huggingface.co/jerryzh168/llama3-8b-autoquant\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* add test\n\n* ruff fix\n\n* ruff reformat\n\n* add docs and min_sqnr support\n\n* format\n\n* format\n\n* fix test\n\n* update doc\n\n* format\n\n* remove disable_compile\n\n* format",
    "sha": "2af272c101acf1401d07ae9f72d1dedf019edc8a",
    "files": [
        {
            "sha": "06017c3f3ec981bdf930d14a731402f3ca28f809",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 55,
            "deletions": 13,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/2af272c101acf1401d07ae9f72d1dedf019edc8a/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2af272c101acf1401d07ae9f72d1dedf019edc8a/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=2af272c101acf1401d07ae9f72d1dedf019edc8a",
            "patch": "@@ -22,6 +22,12 @@ pip install --upgrade torch torchao transformers\n \n By default, the weights are loaded in full precision (torch.float32) regardless of the actual data type the weights are stored in such as torch.float16. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a model's `config.json` file to automatically load the most memory-optimal data type.\n \n+## Manually Choose Quantization Types and Settings\n+\n+`torchao` Provides many commonly used types of quantization, including different dtypes like int4, float8 and different flavors like weight only, dynamic quantization etc., only `int4_weight_only`, `int8_weight_only` and `int8_dynamic_activation_int8_weight` are integrated into hugigngface transformers currently, but we can add more when needed.\n+\n+Users can manually specify the quantization types and settings they want to use:\n+\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n@@ -41,19 +47,14 @@ output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implemen\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n # benchmark the performance\n-import torch.utils.benchmark as benchmark\n-\n-def benchmark_fn(f, *args, **kwargs):\n-    # Manual warmup\n-    for _ in range(5):\n-        f(*args, **kwargs)\n-        \n-    t0 = benchmark.Timer(\n-        stmt=\"f(*args, **kwargs)\",\n-        globals={\"args\": args, \"kwargs\": kwargs, \"f\": f},\n-        num_threads=torch.get_num_threads(),\n-    )\n-    return f\"{(t0.blocked_autorange().mean):.3f}\"\n+from torch._inductor.utils import do_bench_using_profiling\n+from typing import Callable\n+\n+def benchmark_fn(func: Callable, *args, **kwargs) -> float:\n+    \"\"\"Thin wrapper around do_bench_using_profiling\"\"\"\n+    no_args = lambda: func(*args, **kwargs)\n+    time = do_bench_using_profiling(no_args)\n+    return time * 1e3\n \n MAX_NEW_TOKENS = 1000\n print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n@@ -64,6 +65,47 @@ print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_toke\n \n ```\n \n+## Automatically Select Quantization Types\n+\n+`torchao` also provies `autoquant` feature that automatically chooses a quantization type for quantizable layers such as linear based on microbenchmarks of quantizing and compiling a single linear layer.\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"meta-llama/Meta-Llama-3-8B\"\n+quantization_config = TorchAoConfig(\"autoquant\", min_sqnr=None)\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speedup\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+# Due to some implementation details we are explicitly calling this now, we may refactor our code and remove this in the future\n+quantized_model.finalize_autoquant()\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+# benchmark the performance\n+from torch._inductor.utils import do_bench_using_profiling\n+from typing import Callable\n+\n+def benchmark_fn(func: Callable, *args, **kwargs) -> float:\n+    \"\"\"Thin wrapper around do_bench_using_profiling\"\"\"\n+    no_args = lambda: func(*args, **kwargs)\n+    time = do_bench_using_profiling(no_args)\n+    return time * 1e3\n+\n+MAX_NEW_TOKENS = 1000\n+print(\"autoquantized model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n+\n+bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n+output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n+print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n+\n+```\n+\n ## Serialization and Deserialization\n torchao quantization is implemented with [tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor), it only work with huggingface non-safetensor serialization and deserialization. It relies on `torch.load(..., weights_only=True)` to avoid arbitrary user code execution during load time and use [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals) to allowlist some known user functions.\n "
        },
        {
            "sha": "0d03de2addd33f5c2507268d118b0c5f3e1fc610",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2af272c101acf1401d07ae9f72d1dedf019edc8a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2af272c101acf1401d07ae9f72d1dedf019edc8a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=2af272c101acf1401d07ae9f72d1dedf019edc8a",
            "patch": "@@ -4914,7 +4914,7 @@ def _find_mismatched_keys(\n                     device_map is not None\n                     and hf_quantizer is not None\n                     and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n-                    and hf_quantizer.quantization_config.quant_type == \"int4_weight_only\"\n+                    and hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n                 ):\n                     map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n                 state_dict = load_state_dict("
        },
        {
            "sha": "8439e68a908f43f76116b86d8d3f07fb50e8e714",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2af272c101acf1401d07ae9f72d1dedf019edc8a/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2af272c101acf1401d07ae9f72d1dedf019edc8a/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=2af272c101acf1401d07ae9f72d1dedf019edc8a",
            "patch": "@@ -129,6 +129,7 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n                 \"int4_weight_only\": CustomDtype.INT4,\n                 \"int8_weight_only\": torch.int8,\n                 \"int8_dynamic_activation_int8_weight\": torch.int8,\n+                \"autoquant\": None,\n             }\n             return map_to_target_dtype[self.quantization_config.quant_type]\n         else:\n@@ -161,6 +162,9 @@ def check_quantized_param(\n         state_dict: Dict[str, Any],\n         **kwargs,\n     ) -> bool:\n+        if self.quantization_config.quant_type == \"autoquant\":\n+            return False\n+\n         param_device = kwargs.pop(\"param_device\", None)\n         # check if the param_name is not in self.modules_to_not_convert\n         if any((key + \".\" in param_name) or (key == param_name) for key in self.modules_to_not_convert):\n@@ -186,6 +190,9 @@ def create_quantized_param(\n         Each nn.Linear layer that needs to be quantized is processsed here.\n         First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.\n         \"\"\"\n+        if self.quantization_config.quant_type == \"autoquant\":\n+            return\n+\n         from torchao.quantization import quantize_\n \n         module, tensor_name = get_module_from_name(model, param_name)\n@@ -200,6 +207,15 @@ def create_quantized_param(\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n+        if self.quantization_config.quant_type == \"autoquant\":\n+            from torchao import autoquant\n+            from torchao.quantization import ALL_AUTOQUANT_CLASS_LIST\n+\n+            model = torch.compile(model, mode=\"max-autotune\")\n+            model = autoquant(\n+                model, qtensor_class_list=ALL_AUTOQUANT_CLASS_LIST, **self.quantization_config.quant_type_kwargs\n+            )\n+            return model\n         return\n \n     def is_serializable(self, safe_serialization=None):"
        },
        {
            "sha": "2ac53dc3151cc4a185d129107df8c7d9d10bec92",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 27,
            "deletions": 3,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/2af272c101acf1401d07ae9f72d1dedf019edc8a/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2af272c101acf1401d07ae9f72d1dedf019edc8a/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=2af272c101acf1401d07ae9f72d1dedf019edc8a",
            "patch": "@@ -1453,7 +1453,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n \n     Args:\n         quant_type (`str`):\n-            The type of quantization we want to use, currently supporting: `int4_weight_only`, `int8_weight_only` and `int8_dynamic_activation_int8_weight`.\n+            The type of quantization we want to use, currently supporting: `int4_weight_only`, `int8_weight_only`, `int8_dynamic_activation_int8_weight` and `autoquant`.\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision.\n@@ -1465,9 +1465,31 @@ class TorchAoConfig(QuantizationConfigMixin):\n     Example:\n \n     ```python\n+    from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+    # specific quantization method\n     quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n     # int4_weight_only quant is only working with *torch.bfloat16* dtype right now\n     model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n+\n+    # autoquant\n+    # `autoquant` is a convenient way for users to search for the best quantization for each layer\n+    # `min_sqnr` is an option to control the accuracy of the model, higher value means the model is more\n+    # accurate, we can start with 30 and adjust it to larger or smaller (e.g. 40, 20)\n+    # defaults to None, which means we'll try to get the best performing quantized model without\n+    # considering accuracy\n+    quantization_config = TorchAoConfig(\"autoquant\", min_sqnr=30)\n+    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n+    # run through example inputs, quantization methods will be selected based on the shape of example input\n+    tokenizer = AutoTokenizer.from_pretrained(model_name)\n+    input_text = \"What are we having for dinner?\"\n+    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+    MAX_NEW_TOKENS = 1000\n+    model.generate(**input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\")\n+    # manually ran finalize_autoquant if needed\n+    if hasattr(quantized_model, \"finalize_autoquant\"):\n+      print(\"finalizing autoquant\")\n+      quantized_model.finalize_autoquant()\n     ```\n     \"\"\"\n \n@@ -1488,8 +1510,8 @@ def post_init(self):\n         Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n         \"\"\"\n         if is_torchao_available():\n-            if not version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.4.0\"):\n-                raise ValueError(\"Requires torchao 0.4.0 version and above\")\n+            if not version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.7.0\"):\n+                raise ValueError(\"Requires torchao 0.7.0 version and above\")\n         else:\n             raise ValueError(\n                 \"TorchAoConfig requires torchao to be installed, please install with `pip install torchao`\"\n@@ -1517,6 +1539,7 @@ def post_init(self):\n     def _get_torchao_quant_type_to_method(self):\n         if is_torchao_available():\n             from torchao.quantization import (\n+                autoquant,\n                 int4_weight_only,\n                 int8_dynamic_activation_int8_weight,\n                 int8_weight_only,\n@@ -1526,6 +1549,7 @@ def _get_torchao_quant_type_to_method(self):\n                 \"int4_weight_only\": int4_weight_only,\n                 \"int8_weight_only\": int8_weight_only,\n                 \"int8_dynamic_activation_int8_weight\": int8_dynamic_activation_int8_weight,\n+                \"autoquant\": autoquant,\n             }\n         else:\n             raise ValueError("
        },
        {
            "sha": "60694924cdca4226c8ad360013173cbc428aae29",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 35,
            "deletions": 1,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/2af272c101acf1401d07ae9f72d1dedf019edc8a/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2af272c101acf1401d07ae9f72d1dedf019edc8a/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=2af272c101acf1401d07ae9f72d1dedf019edc8a",
            "patch": "@@ -31,18 +31,25 @@\n     import torch\n \n if is_torchao_available():\n+    # renamed in torchao 0.7.0, please install the latest torchao\n     from torchao.dtypes import (\n         AffineQuantizedTensor,\n         TensorCoreTiledLayout,\n     )\n+    from torchao.quantization.autoquant import AQMixin\n \n \n def check_torchao_quantized(test_module, qlayer, batch_size=1, context_size=1024):\n     weight = qlayer.weight\n     test_module.assertTrue(isinstance(weight, AffineQuantizedTensor))\n     test_module.assertEqual(weight.quant_min, 0)\n     test_module.assertEqual(weight.quant_max, 15)\n-    test_module.assertTrue(isinstance(weight.layout, TensorCoreTiledLayout))\n+    test_module.assertTrue(isinstance(weight._layout, TensorCoreTiledLayout))\n+\n+\n+def check_autoquantized(test_module, qlayer):\n+    weight = qlayer.weight\n+    test_module.assertTrue(isinstance(weight, AQMixin))\n \n \n def check_forward(test_module, model, batch_size=1, context_size=1024):\n@@ -248,6 +255,33 @@ def test_int8_dynamic_activation_int8_weight_quant(self):\n         EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n+    def test_autoquant(self):\n+        \"\"\"\n+        Simple LLM model testing autoquant\n+        \"\"\"\n+        quant_config = TorchAoConfig(\"autoquant\")\n+\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            torch_dtype=torch.bfloat16,\n+            device_map=torch_device,\n+            quantization_config=quant_config,\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        output = quantized_model.generate(\n+            **input_ids, max_new_tokens=self.max_new_tokens, cache_implementation=\"static\"\n+        )\n+        quantized_model.finalize_autoquant()\n+\n+        check_autoquantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n+\n+        EXPECTED_OUTPUT = 'What are we having for dinner?\\n\\n10. \"Dinner is ready'\n+        output = quantized_model.generate(\n+            **input_ids, max_new_tokens=self.max_new_tokens, cache_implementation=\"static\"\n+        )\n+        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n+\n \n @require_torch_gpu\n @require_torchao"
        }
    ],
    "stats": {
        "total": 152,
        "additions": 134,
        "deletions": 18
    }
}