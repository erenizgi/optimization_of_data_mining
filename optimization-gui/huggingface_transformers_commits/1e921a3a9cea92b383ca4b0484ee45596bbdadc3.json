{
    "author": "Codys12",
    "message": "Add optional RMSNorm support to BitNet quantization (config + layers) (#38087)\n\n* enable optional RMS in BitLinear\n\n* Fix naming\n\n* Import RMS from Llama using config.*\n\n* make fix-copies\n\n* ran CI loop\n\n* remove default BitNetQuantConfig values\n\n* Fix BitNetQuantConfig to be Optional\n\n* Fix config docstrings to match Optoinal\n\n* Edit docstrings to match standards\n\n---------\n\nCo-authored-by: steinmetzc <codysteinmetz7@gmail.com>\nCo-authored-by: codys12 <steinmetzc@dh-mgmt4.hpc.msoe.edu>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "1e921a3a9cea92b383ca4b0484ee45596bbdadc3",
    "files": [
        {
            "sha": "a9e7c9bff5bcb551f5beb887da4d06d28fe7ce43",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e921a3a9cea92b383ca4b0484ee45596bbdadc3/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e921a3a9cea92b383ca4b0484ee45596bbdadc3/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=1e921a3a9cea92b383ca4b0484ee45596bbdadc3",
            "patch": "@@ -1584,7 +1584,9 @@ def __init__(\n         self.pattern = pattern\n         self.add_prefix_space = add_prefix_space\n         self.additional_special_tokens = (\n-            additional_special_tokens.keys() if type(additional_special_tokens) is dict else additional_special_tokens\n+            additional_special_tokens.keys()\n+            if isinstance(additional_special_tokens, dict)\n+            else additional_special_tokens\n         )\n \n     def extract_vocab_merges_from_model(self, tiktoken_url: str):"
        },
        {
            "sha": "492d6f123c9d965494933d0b08cbeffa6d835e9e",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 38,
            "deletions": 2,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e921a3a9cea92b383ca4b0484ee45596bbdadc3/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e921a3a9cea92b383ca4b0484ee45596bbdadc3/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=1e921a3a9cea92b383ca4b0484ee45596bbdadc3",
            "patch": "@@ -124,7 +124,16 @@ def unpack_weights(packed: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:\n \n \n class BitLinear(nn.Module):\n-    def __init__(self, in_features: int, out_features: int, bias: bool, device=None, dtype=None):\n+    def __init__(\n+        self,\n+        in_features: int,\n+        out_features: int,\n+        bias: bool,\n+        device=None,\n+        dtype=None,\n+        use_rms_norm: bool = False,\n+        rms_norm_eps: float = 1e-6,\n+    ):\n         super().__init__()\n         self.dtype = dtype\n         self.in_features = in_features\n@@ -150,6 +159,13 @@ def __init__(self, in_features: int, out_features: int, bias: bool, device=None,\n         else:\n             self.bias = None\n \n+        # Optional RMSNorm (applied on the activations before quantization).\n+        self.rms_norm = None\n+        if use_rms_norm:\n+            from ..models.llama.modeling_llama import LlamaRMSNorm\n+\n+            self.rms_norm = LlamaRMSNorm(in_features, eps=rms_norm_eps)\n+\n     @torch.compile\n     def activation_quant(self, input, num_bits=8):\n         \"\"\"\n@@ -180,6 +196,10 @@ def post_quant_process(self, input, input_scale, weight_scale):\n         return out\n \n     def forward(self, input):\n+        # Apply RMSNorm on the input if requested.\n+        if self.rms_norm is not None:\n+            input = self.rms_norm(input)\n+\n         w = self.weight\n         w_quant = unpack_weights(w, dtype=self.dtype)\n         input_quant, input_scale = self.activation_quant(input)\n@@ -245,9 +265,17 @@ def __init__(\n         device=None,\n         dtype=None,\n         online_quant: bool = False,\n+        use_rms_norm: bool = False,\n+        rms_norm_eps: float = 1e-6,\n     ):\n         super().__init__(in_features, out_features, bias)\n         self.online_quant = online_quant\n+        # Optional RMSNorm\n+        self.rms_norm = None\n+        if use_rms_norm:\n+            from ..models.llama.modeling_llama import LlamaRMSNorm\n+\n+            self.rms_norm = LlamaRMSNorm(in_features, eps=rms_norm_eps)\n         if not online_quant:\n             self.register_buffer(\n                 \"weight_scale\",\n@@ -271,6 +299,10 @@ def load_hook(\n         return state_dict\n \n     def forward(self, input):\n+        # Optional RMSNorm on activations prior to quantization.\n+        if self.rms_norm is not None:\n+            input = self.rms_norm(input)\n+\n         if self.online_quant:\n             weight = WeightQuant.apply(self.weight)\n         else:\n@@ -318,6 +350,8 @@ def _replace_with_bitnet_linear(\n                             device=module.weight.device,\n                             dtype=module.weight.dtype,\n                             online_quant=(quantization_config.quantization_mode == \"online\"),\n+                            use_rms_norm=quantization_config.use_rms_norm,\n+                            rms_norm_eps=quantization_config.rms_norm_eps,\n                         )\n                         if quantization_config.quantization_mode == \"offline\":\n                             model._modules[name].requires_grad_(False)\n@@ -328,6 +362,8 @@ def _replace_with_bitnet_linear(\n                             bias=module.bias is not None,\n                             device=module.weight.device,\n                             dtype=module.weight.dtype,\n+                            use_rms_norm=quantization_config.use_rms_norm,\n+                            rms_norm_eps=quantization_config.rms_norm_eps,\n                         )\n                         model._modules[name].requires_grad_(False)\n                     has_been_replaced = True\n@@ -363,7 +399,7 @@ def replace_with_bitnet_linear(\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n         modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n-            Names of the modules to not convert in `EetqLinear`. In practice we keep the `lm_head` in full precision\n+            Names of the modules to not convert in `BitLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n         current_key_name (`List[`str`]`, *optional*):\n             An array to track the current key of the recursion. This is used to check whether the current key (part of"
        },
        {
            "sha": "ee9a9c36af2ff9dc9c8d451e6c2c32c2ea86e1d5",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e921a3a9cea92b383ca4b0484ee45596bbdadc3/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e921a3a9cea92b383ca4b0484ee45596bbdadc3/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=1e921a3a9cea92b383ca4b0484ee45596bbdadc3",
            "patch": "@@ -1791,6 +1791,11 @@ class BitNetQuantConfig(QuantizationConfigMixin):\n             In `offline` mode, quantization parameters are pre-calculated *before* inference.\n             These parameters are then fixed and loaded into the quantized model. This\n             generally results in lower runtime overhead compared to online quantization.\n+        use_rms_norm (`bool`, *optional*, defaults to `False`):\n+            Whether to apply RMSNorm on the activations before quantization. This matches the original BitNet paper's approach\n+            of normalizing activations before quantization/packing.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon value used in the RMSNorm layer for numerical stability.\n         kwargs (`Dict[str, Any]`, *optional*):\n             Additional keyword arguments that may be used by specific quantization\n             backends or future versions.\n@@ -1801,6 +1806,8 @@ def __init__(\n         modules_to_not_convert: Optional[List] = None,\n         linear_class: Optional[str] = \"bitlinear\",\n         quantization_mode: Optional[str] = \"offline\",\n+        use_rms_norm: Optional[bool] = False,\n+        rms_norm_eps: Optional[float] = 1e-6,\n         **kwargs,\n     ):\n         if linear_class not in [\"bitlinear\", \"autobitlinear\"]:\n@@ -1811,6 +1818,8 @@ def __init__(\n         self.modules_to_not_convert = modules_to_not_convert\n         self.linear_class = linear_class\n         self.quantization_mode = quantization_mode\n+        self.use_rms_norm = use_rms_norm\n+        self.rms_norm_eps = rms_norm_eps\n         self.post_init()\n \n     def post_init(self):"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 50,
        "deletions": 3
    }
}