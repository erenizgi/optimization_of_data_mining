{
    "author": "ajeet214",
    "message": "Update: add type hints to check_tokenizers.py (#40094)\n\n* Update check_tokenizers.py\n\nchore(typing): add type hints to check_tokenizers script\r\n\r\n- Annotate params/returns for helper functions\r\n- Keep tokenizer instances as `Any` to avoid runtime coupling\r\n- Make `check_LTR_mark` return `bool` explicitly (no behavior change)\n\n* Update check_tokenizers.py\n\nchore(typing): replace Any with PreTrainedTokenizerBase in check_tokenizers.py\r\n\r\n- Use transformers.tokenization_utils_base.PreTrainedTokenizerBase for `slow` and `fast` params\r\n- Covers both PreTrainedTokenizer and PreTrainedTokenizerFast\r\n- Exposes required methods (encode, decode, encode_plus, tokenize)\r\n- Removes generic Any typing while staying implementation-agnostic",
    "sha": "de437d0d7ac30beb44b196e1413544122df64152",
    "files": [
        {
            "sha": "a099d794c2b4b85f3866d17f2a917c0f3c849eb6",
            "filename": "scripts/check_tokenizers.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de437d0d7ac30beb44b196e1413544122df64152/scripts%2Fcheck_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de437d0d7ac30beb44b196e1413544122df64152/scripts%2Fcheck_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fcheck_tokenizers.py?ref=de437d0d7ac30beb44b196e1413544122df64152",
            "patch": "@@ -5,6 +5,7 @@\n import transformers\n from transformers.convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS\n from transformers.utils import logging\n+from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n \n \n logging.set_verbosity_info()\n@@ -21,7 +22,7 @@\n wrong = 0\n \n \n-def check_diff(spm_diff, tok_diff, slow, fast):\n+def check_diff(spm_diff: list[int], tok_diff: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase) -> bool:\n     if spm_diff == list(reversed(tok_diff)):\n         # AAA -> AA+A vs A+AA case.\n         return True\n@@ -42,17 +43,18 @@ def check_diff(spm_diff, tok_diff, slow, fast):\n     return False\n \n \n-def check_LTR_mark(line, idx, fast):\n+def check_LTR_mark(line: str, idx: int, fast: PreTrainedTokenizerBase) -> bool:\n     enc = fast.encode_plus(line)[0]\n     offsets = enc.offsets\n     curr, prev = offsets[idx], offsets[idx - 1]\n     if curr is not None and line[curr[0] : curr[1]] == \"\\u200f\":\n         return True\n     if prev is not None and line[prev[0] : prev[1]] == \"\\u200f\":\n         return True\n+    return False\n \n \n-def check_details(line, spm_ids, tok_ids, slow, fast):\n+def check_details(line: str, spm_ids: list[int], tok_ids: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase) -> bool:\n     # Encoding can be the same with same result AAA -> A + AA vs AA + A\n     # We can check that we use at least exactly the same number of tokens.\n     for i, (spm_id, tok_id) in enumerate(zip(spm_ids, tok_ids)):\n@@ -111,7 +113,7 @@ def check_details(line, spm_ids, tok_ids, slow, fast):\n     return False\n \n \n-def test_string(slow, fast, text):\n+def test_string(slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase, text: str) -> None:\n     global perfect\n     global imperfect\n     global wrong\n@@ -143,7 +145,7 @@ def test_string(slow, fast, text):\n     ), f\"line {text} : \\n\\n{slow_ids}\\n{fast_ids}\\n\\n{slow.tokenize(text)}\\n{fast.tokenize(text)}\"\n \n \n-def test_tokenizer(slow, fast):\n+def test_tokenizer(slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase) -> None:\n     global batch_total\n     for i in range(len(dataset)):\n         # premise, all languages"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 7,
        "deletions": 5
    }
}