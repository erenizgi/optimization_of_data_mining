{
    "author": "AhmedAlmaghz",
    "message": "[i18n-ar] Translated file : `docs/source/ar/fast_tokenizers.md` into Arabic (#33034)\n\n* Add docs/source/ar/fast_tokenizers.md to Add_docs_source_ar_fast_tokenizers.md\r\n\r\n* Update _toctree.yml\r\n\r\n* Update _toctree.yml\r\n\r\n* Update docs/source/ar/_toctree.yml\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/fast_tokenizers.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>",
    "sha": "a17f287ac039f92835b5cd9bd8ee28b584c9f65e",
    "files": [
        {
            "sha": "bd45925c64cb0e5fdeda929bfb4b26ee20a3e4b0",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a17f287ac039f92835b5cd9bd8ee28b584c9f65e/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a17f287ac039f92835b5cd9bd8ee28b584c9f65e/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=a17f287ac039f92835b5cd9bd8ee28b584c9f65e",
            "patch": "@@ -108,9 +108,9 @@\n #       title: Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù…Ø­ÙØ²Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©\n #     title: Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯\n #   title: Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ù‡Ø§Ù…\n-# - sections:\n-#   - local: fast_tokenizers\n-#     title: Ø§Ø³ØªØ®Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù…Ù† ğŸ¤— Tokenizers\n+- sections:\n+  - local: fast_tokenizers\n+    title: Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¬Ø²Ø¦ÙŠØ§Øª Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù…Ù† ğŸ¤— Tokenizers \n #   - local: multilingual\n #     title: ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù„ØºØ§Øª\n #   - local: create_a_model\n@@ -139,7 +139,7 @@\n #     title: Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ¥ØµÙ„Ø§Ø­Ù‡Ø§\n #   - local: gguf\n #     title: Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù…Ù„ÙØ§Øª GGUF\n-#   title: Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·ÙˆØ±ÙŠÙ†\n+  title: Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·ÙˆØ±ÙŠÙ†\n # - sections:\n #   - local: quantization/overview\n #     title: Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©"
        },
        {
            "sha": "539712969e813ffeeb47c0676320ec6255d20180",
            "filename": "docs/source/ar/fast_tokenizers.md",
            "status": "added",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a17f287ac039f92835b5cd9bd8ee28b584c9f65e/docs%2Fsource%2Far%2Ffast_tokenizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a17f287ac039f92835b5cd9bd8ee28b584c9f65e/docs%2Fsource%2Far%2Ffast_tokenizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ffast_tokenizers.md?ref=a17f287ac039f92835b5cd9bd8ee28b584c9f65e",
            "patch": "@@ -0,0 +1,51 @@\n+# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ø²Ø¦ÙŠØ§Øª Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† ğŸ¤— Tokenizers\n+\n+ÙŠØ¹ØªÙ…Ø¯ [`PreTrainedTokenizerFast`] Ø¹Ù„Ù‰ Ù…ÙƒØªØ¨Ø© [ğŸ¤— Tokenizers](https://huggingface.co/docs/tokenizers). ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¬Ø²Ø¦Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡Ù… Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Tokenizers Ø¨Ø¨Ø³Ø§Ø·Ø© Ø´Ø¯ÙŠØ¯Ø© ÙÙŠ ğŸ¤— Transformers.\n+\n+Ù‚Ø¨Ù„ Ø§Ù„Ø¯Ø®ÙˆÙ„ ÙÙŠ Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¨Ø¯Ø£ Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØ¬Ø²Ù‰Ø¡ Ù„ØºÙˆÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙÙŠ Ø¨Ø¶Ø¹ Ø³Ø·ÙˆØ±:\n+\n+```python\n+>>> from tokenizers import Tokenizer\n+>>> from tokenizers.models import BPE\n+>>> from tokenizers.trainers import BpeTrainer\n+>>> from tokenizers.pre_tokenizers import Whitespace\n+\n+>>> tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n+>>> trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n+\n+>>> tokenizer.pre_tokenizer = Whitespace()\n+>>> files = [...]\n+>>> tokenizer.train(files, trainer)\n+```\n+\n+Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙ†Ø§ Ù…ÙØ¬Ø²Ù‰Ø¡ Ù„ØºÙˆÙŠ Ù…Ø¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ Ø­Ø¯Ø¯Ù†Ø§Ù‡Ø§. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ù…Ø§ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ØŒ Ø£Ùˆ Ø­ÙØ¸Ù‡ ÙÙŠ Ù…Ù„Ù JSON Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§.\n+\n+## ØªØ­Ù…ÙŠÙ„ Ù…ÙØ¬Ø²Ø¦  Ø§Ù„Ù†Ù‘ØµÙˆØµ  Ù…ÙØ¨Ø§Ø´Ø±Ø©Ù‹\n+\n+Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø±Ù‰ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ÙƒØ§Ø¦Ù† (Ù…ÙØ¬Ø²Ø¦ Ø§Ù„Ù†ØµÙˆØµ) ÙÙŠ Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers. ØªØ³Ù…Ø­ ÙØ¦Ø© [`PreTrainedTokenizerFast`] Ø³Ù‡ÙˆÙ„Ø© Ø¥Ù†Ø´Ø§Ø¡ *tokenizer*ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ Ù‚Ø¨ÙˆÙ„ ÙƒØ§Ø¦Ù† *Ø§Ù„Ù…ÙØ¬Ø²Ø¦ Ø§Ù„Ù†ØµÙˆØµ*  Ù…ÙÙ‡ÙŠÙ‘Ø£ Ù…ÙØ³Ø¨Ù‚Ù‹Ø§ ÙƒÙ…Ø¹Ø§Ù…Ù„:\n+\n+```python\n+>>> from transformers import PreTrainedTokenizerFast\n+\n+>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n+```\n+\n+ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ÙƒØ§Ø¦Ù† Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…ÙØ´ØªØ±ÙƒØ© Ø¨ÙŠÙ† Ù…ÙØ¬Ø²Ù‘Ø¦ÙŠ Ø§Ù„Ù†Ù‘ØµÙˆØµ  Ù„Ù€ ğŸ¤— Transformers! Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ [ØµÙØ­Ø© Ù…ÙØ¬Ø²Ù‘Ø¦  Ø§Ù„Ù†Ù‘ØµÙˆØµ](main_classes/tokenizer) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.\n+\n+## Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ù…Ù„Ù JSON\n+\n+Ù„ØªØ­Ù…ÙŠÙ„ Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù JSONØŒ Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¨Ø¯Ø£ Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ø­ÙØ¸ Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù†Ù‘ØµÙˆØµ:\n+\n+```python\n+>>> tokenizer.save(\"tokenizer.json\")\n+```\n+\n+ÙŠÙ…ÙƒÙ† ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø°ÙŠ Ø­ÙØ¸Ù†Ø§ Ø¨Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‡ÙŠØ¦Ø© [`PreTrainedTokenizerFast`] Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØ¹Ø§Ù…Ù„  `tokenizer_file`:\n+\n+```python\n+>>> from transformers import PreTrainedTokenizerFast\n+\n+>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n+```\n+\n+ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ÙƒØ§Ø¦Ù† Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ØªÙŠ ØªØ´ØªØ±Ùƒ ÙÙŠÙ‡Ø§ Ù…ÙØ¬Ø²Ù‘Ø¦ÙŠ  Ø§Ù„Ù†Ù‘ØµÙˆØµ Ù„Ù€ ğŸ¤— Transformers! Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ [ØµÙØ­Ø© Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù†Øµ](main_classes/tokenizer) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 55,
        "deletions": 4
    }
}