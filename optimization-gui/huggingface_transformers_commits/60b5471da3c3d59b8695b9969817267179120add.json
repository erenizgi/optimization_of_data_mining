{
    "author": "cyyever",
    "message": "Enable some ruff checks for performance and readability (#39383)\n\n* Fix inefficient sequence tests\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Enable PERF102\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Enable PLC1802\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Enable PLC0208\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "60b5471da3c3d59b8695b9969817267179120add",
    "files": [
        {
            "sha": "944caad72f3f5651b26eea75c49f86b8e64aef3b",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -754,9 +754,9 @@ def get_parameters(model: nn.Module) -> Iterable[torch.Tensor]:\n     Returns:\n         Iterable[torch.Tensor]: An iterator over all parameters in the model\n     \"\"\"\n-    for name, module in model._modules.items():\n+    for module in model._modules.values():\n         # Look for parameters in module attributes\n-        for attr_name, attr in module.__dict__.items():\n+        for attr in module.__dict__.values():\n             if isinstance(attr, torch.Tensor) and attr.requires_grad:\n                 yield attr\n         # Recursively get parameters from submodules"
        },
        {
            "sha": "83a4bf3ad3c05d3c97752e1eecb673c340c37faa",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -19,10 +19,10 @@ line-length = 119\n \n [tool.ruff.lint]\n # Never enforce `E501` (line length violations).\n-ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\" ]\n+ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\"]\n # RUF013: Checks for the use of implicit Optional\n #  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"UP006\"]\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"UP006\", \"PERF102\", \"PLC1802\", \"PLC0208\"]\n extend-safe-fixes = [\"UP006\"]\n \n # Ignore import violations in all `__init__.py` files."
        },
        {
            "sha": "32a3b57956ca0e147ab2be34e41e9eb90286ca35",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -607,7 +607,7 @@ def from_pretrained(\n         if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n             # sometimes the config has no `base_config_key` if the config is used in several composite models\n             # e.g. LlamaConfig. In that case we try to see if there is match in `model_type` before raising a warning\n-            for k, v in config_dict.items():\n+            for v in config_dict.values():\n                 if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n                     config_dict = v\n "
        },
        {
            "sha": "4672af7165279749703ad692d48edd74710d3aa4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -2166,7 +2166,7 @@ def post_init(self):\n                 self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n \n         if self._tp_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-            for _, v in self._tp_plan.items():\n+            for v in self._tp_plan.values():\n                 if v not in ALL_PARALLEL_STYLES:\n                     raise ValueError(\n                         f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\"\n@@ -2845,7 +2845,7 @@ def tie_encoder_to_decoder_recursively(\n \n                 all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()}\n                 encoder_layer_pos = 0\n-                for name, module in decoder_modules.items():\n+                for name in decoder_modules.keys():\n                     if name.isdigit():\n                         encoder_name = str(int(name) + encoder_layer_pos)\n                         decoder_name = name\n@@ -5830,7 +5830,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n     accelerator_device_map = {\n         param: torch.device(device) for param, device in expanded_device_map.items() if is_accelerator_device(device)\n     }\n-    if not len(accelerator_device_map):\n+    if not accelerator_device_map:\n         return\n \n     tp_plan_regex = ("
        },
        {
            "sha": "fcc93165de83fa5a9c8383cea5a1a36d03053048",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -133,7 +133,7 @@ def feature_extractor_class_from_name(class_name: str):\n             except AttributeError:\n                 continue\n \n-    for _, extractor in FEATURE_EXTRACTOR_MAPPING._extra_content.items():\n+    for extractor in FEATURE_EXTRACTOR_MAPPING._extra_content.values():\n         if getattr(extractor, \"__name__\", None) == class_name:\n             return extractor\n "
        },
        {
            "sha": "d3c4367eca512c5239e09944bf9366222ede2009",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -212,7 +212,7 @@ def get_image_processor_class_from_name(class_name: str):\n             except AttributeError:\n                 continue\n \n-    for _, extractors in IMAGE_PROCESSOR_MAPPING._extra_content.items():\n+    for extractors in IMAGE_PROCESSOR_MAPPING._extra_content.values():\n         for extractor in extractors:\n             if getattr(extractor, \"__name__\", None) == class_name:\n                 return extractor\n@@ -533,7 +533,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 )\n                 use_fast = False\n             if use_fast:\n-                for _, image_processors in IMAGE_PROCESSOR_MAPPING_NAMES.items():\n+                for image_processors in IMAGE_PROCESSOR_MAPPING_NAMES.values():\n                     if image_processor_type in image_processors:\n                         break\n                 else:"
        },
        {
            "sha": "9dd78dbeae40c2573afbfdfda23f45f79d831afb",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -744,7 +744,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             except AttributeError:\n                 continue\n \n-    for config, tokenizers in TOKENIZER_MAPPING._extra_content.items():\n+    for tokenizers in TOKENIZER_MAPPING._extra_content.values():\n         for tokenizer in tokenizers:\n             if getattr(tokenizer, \"__name__\", None) == class_name:\n                 return tokenizer"
        },
        {
            "sha": "619d67c561f8becee31b4f8f4a43a81077baa352",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -84,7 +84,7 @@ def video_processor_class_from_name(class_name: str):\n             except AttributeError:\n                 continue\n \n-    for _, extractor in VIDEO_PROCESSOR_MAPPING._extra_content.items():\n+    for extractor in VIDEO_PROCESSOR_MAPPING._extra_content.values():\n         if getattr(extractor, \"__name__\", None) == class_name:\n             return extractor\n "
        },
        {
            "sha": "3b8313e31e8b0bd47dd71c24f3bf8c1fc55d15a4",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -140,7 +140,7 @@ def attention(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor):\n     def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n         residual_state = hidden_state + self.attention(self.ln_1(hidden_state), attention_mask)\n         hidden_state = self.ln_2(residual_state)\n-        for _, layer in self.mlp.items():\n+        for layer in self.mlp.values():\n             hidden_state = layer(hidden_state)\n         hidden_state = residual_state + hidden_state\n         return hidden_state"
        },
        {
            "sha": "0e39528c7c074750e569ad6eb436a72edfb162f4",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -199,7 +199,7 @@ def token2json(self, tokens, is_inner_value=False, added_vocab=None):\n                 if tokens[:6] == r\"<sep/>\":  # non-leaf nodes\n                     return [output] + self.token2json(tokens[6:], is_inner_value=True, added_vocab=added_vocab)\n \n-        if len(output):\n+        if output:\n             return [output] if is_inner_value else output\n         else:\n             return [] if is_inner_value else {\"text_sequence\": tokens}"
        },
        {
            "sha": "9f3f70bc57ad096aad9b0b0cd0b5dcdd7dc750dc",
            "filename": "src/transformers/models/grounding_dino/convert_grounding_dino_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -239,7 +239,7 @@ def create_rename_keys(state_dict, config):\n     ########################################## DECODER - END\n \n     ########################################## Additional - START\n-    for layer_name, params in state_dict.items():\n+    for layer_name in state_dict.keys():\n         #### TEXT BACKBONE\n         if \"bert\" in layer_name:\n             rename_keys.append((layer_name, layer_name.replace(\"bert\", \"model.text_backbone\")))"
        },
        {
            "sha": "257893980111d9fcb0333eec0646e7cc26540bad",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -177,7 +177,7 @@ def find_supported_resolutions(max_num_chunks: int, patch_size: SizeDict) -> tor\n \n     # get the resolutions multiplied by the patch_size\n     possible_resolutions = []\n-    for key, value in asp_dict.items():\n+    for value in asp_dict.values():\n         for height, depth in value:\n             possible_resolutions.append((height * patch_size, depth * patch_size))\n "
        },
        {
            "sha": "1edcae80f67fd87c189a6629e30c62a64c43196f",
            "filename": "src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -100,7 +100,7 @@ def convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, p\n     state_dict.pop(\"lm_head.decoder.weight\")\n     state_dict.pop(\"lm_head.decoder.bias\")\n     state_dict_for_hugging_face = OrderedDict()\n-    for key, value in state_dict.items():\n+    for key in state_dict.keys():\n         if not (key.startswith(\"lm_head\") or key.startswith(\"entity_predictions\")):\n             state_dict_for_hugging_face[f\"luke.{key}\"] = state_dict[key]\n         else:"
        },
        {
            "sha": "e4e31e4d8ae63696e4d1df648de0475b21aa042a",
            "filename": "src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -100,7 +100,7 @@ def create_rename_keys_vision(state_dict, config):\n     ########################################## VISION BACKBONE - END\n \n     ########################################## ENCODER - START\n-    for layer_name, params in state_dict.items():\n+    for layer_name in state_dict.keys():\n         if \"neck\" in layer_name:\n             layer_name_replace = layer_name.replace(\"neck\", \"encoder\")\n             layer_name_replace = layer_name_replace.replace(\"input_proj\", \"channel_projection_layers\")\n@@ -117,7 +117,7 @@ def create_rename_keys_vision(state_dict, config):\n     ########################################## ENCODER - END\n \n     ########################################## DECODER - START\n-    for layer_name, params in state_dict.items():\n+    for layer_name in state_dict.keys():\n         if layer_name.startswith(\"decoder\"):\n             layer_name_replace = layer_name.replace(\"decoder.decoder.layers\", \"decoder.layers\")\n             layer_name_replace = layer_name_replace.replace(\"input_proj\", \"channel_projection_layers\")"
        },
        {
            "sha": "d43c05cd6220bfa7a68e6ec57d3858a9b13a914e",
            "filename": "src/transformers/models/xmod/convert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -144,7 +144,7 @@ def convert_xmod_checkpoint_to_pytorch(\n \n         if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n             raise AssertionError(\"Lists of language adapters do not match.\")\n-        for lang_code, adapter in xmod_layer.adapter_modules.items():\n+        for lang_code in xmod_layer.adapter_modules.keys():\n             to_adapter = bert_output.adapter_modules[lang_code]\n             from_adapter = xmod_layer.adapter_modules[lang_code]\n             to_adapter.dense1.weight = from_adapter.fc1.weight"
        },
        {
            "sha": "81fcb66afa3d78994dd8c276796cda98c5e4c245",
            "filename": "src/transformers/models/zoedepth/convert_zoedepth_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconvert_zoedepth_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconvert_zoedepth_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconvert_zoedepth_to_hf.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -266,7 +266,7 @@ def convert_state_dict(orig_state_dict):\n \n \n def remove_ignore_keys(state_dict):\n-    for key, _ in state_dict.copy().items():\n+    for key in state_dict.copy().keys():\n         if (\n             \"fc_norm\" in key\n             or \"relative_position_index\" in key"
        },
        {
            "sha": "238467ee292ee1044190b0316c184014d7643d51",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -1288,14 +1288,14 @@ def check_model_type(self, supported_models: Union[list[str], dict]):\n             if self.task in SUPPORTED_PEFT_TASKS:\n                 supported_models_names.extend(SUPPORTED_PEFT_TASKS[self.task])\n \n-            for _, model_name in supported_models.items():\n+            for model_name in supported_models.values():\n                 # Mapping can now contain tuples of models for the same configuration.\n                 if isinstance(model_name, tuple):\n                     supported_models_names.extend(list(model_name))\n                 else:\n                     supported_models_names.append(model_name)\n             if hasattr(supported_models, \"_model_mapping\"):\n-                for _, model in supported_models._model_mapping._extra_content.items():\n+                for model in supported_models._model_mapping._extra_content.values():\n                     if isinstance(model_name, tuple):\n                         supported_models_names.extend([m.__name__ for m in model])\n                     else:"
        },
        {
            "sha": "2e8dfd196ccceac595c18a2001d6361899f7d776",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -232,7 +232,7 @@ def __init__(\n \n         self._encodings = encoding\n \n-        if n_sequences is None and encoding is not None and len(encoding):\n+        if n_sequences is None and encoding is not None and encoding:\n             n_sequences = encoding[0].n_sequences\n \n         self._n_sequences = n_sequences"
        },
        {
            "sha": "84f65c954460816f51d538153b83ba1044e8b328",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -149,7 +149,7 @@ def find_batch_size(tensors):\n             if result is not None:\n                 return result\n     elif isinstance(tensors, Mapping):\n-        for key, value in tensors.items():\n+        for value in tensors.values():\n             result = find_batch_size(value)\n             if result is not None:\n                 return result"
        },
        {
            "sha": "93c4d971ac37d56ef1cfe3e6bb11a93bb52e64c6",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -2183,12 +2183,12 @@ def __init__(\n                 self._modules = self._modules.union(module_keys)\n \n                 for key, values in module.items():\n-                    if len(missing_backends):\n+                    if missing_backends:\n                         self._object_missing_backend[key] = missing_backends\n \n                     for value in values:\n                         self._class_to_module[value] = key\n-                        if len(missing_backends):\n+                        if missing_backends:\n                             self._object_missing_backend[value] = missing_backends\n                     _import_structure.setdefault(key, []).extend(values)\n "
        },
        {
            "sha": "70c0b034a541f8b93016d3273bbb38c9aaeff341",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -1199,7 +1199,7 @@ def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct\n         \"\"\"\n-        for layer_name, layer_param in self.config_for_layers.items():\n+        for layer_param in self.config_for_layers.values():\n             VptqLayerConfig(**layer_param)\n         if self.enable_proxy_error is True:\n             raise ValueError(\"enable_proxy_error should always be False until we support training\")"
        },
        {
            "sha": "876d1b9cad5a8c7310e1abab14a8b32133cbfe02",
            "filename": "tests/models/auto/test_modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -125,7 +125,7 @@ def test_model_for_pretraining_from_pretrained(self):\n         self.assertIsNotNone(model)\n         self.assertIsInstance(model, BertForPreTraining)\n         # Only one value should not be initialized and in the missing keys.\n-        for key, value in loading_info.items():\n+        for value in loading_info.values():\n             self.assertEqual(len(value), 0)\n \n     @slow"
        },
        {
            "sha": "673963129411667b9889ba424541e5e3a097df48",
            "filename": "tests/models/auto/test_tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -70,7 +70,7 @@ def setUp(self):\n \n     @slow\n     def test_tokenizer_from_pretrained(self):\n-        for model_name in {\"google-bert/bert-base-uncased\", \"google-bert/bert-base-cased\"}:\n+        for model_name in (\"google-bert/bert-base-uncased\", \"google-bert/bert-base-cased\"):\n             tokenizer = AutoTokenizer.from_pretrained(model_name)\n             self.assertIsNotNone(tokenizer)\n             self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast))"
        },
        {
            "sha": "7cc282e49ab086f0a452c44ddb4cf04798a8455e",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -897,7 +897,7 @@ def test_inference_base_model(self):\n         encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors=\"pt\")\n \n         # move all values to device\n-        for key, value in encoding.items():\n+        for key in encoding.keys():\n             encoding[key] = encoding[key].to(torch_device)\n \n         outputs = model(**encoding)\n@@ -932,7 +932,7 @@ def test_inference_large_model(self):\n         encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors=\"pt\")\n \n         # move all values to device\n-        for key, value in encoding.items():\n+        for key in encoding.keys():\n             encoding[key] = encoding[key].to(torch_device)\n \n         outputs = model(**encoding)"
        },
        {
            "sha": "0ce14f041292812c745f1b1764391ecdb5dae60b",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -757,7 +757,7 @@ def test_peft_load_adapter_training_inference_mode_false(self):\n                     model.load_adapter(tmpdirname, is_trainable=True)\n \n                     for name, module in model.named_modules():\n-                        if len(list(module.children())):\n+                        if list(module.children()):\n                             # only check leaf modules\n                             continue\n "
        },
        {
            "sha": "fbb8d5f541a4a2db6b7a7acfb748b2cbf9fc396b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -2535,7 +2535,7 @@ def test_can_use_safetensors(self):\n \n                 shared_ptrs = {k: v for k, v in ptrs.items() if len(v) > 1}\n \n-                for _, shared_names in shared_ptrs.items():\n+                for shared_names in shared_ptrs.values():\n                     reloaded_ptrs = {reloaded_state[k].data_ptr() for k in shared_names}\n                     self.assertEqual(\n                         len(reloaded_ptrs),"
        },
        {
            "sha": "2cad0b90527e212b17f78c6c02ee406e1f83f3de",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -139,7 +139,7 @@\n     \"zero-shot-image-classification\": (ZeroShotImageClassificationPipeline, ZeroShotImageClassificationInput),\n }\n \n-for task, task_info in pipeline_test_mapping.items():\n+for task_info in pipeline_test_mapping.values():\n     test = task_info[\"test\"]\n     task_info[\"mapping\"] = {\n         \"pt\": getattr(test, \"model_mapping\", None),"
        },
        {
            "sha": "d6382a5fdf11b3084ca853aeb17136148a11be49",
            "filename": "tests/utils/test_import_structure.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/tests%2Futils%2Ftest_import_structure.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/tests%2Futils%2Ftest_import_structure.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_import_structure.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -96,7 +96,7 @@ def test_transformers_specific_model_import(self):\n             with self.subTest(f\"Testing arch {architecture}\"):\n                 import_structure = define_import_structure(self.models_path / architecture)\n                 backend_agnostic_import_structure = {}\n-                for requirement, module_object_mapping in import_structure.items():\n+                for module_object_mapping in import_structure.values():\n                     for module, objects in module_object_mapping.items():\n                         if module not in backend_agnostic_import_structure:\n                             backend_agnostic_import_structure[module] = []"
        },
        {
            "sha": "f036e44495e4cba6457bdfa156957afd5e0782f8",
            "filename": "utils/add_pipeline_model_mapping_to_test.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/utils%2Fadd_pipeline_model_mapping_to_test.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/utils%2Fadd_pipeline_model_mapping_to_test.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fadd_pipeline_model_mapping_to_test.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -37,7 +37,7 @@\n \n \n PIPELINE_TEST_MAPPING = {}\n-for task, _ in pipeline_test_mapping.items():\n+for task in pipeline_test_mapping.keys():\n     PIPELINE_TEST_MAPPING[task] = {\"pt\": None, \"tf\": None}\n \n "
        },
        {
            "sha": "5c79e0a228d8b77ba6da9b40351911cdf7deef4b",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -790,7 +790,7 @@ def check_all_auto_object_names_being_defined():\n         mappings_to_check.update({name: getattr(module, name) for name in mapping_names})\n \n     for name, mapping in mappings_to_check.items():\n-        for _, class_names in mapping.items():\n+        for class_names in mapping.values():\n             if not isinstance(class_names, tuple):\n                 class_names = (class_names,)\n                 for class_name in class_names:"
        },
        {
            "sha": "5802cf23a6ebd5078fc2e6365aa259a9824ddb20",
            "filename": "utils/notification_service_doc_tests.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/60b5471da3c3d59b8695b9969817267179120add/utils%2Fnotification_service_doc_tests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60b5471da3c3d59b8695b9969817267179120add/utils%2Fnotification_service_doc_tests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service_doc_tests.py?ref=60b5471da3c3d59b8695b9969817267179120add",
            "patch": "@@ -332,7 +332,7 @@ def add_path(self, path: str):\n \n     doc_test_results = {}\n     # `artifact_key` is the artifact path\n-    for artifact_key, artifact_obj in available_artifacts.items():\n+    for artifact_obj in available_artifacts.values():\n         artifact_path = artifact_obj.paths[0]\n         if not artifact_path[\"path\"].startswith(\"doc_tests_gpu_test_reports_\"):\n             continue"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 40,
        "deletions": 40
    }
}