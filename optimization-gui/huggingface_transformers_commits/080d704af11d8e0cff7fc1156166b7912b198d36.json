{
    "author": "cyyever",
    "message": "Fix Pylint warnings (#41644)\n\n* Fix pylint warnings\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Raise with an exception\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "080d704af11d8e0cff7fc1156166b7912b198d36",
    "files": [
        {
            "sha": "795da2efe4b4509e74201f514798de826ea67032",
            "filename": "examples/legacy/seq2seq/run_distributed_eval.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -252,8 +252,7 @@ def gather_results_from_each_node(num_replicas, save_dir, timeout) -> list[dict[\n             return json_data\n         except JSONDecodeError:\n             continue\n-    else:\n-        raise TimeoutError(\"Rank 0 gave up on waiting for other processes\")\n+    raise TimeoutError(\"Rank 0 gave up on waiting for other processes\")\n     # Unreachable\n \n "
        },
        {
            "sha": "a76547809572006046ed852c3f901bbc9859dab5",
            "filename": "src/transformers/models/edgetam/configuration_edgetam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -93,8 +93,6 @@ def __init__(\n         if isinstance(backbone_config, dict):\n             backbone_config[\"model_type\"] = backbone_config.get(\"model_type\", \"timm_wrapper\")\n             backbone_config = CONFIG_MAPPING[backbone_config[\"model_type\"]](**backbone_config)\n-        elif isinstance(backbone_config, AutoConfig):\n-            backbone_config = backbone_config\n         elif backbone_config is None:\n             backbone_config = AutoConfig.from_pretrained(\n                 \"timm/repvit_m1.dist_in1k\","
        },
        {
            "sha": "d432a725b021e4c2596a96edfb78dc2af386292b",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -116,8 +116,6 @@ def __init__(\n         if isinstance(backbone_config, dict):\n             backbone_config[\"model_type\"] = backbone_config.get(\"model_type\", \"timm_wrapper\")\n             backbone_config = CONFIG_MAPPING[backbone_config[\"model_type\"]](**backbone_config)\n-        elif isinstance(backbone_config, AutoConfig):\n-            backbone_config = backbone_config\n         elif backbone_config is None:\n             backbone_config = AutoConfig.from_pretrained(\n                 \"timm/repvit_m1.dist_in1k\","
        },
        {
            "sha": "7153154048b6ed5a6ded34177b04655ccdd850c9",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -162,7 +162,7 @@ def sample_frames(\n                 )\n             max_frames = math.floor(min(max_frames, total_num_frames) / temporal_patch_size) * temporal_patch_size\n             num_frames = total_num_frames / metadata.fps * fps\n-            num_frames = min(min(max(num_frames, min_frames), max_frames), total_num_frames)\n+            num_frames = min(max(num_frames, min_frames), max_frames, total_num_frames)\n             num_frames = math.floor(num_frames / temporal_patch_size) * temporal_patch_size\n \n         if num_frames > total_num_frames:"
        },
        {
            "sha": "8a70a1a6858418f6f0a801fe4996234514aaa43e",
            "filename": "src/transformers/models/qwen3_vl/video_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -164,7 +164,7 @@ def sample_frames(\n                     \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n                 )\n             num_frames = int(total_num_frames / metadata.fps * fps)\n-            num_frames = min(min(max(num_frames, self.min_frames), self.max_frames), total_num_frames)\n+            num_frames = min(max(num_frames, self.min_frames), self.max_frames, total_num_frames)\n \n         if num_frames is None:\n             num_frames = min(max(total_num_frames, self.min_frames), self.max_frames)"
        },
        {
            "sha": "008cd020f5282ba638566d224742c323252d41e8",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -230,7 +230,6 @@ def __init__(self, config: SwitchTransformersConfig, is_sparse=False):\n     def forward(self, hidden_states, **kwargs):\n         forwarded_states = self.layer_norm(hidden_states)\n         forwarded_states = self.mlp(forwarded_states)\n-        forwarded_states = forwarded_states\n         output = hidden_states + self.dropout(forwarded_states)\n         return output\n "
        },
        {
            "sha": "ccb1fe739bd08112466090b385b353420eeabef1",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -250,7 +250,6 @@ def __init__(self, config: SwitchTransformersConfig, is_sparse=False):\n     def forward(self, hidden_states, **kwargs):\n         forwarded_states = self.layer_norm(hidden_states)\n         forwarded_states = self.mlp(forwarded_states)\n-        forwarded_states = forwarded_states\n         output = hidden_states + self.dropout(forwarded_states)\n         return output\n "
        },
        {
            "sha": "d55b2ab66564a870db23e252716228db84ac1df1",
            "filename": "src/transformers/models/video_llama_3/video_processing_video_llama_3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -163,7 +163,7 @@ def sample_frames(\n                 )\n             max_frames = math.floor(min(max_frames, total_num_frames) / temporal_patch_size) * temporal_patch_size\n             num_frames = total_num_frames / metadata.fps * fps\n-            num_frames = min(min(max(num_frames, min_frames), max_frames), total_num_frames)\n+            num_frames = min(max(num_frames, min_frames), max_frames, total_num_frames)\n             num_frames = math.floor(num_frames / temporal_patch_size) * temporal_patch_size\n \n         if num_frames > total_num_frames:"
        },
        {
            "sha": "51d4203fcd567b50243d7caf7bd3e82bc743b6c2",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -86,10 +86,10 @@ def decode_spans(\n \n \n def select_starts_ends(\n-    start,\n-    end,\n-    p_mask,\n-    attention_mask,\n+    start: np.ndarray,\n+    end: np.ndarray,\n+    p_mask: np.ndarray,\n+    attention_mask: np.ndarray,\n     min_null_score=1000000,\n     top_k=1,\n     handle_impossible_answer=False,"
        },
        {
            "sha": "811b9c178e934a02b73b2ae5b9544c467b8e28f4",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -514,7 +514,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "fbc2febb8af2ecb875269ad52cfdbd5696151bcd",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -270,7 +270,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "f107cbefcdd5d0d5de2c84864258c992ab5c0e07",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -275,7 +275,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "095b912865758d34a017cf33b8c09701b25203b2",
            "filename": "tests/models/fsmt/test_modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -414,7 +414,7 @@ def _assert_tensors_equal(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         if len(prefix) > 0:\n             prefix = f\"{prefix}: \""
        },
        {
            "sha": "2a17cb4d8a4178bb035515d3eedeaaf0cfd21cc5",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -476,7 +476,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "0ec3f6e610078871a419aae695d97705073a502b",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -346,7 +346,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "797ecda798ebd8b431f3b6a96d61cb4d1927bbaa",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -379,7 +379,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "53bcc3e771620259bb9d477981b7ebe7773f7017",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -521,7 +521,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "607e6c1912463c66d7494855a3129c54bf28800b",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -341,7 +341,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "327e55f38942e9cb5511873825ba990635910e65",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -300,7 +300,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "34e0828fd6dd02e92e93082c6957150c818e6b6d",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -545,7 +545,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "e76b5e02e634504281772fd262e2b546d8549406",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -330,7 +330,7 @@ def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()\n         if a.numel() > 100:"
        },
        {
            "sha": "3349fc0dc9fb950ed12f041a44aa0fce14ca8a4e",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -75,7 +75,7 @@ def _assert_tensors_equal(a, b, atol=1e-12, prefix=\"\"):\n     try:\n         if torch.allclose(a, b, atol=atol):\n             return True\n-        raise\n+        raise Exception\n     except Exception:\n         msg = f\"{a} != {b}\"\n         if prefix:"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 21,
        "deletions": 28
    }
}