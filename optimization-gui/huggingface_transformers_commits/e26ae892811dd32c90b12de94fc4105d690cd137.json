{
    "author": "zucchini-nlp",
    "message": "[docs] update cache docs with new info (#38775)\n\n* update docs with new info\n\n* Update docs/source/en/kv_cache.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "e26ae892811dd32c90b12de94fc4105d690cd137",
    "files": [
        {
            "sha": "14a0d4901d704532f2cdf6efb226c005cd7d5fad",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e26ae892811dd32c90b12de94fc4105d690cd137/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e26ae892811dd32c90b12de94fc4105d690cd137/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=e26ae892811dd32c90b12de94fc4105d690cd137",
            "patch": "@@ -261,7 +261,9 @@ A cache can also work in iterative generation settings where there is back-and-f\n \n For iterative generation with a cache, start by initializing an empty cache class and then you can feed in your new prompts. Keep track of dialogue history with a [chat template](./chat_templating).\n \n-The example below demonstrates how to use a cache for iterative generation.\n+The following example demonstrates [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). If youâ€™re using a different chat-style model, [`~PreTrainedTokenizer.apply_chat_template`] may process messages differently. It might cut out important tokens depending on how the Jinja template is written.\n+\n+For example, some models use special `<think> ... </think>` tokens during reasoning. These could get lost during re-encoding, causing indexing issues. You might need to manually remove or adjust extra tokens from the completions to keep things stable.\n \n ```py\n import torch\n@@ -281,7 +283,6 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n \n past_key_values = DynamicCache()\n-max_cache_length = past_key_values.get_max_length()\n \n messages = []\n for prompt in user_prompts:"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 3,
        "deletions": 2
    }
}