{
    "author": "vasqu",
    "message": "Mamba2 conversion script for original models (#32580)\n\n* first attempt at allowing both conversions from codestral and from the original mamba ssm\r\n\r\n* allow fp16, seems default for mamba2\r\n\r\n* dtype fix\r\n\r\n* simplify codestral check, dont overwrite pad/eos/bos when codestral\r\n\r\n* change file -> directory\r\n\r\n* use path join to be safe\r\n\r\n* style\r\n\r\n* apply code review\r\n- add util mamba2 tokenizer (gptneox with left padding)\r\n- add models dict\r\n\r\n* fix copies\r\n\r\n* add tokenizer to docs\r\n\r\n* empty commit to check for weird err\r\n\r\n* make conversion user dependent on model type, defaults for original paper models\r\n\r\n* small comment nit\r\n\r\n* remove norm_before_gate in conversion\r\n\r\n* simplify model dict by using shared keys directly + remove unnecessary attributes\r\n\r\n* fix tokenization: remove separate mamba2 tokenizer, add padding option as kwarg to gptneox one and reuse it for the conversion script\r\n\r\n* simplify even further as we pass padding side via **kwargs already",
    "sha": "92a75ff6b15837767247f2a2d7afca67b4249d5e",
    "files": [
        {
            "sha": "f68e9bd4904b2035cdc7e73c68db67ab8ac8f864",
            "filename": "src/transformers/models/mamba2/convert_mamba2_ssm_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 141,
            "deletions": 17,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/92a75ff6b15837767247f2a2d7afca67b4249d5e/src%2Ftransformers%2Fmodels%2Fmamba2%2Fconvert_mamba2_ssm_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/92a75ff6b15837767247f2a2d7afca67b4249d5e/src%2Ftransformers%2Fmodels%2Fmamba2%2Fconvert_mamba2_ssm_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fconvert_mamba2_ssm_checkpoint_to_pytorch.py?ref=92a75ff6b15837767247f2a2d7afca67b4249d5e",
            "patch": "@@ -15,55 +15,179 @@\n \"\"\"This script can be used to convert checkpoints provided in the `mamba2_ssm` library into the format provided in HuggingFace `transformers`. It depends on the `mamba2_ssm` package to be installed.\"\"\"\n \n import argparse\n+import json\n+from functools import partial\n+from os import path\n+from typing import Dict, Optional\n \n import torch\n from safetensors import safe_open\n+from safetensors.torch import save_model\n \n-from transformers import LlamaTokenizerFast, Mamba2Config, Mamba2ForCausalLM\n+from transformers import GPTNeoXTokenizerFast, LlamaTokenizerFast, Mamba2Config, Mamba2ForCausalLM\n \n \n-def convert_mamba2_checkpoint_file_to_huggingface_model_file(\n-    mamba2_checkpoint_path: str, tokenizer_model_path: str, output_dir: str\n-) -> None:\n-    hf_config = Mamba2Config()\n-    hf_model = Mamba2ForCausalLM(hf_config)\n+def load_state_dict_from_safetensors(mamba2_checkpoint_path: str, ckpt_name: str) -> Dict[str, torch.Tensor]:\n     # Load weights and config from paths\n     original_state_dict = {}\n-    with safe_open(mamba2_checkpoint_path, framework=\"pt\") as f:\n+    with safe_open(path.join(mamba2_checkpoint_path, ckpt_name), framework=\"pt\") as f:\n         for k in f.keys():\n             newk = k.removeprefix(\"model.\")\n             original_state_dict[newk] = f.get_tensor(k).clone()\n+    return original_state_dict\n+\n+\n+def load_state_dict_from_torch(mamba2_checkpoint_path: str, ckpt_name: str) -> Dict[str, torch.Tensor]:\n+    return torch.load(path.join(mamba2_checkpoint_path, ckpt_name), map_location=\"cpu\")\n+\n+\n+def convert_ssm_config_to_hf_config(config_ssm: Dict, mamba2_model_dict: Dict) -> Mamba2Config:\n+    \"\"\"Convert a Mamba2Config from mamba_ssm to a Mamba2Config from here.\"\"\"\n+    hf_config = Mamba2Config()\n+\n+    # Switch to a different dict depending on model type\n+    config_dict = mamba2_model_dict\n+\n+    # Set important values from config and recalculate other resulting entries\n+    hf_config.hidden_size = config_ssm[config_dict[\"hidden_size\"]]\n+    hf_config.num_heads = (hf_config.hidden_size * hf_config.expand) // hf_config.head_dim\n+    hf_config.num_hidden_layers = config_ssm[config_dict[\"num_hidden_layers\"]]\n+    hf_config.n_groups = config_ssm.get(config_dict[\"n_groups\"], 1)\n+    hf_config.tie_word_embeddings = config_ssm[\"tie_embeddings\"]\n+    hf_config.bos_token_id = config_dict[\"bos_token_id\"]\n+    hf_config.pad_token_id = config_dict[\"pad_token_id\"]\n+    hf_config.eos_token_id = config_dict[\"eos_token_id\"]\n+\n+    # Padded vocab size, mostly of 16 but 32 is also very common in different models\n+    vocab_size = config_ssm[\"vocab_size\"]\n+    pad_vocab_size_multiple = config_ssm[\"pad_vocab_size_multiple\"]\n+    if (vocab_size % pad_vocab_size_multiple) != 0:\n+        vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n+    hf_config.vocab_size = vocab_size\n+\n+    return hf_config\n+\n+\n+def load_and_save_tokenizer(\n+    mamba2_model_type: str,\n+    output_dir: str,\n+    tokenizer_model_path: Optional[str] = None,\n+) -> None:\n+    tokenizer = None\n+\n+    # Load tokenizer\n+    if tokenizer_model_path is not None and mamba2_model_type == \"codestral\":\n+        tokenizer_class = LlamaTokenizerFast\n+        tokenizer = tokenizer_class(tokenizer_model_path, legacy=False, from_slow=True)\n+    elif mamba2_model_type == \"mamba_ssm\":\n+        tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"state-spaces/mamba-130m-hf\", padding_side=\"left\")\n+\n+    # Save tokenizer\n+    if tokenizer is not None:\n+        tokenizer.save_pretrained(output_dir)\n \n+\n+_MAMBA2_MODELS_DICT = {\n+    \"codestral\": {\n+        \"hidden_size\": \"dim\",\n+        \"num_hidden_layers\": \"n_layers\",\n+        \"n_groups\": \"n_groups\",\n+        \"bos_token_id\": 0,\n+        \"pad_token_id\": 1,\n+        \"eos_token_id\": 2,\n+        \"config_name\": \"params.json\",\n+        \"load_state_dict\": partial(load_state_dict_from_safetensors, ckpt_name=\"consolidated.safetensors\"),\n+        \"load_and_save_tokenizer\": partial(load_and_save_tokenizer, \"codestral\"),\n+    },\n+    \"mamba_ssm\": {\n+        \"hidden_size\": \"d_model\",\n+        \"num_hidden_layers\": \"n_layer\",\n+        \"n_groups\": \"ngroups\",\n+        \"bos_token_id\": 0,\n+        \"pad_token_id\": 0,\n+        \"eos_token_id\": 0,\n+        \"config_name\": \"config.json\",\n+        \"load_state_dict\": partial(load_state_dict_from_torch, ckpt_name=\"pytorch_model.bin\"),\n+        \"load_and_save_tokenizer\": partial(load_and_save_tokenizer, \"mamba_ssm\"),\n+    },\n+}\n+\n+\n+def convert_mamba2_checkpoint_file_to_huggingface_model_file(\n+    mamba2_checkpoint_path: str,\n+    mamba2_model_type: str,\n+    precision: str,\n+    output_dir: str,\n+    tokenizer_model_path: Optional[str] = None,\n+) -> None:\n+    mamba2_model_dict = _MAMBA2_MODELS_DICT[mamba2_model_type]\n+\n+    # Load and save config based on name\n+    config_path = path.join(mamba2_checkpoint_path, mamba2_model_dict[\"config_name\"])\n+    with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n+        config = json.load(json_file)\n+    hf_config = convert_ssm_config_to_hf_config(config_ssm=config, mamba2_model_dict=mamba2_model_dict)\n+    hf_config.save_pretrained(output_dir)\n+\n+    # Load state dict of the original model and transfer to hf model\n+    original_state_dict = mamba2_model_dict[\"load_state_dict\"](mamba2_checkpoint_path=mamba2_checkpoint_path)\n+    hf_model = Mamba2ForCausalLM(hf_config)\n     hf_model.load_state_dict(original_state_dict)\n \n     # Save new model to pytorch_dump_path\n-    hf_model.to(torch.bfloat16).save_pretrained(output_dir)\n-    tokenizer_class = LlamaTokenizerFast\n-    tokenizer = tokenizer_class(tokenizer_model_path, legacy=False, from_slow=True)\n-    tokenizer.save_pretrained(output_dir)\n+    dtype = torch.float32 if precision == \"fp32\" else (torch.bfloat16 if precision == \"bf16\" else torch.float16)\n+    save_model(hf_model.to(dtype), path.join(output_dir, \"model.safetensors\"), metadata={\"format\": \"pt\"})\n+\n+    # Load and save tokenizer\n+    mamba2_model_dict[\"load_and_save_tokenizer\"](output_dir=output_dir, tokenizer_model_path=tokenizer_model_path)\n \n \n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"-i\",\n-        \"--mamba2_checkpoint_file\",\n+        \"--mamba2_checkpoint_directory\",\n         type=str,\n         required=True,\n-        help=\"Path to a `pytorch_model.bin` mamba2_ssm checkpoint file to be converted.\",\n+        help=\"Path to a directory containing the `pytorch_model.bin` or `.safetensors` mamba2_ssm checkpoint file to be converted.\",\n     )\n     parser.add_argument(\n-        \"-c\",\n-        \"--tokenizer_model_path\",\n+        \"-m\",\n+        \"--mamba2_model_type\",\n+        type=str,\n+        default=\"mamba_ssm\",\n+        const=\"mamba_ssm\",\n+        required=True,\n+        choices=(\"codestral\", \"mamba_ssm\"),\n+        help=\"The model type the conversion will be performed on. Can choose from either `codestral` or `mamba_ssm`.\",\n+    )\n+    parser.add_argument(\n+        \"-p\",\n+        \"--precision\",\n         type=str,\n+        default=\"fp16\",\n+        const=\"fp16\",\n         required=True,\n-        help=\"Path to a `config.json` file corresponding to a Mamba2Config of the original mamba2_ssm model.\",\n+        choices=(\"fp32\", \"fp16\", \"bf16\"),\n+        help=\"The precision the model will be saved in. Select from fp32, fp16 or bf16.\",\n     )\n     parser.add_argument(\n         \"-o\", \"--output_dir\", type=str, required=True, help=\"Path to directory to save the converted output model to.\"\n     )\n+    parser.add_argument(\n+        \"-t\",\n+        \"--tokenizer_model_path\",\n+        type=str,\n+        default=None,\n+        required=False,\n+        help=\"Path to a `codestral` tokenizer file.\",\n+    )\n     args = parser.parse_args()\n \n     convert_mamba2_checkpoint_file_to_huggingface_model_file(\n-        args.mamba2_checkpoint_file, args.tokenizer_model_path, args.output_dir\n+        args.mamba2_checkpoint_directory,\n+        args.mamba2_model_type,\n+        args.precision,\n+        args.output_dir,\n+        args.tokenizer_model_path,\n     )"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 141,
        "deletions": 17
    }
}