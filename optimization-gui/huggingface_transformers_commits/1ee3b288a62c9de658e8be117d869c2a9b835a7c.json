{
    "author": "ArthurZucker",
    "message": "[`from_pretrained`] Small refactor `from_pretrained`: move around unrelated stuff (#41445)\n\n* drafts\n\n* up\n\n* simplify modeling utils\n\n* more simplifications\n\n* type kwargs\n\n* up\n\n* move more accelerate related stuff\n\n* safeguarding?\n\n* nits\n\n* remove func when func is NOPE\n\n* more\n\n* nits\n\n* styling\n\n* yups\n\n* up\n\n* ups\n\n* revert\n\n* protect trainer utils iport\n\n* fix doc\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\n\n* review\n\n* update\n\n* ?\n\n* fixx\n\n* update\n\n* super small update\n\n* ups\n\n* style\n\n* this is stupid\n\n* :facepalm: well this was the issue\n\n* small nit\n\n* fix\n\n* nit\n\n* damn the missing return\n\n* one last stupid fix\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "1ee3b288a62c9de658e8be117d869c2a9b835a7c",
    "files": [
        {
            "sha": "752010f8ab6dd43585d72cc9fa0352b4cda88bf4",
            "filename": "docs/source/en/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -42,7 +42,3 @@ set this to `False`.\n ## Pushing to the Hub\n \n [[autodoc]] utils.PushToHubMixin\n-\n-## Sharded checkpoints\n-\n-[[autodoc]] modeling_utils.load_sharded_checkpoint"
        },
        {
            "sha": "11711d5529a1e4e76477f730385e9f8acef2d002",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -14,6 +14,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import copy\n+import functools\n import inspect\n import os\n import warnings\n@@ -365,6 +366,58 @@ class GenerationMixin(ContinuousMixin):\n     To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n     \"\"\"\n \n+    def adjust_generation_fn(\n+        self,\n+        generation_config,\n+        from_auto_class,\n+        from_pipeline,\n+        pretrained_model_name_or_path,\n+        cache_dir,\n+        force_download,\n+        proxies,\n+        local_files_only,\n+        token,\n+        revision,\n+        subfolder,\n+        trust_remote_code,\n+        **kwargs,\n+    ):\n+        if self.can_generate() and generation_config is not None:\n+            self.generation_config = self.generation_config.from_dict(generation_config.to_dict())\n+        elif self.can_generate() and pretrained_model_name_or_path is not None:\n+            repo_loading_kwargs = {\n+                \"cache_dir\": cache_dir,\n+                \"force_download\": force_download,\n+                \"proxies\": proxies,\n+                \"local_files_only\": local_files_only,\n+                \"token\": token,\n+                \"revision\": revision,\n+                \"subfolder\": subfolder,\n+                **kwargs,\n+            }\n+            # Load generation config\n+            try:\n+                self.generation_config = GenerationConfig.from_pretrained(\n+                    pretrained_model_name_or_path,\n+                    _from_auto=from_auto_class,\n+                    _from_pipeline=from_pipeline,\n+                    **repo_loading_kwargs,\n+                )\n+            except OSError:\n+                logger.info(\n+                    \"Generation config file not found, using a generation config created from the model config.\"\n+                )\n+                pass\n+            # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n+            if hasattr(self, \"load_custom_generate\"):\n+                try:\n+                    custom_generate = self.load_custom_generate(\n+                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **repo_loading_kwargs\n+                    )\n+                    self.generate = functools.partial(custom_generate, model=self)\n+                except OSError:  # there is no custom generate function\n+                    pass\n+\n     def load_custom_generate(\n         self,\n         pretrained_model_name_or_path: Optional[Union[str, os.PathLike]] = None,"
        },
        {
            "sha": "7d93ca494444cb25fd4ab5315cf1cdcb4b4e4773",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 157,
            "deletions": 1,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -20,15 +20,24 @@\n `find_tied_parameters` was copied from `accelerate.utils.modeling.py`\n \"\"\"\n \n+import collections\n+import inspect\n+import os\n from contextlib import contextmanager\n \n-from ..utils import is_torch_available, logging\n+from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils.quantization_config import QuantizationMethod\n+from .deepspeed import is_deepspeed_zero3_enabled\n+from .fsdp import is_fsdp_enabled\n \n \n if is_torch_available():\n     import torch\n     import torch.nn as nn\n \n+if is_accelerate_available():\n+    from accelerate import dispatch_model\n+\n \n logger = logging.get_logger(__name__)\n \n@@ -194,3 +203,150 @@ def find_tied_parameters(model: \"nn.Module\", **kwargs):\n                 tied_param_groups[param_name].append(tied_param_name)\n \n     return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]\n+\n+\n+def check_and_set_device_map(device_map):\n+    from ..modeling_utils import get_torch_context_manager_or_global_device\n+\n+    # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n+    if device_map is None and not is_deepspeed_zero3_enabled():\n+        device_in_context = get_torch_context_manager_or_global_device()\n+        if device_in_context == torch.device(\"meta\"):\n+            raise RuntimeError(\n+                \"You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\\n\"\n+                \"This is an anti-pattern as `from_pretrained` wants to load existing weights.\\nIf you want to initialize an \"\n+                \"empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`\"\n+            )\n+        device_map = device_in_context\n+\n+    # change device_map into a map if we passed an int, a str or a torch.device\n+    if isinstance(device_map, torch.device):\n+        device_map = {\"\": device_map}\n+    elif isinstance(device_map, str) and device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n+        try:\n+            device_map = {\"\": torch.device(device_map)}\n+        except RuntimeError:\n+            raise ValueError(\n+                \"When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \"\n+                f\"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}.\"\n+            )\n+    elif isinstance(device_map, int):\n+        if device_map < 0:\n+            raise ValueError(\n+                \"You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' \"\n+            )\n+        else:\n+            device_map = {\"\": device_map}\n+\n+    if device_map is not None:\n+        if is_deepspeed_zero3_enabled():\n+            raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n+        if not is_accelerate_available():\n+            raise ValueError(\n+                \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n+                \"requires `accelerate`. You can install it with `pip install accelerate`\"\n+            )\n+    return device_map\n+\n+\n+def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers):\n+    device_map_kwargs = {\n+        \"device_map\": device_map,\n+        \"offload_dir\": offload_folder,\n+        \"offload_index\": offload_index,\n+        \"offload_buffers\": offload_buffers,\n+    }\n+    if \"skip_keys\" in inspect.signature(dispatch_model).parameters:\n+        device_map_kwargs[\"skip_keys\"] = model._skip_keys_device_placement\n+    # For HQQ method we force-set the hooks for single GPU envs\n+    if (\n+        \"force_hooks\" in inspect.signature(dispatch_model).parameters\n+        and hf_quantizer is not None\n+        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ\n+    ):\n+        device_map_kwargs[\"force_hooks\"] = True\n+    if (\n+        hf_quantizer is not None\n+        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.FBGEMM_FP8\n+        and isinstance(device_map, dict)\n+        and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n+    ):\n+        device_map_kwargs[\"offload_buffers\"] = True\n+\n+    if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n+        dispatch_model(model, **device_map_kwargs)\n+\n+\n+def get_disk_only_shard_files(device_map, weight_map):\n+    \"\"\"\n+    Returns the list of shard files containing only weights offloaded to disk.\n+    \"\"\"\n+    files_content = collections.defaultdict(list)\n+    for weight_name, filename in weight_map.items():\n+        while len(weight_name) > 0 and weight_name not in device_map:\n+            weight_name = \".\".join(weight_name.split(\".\")[:-1])\n+        files_content[filename].append(device_map[weight_name])\n+\n+    return [fname for fname, devices in files_content.items() if set(devices) == {\"disk\"}]\n+\n+\n+def expand_device_map(device_map, param_names):\n+    \"\"\"\n+    Expand a device map to return the correspondence parameter name to device.\n+    \"\"\"\n+    new_device_map = {}\n+    for module, device in device_map.items():\n+        new_device_map.update(\n+            {p: device for p in param_names if p == module or p.startswith(f\"{module}.\") or module == \"\"}\n+        )\n+    return new_device_map\n+\n+\n+def accelerate_disk_offload(\n+    disk_offload_folder,\n+    checkpoint_files,\n+    device_map,\n+    checkpoint_keys,\n+    key_renaming_mapping,\n+    sharded_metadata,\n+    dtype,\n+    reverse_key_renaming_mapping,\n+):\n+    disk_only_shard_files = []\n+    if disk_offload_folder is not None:\n+        os.makedirs(disk_offload_folder, exist_ok=True)\n+    is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n+    if disk_offload_folder is None and not is_offloaded_safetensors:\n+        raise ValueError(\n+            \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\"\n+            \" for them. Alternatively, make sure you have `safetensors` installed if the model you are using\"\n+            \" offers the weights in this format.\"\n+        )\n+    if is_offloaded_safetensors:\n+        param_device_map = expand_device_map(device_map, checkpoint_keys)\n+        str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n+        if sharded_metadata is None:\n+            weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n+        else:\n+            folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n+            # Fix the weight map keys according to the key mapping\n+            weight_map = {\n+                key_renaming_mapping[k]: v\n+                for k, v in sharded_metadata[\"weight_map\"].items()\n+                if k in key_renaming_mapping\n+            }\n+            weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n+            # Find potential checkpoints containing only offloaded weights\n+            disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n+        disk_offload_index = {\n+            name: {\n+                \"safetensors_file\": file,\n+                \"weight_name\": reverse_key_renaming_mapping[name],\n+                \"dtype\": str_dtype,\n+            }\n+            for name, file in weight_map.items()\n+            if param_device_map[name] == \"disk\"\n+        }\n+    else:\n+        disk_offload_index = {}\n+    return disk_offload_index, disk_only_shard_files, is_offloaded_safetensors"
        },
        {
            "sha": "3198cff771469811c4def903869b1c61a5985a4b",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 59,
            "deletions": 2,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -12,21 +12,27 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import importlib\n+import importlib.metadata\n import inspect\n+import json\n+import os\n import re\n from typing import Any, Optional, Union\n \n from packaging import version\n \n from ..utils import (\n+    CONFIG_NAME,\n+    cached_file,\n     check_peft_version,\n+    extract_commit_hash,\n     find_adapter_config_file,\n     is_accelerate_available,\n     is_peft_available,\n     is_torch_available,\n     logging,\n )\n+from ..utils.hub import DownloadKwargs\n \n \n if is_torch_available():\n@@ -249,7 +255,7 @@ def load_adapter(\n             else:\n                 new_key = key\n \n-            if key_mapping:\n+            if key_mapping:  # TODO dynamic weight loader for adapters\n                 for pattern, replacement in key_mapping.items():\n                     new_key, n_replace = re.subn(pattern, replacement, new_key)\n                     # Early exit of the loop\n@@ -614,3 +620,54 @@ def old_delete_adapter(model, adapter_name, prefix=None):\n         if len(self.peft_config) == 0:\n             del self.peft_config\n             self._hf_peft_config_loaded = False\n+\n+\n+def maybe_load_adapters(\n+    pretrained_model_name_or_path,\n+    download_kwargs: DownloadKwargs,\n+    **adapter_kwargs,\n+):\n+    if pretrained_model_name_or_path is None or not is_peft_available():\n+        return None, pretrained_model_name_or_path\n+\n+    token = download_kwargs.get(\"token\")\n+\n+    if download_kwargs.get(\"commit_hash\") is None:\n+        resolved_config_file = cached_file(\n+            pretrained_model_name_or_path,\n+            CONFIG_NAME,\n+            cache_dir=download_kwargs.get(\"cache_dir\"),\n+            force_download=bool(download_kwargs.get(\"force_download\", False)),\n+            proxies=download_kwargs.get(\"proxies\"),\n+            local_files_only=bool(download_kwargs.get(\"local_files_only\", False)),\n+            token=token,\n+            revision=download_kwargs.get(\"revision\"),\n+            subfolder=download_kwargs.get(\"subfolder\"),\n+            _raise_exceptions_for_gated_repo=False,\n+            _raise_exceptions_for_missing_entries=False,\n+            _raise_exceptions_for_connection_errors=False,\n+        )\n+        download_kwargs[\"commit_hash\"] = extract_commit_hash(resolved_config_file, None)\n+\n+    _adapter_model_path = adapter_kwargs.pop(\"_adapter_model_path\", None)\n+\n+    if _adapter_model_path is None:\n+        _adapter_model_path = find_adapter_config_file(\n+            pretrained_model_name_or_path,\n+            cache_dir=download_kwargs.get(\"cache_dir\"),\n+            force_download=bool(download_kwargs.get(\"force_download\", False)),\n+            proxies=download_kwargs.get(\"proxies\"),\n+            token=token,\n+            revision=download_kwargs.get(\"revision\"),\n+            local_files_only=bool(download_kwargs.get(\"local_files_only\", False)),\n+            subfolder=download_kwargs.get(\"subfolder\", \"\"),\n+            _commit_hash=download_kwargs.get(\"commit_hash\"),\n+            **adapter_kwargs,\n+        )\n+\n+    if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):\n+        with open(_adapter_model_path, \"r\", encoding=\"utf-8\") as f:\n+            _adapter_model_path = pretrained_model_name_or_path\n+            pretrained_model_name_or_path = json.load(f)[\"base_model_name_or_path\"]\n+\n+    return _adapter_model_path, pretrained_model_name_or_path"
        },
        {
            "sha": "9e02c34ffa91eb9218d85203f2c347cd8a88d00a",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 68,
            "deletions": 47,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -38,59 +38,80 @@\n     from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n \n \n-def initialize_tensor_parallelism(tp_plan, tp_size=None):\n+def initialize_tensor_parallelism(tp_plan, tp_size=None, device_mesh=None, device_map=None):\n     r\"\"\"\n     Sets up the device mesh and initialized the backend for tensor parallelism.\n     This function is called when the model is loaded and the TP plan is set to 'auto'.\n     \"\"\"\n-    if tp_plan is None:\n-        return None, None, None\n+    if tp_size is not None and tp_plan is None:\n+        raise ValueError(\"tp_plan has to be set when tp_size is passed.\")\n+    if tp_plan is not None and tp_plan != \"auto\":\n+        # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n+        raise ValueError(f\"tp_plan supports 'auto' only for now but got {tp_plan}.\")\n+    if tp_plan is not None and device_map is not None:\n+        raise ValueError(\"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\")\n+    if device_mesh is None:\n+        if not is_torch_greater_or_equal(\"2.5\"):\n+            raise OSError(\"Tensor parallel is only supported for `torch>=2.5`.\")\n+\n+        # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n+        device_type = torch._C._get_accelerator().type\n+        if device_type == \"mps\":\n+            device_type = \"cpu\"  # fallback\n+        current_device = getattr(torch, device_type)\n+        if not torch.distributed.is_initialized():\n+            try:\n+                rank = int(os.environ[\"RANK\"])\n+                local_rank = int(os.environ[\"LOCAL_RANK\"])\n+                world_size = int(os.environ[\"WORLD_SIZE\"])\n+\n+                backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"xccl\", \"hpu\": \"hccl\"}\n+                backend = backend_map.get(device_type)\n+                if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", \"0\")):\n+                    backend = \"ccl\"\n+                if device_type == \"xpu\" and not is_torch_greater_or_equal(\"2.8\", accept_dev=True):\n+                    backend = \"ccl\"\n+\n+                torch.distributed.init_process_group(backend=backend, rank=rank, world_size=world_size)\n+                current_device = getattr(torch, device_type)\n+                if device_type != \"cpu\":\n+                    current_device.set_device(local_rank)\n+\n+            except Exception as e:\n+                raise OSError(\n+                    \"We tried to initialize torch.distributed for you, but it failed. Make \"\n+                    \"sure you init torch distributed in your script to use `tp_plan='auto'`.\"\n+                ) from e\n+\n+        if device_type != \"cpu\":\n+            current_device.set_device(int(os.environ[\"LOCAL_RANK\"]))\n+            index = current_device.current_device()\n+            tp_device = torch.device(device_type, index)\n+            device_map = tp_device\n+            # Silence output for non-primary ranks\n+            if index > 0:\n+                import sys\n+\n+                sys.stdout = open(os.devnull, \"w\")\n+                sys.stderr = open(os.devnull, \"w\")\n \n-    if not is_torch_greater_or_equal(\"2.5\"):\n-        raise OSError(\"Tensor parallel is only supported for `torch>=2.5`.\")\n+        else:\n+            tp_device = torch.device(device_type)\n+            device_map = device_type or {}\n+\n+        tp_size = tp_size if tp_size is not None else torch.distributed.get_world_size()\n+        device_mesh = torch.distributed.init_device_mesh(tp_device.type, (tp_size,))\n+    else:\n+        if device_mesh.ndim > 1:\n+            if \"tp\" not in device_mesh.mesh_dim_names:\n+                raise ValueError(\n+                    \"When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. \"\n+                    \"Please provide a valid `device_mesh`.\"\n+                )\n+            device_mesh = device_mesh[\"tp\"]\n+        tp_size = device_mesh.size()\n+        device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n \n-    # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n-    device_type = torch._C._get_accelerator().type\n-    current_device = getattr(torch, device_type)\n-    if not torch.distributed.is_initialized():\n-        try:\n-            rank = int(os.environ[\"RANK\"])\n-            local_rank = int(os.environ[\"LOCAL_RANK\"])\n-            world_size = int(os.environ[\"WORLD_SIZE\"])\n-\n-            backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"xccl\", \"hpu\": \"hccl\"}\n-            backend = backend_map.get(device_type)\n-            if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", \"0\")):\n-                backend = \"ccl\"\n-            if device_type == \"xpu\" and not is_torch_greater_or_equal(\"2.8\", accept_dev=True):\n-                backend = \"ccl\"\n-\n-            torch.distributed.init_process_group(backend=backend, rank=rank, world_size=world_size)\n-            current_device = getattr(torch, device_type)\n-            if device_type != \"cpu\":\n-                current_device.set_device(local_rank)\n-\n-        except Exception as e:\n-            raise OSError(\n-                \"We tried to initialize torch.distributed for you, but it failed. Make \"\n-                \"sure you init torch distributed in your script to use `tp_plan='auto'`.\"\n-            ) from e\n-\n-    if device_type != \"cpu\":\n-        current_device.set_device(int(os.environ[\"LOCAL_RANK\"]))\n-    index = current_device.current_device() if device_type != \"cpu\" else None\n-    tp_device = torch.device(device_type, index)\n-\n-    # Silence output for non-primary ranks\n-    if index is not None and index > 0:\n-        import sys\n-\n-        sys.stdout = open(os.devnull, \"w\")\n-        sys.stderr = open(os.devnull, \"w\")\n-\n-    device_map = tp_device\n-    tp_size = tp_size if tp_size is not None else torch.distributed.get_world_size()\n-    device_mesh = torch.distributed.init_device_mesh(tp_device.type, (tp_size,))\n     return tp_device, device_map, device_mesh, tp_size\n \n "
        },
        {
            "sha": "2821b60728a0f0ae084580d1f14ed73442884ee9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 157,
            "deletions": 460,
            "changes": 617,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -39,7 +39,6 @@\n from huggingface_hub import split_torch_state_dict_into_shards\n from packaging import version\n from safetensors import safe_open\n-from safetensors.torch import load_file as safe_load_file\n from safetensors.torch import save_file as safe_save_file\n from torch import Tensor, nn\n from torch.distributions import constraints\n@@ -50,13 +49,20 @@\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled, is_fsdp_enabled\n-from .integrations.accelerate import find_tied_parameters, init_empty_weights\n+from .integrations.accelerate import (\n+    accelerate_disk_offload,\n+    accelerate_dispatch,\n+    check_and_set_device_map,\n+    find_tied_parameters,\n+    init_empty_weights,\n+)\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n from .integrations.eager_paged import eager_paged_attention_forward\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flash_paged import paged_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.hub_kernels import is_kernel, load_and_register_attn_kernel\n+from .integrations.peft import maybe_load_adapters\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.sdpa_paged import sdpa_attention_paged_forward\n from .integrations.tensor_parallel import (\n@@ -78,7 +84,6 @@\n from .utils import (\n     ADAPTER_SAFE_WEIGHTS_NAME,\n     ADAPTER_WEIGHTS_NAME,\n-    CONFIG_NAME,\n     DUMMY_INPUTS,\n     SAFE_WEIGHTS_INDEX_NAME,\n     SAFE_WEIGHTS_NAME,\n@@ -91,15 +96,13 @@\n     check_torch_load_is_safe,\n     copy_func,\n     download_url,\n-    extract_commit_hash,\n     has_file,\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n     is_kernels_available,\n     is_offline_mode,\n-    is_peft_available,\n     is_remote_url,\n     is_torch_flex_attn_available,\n     is_torch_greater_or_equal,\n@@ -110,7 +113,7 @@\n     logging,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n-from .utils.hub import create_and_tag_model_card, get_checkpoint_shard_files\n+from .utils.hub import DownloadKwargs, create_and_tag_model_card, get_checkpoint_shard_files\n from .utils.import_utils import (\n     ENV_VARS_TRUE_VALUES,\n     is_huggingface_hub_greater_or_equal,\n@@ -122,7 +125,7 @@\n \n \n if is_accelerate_available():\n-    from accelerate import dispatch_model, infer_auto_device_map\n+    from accelerate import infer_auto_device_map\n     from accelerate.hooks import add_hook_to_module\n     from accelerate.utils import (\n         check_tied_parameters_on_same_device,\n@@ -134,8 +137,6 @@\n     )\n     from accelerate.utils.modeling import get_state_dict_from_offload\n \n-if is_peft_available():\n-    from .utils import find_adapter_config_file\n \n _torch_distributed_available = torch.distributed.is_available()\n _is_dtensor_available = _torch_distributed_available and is_torch_greater_or_equal(\"2.5\")\n@@ -276,6 +277,27 @@ def _wrapper(*args, **kwargs):\n     return _wrapper\n \n \n+def get_keep_in_fp32_regex(model, hf_quantizer, dtype):\n+    # Find fp32 modules if needed\n+    keep_in_fp32_modules = []\n+    # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n+    # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n+    # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n+    if model._keep_in_fp32_modules is not None and (\n+        dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+    ):\n+        keep_in_fp32_modules.extend(model._keep_in_fp32_modules)\n+\n+    if model._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n+        keep_in_fp32_modules.extend(model._keep_in_fp32_modules_strict)\n+\n+    keep_in_fp32_regex = None\n+    if keep_in_fp32_modules:\n+        # We need to match exact layers, so we add either `.` on each side, or start/end of string\n+        keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n+    return keep_in_fp32_regex\n+\n+\n def get_torch_context_manager_or_global_device():\n     \"\"\"\n     Test if a device context manager is currently in use, or if it is not the case, check if the default device\n@@ -368,81 +390,6 @@ def get_state_dict_dtype(state_dict):\n     return next(state_dict.values()).dtype\n \n \n-def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n-    \"\"\"\n-    This is the same as\n-    [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)\n-    but for a sharded checkpoint.\n-\n-    This load is performed efficiently: each checkpoint shard is loaded one by one in RAM and deleted after being\n-    loaded in the model.\n-\n-    Args:\n-        model (`torch.nn.Module`): The model in which to load the checkpoint.\n-        folder (`str` or `os.PathLike`): A path to a folder containing the sharded checkpoint.\n-        strict (`bool`, *optional*, defaults to `True`):\n-            Whether to strictly enforce that the keys in the model state dict match the keys in the sharded checkpoint.\n-        prefer_safe (`bool`, *optional*, defaults to `False`):\n-            If both safetensors and PyTorch save files are present in checkpoint and `prefer_safe` is True, the\n-            safetensors files will be loaded. Otherwise, PyTorch files are always loaded when possible.\n-\n-    Returns:\n-        `NamedTuple`: A named tuple with `missing_keys` and `unexpected_keys` fields\n-            - `missing_keys` is a list of str containing the missing keys\n-            - `unexpected_keys` is a list of str containing the unexpected keys\n-    \"\"\"\n-    # Load the index\n-    index_file = os.path.join(folder, WEIGHTS_INDEX_NAME)\n-    safe_index_file = os.path.join(folder, SAFE_WEIGHTS_INDEX_NAME)\n-\n-    index_present = os.path.isfile(index_file)\n-    safe_index_present = os.path.isfile(safe_index_file)\n-\n-    if not index_present and not safe_index_present:\n-        filenames = (WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_INDEX_NAME)\n-        raise ValueError(f\"Can't find a checkpoint index ({' or '.join(filenames)}) in {folder}.\")\n-\n-    load_safe = safe_index_present and (prefer_safe or not index_present)\n-    load_index = safe_index_file if load_safe else index_file\n-\n-    with open(load_index, \"r\", encoding=\"utf-8\") as f:\n-        index = json.load(f)\n-\n-    shard_files = list(set(index[\"weight_map\"].values()))\n-\n-    # If strict=True, error before loading any of the state dicts.\n-    loaded_keys = index[\"weight_map\"].keys()\n-    model_keys = model.state_dict().keys()\n-    missing_keys = [key for key in model_keys if key not in loaded_keys]\n-    unexpected_keys = [key for key in loaded_keys if key not in model_keys]\n-    if strict and (len(missing_keys) > 0 or len(unexpected_keys) > 0):\n-        error_message = f\"Error(s) in loading state_dict for {model.__class__.__name__}\"\n-        if len(missing_keys) > 0:\n-            str_missing_keys = \",\".join([f'\"{k}\"' for k in missing_keys])\n-            error_message += f\"\\nMissing key(s): {str_missing_keys}.\"\n-        if len(unexpected_keys) > 0:\n-            str_unexpected_keys = \",\".join([f'\"{k}\"' for k in unexpected_keys])\n-            error_message += f\"\\nMissing key(s): {str_unexpected_keys}.\"\n-        raise RuntimeError(error_message)\n-\n-    if load_safe:\n-        loader = safe_load_file\n-    else:\n-        check_torch_load_is_safe()\n-        loader = partial(torch.load, map_location=\"cpu\", weights_only=True)\n-\n-    for shard_file in shard_files:\n-        state_dict = loader(os.path.join(folder, shard_file))\n-        model.load_state_dict(state_dict, strict=False)\n-\n-        # Make sure memory is freed before we load the next state dict.\n-        del state_dict\n-        gc.collect()\n-\n-    # Return the same thing as PyTorch load_state_dict function.\n-    return torch.nn.modules.module._IncompatibleKeys(missing_keys, unexpected_keys)\n-\n-\n str_to_torch_dtype = {\n     \"BOOL\": torch.bool,\n     \"U8\": torch.uint8,\n@@ -906,25 +853,36 @@ def update_key_name(keys):\n \n def _get_resolved_checkpoint_files(\n     pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n-    subfolder: str,\n     variant: Optional[str],\n     gguf_file: Optional[str],\n     use_safetensors: Optional[bool],\n-    cache_dir: str,\n-    force_download: bool,\n-    proxies: Optional[dict[str, str]],\n-    local_files_only: bool,\n-    token: Optional[Union[str, bool]],\n+    download_kwargs: DownloadKwargs,\n     user_agent: dict,\n-    revision: str,\n-    commit_hash: Optional[str],\n     is_remote_code: bool,  # Because we can't determine this inside this function, we need it to be passed in\n     transformers_explicit_filename: Optional[str] = None,\n ) -> tuple[Optional[list[str]], Optional[dict]]:\n     \"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\n     checkpoints are sharded.\n     This function will download the data if necessary.\n     \"\"\"\n+    cache_dir = download_kwargs.get(\"cache_dir\")\n+    force_download = download_kwargs.get(\"force_download\", False)\n+    proxies = download_kwargs.get(\"proxies\")\n+    local_files_only = download_kwargs.get(\"local_files_only\", False)\n+    token = download_kwargs.get(\"token\")\n+    revision = download_kwargs.get(\"revision\") or \"main\"\n+    subfolder = download_kwargs.get(\"subfolder\", \"\")\n+    commit_hash = download_kwargs.get(\"commit_hash\")\n+    if transformers_explicit_filename is not None:\n+        if not transformers_explicit_filename.endswith(\".safetensors\") and not transformers_explicit_filename.endswith(\n+            \".safetensors.index.json\"\n+        ):\n+            raise ValueError(\n+                \"The transformers file in the config seems to be incorrect: it is neither a safetensors file \"\n+                \"(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \"\n+                f\"{transformers_explicit_filename}\"\n+            )\n+\n     is_sharded = False\n \n     if pretrained_model_name_or_path is not None and gguf_file is None:\n@@ -4212,6 +4170,31 @@ def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n \n         return init_contexts\n \n+    def set_use_kernels(self, use_kernels, kernel_config):\n+        if use_kernels:\n+            if not is_kernels_available():\n+                raise ValueError(\n+                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n+                )\n+            from kernels import use_kernel_mapping\n+\n+            if kernel_config is not None and isinstance(kernel_config, KernelConfig):\n+                # This will make sure the mapping is valid, and the layers are registered in the model\n+                kernel_config.sanitize_kernel_mapping(self)\n+\n+                # This will create a compatible mapping for the model with the kernels library\n+                kernel_config.create_compatible_mapping(self)\n+\n+                # This is a context manager to override the default kernel mapping\n+                # We are calling kernelize inside this context manager using the use_kernels setter\n+                with use_kernel_mapping(kernel_config.kernel_mapping):\n+                    self.use_kernels = True\n+            # We use the default kernel mapping in .integrations.hub_kernels\n+            else:\n+                self.use_kernels = True\n+        else:\n+            self.use_kernels = False\n+\n     @classmethod\n     @restore_default_dtype\n     def from_pretrained(\n@@ -4431,7 +4414,6 @@ def from_pretrained(\n         state_dict = kwargs.pop(\"state_dict\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         output_loading_info = kwargs.pop(\"output_loading_info\", False)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n         dtype = kwargs.pop(\"dtype\", None)\n@@ -4457,7 +4439,7 @@ def from_pretrained(\n         kernel_config = kwargs.pop(\"kernel_config\", None)\n \n         key_mapping = kwargs.pop(\"key_mapping\", None)\n-        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model\n+        # Load models with key mapping\n         if key_mapping is None and any(\n             allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n         ):\n@@ -4467,32 +4449,31 @@ def from_pretrained(\n             tp_plan = \"auto\"\n \n         # Not used anymore -- remove them from the kwargs\n-        _ = kwargs.pop(\"mirror\", None)\n-        _ = kwargs.pop(\"_fast_init\", None)\n-        _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n-        _ = kwargs.pop(\"from_tf\", None)\n-        _ = kwargs.pop(\"from_flax\", None)\n-        _ = kwargs.pop(\"offload_state_dict\", None)\n+        for name in [\"mirror\", \"_fast_init\", \"low_cpu_mem_usage\", \"from_tf\", \"from_flax\", \"offload_state_dict\"]:\n+            _ = kwargs.pop(name, None)\n \n         # For BC on torch_dtype argument\n         if torch_dtype is not None:\n-            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n-            # If both kwargs are provided, use `dtype`\n             dtype = dtype if dtype is not None else torch_dtype\n \n+        if is_offline_mode() and not local_files_only:\n+            local_files_only = True\n+\n+        download_kwargs = {\n+            \"cache_dir\": cache_dir,\n+            \"force_download\": force_download,\n+            \"proxies\": proxies,\n+            \"local_files_only\": local_files_only,\n+            \"token\": token,\n+            \"revision\": revision,\n+            \"subfolder\": subfolder,\n+        }\n+        download_kwargs_with_commit = {**download_kwargs, \"commit_hash\": commit_hash}\n+\n         if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):\n             raise ValueError(\n                 \"`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies.\"\n             )\n-        if tp_size is not None and tp_plan is None:\n-            raise ValueError(\"tp_plan has to be set when tp_size is passed.\")\n-        if tp_plan is not None and tp_plan != \"auto\":\n-            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n-            raise ValueError(f\"tp_plan supports 'auto' only for now but got {tp_plan}.\")\n-        if tp_plan is not None and device_map is not None:\n-            raise ValueError(\n-                \"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\"\n-            )\n \n         if device_map == \"auto\" and int(os.environ.get(\"WORLD_SIZE\", \"0\")):\n             logger.info(\n@@ -4501,221 +4482,84 @@ def from_pretrained(\n                 \": PartialState().process_index} where PartialState comes from accelerate library\"\n             )\n \n-        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n-        # `device_map` pointing to the correct device\n-        if tp_plan is not None:\n-            if device_mesh is None:\n-                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)\n-            else:\n-                if device_mesh.ndim > 1:\n-                    if \"tp\" not in device_mesh.mesh_dim_names:\n-                        raise ValueError(\n-                            \"When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. \"\n-                            \"Please provide a valid `device_mesh`.\"\n-                        )\n-                    device_mesh = device_mesh[\"tp\"]\n-                tp_size = device_mesh.size()\n-                device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n-\n-            if tp_size is None:\n-                tp_size = torch.distributed.get_world_size()\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n+        if tp_plan is not None or tp_size is not None:  # TP warnings, and setup\n+            tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(\n+                tp_plan, tp_size=tp_size, device_mesh=device_mesh, device_map=device_map\n             )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n-        if token is not None and adapter_kwargs is not None and \"token\" not in adapter_kwargs:\n-            adapter_kwargs[\"token\"] = token\n \n         if gguf_file is not None and not is_accelerate_available():\n             raise ValueError(\"accelerate is required when loading a GGUF file `pip install accelerate`.\")\n \n-        if commit_hash is None:\n-            if not isinstance(config, PreTrainedConfig):\n-                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n-                resolved_config_file = cached_file(\n-                    pretrained_model_name_or_path,\n-                    CONFIG_NAME,\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    proxies=proxies,\n-                    local_files_only=local_files_only,\n-                    token=token,\n-                    revision=revision,\n-                    subfolder=subfolder,\n-                    _raise_exceptions_for_gated_repo=False,\n-                    _raise_exceptions_for_missing_entries=False,\n-                    _raise_exceptions_for_connection_errors=False,\n-                )\n-                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n-            else:\n-                commit_hash = getattr(config, \"_commit_hash\", None)\n-\n-        if is_peft_available():\n-            _adapter_model_path = adapter_kwargs.pop(\"_adapter_model_path\", None)\n-\n-            if _adapter_model_path is None:\n-                _adapter_model_path = find_adapter_config_file(\n-                    pretrained_model_name_or_path,\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    proxies=proxies,\n-                    local_files_only=local_files_only,\n-                    _commit_hash=commit_hash,\n-                    **adapter_kwargs,\n-                )\n-            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):\n-                with open(_adapter_model_path, \"r\", encoding=\"utf-8\") as f:\n-                    _adapter_model_path = pretrained_model_name_or_path\n-                    pretrained_model_name_or_path = json.load(f)[\"base_model_name_or_path\"]\n-        else:\n-            _adapter_model_path = None\n-\n-        # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n-        if device_map is None and not is_deepspeed_zero3_enabled():\n-            device_in_context = get_torch_context_manager_or_global_device()\n-            if device_in_context == torch.device(\"meta\"):\n-                raise RuntimeError(\n-                    \"You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\\n\"\n-                    \"This is an anti-pattern as `from_pretrained` wants to load existing weights.\\nIf you want to initialize an \"\n-                    \"empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`\"\n-                )\n-            device_map = device_in_context\n+        if adapter_kwargs is None:\n+            adapter_kwargs = {}\n \n-        # change device_map into a map if we passed an int, a str or a torch.device\n-        if isinstance(device_map, torch.device):\n-            device_map = {\"\": device_map}\n-        elif isinstance(device_map, str) and device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n-            try:\n-                device_map = {\"\": torch.device(device_map)}\n-            except RuntimeError:\n-                raise ValueError(\n-                    \"When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \"\n-                    f\"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}.\"\n-                )\n-        elif isinstance(device_map, int):\n-            if device_map < 0:\n-                raise ValueError(\n-                    \"You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' \"\n-                )\n-            else:\n-                device_map = {\"\": device_map}\n-\n-        if device_map is not None:\n-            if is_deepspeed_zero3_enabled():\n-                raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n-            if not is_accelerate_available():\n-                raise ValueError(\n-                    \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n-                    \"requires `accelerate`. You can install it with `pip install accelerate`\"\n-                )\n+        _adapter_model_path, pretrained_model_name_or_path = maybe_load_adapters(\n+            pretrained_model_name_or_path,\n+            download_kwargs_with_commit,\n+            **adapter_kwargs,\n+        )\n+        device_map = check_and_set_device_map(device_map)  # warn, error and fix the device map\n \n         user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline\n \n-        if is_offline_mode() and not local_files_only:\n-            logger.info(\"Offline mode: forcing local_files_only=True\")\n-            local_files_only = True\n-\n         # Load config if we don't provide a configuration\n         if not isinstance(config, PreTrainedConfig):\n             config_path = config if config is not None else pretrained_model_name_or_path\n             config, model_kwargs = cls.config_class.from_pretrained(\n                 config_path,\n-                cache_dir=cache_dir,\n                 return_unused_kwargs=True,\n-                force_download=force_download,\n-                proxies=proxies,\n-                local_files_only=local_files_only,\n-                token=token,\n-                revision=revision,\n-                subfolder=subfolder,\n                 gguf_file=gguf_file,\n                 _from_auto=from_auto_class,\n                 _from_pipeline=from_pipeline,\n+                **download_kwargs,\n                 **kwargs,\n             )\n             if \"gguf_file\" in model_kwargs:\n                 model_kwargs.pop(\"gguf_file\")\n+            commit_hash = model_kwargs.pop(\"_commit_hash\", commit_hash)\n         else:\n             config = copy.deepcopy(config)\n             model_kwargs = kwargs\n+            commit_hash = getattr(config, \"_commit_hash\", commit_hash)\n+\n+        download_kwargs_with_commit[\"commit_hash\"] = commit_hash\n \n         # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call\n         # to correctly redispatch recursively if the kwarg is provided\n         if \"attn_implementation\" in kwargs:\n             config._attn_implementation = kwargs.pop(\"attn_implementation\")\n \n-        transformers_explicit_filename = getattr(config, \"transformers_weights\", None)\n-\n-        if transformers_explicit_filename is not None:\n-            if not transformers_explicit_filename.endswith(\n-                \".safetensors\"\n-            ) and not transformers_explicit_filename.endswith(\".safetensors.index.json\"):\n-                raise ValueError(\n-                    \"The transformers file in the config seems to be incorrect: it is neither a safetensors file \"\n-                    \"(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \"\n-                    f\"{transformers_explicit_filename}\"\n-                )\n-\n         hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n             config, quantization_config, dtype, device_map, weights_only, user_agent\n         )\n \n-        if gguf_file is not None and hf_quantizer is not None:\n-            raise ValueError(\n-                \"You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\"\n-            )\n-\n-        if (\n-            gguf_file\n-            and device_map is not None\n-            and ((isinstance(device_map, dict) and \"disk\" in device_map.values()) or \"disk\" in device_map)\n-        ):\n-            raise RuntimeError(\n-                \"One or more modules is configured to be mapped to disk. Disk offload is not supported for models \"\n-                \"loaded from GGUF files.\"\n-            )\n+        if gguf_file:\n+            if hf_quantizer is not None:\n+                raise ValueError(\n+                    \"You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\"\n+                )\n+            if device_map is not None and (\n+                (isinstance(device_map, dict) and \"disk\" in device_map.values()) or \"disk\" in device_map\n+            ):\n+                raise RuntimeError(\n+                    \"One or more modules is configured to be mapped to disk. Disk offload is not supported for models \"\n+                    \"loaded from GGUF files.\"\n+                )\n \n         checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n             pretrained_model_name_or_path=pretrained_model_name_or_path,\n-            subfolder=subfolder,\n             variant=variant,\n             gguf_file=gguf_file,\n             use_safetensors=use_safetensors,\n-            cache_dir=cache_dir,\n-            force_download=force_download,\n-            proxies=proxies,\n-            local_files_only=local_files_only,\n-            token=token,\n+            download_kwargs=download_kwargs_with_commit,\n             user_agent=user_agent,\n-            revision=revision,\n-            commit_hash=commit_hash,\n             is_remote_code=cls._auto_class is not None,\n-            transformers_explicit_filename=transformers_explicit_filename,\n+            transformers_explicit_filename=getattr(config, \"transformers_weights\", None),\n         )\n \n         is_quantized = hf_quantizer is not None\n-        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n-\n-        # Just a helpful message in case we try to load safetensors files coming from old Transformers tf/flax classes\n-        if is_from_file and checkpoint_files[0].endswith(\".safetensors\"):\n-            with safe_open(checkpoint_files[0], framework=\"pt\") as f:\n-                metadata = f.metadata()\n-            if metadata is not None and metadata.get(\"format\") in [\"tf\", \"flax\"]:\n-                logger.warning(\n-                    \"The safetensors checkpoint found has format `tf` or `flax`. This mean that the keys will very\"\n-                    \"likely not match to the model you are trying to load, and will be newly initialized. If it's the case \"\n-                    \"another warning will be raised later. Consider converting your checkpoint to the correct format.\"\n-                )\n \n         if gguf_file:\n             from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n@@ -4746,52 +4590,19 @@ def from_pretrained(\n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n \n-        # Find fp32 modules if needed\n-        keep_in_fp32_modules = []\n-        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n-        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n-        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n-        if model._keep_in_fp32_modules is not None and (\n-            dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n-        ):\n-            keep_in_fp32_modules.extend(model._keep_in_fp32_modules)\n-\n-        if model._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n-            keep_in_fp32_modules.extend(model._keep_in_fp32_modules_strict)\n-\n-        keep_in_fp32_regex = None\n-        if keep_in_fp32_modules:\n-            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n-            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n-\n-        if hf_quantizer is not None:\n+        # Regex to keep a fixed dtype\n+        keep_in_fp32_regex = get_keep_in_fp32_regex(model, hf_quantizer, dtype)\n+        if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n             hf_quantizer.preprocess_model(\n                 model=model,\n                 device_map=device_map,\n                 keep_in_fp32_modules=model._keep_in_fp32_modules,\n                 config=config,\n+                checkpoint_files=checkpoint_files,\n                 use_kernels=use_kernels,\n             )\n-            # We store the original dtype for quantized models as we cannot easily retrieve it\n-            # once the weights have been quantized\n-            # Note that once you have loaded a quantized model, you can't change its dtype so this will\n-            # remain a single source of truth\n-            original_dtype = dtype if dtype is not None else torch.get_default_dtype()\n-\n-            def _assign_original_dtype(module):\n-                for child in module.children():\n-                    if isinstance(child, PreTrainedModel):\n-                        child.config._pre_quantization_dtype = original_dtype\n-                    _assign_original_dtype(child)\n-\n-            config._pre_quantization_dtype = original_dtype\n-            _assign_original_dtype(model)\n-\n-            # Torchao needs access to all metadata later\n-            if hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO:\n-                hf_quantizer.set_metadata(checkpoint_files)\n-\n-        if _torch_distributed_available and device_mesh is not None:\n+\n+        if _torch_distributed_available and device_mesh is not None:  # add hooks to nn.Modules: no weights\n             model = distribute_model(model, distributed_config, device_mesh, tp_size)\n \n         # Prepare the full device map\n@@ -4820,109 +4631,34 @@ def _assign_original_dtype(module):\n             weights_only=weights_only,\n         )\n \n-        # make sure token embedding weights are still tied if needed\n-        model.tie_weights()\n-\n-        # Set model in evaluation mode to deactivate DropOut modules by default\n-        model.eval()\n-\n-        # check if using kernels\n-        if use_kernels:\n-            if not is_kernels_available():\n-                raise ValueError(\n-                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n-                )\n-            from kernels import use_kernel_mapping\n-\n-            if kernel_config is not None and isinstance(kernel_config, KernelConfig):\n-                # This will make sure the mapping is valid, and the layers are registered in the model\n-                kernel_config.sanitize_kernel_mapping(model)\n-\n-                # This will create a compatible mapping for the model with the kernels library\n-                kernel_config.create_compatible_mapping(model)\n-\n-                # This is a context manager to override the default kernel mapping\n-                # We are calling kernelize inside this context manager using the use_kernels setter\n-                with use_kernel_mapping(kernel_config.kernel_mapping):\n-                    model.use_kernels = True\n-            # We use the default kernel mapping in .integrations.hub_kernels\n-            else:\n-                model.use_kernels = True\n+        model.tie_weights()  # make sure token embedding weights are still tied if needed\n+        model.eval()  # Set model in evaluation mode to deactivate DropOut modules by default\n+        model.set_use_kernels(use_kernels, kernel_config)\n \n         # If it is a model with generation capabilities, attempt to load generation files (generation config,\n         # custom generate function)\n-        if model.can_generate() and generation_config is not None:\n-            logger.info(\"The user-defined `generation_config` will be used to override the default generation config.\")\n-            model.generation_config = model.generation_config.from_dict(generation_config.to_dict())\n-        elif model.can_generate() and pretrained_model_name_or_path is not None:\n-            repo_loading_kwargs = {\n-                \"cache_dir\": cache_dir,\n-                \"force_download\": force_download,\n-                \"proxies\": proxies,\n-                \"local_files_only\": local_files_only,\n-                \"token\": token,\n-                \"revision\": revision,\n-                \"subfolder\": subfolder,\n+        if model.can_generate() and hasattr(model, \"adjust_generation_fn\"):\n+            model.adjust_generation_fn(\n+                generation_config,\n+                from_auto_class,\n+                from_pipeline,\n+                pretrained_model_name_or_path,\n+                **download_kwargs,\n+                trust_remote_code=trust_remote_code,\n                 **kwargs,\n-            }\n-            # Load generation config\n-            try:\n-                model.generation_config = GenerationConfig.from_pretrained(\n-                    pretrained_model_name_or_path,\n-                    _from_auto=from_auto_class,\n-                    _from_pipeline=from_pipeline,\n-                    **repo_loading_kwargs,\n-                )\n-            except OSError:\n-                logger.info(\n-                    \"Generation config file not found, using a generation config created from the model config.\"\n-                )\n-                pass\n-            # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n-            if hasattr(model, \"load_custom_generate\"):\n-                try:\n-                    custom_generate = model.load_custom_generate(\n-                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **repo_loading_kwargs\n-                    )\n-                    model.generate = functools.partial(custom_generate, model=model)\n-                except OSError:  # there is no custom generate function\n-                    pass\n+            )\n \n-        # Dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n-        # harm performances)\n+        # for device_map=\"auto\" : dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n+        # harm performances).\n         if device_map is not None and device_mesh is None:\n-            device_map_kwargs = {\n-                \"device_map\": device_map,\n-                \"offload_dir\": offload_folder,\n-                \"offload_index\": offload_index,\n-                \"offload_buffers\": offload_buffers,\n-            }\n-            if \"skip_keys\" in inspect.signature(dispatch_model).parameters:\n-                device_map_kwargs[\"skip_keys\"] = model._skip_keys_device_placement\n-            # For HQQ method we force-set the hooks for single GPU envs\n-            if (\n-                \"force_hooks\" in inspect.signature(dispatch_model).parameters\n-                and hf_quantizer is not None\n-                and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ\n-            ):\n-                device_map_kwargs[\"force_hooks\"] = True\n-            if (\n-                hf_quantizer is not None\n-                and hf_quantizer.quantization_config.quant_method == QuantizationMethod.FBGEMM_FP8\n-                and isinstance(device_map, dict)\n-                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-            ):\n-                device_map_kwargs[\"offload_buffers\"] = True\n-\n-            if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n-                dispatch_model(model, **device_map_kwargs)\n+            accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers)\n \n         if hf_quantizer is not None:\n             model.hf_quantizer = hf_quantizer\n-            hf_quantizer.postprocess_model(model, config=config)\n+            hf_quantizer.postprocess_model(model, config=config)  # usually a no-op\n \n         if _adapter_model_path is not None:\n-            adapter_kwargs[\"key_mapping\"] = key_mapping\n+            adapter_kwargs[\"key_mapping\"] = key_mapping  # TODO: Dynamic weight loader for adapters\n             model.load_adapter(\n                 _adapter_model_path,\n                 adapter_name=adapter_name,\n@@ -5081,7 +4817,7 @@ def _load_pretrained_model(\n             QuantizationMethod.QUARK,\n         }\n \n-        # Get all the keys of the state dicts that we have to initialize the model\n+        # Get all the keys of the state dicts that we have to initialize the model with\n         if sharded_metadata is not None:\n             original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n         elif state_dict is not None:\n@@ -5152,43 +4888,16 @@ def _load_pretrained_model(\n         disk_only_shard_files = []\n         # Prepare parameters offloading if needed\n         if device_map is not None and \"disk\" in device_map.values():\n-            if disk_offload_folder is not None:\n-                os.makedirs(disk_offload_folder, exist_ok=True)\n-            is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n-            if disk_offload_folder is None and not is_offloaded_safetensors:\n-                raise ValueError(\n-                    \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\"\n-                    \" for them. Alternatively, make sure you have `safetensors` installed if the model you are using\"\n-                    \" offers the weights in this format.\"\n-                )\n-            if is_offloaded_safetensors:\n-                param_device_map = expand_device_map(device_map, checkpoint_keys)\n-                str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n-                if sharded_metadata is None:\n-                    weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n-                else:\n-                    folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n-                    # Fix the weight map keys according to the key mapping\n-                    weight_map = {\n-                        key_renaming_mapping[k]: v\n-                        for k, v in sharded_metadata[\"weight_map\"].items()\n-                        if k in key_renaming_mapping\n-                    }\n-                    weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n-                    # Find potential checkpoints containing only offloaded weights\n-                    disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n-                disk_offload_index = {\n-                    name: {\n-                        \"safetensors_file\": file,\n-                        \"weight_name\": reverse_key_renaming_mapping[name],\n-                        \"dtype\": str_dtype,\n-                    }\n-                    for name, file in weight_map.items()\n-                    if param_device_map[name] == \"disk\"\n-                }\n-            else:\n-                disk_offload_index = {}\n-\n+            disk_offload_index, disk_only_shard_files, is_offloaded_safetensors = accelerate_disk_offload(\n+                disk_offload_folder,\n+                checkpoint_files,\n+                device_map,\n+                checkpoint_keys,\n+                key_renaming_mapping,\n+                sharded_metadata,\n+                dtype,\n+                reverse_key_renaming_mapping,\n+            )\n         # To be able to iterate, even if we don't use it if the state_dict is already provided\n         elif state_dict is not None:\n             checkpoint_files = [\"\"]\n@@ -5286,6 +4995,7 @@ def _load_pretrained_model(\n             missing_keys, unexpected_keys, loading_task_model_from_base_state_dict\n         )\n \n+        # TODO: separate this in another function: it's not core....\n         # All potential warnings/infos\n         if len(error_msgs) > 0:\n             error_msg = \"\\n\\t\".join(error_msgs)\n@@ -5787,19 +5497,6 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n         _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n \n \n-def get_disk_only_shard_files(device_map, weight_map):\n-    \"\"\"\n-    Returns the list of shard files containing only weights offloaded to disk.\n-    \"\"\"\n-    files_content = collections.defaultdict(list)\n-    for weight_name, filename in weight_map.items():\n-        while len(weight_name) > 0 and weight_name not in device_map:\n-            weight_name = \".\".join(weight_name.split(\".\")[:-1])\n-        files_content[filename].append(device_map[weight_name])\n-\n-    return [fname for fname, devices in files_content.items() if set(devices) == {\"disk\"}]\n-\n-\n class AttentionInterface(GeneralInterface):\n     \"\"\"\n     Dict-like object keeping track of allowed attention functions. You can easily add a new attention function"
        },
        {
            "sha": "fe7701bc985f89d241a62b844d3fe5eb3996db2f",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -31,6 +31,13 @@\n logger = logging.get_logger(__file__)\n \n \n+def _assign_original_dtype(module, original_dtype):\n+    for child in module.children():\n+        if isinstance(child, PreTrainedModel):\n+            child.config._pre_quantization_dtype = original_dtype\n+        _assign_original_dtype(child, original_dtype)\n+\n+\n class HfQuantizer(ABC):\n     \"\"\"\n     Abstract class of the HuggingFace quantizer. Supports for now quantizing HF transformers models for inference and/or quantization.\n@@ -206,7 +213,7 @@ def update_ep_plan(self, config):\n         \"updates the tp plan for the scales\"\n         return config\n \n-    def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n+    def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n         \"\"\"\n         Setting model attributes and/or converting model before weights loading. At this point\n         the model should be initialized on the meta device so you can freely manipulate the skeleton\n@@ -222,7 +229,18 @@ def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n         model.quantization_method = self.quantization_config.quant_method\n         if self.pre_quantized:\n             self._convert_model_for_quantization(model)\n-        return self._process_model_before_weight_loading(model, **kwargs)\n+        self._process_model_before_weight_loading(model, **kwargs)\n+\n+        # We store the original dtype for quantized models as we cannot easily retrieve it\n+        # once the weights have been quantized\n+        # Note that once you have loaded a quantized model, you can't change its dtype so this will\n+        # remain a single source of truth\n+        original_dtype = dtype if dtype is not None else torch.get_default_dtype()\n+        config._pre_quantization_dtype = original_dtype\n+        _assign_original_dtype(model, original_dtype)\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        return model\n \n     def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n         \"\"\""
        },
        {
            "sha": "ec3fd033b7b3e05d8c43f8669984b5423517896c",
            "filename": "src/transformers/quantizers/quantizer_aqlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -79,9 +79,6 @@ def _process_model_before_weight_loading(\n         )\n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     @property\n     def is_trainable(self) -> bool:\n         aqlm_supports_training = version.parse(importlib.metadata.version(\"aqlm\")) >= version.parse(\"1.0.2\")"
        },
        {
            "sha": "ef55e9c3f008910895141664e02048e88a2d16c2",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -69,9 +69,6 @@ def validate_environment(self, *args, **kwargs):\n                     \"This is not supported. Please remove the CPU or disk device from the device_map.\"\n                 )\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\","
        },
        {
            "sha": "829b01ae8201cbe716b346ef80ee513f980f19e9",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -137,9 +137,6 @@ def create_quantized_param(\n         module._buffers[tensor_name] = new_value.to(target_device)\n         module.register(\"weight_scales\", weight_scale.to(target_device))\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\","
        },
        {
            "sha": "9c04d8f845d42780149c091561ecb569acbe7e89",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -192,9 +192,6 @@ def create_quantized_param(\n \n         del param_name\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\","
        },
        {
            "sha": "326ee8c015ab987ba864c3e0bf693b3c0e24fdbb",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -167,9 +167,6 @@ def _process_model_before_weight_loading(\n \n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n         from ..integrations import FP8Linear\n "
        },
        {
            "sha": "c0cbad1d5bc937270f6cb001f3cb34a72b5a468a",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -140,9 +140,6 @@ def _process_model_before_weight_loading(\n         )\n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n         from fp_quant import FPQuantLinear\n "
        },
        {
            "sha": "3f5379a49c31fe27ad3fbf3c9dcb9f0fce5b023f",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -157,9 +157,6 @@ def _process_model_before_weight_loading(\n         )\n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model, **kwargs):\n-        return model\n-\n     @property\n     def is_trainable(self) -> bool:\n         return True"
        },
        {
            "sha": "931649f86f537b3d89df3fbefaf235f53c2b830e",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -88,9 +88,6 @@ def create_quantized_param(self, model, param, param_name, param_device, **kwarg\n \n         _load_parameter_into_model(model, param_name, param.to(param_device))\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     def is_serializable(self, safe_serialization=None):\n         return False\n "
        },
        {
            "sha": "d2c321f855539ddcc7cf78eb927c822ff74606bd",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -79,9 +79,6 @@ def _process_model_before_weight_loading(\n         )\n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     @property\n     def is_trainable(self):\n         return False"
        },
        {
            "sha": "5358478e0be2af5963391f5686d012f8b8316439",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -350,6 +350,22 @@ def create_quantized_param(\n \n             quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n+    def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n+        \"\"\"\n+        Setting model attributes and/or converting model before weights loading. At this point\n+        the model should be initialized on the meta device so you can freely manipulate the skeleton\n+        of the model in order to replace modules in-place. Make sure to override the abstract method `_process_model_before_weight_loading`.\n+\n+        Args:\n+            model (`~transformers.PreTrainedModel`):\n+                The model to quantize\n+            kwargs (`dict`, *optional*):\n+                The keyword arguments that are passed along `_process_model_before_weight_loading`.\n+        \"\"\"\n+        super().preprocess_model(model, config, dtype, checkpoint_files, **kwargs)\n+        # Torchao needs access to all metadata later\n+        model.set_metadata(checkpoint_files)\n+\n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n         if self.quantization_config.quant_type == \"autoquant\":"
        },
        {
            "sha": "f593d4a3de8d6df70d222ec0e9ff7bf81793e269",
            "filename": "src/transformers/quantizers/quantizer_vptq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -88,9 +88,6 @@ def _process_model_before_weight_loading(\n         )\n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        return model\n-\n     @property\n     def is_trainable(self) -> bool:\n         return False"
        },
        {
            "sha": "d3a02a1409f4f218576c12323b002442b11c4315",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -65,7 +65,7 @@\n from .integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n from .integrations.tpu import tpu_spmd_dataloader\n from .modelcard import TrainingSummary\n-from .modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n+from .modeling_utils import PreTrainedModel, unwrap_model\n from .models.auto.modeling_auto import (\n     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n     MODEL_MAPPING_NAMES,\n@@ -123,6 +123,7 @@\n     find_executable_batch_size,\n     get_last_checkpoint,\n     has_length,\n+    load_sharded_checkpoint,\n     neftune_post_forward_hook,\n     number_of_arguments,\n     seed_worker,"
        },
        {
            "sha": "99ffe74ae27128609c07d8987b0cf264f43b87ed",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -19,18 +19,23 @@\n import functools\n import gc\n import inspect\n+import json\n import os\n import random\n import re\n import threading\n import time\n from collections.abc import Callable\n+from functools import partial\n from typing import Any, NamedTuple, Optional, Union\n \n import numpy as np\n \n from .utils import (\n+    SAFE_WEIGHTS_INDEX_NAME,\n+    WEIGHTS_INDEX_NAME,\n     ExplicitEnum,\n+    check_torch_load_is_safe,\n     is_psutil_available,\n     is_torch_available,\n     is_torch_cuda_available,\n@@ -47,6 +52,7 @@\n \n if is_torch_available():\n     import torch\n+    from safetensors.torch import load_file as safe_load_file\n \n \n def seed_worker(worker_id: int, num_workers: int, rank: int):\n@@ -873,3 +879,79 @@ def check_target_module_exists(optim_target_modules, key: str, return_is_regex:\n         return target_module_found, is_regex\n \n     return target_module_found\n+\n+\n+def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n+    \"\"\"\n+    This is the same as\n+    [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)\n+    but for a sharded checkpoint.\n+\n+    This load is performed efficiently: each checkpoint shard is loaded one by one in RAM and deleted after being\n+    loaded in the model.\n+\n+    Args:\n+        model (`torch.nn.Module`): The model in which to load the checkpoint.\n+        folder (`str` or `os.PathLike`): A path to a folder containing the sharded checkpoint.\n+        strict (`bool`, *optional*, defaults to `True`):\n+            Whether to strictly enforce that the keys in the model state dict match the keys in the sharded checkpoint.\n+        prefer_safe (`bool`, *optional*, defaults to `False`):\n+            If both safetensors and PyTorch save files are present in checkpoint and `prefer_safe` is True, the\n+            safetensors files will be loaded. Otherwise, PyTorch files are always loaded when possible.\n+\n+    Returns:\n+        `NamedTuple`: A named tuple with `missing_keys` and `unexpected_keys` fields\n+            - `missing_keys` is a list of str containing the missing keys\n+            - `unexpected_keys` is a list of str containing the unexpected keys\n+    \"\"\"\n+    # Load the index\n+    index_file = os.path.join(folder, WEIGHTS_INDEX_NAME)\n+    safe_index_file = os.path.join(folder, SAFE_WEIGHTS_INDEX_NAME)\n+\n+    index_present = os.path.isfile(index_file)\n+    safe_index_present = os.path.isfile(safe_index_file)\n+\n+    if not index_present and not safe_index_present:\n+        filenames = (WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_INDEX_NAME)\n+        raise ValueError(f\"Can't find a checkpoint index ({' or '.join(filenames)}) in {folder}.\")\n+\n+    load_safe = safe_index_present and (prefer_safe or not index_present)\n+    load_index = safe_index_file if load_safe else index_file\n+\n+    with open(load_index, \"r\", encoding=\"utf-8\") as f:\n+        index = json.load(f)\n+\n+    shard_files = list(set(index[\"weight_map\"].values()))\n+\n+    # If strict=True, error before loading any of the state dicts.\n+    # TODO: Here, update the weigth map with the config.dynamic_weight_conversion\n+    loaded_keys = index[\"weight_map\"].keys()\n+    model_keys = model.state_dict().keys()\n+    missing_keys = [key for key in model_keys if key not in loaded_keys]\n+    unexpected_keys = [key for key in loaded_keys if key not in model_keys]\n+    if strict and (len(missing_keys) > 0 or len(unexpected_keys) > 0):\n+        error_message = f\"Error(s) in loading state_dict for {model.__class__.__name__}\"\n+        if len(missing_keys) > 0:\n+            str_missing_keys = \",\".join([f'\"{k}\"' for k in missing_keys])\n+            error_message += f\"\\nMissing key(s): {str_missing_keys}.\"\n+        if len(unexpected_keys) > 0:\n+            str_unexpected_keys = \",\".join([f'\"{k}\"' for k in unexpected_keys])\n+            error_message += f\"\\nMissing key(s): {str_unexpected_keys}.\"\n+        raise RuntimeError(error_message)\n+\n+    if load_safe:\n+        loader = safe_load_file\n+    else:\n+        check_torch_load_is_safe()\n+        loader = partial(torch.load, map_location=\"cpu\", weights_only=True)\n+\n+    for shard_file in shard_files:\n+        state_dict = loader(os.path.join(folder, shard_file))\n+        model.load_state_dict(state_dict, strict=False)\n+\n+        # Make sure memory is freed before we load the next state dict.\n+        del state_dict\n+        gc.collect()\n+\n+    # Return the same thing as PyTorch load_state_dict function.\n+    return torch.nn.modules.module._IncompatibleKeys(missing_keys, unexpected_keys)"
        },
        {
            "sha": "256aa4335eb5643c23892a7a9db1bc64c144b099",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ee3b288a62c9de658e8be117d869c2a9b835a7c/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=1ee3b288a62c9de658e8be117d869c2a9b835a7c",
            "patch": "@@ -23,7 +23,7 @@\n import warnings\n from concurrent import futures\n from pathlib import Path\n-from typing import Optional, Union\n+from typing import Optional, TypedDict, Union\n from urllib.parse import urlparse\n from uuid import uuid4\n \n@@ -75,6 +75,18 @@\n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n+\n+class DownloadKwargs(TypedDict, total=False):\n+    cache_dir: Optional[Union[str, os.PathLike]]\n+    force_download: bool\n+    proxies: Optional[dict[str, str]]\n+    local_files_only: bool\n+    token: Optional[Union[str, bool]]\n+    revision: Optional[str]\n+    subfolder: str\n+    commit_hash: Optional[str]\n+\n+\n _is_offline_mode = huggingface_hub.constants.HF_HUB_OFFLINE\n \n "
        }
    ],
    "stats": {
        "total": 1175,
        "additions": 627,
        "deletions": 548
    }
}