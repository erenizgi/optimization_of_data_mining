{
    "author": "qykong",
    "message": "[bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU â€¦ (#38876)\n\n[bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU setups",
    "sha": "c55d8063557d4cfa6a15df44bdb2f29c7de51247",
    "files": [
        {
            "sha": "dd8a6dc5d07b6e46053c3635fc5c9711b7cd845f",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c55d8063557d4cfa6a15df44bdb2f29c7de51247/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c55d8063557d4cfa6a15df44bdb2f29c7de51247/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=c55d8063557d4cfa6a15df44bdb2f29c7de51247",
            "patch": "@@ -38,7 +38,14 @@\n         \"or 3 (down-right aligned causal mask).\"\n     )\n \n-ATTN_MASK_NPU = None\n+ATTN_MASK_NPU_CACHE = {}\n+\n+\n+def get_attn_mask_npu(device):\n+    \"\"\"Get or create attention mask for the specified device.\"\"\"\n+    if device not in ATTN_MASK_NPU_CACHE:\n+        ATTN_MASK_NPU_CACHE[device] = torch.triu(torch.ones([2048, 2048], device=device), diagonal=1).bool()\n+    return ATTN_MASK_NPU_CACHE[device]\n \n \n def is_npu_fa2_top_left_aligned_causal_mask():\n@@ -174,9 +181,7 @@ def npu_flash_attn_func(\n         head_num = q.shape[2]\n         output = torch_npu.npu_fusion_attention(q, k, v, head_num, \"BSND\", keep_prob=keep_prob, scale=softmax_scale)[0]\n     else:\n-        global ATTN_MASK_NPU\n-        if ATTN_MASK_NPU is None:\n-            ATTN_MASK_NPU = torch.triu(torch.ones([2048, 2048], device=q.device), diagonal=1).bool()\n+        attn_mask_npu = get_attn_mask_npu(q.device)\n         head_num = q.shape[2]\n         output = torch_npu.npu_fusion_attention(\n             q,\n@@ -186,7 +191,7 @@ def npu_flash_attn_func(\n             \"BSND\",\n             keep_prob=keep_prob,\n             scale=softmax_scale,\n-            atten_mask=ATTN_MASK_NPU,\n+            atten_mask=attn_mask_npu,\n             sparse_mode=SPARSE_MODE,\n         )[0]\n \n@@ -227,9 +232,7 @@ def npu_flash_attn_varlen_func(\n             actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),\n         )[0]\n     else:\n-        global ATTN_MASK_NPU\n-        if ATTN_MASK_NPU is None:\n-            ATTN_MASK_NPU = torch.triu(torch.ones([2048, 2048], device=q.device), diagonal=1).bool()\n+        attn_mask_npu = get_attn_mask_npu(q.device)\n         head_num = q.shape[1]\n         output = torch_npu.npu_fusion_attention(\n             q,\n@@ -238,7 +241,7 @@ def npu_flash_attn_varlen_func(\n             head_num,\n             pse=None,\n             padding_mask=None,\n-            atten_mask=ATTN_MASK_NPU,\n+            atten_mask=attn_mask_npu,\n             scale=softmax_scale,\n             keep_prob=keep_prob,\n             input_layout=\"TND\","
        }
    ],
    "stats": {
        "total": 21,
        "additions": 12,
        "deletions": 9
    }
}