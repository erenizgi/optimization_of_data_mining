{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ðŸš¨ Fix and simplify attention implementation dispatch and subconfigs handling (#39423)\n\n* first try\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* big refactor\n\n* Update modeling_utils.py\n\n* style\n\n* docstrings and simplify inner workings of configs\n\n* remove all trace of _internal\n\n* Update modeling_utils.py\n\n* fix logic error\n\n* Update modeling_utils.py\n\n* recursive on config\n\n* Update configuration_utils.py\n\n* fix\n\n* Update configuration_dpt.py\n\n* Update configuration_utils.py\n\n* Update configuration_utils.py\n\n* Update modeling_idefics.py\n\n* Update modeling_utils.py\n\n* fix for old models\n\n* more old models fixup\n\n* Update modeling_utils.py\n\n* Update configuration_utils.py\n\n* Remove outdated test\n\n* remove the deepcopy!! ðŸ¥µðŸ¥µ\n\n* Update test_modeling_gpt_bigcode.py\n\n* fix qwen dispatch\n\n* restrict to only models supporting it\n\n* style\n\n* switch name\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* add tests!\n\n* fix\n\n* rypo\n\n* remove bad copies\n\n* fix\n\n* Update modeling_utils.py\n\n* additional check\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* fix\n\n* skip",
    "sha": "4ded9a41138773af49c70eac3fca7ad6ba9ede03",
    "files": [
        {
            "sha": "034686ad2c8a687f58b1b8dff935f3a427ff3e30",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -60,11 +60,11 @@ You will see it prints \"I just entered the attention computation\" as many times\n \n ## Dynamically switching attention function\n \n-You could dynamically change the model's attention function as well, by overriding the `config._attn_implementation` field:\n+You could dynamically change the model's attention function as well:\n \n ```python\n # Back to use original sdpa implementation\n-model.config._attn_implementation = \"sdpa\"\n+model.set_attn_implementation(\"sdpa\")\n \n model(torch.ones(1, 5, dtype=int))\n ```"
        },
        {
            "sha": "243622d895fe01d934642d6eaf2333f28834bc80",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 18,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -323,9 +323,8 @@ def __init__(\n         self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n         self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n \n-        # Attention implementation to use, if relevant.\n-        self._attn_implementation_internal = kwargs.pop(\"attn_implementation\", None)\n-        self._attn_implementation_autoset = False\n+        # Attention implementation to use, if relevant (it sets it recursively on sub-configs)\n+        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n \n         # Drop the transformers version info\n         self.transformers_version = kwargs.pop(\"transformers_version\", None)\n@@ -370,8 +369,11 @@ def output_attentions(self):\n         return self._output_attentions\n \n     @output_attentions.setter\n-    def output_attentions(self, value):\n-        if value is True and self._attn_implementation != \"eager\":\n+    def output_attentions(self, value: bool):\n+        # If we set `output_attentions` explictily before the attn implementation, dispatch eager\n+        if value and self._attn_implementation is None:\n+            self._attn_implementation = \"eager\"\n+        if value and self._attn_implementation != \"eager\":\n             raise ValueError(\n                 \"The `output_attentions` attribute is not supported when using the `attn_implementation` set to \"\n                 f\"{self._attn_implementation}. Please set it to 'eager' instead.\"\n@@ -402,19 +404,23 @@ def num_labels(self, num_labels: int):\n \n     @property\n     def _attn_implementation(self):\n-        # This property is made private for now (as it cannot be changed and a PreTrainedModel.use_attn_implementation method needs to be implemented.)\n-        if hasattr(self, \"_attn_implementation_internal\"):\n-            if self._attn_implementation_internal is None:\n-                # `config.attn_implementation` should never be None, for backward compatibility.\n-                return \"eager\"\n-            else:\n-                return self._attn_implementation_internal\n-        else:\n-            return \"eager\"\n+        return self._attn_implementation_internal\n \n     @_attn_implementation.setter\n-    def _attn_implementation(self, value):\n-        self._attn_implementation_internal = value\n+    def _attn_implementation(self, value: Optional[Union[str, dict]]):\n+        \"\"\"We set it recursively on the sub-configs as well\"\"\"\n+        # Set if for current config\n+        attn_implementation = value if not isinstance(value, dict) else value.get(\"\", self._attn_implementation)\n+        self._attn_implementation_internal = attn_implementation\n+\n+        # Set it recursively on the subconfigs\n+        for subconfig_key in self.sub_configs:\n+            subconfig = getattr(self, subconfig_key, None)\n+            if subconfig is not None:\n+                sub_implementation = (\n+                    value if not isinstance(value, dict) else value.get(subconfig_key, subconfig._attn_implementation)\n+                )\n+                subconfig._attn_implementation = sub_implementation\n \n     def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n         \"\"\"\n@@ -1053,8 +1059,6 @@ def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n             del d[\"_commit_hash\"]\n         if \"_attn_implementation_internal\" in d:\n             del d[\"_attn_implementation_internal\"]\n-        if \"_attn_implementation_autoset\" in d:\n-            del d[\"_attn_implementation_autoset\"]\n         # Do not serialize `base_model_tp_plan` for now\n         if \"base_model_tp_plan\" in d:\n             del d[\"base_model_tp_plan\"]"
        },
        {
            "sha": "9d2f70a497d0a0a5a6fc4bfb01b8e61533c8fed7",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 284,
            "deletions": 212,
            "changes": 496,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -24,6 +24,7 @@\n import os\n import re\n import shutil\n+import sys\n import tempfile\n import warnings\n from abc import abstractmethod\n@@ -2094,12 +2095,11 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n             )\n         self.config = config\n \n-        # The `hasattr` here is used as some Transformers tests for some reason do not call\n-        # PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)\n-        if hasattr(config, \"_attn_implementation_internal\") and not getattr(\n-            config, \"_attn_implementation_autoset\", False\n-        ):\n-            self.set_attention_implementation(self.config._attn_implementation_internal)\n+        # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n+        # setting it recursively)\n+        self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n+            self.config._attn_implementation, is_init_check=True\n+        )\n \n         # for initialization of the loss\n         loss_type = self.__class__.__name__\n@@ -2244,14 +2244,9 @@ def _from_config(cls, config, **kwargs):\n         if torch_dtype is not None:\n             dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n \n-        config = copy.deepcopy(config)  # We do not want to modify the config inplace in _from_config.\n-\n-        if config._attn_implementation_internal is not None:\n-            # In this case, the config has been created with the attn_implementation set by the user, which we should respect.\n-            attn_implementation = config._attn_implementation_internal\n-        else:\n-            attn_implementation = None\n-        config._attn_implementation = kwargs.pop(\"attn_implementation\", attn_implementation)\n+        # If passing `attn_implementation` as kwargs, respect it (it will be applied recursively on subconfigs)\n+        if \"attn_implementation\" in kwargs:\n+            config._attn_implementation = kwargs.pop(\"attn_implementation\")\n \n         if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n@@ -2272,101 +2267,6 @@ def _from_config(cls, config, **kwargs):\n \n         return model\n \n-    @classmethod\n-    def _check_attn_implementation(cls, attn_implementation: Union[dict, str]) -> Union[dict, str]:\n-        \"\"\"\n-        Checks that the requested attention implementation exists and tries to get the kernel from hub\n-        if `attn_implementation` matches hf kernels pattern.\n-        \"\"\"\n-        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n-            if not is_kernels_available():\n-                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n-\n-            # Extract repo_id and kernel_name from the string\n-            repo_id, kernel_name = attn_implementation.split(\":\")\n-            kernel_name = kernel_name.strip()\n-            repo_id = repo_id.strip()\n-\n-            try:\n-                kernel = get_kernel(repo_id)\n-                ALL_ATTENTION_FUNCTIONS.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n-                attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n-            except FileNotFoundError as e:\n-                logger.warning(\n-                    f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n-                )\n-                attn_implementation = None  # try to dispatch SDPA and fallback eager if not available\n-            except AttributeError:\n-                raise ValueError(\n-                    \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n-                                 Please check the documentation for the correct format, \\\n-                                 and check that the kernel exports the class and the function correctly.\"\n-                )\n-        if (\n-            not isinstance(attn_implementation, dict)\n-            and attn_implementation not in [\"eager\", None] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n-        ):\n-            message = f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n-            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n-            if cls._supports_flash_attn or getattr(cls, \"_supports_flash_attn_2\", False):\n-                message += (\n-                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n-                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n-                )\n-            if cls._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n-            if cls._supports_flex_attn:\n-                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n-            raise ValueError(message + \".\")\n-\n-        return attn_implementation\n-\n-    def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n-        \"\"\"\n-        Checks and dispatches to the requested attention implementation.\n-        \"\"\"\n-        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n-\n-        # Composite models consisting of several PretrainedModels can specify attention implementation as a dict where\n-        # keys are sub-config names. But most people will specify one `str` which means that should dispatch it for all sub-models.\n-        # See https://github.com/huggingface/transformers/pull/32238\n-        for key in self.config.sub_configs.keys():\n-            sub_config = getattr(self.config, key)\n-            curr_attn_implementation = (\n-                requested_attn_implementation\n-                if not isinstance(requested_attn_implementation, dict)\n-                else requested_attn_implementation.get(key, None)\n-            )\n-            # For models with backbone sub-config might be not initialized. Set the requested att\n-            # if the config hasn't got any attn pre-set and the requested attn in not `None` (i.e not the default attn)\n-            if (\n-                sub_config is not None\n-                and sub_config._attn_implementation_internal is None\n-                and curr_attn_implementation is not None\n-            ):\n-                sub_config._attn_implementation_internal = curr_attn_implementation\n-\n-        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n-            self.config._attn_implementation = \"flash_attention_3\"\n-        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n-            self.config._attn_implementation = \"flash_attention_2\"\n-        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n-            self.config._attn_implementation = \"flex_attention\"\n-        elif (\n-            requested_attn_implementation in [None, \"sdpa\"]\n-            and not is_torch_xla_available()\n-            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n-        ):\n-            self.config._attn_implementation = \"sdpa\"\n-        elif requested_attn_implementation in ALL_ATTENTION_FUNCTIONS.valid_keys():\n-            self.config._attn_implementation = requested_attn_implementation\n-        elif isinstance(requested_attn_implementation, dict):\n-            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n-        else:\n-            self.config._attn_implementation = \"eager\"\n-\n-        self.config._attn_implementation_autoset = True\n-\n     @classmethod\n     def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n         \"\"\"\n@@ -2439,15 +2339,18 @@ def can_generate(cls) -> bool:\n         # Otherwise, can't generate\n         return False\n \n-    def _flash_attn_2_can_dispatch(self) -> bool:\n+    def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n         \"\"\"\n-        Checks the availability of Flash Attention 2 and compatibility with the current model.\n+        Check the availability of Flash Attention 2 for a given model.\n \n-        If all checks pass and `hard_check_only` is False, the method will set the config attribute `attn_implementation` to \"flash_attention_2\" so that the model can initialize the correct attention module.\n+        Args:\n+            is_init_check (`bool`, *optional*):\n+                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n+                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n+                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n+                before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n-        # Config always has `torch_dtype` but we need the next line for `no_super_init()` tests\n-        torch_dtype = self.config.torch_dtype if hasattr(self.config, \"torch_dtype\") else torch.get_default_dtype()\n-        device_map = self.hf_device_map if hasattr(self, \"hf_device_map\") else None\n+        torch_dtype = self.config.torch_dtype\n \n         # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n         if not (self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False)):\n@@ -2486,68 +2389,62 @@ def _flash_attn_2_can_dispatch(self) -> bool:\n                     else:\n                         raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n \n-        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n-        if _is_bettertransformer:\n-            raise ValueError(\n-                \"Flash Attention 2 and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n-            )\n-\n         if torch_dtype is None:\n             logger.warning_once(\n-                \"You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\"\n+                \"You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\"\n             )\n         elif torch_dtype is not None and torch_dtype not in [torch.float16, torch.bfloat16]:\n             logger.warning_once(\n-                \"Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n+                \"Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n                 f\" the current dype in {self.__class__.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                 ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`'\n             )\n \n-        # The check `torch.empty(0).device.type != \"cuda\"` is needed as the model may be initialized after `torch.set_default_device` has been called,\n-        # or the model may be initialized under the context manager `with torch.device(\"cuda\"):`.\n-        if device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n-            if torch.cuda.is_available():\n-                logger.warning_once(\n-                    \"You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU\"\n-                    \" after initializing it on CPU with `model.to('cuda')`.\"\n-                )\n-            elif is_torch_mlu_available():\n-                logger.warning_once(\n-                    \"You are attempting to use Flash Attention 2.0 with a model not initialized on MLU. Make sure to move the model to MLU\"\n-                    \" after initializing it on CPU with `model.to('mlu')`.\"\n-                )\n-            else:\n+        # With the early check, the parameters are not yet initalized correctly\n+        if not is_init_check:\n+            if getattr(self, \"use_bettertransformer\", False):\n                 raise ValueError(\n-                    \"You are attempting to use Flash Attention 2.0 with a model not initialized on GPU and with no GPU available. \"\n-                    \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n-                    \"or initialising the model on CPU and then moving it to GPU.\"\n+                    \"Flash Attention 2 and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n                 )\n-        elif (\n-            device_map is not None\n-            and isinstance(device_map, dict)\n-            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-        ):\n-            raise ValueError(\n-                \"You are attempting to use Flash Attention 2.0 with a model dispatched on CPU or disk. This is not supported. Please make sure to \"\n-                \"initialise the model on a GPU by passing a device_map that contains only GPU devices as keys.\"\n-            )\n+\n+            param_devices = list({param.device for param in self.parameters()})\n+            if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n+                if torch.cuda.is_available():\n+                    logger.warning_once(\n+                        \"You are attempting to use Flash Attention 2 with a model not initialized on GPU. Make sure to move the model to GPU\"\n+                        \" after initializing it on CPU with `model.to('cuda')`.\"\n+                    )\n+                elif is_torch_mlu_available():\n+                    logger.warning_once(\n+                        \"You are attempting to use Flash Attention 2 with a model not initialized on MLU. Make sure to move the model to MLU\"\n+                        \" after initializing it on CPU with `model.to('mlu')`.\"\n+                    )\n+                else:\n+                    raise ValueError(\n+                        \"You are attempting to use Flash Attention 2 with a model not initialized on GPU and with no GPU available. \"\n+                        \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n+                        \"or initialising the model on CPU and then moving it to GPU.\"\n+                    )\n \n         # If no error raise by this point, we can return `True`\n         return True\n \n-    def _flash_attn_3_can_dispatch(self) -> bool:\n+    def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n         \"\"\"\n-        Checks the availability of Flash Attention 3 and compatibility with the current model.\n+        Check the availability of Flash Attention 3 for a given model.\n \n-        If all checks pass and `hard_check_only` is False, the method will set the config attribute `attn_implementation` to \"flash_attention_3\" so that the model can initialize the correct attention module.\n+        Args:\n+            is_init_check (`bool`, *optional*):\n+                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n+                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n+                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n+                before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n-        # Config always has `torch_dtype` but we need the next line for `no_super_init()` tests\n-        torch_dtype = self.config.torch_dtype if hasattr(self.config, \"torch_dtype\") else torch.get_default_dtype()\n-        device_map = self.hf_device_map if hasattr(self, \"hf_device_map\") else None\n+        torch_dtype = self.config.torch_dtype\n \n         if not self._supports_flash_attn:\n             raise ValueError(\n-                f\"{self.__class__.__name__} does not support Flash Attention 3.0 yet. Please request to add support where\"\n+                f\"{self.__class__.__name__} does not support Flash Attention 3 yet. Please request to add support where\"\n                 f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                 \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n             )\n@@ -2591,48 +2488,43 @@ def _flash_attn_3_can_dispatch(self) -> bool:\n                 f\"Model has attention_dropout={self.config.attention_dropout}, which is not supported by Flash Attention 3.\"\n             )\n \n-        # The check `torch.empty(0).device.type != \"cuda\"` is needed as the model may be initialized after `torch.set_default_device` has been called,\n-        # or the model may be initialized under the context manager `with torch.device(\"cuda\"):`.\n-        if device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n-            if torch.cuda.is_available():\n-                logger.warning_once(\n-                    \"You are attempting to use Flash Attention 3 with a model not initialized on GPU. Make sure to move the model to GPU\"\n-                    \" after initializing it on CPU with `model.to('cuda')`.\"\n-                )\n-            else:\n-                raise ValueError(\n-                    \"You are attempting to use Flash Attention 3 with a model not initialized on GPU and with no GPU available. \"\n-                    \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n-                    \"or initialising the model on CPU and then moving it to GPU.\"\n-                )\n-        elif (\n-            device_map is not None\n-            and isinstance(device_map, dict)\n-            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-        ):\n-            raise ValueError(\n-                \"You are attempting to use Flash Attention 3 with a model dispatched on CPU or disk. This is not supported. Please make sure to \"\n-                \"initialise the model on a GPU by passing a device_map that contains only GPU devices as keys.\"\n-            )\n+        # With the early check, the parameters are not yet initalized correctly\n+        if not is_init_check:\n+            param_devices = list({param.device for param in self.parameters()})\n+            if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n+                if torch.cuda.is_available():\n+                    logger.warning_once(\n+                        \"You are attempting to use Flash Attention 3 with a model not initialized on GPU. Make sure to move the model to GPU\"\n+                        \" after initializing it on CPU with `model.to('cuda')`.\"\n+                    )\n+                else:\n+                    raise ValueError(\n+                        \"You are attempting to use Flash Attention 3 with a model not initialized on GPU and with no GPU available. \"\n+                        \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n+                        \"or initialising the model on CPU and then moving it to GPU.\"\n+                    )\n+\n         return True\n \n-    def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n+    def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n         \"\"\"\n-        Checks the availability of SDPA for a given model.\n+        Check the availability of SDPA for a given model.\n \n-        If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"sdpa\" so that the model can initialize the correct attention module.\n+        Args:\n+            is_init_check (`bool`, *optional*):\n+                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n+                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n+                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n+                before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n-        if hard_check_only:\n-            if not self._supports_sdpa:\n-                raise ValueError(\n-                    f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n-                    \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n-                    ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n-                )\n-            if not is_torch_sdpa_available():\n-                raise ImportError(\n-                    \"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\"\n-                )\n+        if not self._supports_sdpa:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n+                \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n+                ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n+            )\n+        if not is_torch_sdpa_available():\n+            raise ImportError(\"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\")\n \n         if (\n             torch.version.hip is not None\n@@ -2644,18 +2536,24 @@ def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n             )\n             torch.backends.cuda.enable_flash_sdp(False)\n \n-        # This means we have `hard_check_only=False` and fallback to eager if SDPA isn't supported\n-        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n-        if not is_torch_sdpa_available() or not self._supports_sdpa or _is_bettertransformer:\n-            return False\n+        if not is_init_check:\n+            if getattr(self, \"use_bettertransformer\", False):\n+                raise ValueError(\n+                    \"SDPA and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n+                )\n \n         return True\n \n-    def _flex_attn_can_dispatch(self) -> bool:\n+    def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n         \"\"\"\n-        Checks the availability of Flex Attention for a given model.\n+        Check the availability of Flex Attention for a given model.\n \n-        If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"flex_attention\" so that the model can initialize the correct attention module.\n+        Args:\n+            is_init_check (`bool`, *optional*):\n+                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n+                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n+                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n+                before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n         if not self._supports_flex_attn:\n             raise ValueError(\n@@ -2670,9 +2568,190 @@ def _flex_attn_can_dispatch(self) -> bool:\n                 \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n             )\n \n+        if not is_init_check:\n+            if getattr(self, \"use_bettertransformer\", False):\n+                raise ValueError(\n+                    \"FlexAttention and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n+                )\n+\n         # If no error raise by this point, we can return `True`\n         return True\n \n+    def _check_and_adjust_attn_implementation(\n+        self, attn_implementation: Optional[str], is_init_check: bool = False\n+    ) -> str:\n+        \"\"\"\n+        Check that the `attn_implementation` exists and is supported by the models, and try to get the kernel from hub if\n+        it matches hf kernels pattern.\n+\n+        Args:\n+            attn_implementation (`str` or `None`):\n+                The attention implementation to check for existence/validity.\n+            is_init_check (`bool`, *optional*):\n+                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n+                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n+                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n+                before instantiating the full models if we know that the model does not support the requested attention.\n+\n+        Returns:\n+            `str`: The final attention implementation to use, including potential fallbacks from sdpa to eager, or from\n+            None to sdpa (to potentially eager).\n+        \"\"\"\n+        applicable_attn_implementation = \"sdpa\" if attn_implementation is None else attn_implementation\n+        if re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", applicable_attn_implementation):\n+            if not is_kernels_available():\n+                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n+\n+            # Extract repo_id and kernel_name from the string\n+            repo_id, kernel_name = applicable_attn_implementation.split(\":\")\n+            kernel_name = kernel_name.strip()\n+            repo_id = repo_id.strip()\n+\n+            try:\n+                kernel = get_kernel(repo_id)\n+                ALL_ATTENTION_FUNCTIONS.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n+                applicable_attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n+            except FileNotFoundError as e:\n+                logger.warning_once(\n+                    f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n+                    \"default attention implementation instead (sdpa if available, eager otherwise).\"\n+                )\n+                applicable_attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n+            except AttributeError:\n+                raise ValueError(\n+                    \"the kernel function name or class specified in the attn_implementation argument is not valid. Please check \"\n+                    \"the documentation for the correct format, and check that the kernel exports the class and the function correctly.\"\n+                )\n+        if applicable_attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n+            message = (\n+                f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are '\n+                '`attn_implementation=\"eager\"` (manual attention implementation)'\n+            )\n+            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n+            if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n+                message += (\n+                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n+                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n+                )\n+            if self._supports_sdpa:\n+                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n+            if self._supports_flex_attn:\n+                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n+            raise ValueError(message + \".\")\n+\n+        # Perform relevant checks\n+        if applicable_attn_implementation == \"flash_attention_2\":\n+            self._flash_attn_2_can_dispatch(is_init_check)\n+        elif applicable_attn_implementation == \"flash_attention_3\":\n+            self._flash_attn_3_can_dispatch(is_init_check)\n+        elif applicable_attn_implementation == \"flex_attention\":\n+            self._flex_attn_can_dispatch(is_init_check)\n+        elif applicable_attn_implementation == \"sdpa\":\n+            # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n+            try:\n+                self._sdpa_can_dispatch(is_init_check)\n+            except (ValueError, ImportError) as e:\n+                # In this case, sdpa was requested explicitly, but we can't use it, so let's raise\n+                if attn_implementation == \"sdpa\":\n+                    raise e\n+                applicable_attn_implementation = \"eager\"\n+\n+        return applicable_attn_implementation\n+\n+    @classmethod\n+    def _can_set_attn_implementation(cls) -> bool:\n+        \"\"\"Detect whether the class supports setting its attention implementation dynamically. It is an ugly check based on\n+        opening the file, but avoids maintaining yet another property flag.\n+        \"\"\"\n+        class_file = sys.modules[cls.__module__].__file__\n+        with open(class_file, \"r\") as f:\n+            code = f.read()\n+        # heuristic -> if we find those patterns, the model uses the correct interface\n+        return (\n+            \"eager_attention_forward\" in code and \"ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\" in code\n+        )\n+\n+    def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n+        \"\"\"\n+        Set the requested `attn_implementation` for this model.\n+\n+        Args:\n+            attn_implementation (`str` or `dict`):\n+                The attention implementation to set for this model. It can be either a `str`, in which case it will be\n+                dispatched to all submodels if relevant, or a `dict` where keys are the sub_configs name, in which case each\n+                submodel will dispatch the corresponding value.\n+        \"\"\"\n+        requested_implementation = (\n+            attn_implementation\n+            if not isinstance(attn_implementation, dict)\n+            else attn_implementation.get(\"\", self.config._attn_implementation)\n+        )\n+\n+        # At this point, the model was already instantiated, so instead of crashing on bad value, let's simply\n+        # warn the user that the requested value is not working\n+        if requested_implementation != self.config._attn_implementation:\n+            # In this case, raise\n+            if not self._can_set_attn_implementation():\n+                logger.warning(\n+                    f\"{self.__class__.__name__} does not support setting its attention implementation dynamically, because it \"\n+                    \"does not follow the functional approach based on AttentionInterface \"\n+                    \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n+                )\n+            else:\n+                try:\n+                    applicable_attn_implementation = self._check_and_adjust_attn_implementation(\n+                        requested_implementation, is_init_check=False\n+                    )\n+                    # Apply the change (on the internal attr, to avoid setting it recursively)\n+                    self.config._attn_implementation_internal = applicable_attn_implementation\n+                except (ValueError, ImportError) as e:\n+                    logger.warning(\n+                        f\"Impossible to set the requested `attn_implementation`. The following error was captured: {str(e)}\"\n+                    )\n+\n+        subconfigs_changed = set()\n+        # Apply it to all submodels as well\n+        for submodule in self.modules():\n+            # We found a submodel (which is not self) with a different config (otherwise, it may be the same \"actual model\",\n+            # e.g. ForCausalLM has a Model inside, but no need to check it again)\n+            if (\n+                submodule is not self\n+                and isinstance(submodule, PreTrainedModel)\n+                and submodule.config.__class__ != self.config.__class__\n+            ):\n+                sub_implementation = attn_implementation\n+                if isinstance(attn_implementation, dict):\n+                    for subconfig_key in self.config.sub_configs:\n+                        # We need to check for exact object match here, with `is`\n+                        if getattr(self.config, subconfig_key) is submodule.config:\n+                            sub_implementation = attn_implementation.get(\n+                                subconfig_key, submodule.config._attn_implementation\n+                            )\n+                            break\n+                submodule.set_attn_implementation(sub_implementation)\n+                subconfigs_changed.add(submodule.config.__class__)\n+\n+        # We need this as some old and badly designed models use subconfigs without declaring the corresponding modules as PreTrainedModel\n+        for subconfig_key in self.config.sub_configs:\n+            subconfig = getattr(self.config, subconfig_key)\n+            requested_implementation = (\n+                attn_implementation\n+                if not isinstance(attn_implementation, dict)\n+                else attn_implementation.get(subconfig_key, subconfig._attn_implementation)\n+            )\n+            # This means we did not perform any check above for this particular subconfig -> set it in the dark if it is registered\n+            if (\n+                subconfig.__class__ not in subconfigs_changed\n+                and requested_implementation != subconfig._attn_implementation\n+                and requested_implementation in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n+            ):\n+                subconfig._attn_implementation_internal = requested_implementation\n+                logger.warning(\n+                    f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{requested_implementation}` \"\n+                    \"without finding the associated sub-model. For this reason we could not check if the model supports it. \"\n+                    \"You may encounter undefined behavior.\"\n+                )\n+\n     def enable_input_require_grads(self):\n         \"\"\"\n         Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n@@ -4601,21 +4680,14 @@ def from_pretrained(\n             if \"gguf_file\" in model_kwargs:\n                 model_kwargs.pop(\"gguf_file\")\n         else:\n-            # In case one passes a config to `from_pretrained` + \"attn_implementation\"\n-            # override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\n-            # Please see: https://github.com/huggingface/transformers/issues/28038\n-\n-            # Overwrite `config._attn_implementation` by the one from the kwargs --> in auto-factory\n-            # we pop attn_implementation from the kwargs but this handles the case where users\n-            # passes manually the config to `from_pretrained`.\n             config = copy.deepcopy(config)\n-\n-            kwarg_attn_imp = kwargs.pop(\"attn_implementation\", None)\n-            if kwarg_attn_imp is not None:\n-                config._attn_implementation = kwarg_attn_imp\n-\n             model_kwargs = kwargs\n \n+        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call\n+        # to correctly redispatch recursively if the kwarg is provided\n+        if \"attn_implementation\" in kwargs:\n+            config._attn_implementation = kwargs.pop(\"attn_implementation\")\n+\n         transformers_explicit_filename = getattr(config, \"transformers_weights\", None)\n \n         if transformers_explicit_filename is not None:"
        },
        {
            "sha": "296d8a5d8090baa13920585c4f44c3258a3aa74d",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1226,6 +1226,8 @@ def __init__(self, config: AltCLIPConfig):\n \n         text_config = config.text_config\n         vision_config = config.vision_config\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        vision_config._attn_implementation = config._attn_implementation\n \n         self.projection_dim = config.projection_dim\n         self.text_embed_dim = text_config.project_dim"
        },
        {
            "sha": "45843732418966f94f21170ea9809b51407e32d3",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch BART model.\"\"\"\n \n-import copy\n import math\n import warnings\n from typing import Callable, Optional, Union\n@@ -1842,7 +1841,6 @@ class BartForCausalLM(BartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "8146d7b018c9bfe49752207a9a661eb72ae1bff3",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch BigBirdPegasus model.\"\"\"\n \n-import copy\n import math\n from typing import Callable, Optional, Union\n \n@@ -2923,7 +2922,6 @@ class BigBirdPegasusForCausalLM(BigBirdPegasusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "3b9467e6e2ab158c40a475911d4a236686c831ec",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch Blenderbot model.\"\"\"\n \n-import copy\n import math\n import os\n import warnings\n@@ -1488,7 +1487,6 @@ class BlenderbotForCausalLM(BlenderbotPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "7f7701d9dacb628d31f7fd65d33f10aa267af5c6",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch BlenderbotSmall model.\"\"\"\n \n-import copy\n import math\n from typing import Callable, Optional, Union\n \n@@ -1447,7 +1446,6 @@ class BlenderbotSmallForCausalLM(BlenderbotSmallPreTrainedModel, GenerationMixin\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "cf134e22c66c50159621e9d46769f13d7e3b4ab4",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1046,6 +1046,8 @@ def __init__(self, config: ChineseCLIPConfig):\n \n         text_config = config.text_config\n         vision_config = config.vision_config\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        vision_config._attn_implementation = config._attn_implementation\n \n         self.projection_dim = config.projection_dim\n         self.text_embed_dim = text_config.hidden_size"
        },
        {
            "sha": "34d19ffaf3873873758a445b4634c8c4712e3c69",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -828,6 +828,10 @@ def __init__(self, config: CLIPSegConfig):\n \n         text_config = config.text_config\n         vision_config = config.vision_config\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        text_config._attn_implementation = config._attn_implementation\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        vision_config._attn_implementation = config._attn_implementation\n \n         self.projection_dim = config.projection_dim\n         self.text_embed_dim = text_config.hidden_size"
        },
        {
            "sha": "fac33b081837bdeaf923a9f0b67054a92bcb8e8e",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch GPTSANJapanese model.\"\"\"\n \n-import copy\n from typing import Optional, Union\n \n import torch\n@@ -849,7 +848,6 @@ class GPTSanJapaneseModel(GPTSanJapanesePreTrainedModel):\n     def __init__(self, config: GPTSanJapaneseConfig):\n         super().__init__(config)\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.d_model)\n-        self.config = copy.deepcopy(config)\n         self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model)\n         self.last_project = nn.Linear(config.d_model, config.d_model, bias=True)\n         self.act = ACT2FN[\"swish\"]"
        },
        {
            "sha": "aa248038f6cd3bf24db53e302b35e04eb91aaae6",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch Speech2Text2 model.\"\"\"\n \n-import copy\n import math\n from typing import Optional, Union\n \n@@ -682,7 +681,6 @@ class Speech2Text2ForCausalLM(Speech2Text2PreTrainedModel):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "70e46f2320223b595797cff6946bd9987f477a1f",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -294,7 +294,11 @@ def to_dict(self):\n \n     @property\n     def sub_configs(self):\n-        return {\"backbone_config\": type(self.backbone_config)} if self.backbone_config is not None else {}\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n \n \n __all__ = [\"DPTConfig\"]"
        },
        {
            "sha": "ec752c2b1df25e5eb35db3118a4aed28adf52861",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -942,6 +942,8 @@ def __init__(self, config: IdeficsConfig):\n \n         self.image_size = config.vision_config.image_size\n         self.vision_config = config.vision_config\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        self.vision_config._attn_implementation = config._attn_implementation\n         self.vision_model = IdeficsVisionTransformer(config.vision_config)\n \n         # Perceiver Resampler"
        },
        {
            "sha": "3bcc64db47cef70e68af03d4703106d9bd5f5de3",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1601,7 +1601,6 @@ class MarianForCausalLM(MarianPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "c02de1d4fee4b17cb3676709eb1d130c93ce96c1",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch MBART model.\"\"\"\n \n-import copy\n import math\n from typing import Callable, Optional, Union\n \n@@ -1799,7 +1798,6 @@ class MBartForCausalLM(MBartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "6b9be40ebf29b91faba7327a97159b444e104fde",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1882,7 +1882,7 @@ def __init__(self, config: MT5Config):\n         super().__init__(config)\n         self.shared = nn.Embedding(config.vocab_size, config.d_model)\n \n-        encoder_config = copy.deepcopy(config)\n+        encoder_config = config\n         encoder_config.use_cache = False\n         encoder_config.is_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config, self.shared)"
        },
        {
            "sha": "370afff3917f350c8217786a6b4e16a764b094c0",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch MVP model.\"\"\"\n \n-import copy\n import math\n from typing import Optional, Union\n \n@@ -1680,7 +1679,6 @@ class MvpForCausalLM(MvpPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "07cb8835bb331be1b71bf5f50efada83f0b3466a",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import copy\n import math\n from typing import Callable, Optional, Union\n \n@@ -1630,7 +1629,6 @@ class PLBartForCausalLM(PLBartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "6ddf829ad440fc4e857bc832f89d9fa470f1064a",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1689,18 +1689,10 @@ class Qwen2_5OmniThinkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForCo\n \n     def __init__(self, config: Qwen2_5OmniThinkerConfig):\n         super().__init__(config)\n-        self.audio_tower = Qwen2_5OmniAudioEncoder._from_config(\n-            config.audio_config, attn_implementation=config._attn_implementation\n-        )\n-\n-        self.visual = Qwen2_5OmniVisionEncoder._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n-\n+        self.audio_tower = Qwen2_5OmniAudioEncoder._from_config(config.audio_config)\n+        self.visual = Qwen2_5OmniVisionEncoder._from_config(config.vision_config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.model = Qwen2_5OmniThinkerTextModel._from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.model = Qwen2_5OmniThinkerTextModel._from_config(config.text_config)\n         self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.spatial_merge_size = config.vision_config.spatial_merge_size\n@@ -2953,7 +2945,6 @@ def __init__(self, config: Qwen2_5OmniDiTConfig):\n         self.heads = config.num_attention_heads\n         self.inner_dim = config.head_dim * config.num_attention_heads\n         self.dropout = config.dropout\n-        self._attn_implementation = config._attn_implementation\n         self.is_causal = False\n \n         self.to_q = nn.Linear(config.hidden_size, self.inner_dim)\n@@ -2987,7 +2978,7 @@ def forward(\n         cos, sin = position_embeddings\n         query[:, :1], key[:, :1] = apply_rotary_pos_emb(query[:, :1], key[:, :1], cos, sin)\n \n-        attention_interface = ALL_ATTENTION_FUNCTIONS[self._attn_implementation]\n+        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n         attention_weights, _ = attention_interface(\n             self,\n             query,"
        },
        {
            "sha": "e315a84583f37f26fe7f42401968b6ac2426c8fc",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -2144,18 +2144,10 @@ class Qwen2_5OmniThinkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForCo\n \n     def __init__(self, config: Qwen2_5OmniThinkerConfig):\n         super().__init__(config)\n-        self.audio_tower = Qwen2_5OmniAudioEncoder._from_config(\n-            config.audio_config, attn_implementation=config._attn_implementation\n-        )\n-\n-        self.visual = Qwen2_5OmniVisionEncoder._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n-\n+        self.audio_tower = Qwen2_5OmniAudioEncoder._from_config(config.audio_config)\n+        self.visual = Qwen2_5OmniVisionEncoder._from_config(config.vision_config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.model = Qwen2_5OmniThinkerTextModel._from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.model = Qwen2_5OmniThinkerTextModel._from_config(config.text_config)\n         self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.spatial_merge_size = config.vision_config.spatial_merge_size\n@@ -3270,7 +3262,6 @@ def __init__(self, config: Qwen2_5OmniDiTConfig):\n         self.heads = config.num_attention_heads\n         self.inner_dim = config.head_dim * config.num_attention_heads\n         self.dropout = config.dropout\n-        self._attn_implementation = config._attn_implementation\n         self.is_causal = False\n \n         self.to_q = nn.Linear(config.hidden_size, self.inner_dim)\n@@ -3304,7 +3295,7 @@ def forward(\n         cos, sin = position_embeddings\n         query[:, :1], key[:, :1] = apply_rotary_pos_emb(query[:, :1], key[:, :1], cos, sin)\n \n-        attention_interface = ALL_ATTENTION_FUNCTIONS[self._attn_implementation]\n+        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n         attention_weights, _ = attention_interface(\n             self,\n             query,"
        },
        {
            "sha": "b4446d68f51dadda2690521be0b2dda9e1d55d39",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1149,6 +1149,8 @@ def __init__(self, config):\n \n         self.vision_encoder = SamVisionEncoder(config.vision_config)\n         self.prompt_encoder = SamPromptEncoder(config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n         self.mask_decoder = SamMaskDecoder(config.mask_decoder_config)\n \n         self.post_init()"
        },
        {
            "sha": "6418e717202f2d6791d096b72d3a97aadf60bba5",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1274,6 +1274,8 @@ def __init__(self, config):\n         self.shared_image_embedding = SamHQPositionalEmbedding(config.vision_config)\n         self.vision_encoder = SamHQVisionEncoder(config.vision_config)\n         self.prompt_encoder = SamHQPromptEncoder(config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n \n         self.mask_decoder = SamHQMaskDecoder(config.mask_decoder_config)\n "
        },
        {
            "sha": "ba5741a904be1e458b0fd365847f57a98d1f3f50",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1836,7 +1836,7 @@ def __init__(self, config: T5Config):\n         super().__init__(config)\n         self.shared = nn.Embedding(config.vocab_size, config.d_model)\n \n-        encoder_config = copy.deepcopy(config)\n+        encoder_config = config\n         encoder_config.use_cache = False\n         encoder_config.is_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config, self.shared)"
        },
        {
            "sha": "1bc780b3d06859a0c80e2b136ee8f4f5da9edb5f",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch TrOCR decoder model (based on RoBERTa).\"\"\"\n \n-import copy\n import math\n from typing import Optional, Union\n \n@@ -715,7 +714,6 @@ class TrOCRForCausalLM(TrOCRPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"output_projection.weight\"]\n \n     def __init__(self, config):\n-        config = copy.deepcopy(config)\n         config.is_decoder = True\n         config.is_encoder_decoder = False\n         super().__init__(config)"
        },
        {
            "sha": "f6c5c51a27c5727a726882060855621b201f5416",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1173,6 +1173,10 @@ def __init__(self, config: XCLIPConfig):\n \n         text_config = config.text_config\n         vision_config = config.vision_config\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        text_config._attn_implementation = config._attn_implementation\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        vision_config._attn_implementation = config._attn_implementation\n \n         self.projection_dim = config.projection_dim\n         self.text_embed_dim = text_config.hidden_size"
        },
        {
            "sha": "f923273c0250f271168a8d4d20a84973965ea303",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -1085,6 +1085,10 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n+    @unittest.skip(\"T5 backbone deepcopies the configs, and fixing it would be more involved\")\n+    def test_internal_model_config_and_subconfig_are_same(self):\n+        pass\n+\n \n class Blip2TextModelWithProjectionTester:\n     def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=True):"
        },
        {
            "sha": "26737a647327cc3ad2929f2454387c9efcde35d2",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -542,6 +542,8 @@ def get_attention(self, multi_query):\n             attn_pdrop=0,\n             resid_pdrop=0,\n         )\n+        # We need to set it here as it's normally set by the Model's __init__\n+        config._attn_implementation = \"sdpa\"\n         return GPTBigCodeAttention(config)\n \n     @parameterized.expand([(seed, is_train_mode) for seed in range(5) for is_train_mode in [True, False]])"
        },
        {
            "sha": "8539e8371303491dd62a79f30382d81a7801f7d0",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 120,
            "deletions": 0,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -4783,6 +4783,126 @@ def test_can_load_with_meta_device_context_manager(self):\n                 f\"All parameters should be on meta device, but found {unique_devices}.\",\n             )\n \n+    def test_internal_model_config_and_subconfig_are_same(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        subconfig_keys = list(config.sub_configs.keys())\n+        for model_class in self.all_model_classes:\n+            if len(config.sub_configs) == 0:\n+                self.skipTest(reason=\"No subconfigs so the test does not make sense\")\n+            # Need to deepcopy here to avoid changing the _attn_implementation in-place\n+            model = model_class(copy.deepcopy(config))\n+\n+            for submodule in model.modules():\n+                # This is a submodel\n+                if isinstance(submodule, PreTrainedModel) and submodule.config.__class__ != model.config.__class__:\n+                    subconfig_from_model_internal = submodule.config\n+                    matching_sub_configs = []\n+                    for subconfig_key in subconfig_keys:\n+                        # Get the subconfig from the model config\n+                        subconfig_from_model_config = getattr(model.config, subconfig_key)\n+                        if subconfig_from_model_config.__class__ == subconfig_from_model_internal.__class__:\n+                            # Since some composite models have different submodels parameterized by 2 of the same config\n+                            # class instances, we need to check against a list of matching classes, and check that at least\n+                            # 1 is the exact object (instead of checking immediately for similar object)\n+                            matching_sub_configs.append(subconfig_from_model_config)\n+\n+                    # Both should be exactly the same object, that is when instantiating the submodel when should\n+                    # absolutely not copy the subconfig\n+                    if len(matching_sub_configs) > 0:\n+                        self.assertTrue(\n+                            any(\n+                                subconfig_from_model_config is subconfig_from_model_internal\n+                                for subconfig_from_model_config in matching_sub_configs\n+                            )\n+                        )\n+\n+    def test_can_set_attention_dynamically(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            if not model_class._can_set_attn_implementation():\n+                self.skipTest(reason=\"This model does not support setting its attention dynamically\")\n+\n+            # Need to deepcopy here to avoid changing the _attn_implementation in-place\n+            model_config = copy.deepcopy(config)\n+            # Set eager everywhere (it sets it recursively on subconfigs)\n+            model_config._attn_implementation = \"eager\"\n+            model = model_class(model_config)\n+\n+            # sanity check to make sure everything is correctly eager\n+            self.assertTrue(model.config._attn_implementation == \"eager\")\n+            for subconfig_key in model.config.sub_configs:\n+                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n+\n+            if not all(\n+                submodule._can_set_attn_implementation()\n+                for submodule in model.modules()\n+                if isinstance(submodule, PreTrainedModel)\n+            ):\n+                self.skipTest(reason=\"Parts of this model cannot set attention dynamically\")\n+            # Some old models technically should support switching, but don't have the flags active...\n+            if not all(\n+                submodule._supports_sdpa for submodule in model.modules() if isinstance(submodule, PreTrainedModel)\n+            ):\n+                self.skipTest(reason=\"Parts of this model don't support sdpa\")\n+\n+            # Now, set it to sdpa\n+            model.set_attn_implementation(\"sdpa\")\n+\n+            # Check everything was correctly changed\n+            self.assertTrue(model.config._attn_implementation == \"sdpa\")\n+            for subconfig_key in model.config.sub_configs:\n+                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n+\n+            # Check we cannot set it to random values, and it raises a warning (but no crash)\n+            with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as cm:\n+                model.set_attn_implementation(\"foo\")\n+                self.assertTrue(\n+                    any(\n+                        \"Impossible to set the requested `attn_implementation`. The following error was captured:\"\n+                        in warning\n+                        for warning in cm.output\n+                    )\n+                )\n+\n+            # Should still be sdpa everywhere\n+            self.assertTrue(model.config._attn_implementation == \"sdpa\")\n+            for subconfig_key in model.config.sub_configs:\n+                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n+\n+    def test_can_set_attention_dynamically_composite_model(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            if not model_class._can_set_attn_implementation():\n+                self.skipTest(reason=\"This model does not support setting its attention dynamically\")\n+            if not self._is_composite:\n+                self.skipTest(reason=\"This model is not composite\")\n+\n+            # Need to deepcopy here to avoid changing the _attn_implementation in-place\n+            model_config = copy.deepcopy(config)\n+            # Set eager everywhere (it sets it recursively on subconfigs)\n+            model_config._attn_implementation = \"eager\"\n+            model = model_class(model_config)\n+\n+            # sanity check to make sure everything is correctly eager\n+            self.assertTrue(model.config._attn_implementation == \"eager\")\n+            for subconfig_key in model.config.sub_configs:\n+                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n+\n+            if not all(\n+                submodule._can_set_attn_implementation()\n+                for submodule in model.modules()\n+                if isinstance(submodule, PreTrainedModel)\n+            ):\n+                self.skipTest(reason=\"Parts of this model cannot set attention dynamically\")\n+\n+            # Now, set only top-most to sdpa (should support it if it supports the dynamic switch)\n+            model.set_attn_implementation({\"\": \"sdpa\"})\n+\n+            # Check only top-most was correctly changed\n+            self.assertTrue(model.config._attn_implementation == \"sdpa\")\n+            for subconfig_key in model.config.sub_configs:\n+                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n+\n \n global_rng = random.Random()\n "
        },
        {
            "sha": "2bfd493993908a026c326afbada4c72cf7df4e3f",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -194,7 +194,6 @@ def test_config_common_kwargs_is_complete(self):\n                 \"_name_or_path\",\n                 \"_commit_hash\",\n                 \"_attn_implementation_internal\",\n-                \"_attn_implementation_autoset\",\n                 \"transformers_version\",\n             ],\n         )"
        },
        {
            "sha": "b58629757e8cd736682a7ffbd368184e58298217",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -83,12 +83,13 @@\n \n sys.path.append(str(Path(__file__).parent.parent.parent / \"utils\"))\n \n-from test_module.custom_configuration import CustomConfig, NoSuperInitConfig  # noqa E402\n+from test_module.custom_configuration import CustomConfig\n+\n \n if is_torch_available():\n     import torch\n     from safetensors.torch import save_file as safe_save_file\n-    from test_module.custom_modeling import CustomModel, NoSuperInitModel\n+    from test_module.custom_modeling import CustomModel\n     from torch import nn\n \n     from transformers import (\n@@ -732,36 +733,21 @@ def test_model_from_config_attn_implementation(self):\n             config = AutoConfig.from_pretrained(TINY_MISTRAL, attn_implementation=requested_attn_implementation)\n             # Ensure the config was set correctly\n             self.assertEqual(config._attn_implementation, requested_attn_implementation)\n-            self.assertEqual(config._attn_implementation_internal, requested_attn_implementation)\n             model = AutoModelForCausalLM.from_config(config)\n             self.assertEqual(model.config._attn_implementation, requested_attn_implementation)\n \n             config = AutoConfig.from_pretrained(TINY_MISTRAL)\n             # When the config is not set, the default is \"eager\"\n-            self.assertEqual(config._attn_implementation, \"eager\")\n-            self.assertEqual(config._attn_implementation_internal, None)\n+            self.assertEqual(config._attn_implementation, None)\n             model = AutoModelForCausalLM.from_config(config=config, attn_implementation=requested_attn_implementation)\n             self.assertEqual(model.config._attn_implementation, requested_attn_implementation)\n \n             # Set a nonsense attn_implementation in the config, which should be overridden by the explicit argument\n             config = AutoConfig.from_pretrained(TINY_MISTRAL, attn_implementation=\"foo-bar-baz\")\n             self.assertEqual(config._attn_implementation, \"foo-bar-baz\")\n-            self.assertEqual(config._attn_implementation_internal, \"foo-bar-baz\")\n             model = AutoModelForCausalLM.from_config(config=config, attn_implementation=requested_attn_implementation)\n             self.assertEqual(model.config._attn_implementation, requested_attn_implementation)\n \n-    def test_no_super_init_config_and_model(self):\n-        config = NoSuperInitConfig(attribute=32)\n-        model = NoSuperInitModel(config)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir)\n-\n-            new_model = NoSuperInitModel.from_pretrained(tmp_dir)\n-\n-        for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.equal(p1, p2))\n-\n     def test_checkpoint_sharding_local_bin(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n "
        },
        {
            "sha": "4bb0fe6a15dc15202318c099d32ff4a35b846930",
            "filename": "utils/test_module/custom_configuration.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/utils%2Ftest_module%2Fcustom_configuration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/utils%2Ftest_module%2Fcustom_configuration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftest_module%2Fcustom_configuration.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -7,10 +7,3 @@ class CustomConfig(PretrainedConfig):\n     def __init__(self, attribute=1, **kwargs):\n         self.attribute = attribute\n         super().__init__(**kwargs)\n-\n-\n-class NoSuperInitConfig(PretrainedConfig):\n-    model_type = \"custom\"\n-\n-    def __init__(self, attribute=1, **kwargs):\n-        self.attribute = attribute"
        },
        {
            "sha": "fafa7bff253d8e686a459f64294c3eb070fae792",
            "filename": "utils/test_module/custom_modeling.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ded9a41138773af49c70eac3fca7ad6ba9ede03/utils%2Ftest_module%2Fcustom_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ded9a41138773af49c70eac3fca7ad6ba9ede03/utils%2Ftest_module%2Fcustom_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftest_module%2Fcustom_modeling.py?ref=4ded9a41138773af49c70eac3fca7ad6ba9ede03",
            "patch": "@@ -2,7 +2,7 @@\n \n from transformers import PreTrainedModel\n \n-from .custom_configuration import CustomConfig, NoSuperInitConfig\n+from .custom_configuration import CustomConfig\n \n \n class CustomModel(PreTrainedModel):\n@@ -17,17 +17,3 @@ def forward(self, x):\n \n     def _init_weights(self, module):\n         pass\n-\n-\n-class NoSuperInitModel(PreTrainedModel):\n-    config_class = NoSuperInitConfig\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.linear = torch.nn.Linear(config.attribute, config.attribute)\n-\n-    def forward(self, x):\n-        return self.linear(x)\n-\n-    def _init_weights(self, module):\n-        pass"
        }
    ],
    "stats": {
        "total": 795,
        "additions": 472,
        "deletions": 323
    }
}