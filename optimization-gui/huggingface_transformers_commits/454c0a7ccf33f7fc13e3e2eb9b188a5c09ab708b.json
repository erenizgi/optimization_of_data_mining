{
    "author": "qgallouedec",
    "message": "Use `torch.get_autocast_dtype` instead of `torch.get_autocast_gpu_dtype` (#42055)\n\nUpdate dtype handling for PyTorch 2.4 compatibility in flash attention models",
    "sha": "454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
    "files": [
        {
            "sha": "4d7e15a0a0c14881caeb5c50492d4fe9c3d4a2ae",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -15,7 +15,12 @@ def get_target_dtype(query: torch.Tensor, module: torch.nn.Module) -> torch.dtyp\n     \"\"\"If the query is in float32, return a target dtype compatible with flash attention. Return None otherwise.\"\"\"\n     if query.dtype == torch.float32:\n         if torch.is_autocast_enabled():\n-            return torch.get_autocast_gpu_dtype()\n+            # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n+            return (\n+                torch.get_autocast_dtype(\"cuda\")\n+                if hasattr(torch, \"get_autocast_dtype\")\n+                else torch.get_autocast_gpu_dtype()\n+            )\n         # Handle the case where the model is quantized\n         elif hasattr(module.config, \"_pre_quantization_dtype\"):\n             return module.config._pre_quantization_dtype"
        },
        {
            "sha": "7e3474993c1f464f4a14e5efbef1f7e3cdd621d4",
            "filename": "src/transformers/kernels/falcon_mamba/selective_scan_with_ln_interface.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -250,12 +250,16 @@ def forward(\n         delta_rank = delta_proj_weight.shape[1]\n         d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n         if torch.is_autocast_enabled():\n-            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n-            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n-            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n-            out_proj_bias = (\n-                out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype()) if out_proj_bias is not None else None\n+            # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n+            target_dtype = (\n+                torch.get_autocast_dtype(\"cuda\")\n+                if hasattr(torch, \"get_autocast_dtype\")\n+                else torch.get_autocast_gpu_dtype()\n             )\n+            x_proj_weight = x_proj_weight.to(dtype=target_dtype)\n+            delta_proj_weight = delta_proj_weight.to(dtype=target_dtype)\n+            out_proj_weight = out_proj_weight.to(dtype=target_dtype)\n+            out_proj_bias = out_proj_bias.to(dtype=target_dtype) if out_proj_bias is not None else None\n         if xz.stride(-1) != 1:\n             xz = xz.contiguous()\n         conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")"
        },
        {
            "sha": "07565ef9aedadaae9573bd22b218a56a04aee20a",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -353,6 +353,7 @@ def forward(\n         device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "904881ed7fbdf194eec1eb5c1ec8f285862327d6",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -229,6 +229,7 @@ def forward(\n         device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "2d9f7a20eb1ba36535563b22a510f7af247b29be",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -137,6 +137,7 @@ class EsmForProteinFoldingOutput(ModelOutput):\n \n def is_fp16_enabled(device_type):\n     # Autocast world\n+    # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n     autocast_dtype = (\n         torch.get_autocast_dtype(device_type)\n         if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "085ff65644e3d73276d32e9a99f3e8624ed50d15",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -513,6 +513,7 @@ def forward(\n         device_type = query_layer.device.type if query_layer.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "1d14293fe95c76f83e5b30423d0d1c4b35895207",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -230,6 +230,7 @@ def forward(\n         device_type = query.device.type if query.device.type != \"mps\" else \"cpu\"\n         if query.dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "9e7888e5961d277c0d7ef25f02fe3151df173f32",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -327,6 +327,7 @@ def forward(\n         device_type = query.device.type if query.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "1e95b92d528d8e185e9383f93d620c59db636072",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -592,6 +592,7 @@ def forward(\n         device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "7fca9c22e5f733b4a93f4eba173e46e60d2fd5b5",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -806,6 +806,7 @@ def forward(\n         device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        },
        {
            "sha": "09a2bcbaf43037f4b2dfda03768623bd6b80d0b6",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=454c0a7ccf33f7fc13e3e2eb9b188a5c09ab708b",
            "patch": "@@ -601,6 +601,7 @@ def forward(\n         device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n                 target_dtype = (\n                     torch.get_autocast_dtype(device_type)\n                     if hasattr(torch, \"get_autocast_dtype\")"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 24,
        "deletions": 6
    }
}