{
    "author": "vasqu",
    "message": "[`Kernels Attention`] Change fallback logic to error out on explicit kernels request and include FA3 (#41010)\n\n* fix\n\n* be more strict\n\n* change logic to include fa3\n\n* fix the case where nothing is requested\n\n* modify old tests + add kernels related tests\n\n* style",
    "sha": "c1db38686acb480440e9d5e56a6b5cdbe8613321",
    "files": [
        {
            "sha": "71adbb9188e7139c95d99566e1d8feddc38c46cf",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1db38686acb480440e9d5e56a6b5cdbe8613321/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1db38686acb480440e9d5e56a6b5cdbe8613321/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=c1db38686acb480440e9d5e56a6b5cdbe8613321",
            "patch": "@@ -152,7 +152,10 @@ def load_and_register_kernel(attn_implementation: str) -> None:\n     if not is_kernel(attn_implementation):\n         return\n     if not _kernels_available:\n-        raise ImportError(\"`kernels` is not installed. Please install it with `pip install kernels`.\")\n+        raise ImportError(\n+            \"`kernels` is either not installed or uses an incompatible version. \"\n+            \"Please install the latest version with `pip install -U kernels`.\"\n+        )\n \n     # Need to be imported here as otherwise we have a circular import in `modeling_utils`\n     from ..masking_utils import ALL_MASK_ATTENTION_FUNCTIONS"
        },
        {
            "sha": "3853c369cd49e39222235b82991605ed3ff11463",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1db38686acb480440e9d5e56a6b5cdbe8613321/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1db38686acb480440e9d5e56a6b5cdbe8613321/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c1db38686acb480440e9d5e56a6b5cdbe8613321",
            "patch": "@@ -2574,35 +2574,39 @@ def _check_and_adjust_attn_implementation(\n             None to sdpa (to potentially eager).\n         \"\"\"\n         applicable_attn_implementation = attn_implementation\n+\n         # If FA not installed, do not fail but use kernels instead\n         if (\n-            applicable_attn_implementation == \"flash_attention_2\"\n+            attn_implementation is not None\n+            and attn_implementation.startswith(\"flash_attention\")\n             and self._supports_flash_attn\n-            and not is_flash_attn_2_available()\n+            and not (is_flash_attn_2_available() or is_flash_attn_3_available())\n             and is_kernels_available()\n         ):\n-            applicable_attn_implementation = \"kernels-community/flash-attn\"\n+            if attn_implementation.endswith(\"2\"):\n+                applicable_attn_implementation = \"kernels-community/flash-attn\"\n+            else:\n+                applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n+\n         if is_kernel(applicable_attn_implementation):\n             try:\n                 load_and_register_kernel(applicable_attn_implementation)\n                 # log that we used kernel fallback if successful\n-                if attn_implementation == \"flash_attention_2\":\n+                if attn_implementation.startswith(\"flash_attention\"):\n                     logger.warning_once(\n-                        \"You do not have `flash_attn` installed, using `kernels-community/flash-attn` from the `kernels` \"\n-                        \"library instead!\"\n+                        f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n+                        \"from the `kernels` library instead!\"\n                     )\n             except Exception as e:\n-                if attn_implementation == \"flash_attention_2\":\n-                    self._flash_attn_2_can_dispatch()  # will fail as fa2 is not available but raise the proper exception\n-                logger.warning_once(\n-                    f\"Could not find a kernel matching `{applicable_attn_implementation}` compatible with your device in the \"\n-                    f\"hub:\\n{e}.\\nUsing default attention implementation instead (sdpa if available, eager otherwise).\"\n-                )\n-                try:\n-                    self._sdpa_can_dispatch(is_init_check)\n-                    applicable_attn_implementation = \"sdpa\"\n-                except (ValueError, ImportError):\n-                    applicable_attn_implementation = \"eager\"\n+                # raise the proper exception for requested flash attention\n+                if attn_implementation.startswith(\"flash_attention\"):\n+                    if attn_implementation.endswith(\"2\"):\n+                        self._flash_attn_2_can_dispatch()\n+                    else:\n+                        self._flash_attn_3_can_dispatch()\n+\n+                # error properly out if a kernel was specifically requested\n+                raise e\n         else:\n             applicable_attn_implementation = self.get_correct_attn_implementation(\n                 applicable_attn_implementation, is_init_check"
        },
        {
            "sha": "495e043ee7819edc873fac80407ec5ac00b21256",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1db38686acb480440e9d5e56a6b5cdbe8613321/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1db38686acb480440e9d5e56a6b5cdbe8613321/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=c1db38686acb480440e9d5e56a6b5cdbe8613321",
            "patch": "@@ -87,6 +87,7 @@\n from transformers.utils.import_utils import (\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n+    is_kernels_available,\n     is_torch_npu_available,\n )\n \n@@ -2846,6 +2847,9 @@ def test_not_available_flash(self):\n                 reason=\"FlashAttention2 is supported on Ascend NPU without using package `flash-attn`, ignore this test case.\"\n             )\n \n+        if is_kernels_available():\n+            self.skipTest(reason=\"Please uninstall `kernels` package to run `test_not_available_flash`\")\n+\n         with self.assertRaises(ImportError) as cm:\n             _ = AutoModel.from_pretrained(\n                 \"hf-internal-testing/tiny-random-GPTBigCodeModel\", attn_implementation=\"flash_attention_2\"\n@@ -2861,6 +2865,9 @@ def test_not_available_flash_with_config(self):\n                 reason=\"FlashAttention2 is supported on Ascend NPU without using package `flash-attn`, ignore this test case.\"\n             )\n \n+        if is_kernels_available():\n+            self.skipTest(reason=\"Please uninstall `kernels` package to run `test_not_available_flash_with_config`\")\n+\n         config = AutoConfig.from_pretrained(\"hf-internal-testing/tiny-random-GPTBigCodeModel\")\n \n         with self.assertRaises(ImportError) as cm:\n@@ -2872,6 +2879,41 @@ def test_not_available_flash_with_config(self):\n \n         self.assertTrue(\"the package flash_attn seems to be not installed\" in str(cm.exception))\n \n+    def test_kernels_fallback(self):\n+        if not is_kernels_available():\n+            self.skipTest(reason=\"Please install `kernels` package to run `test_kernels_fallback`\")\n+\n+        if is_flash_attn_2_available():\n+            self.skipTest(reason=\"Please uninstall flash-attn package to run test_kernels_fallback\")\n+\n+        if is_torch_npu_available():\n+            self.skipTest(\n+                reason=\"FlashAttention2 is supported on Ascend NPU without using package `flash-attn`, ignore this test case.\"\n+            )\n+\n+        logger = logging.get_logger(\"transformers.modeling_utils\")\n+        with LoggingLevel(logging.WARNING):\n+            with CaptureLogger(logger) as cl:\n+                _ = AutoModel.from_pretrained(\n+                    \"hf-internal-testing/tiny-random-GPTBigCodeModel\", attn_implementation=\"flash_attention_2\"\n+                )\n+\n+        self.assertTrue(\n+            \"You do not have `flash_attn` installed, using `kernels-community/flash-attn` from the `kernels` library instead!\"\n+            in cl.out\n+        )\n+\n+    def test_not_available_kernels(self):\n+        if is_kernels_available():\n+            self.skipTest(reason=\"Please uninstall `kernels` package to run `test_not_available_kernels`\")\n+\n+        with self.assertRaises(ImportError) as cm:\n+            _ = AutoModel.from_pretrained(\n+                \"hf-tiny-model-private/tiny-random-MCTCTModel\", attn_implementation=\"kernels-community/flash-attn\"\n+            )\n+\n+        self.assertTrue(\"`kernels` is either not installed or uses an incompatible version.\" in str(cm.exception))\n+\n \n @require_torch\n class TestTensorSharing(TestCasePlus):"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 67,
        "deletions": 18
    }
}