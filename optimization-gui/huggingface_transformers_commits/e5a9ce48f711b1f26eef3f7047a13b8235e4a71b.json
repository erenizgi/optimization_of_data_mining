{
    "author": "sbucaille",
    "message": "Add LightGlue model (#31718)\n\n* init\n\n* chore: various changes to LightGlue\n\n* chore: various changes to LightGlue\n\n* chore: various changes to LightGlue\n\n* chore: various changes to LightGlue\n\n* Fixed dynamo bug and image padding tests\n\n* refactor: applied refactoring changes from SuperGlue's concat, batch and stack functions to LightGlue file\n\n* tests: removed sdpa support and changed expected values\n\n* chore: added some docs and refactoring\n\n* chore: fixed copy to superpoint.image_processing_superpoint.convert_to_grayscale\n\n* feat: adding batch implementation\n\n* feat: added validation for preprocess and post process method to LightGlueImageProcessor\n\n* chore: changed convert_lightglue_to_hf script to comply with new standard\n\n* chore: changed lightglue test values to match new lightglue config pushed to hub\n\n* chore: simplified convert_lightglue_to_hf conversion map\n\n* feat: adding batching implementation\n\n* chore: make style\n\n* feat: added threshold to post_process_keypoint_matching method\n\n* fix: added missing instructions that turns keypoints back to absolute coordinate before matching forward\n\n* fix: added typehint and docs\n\n* chore: make style\n\n* [run-slow] lightglue\n\n* fix: add matches different from -1 to compute valid matches in post_process_keypoint_matching\n\n* tests: added CUDA proof tests similar to SuperGlue\n\n* chore: various changes to modeling_lightglue.py\n\n- Added \"Copies from\" statements for copied functions from modeling_superglue.py\n- Added missing docstrings\n- Removed unused functions or classes\n- Removed unnecessary statements\n- Added missing typehints\n- Added comments to the main forward method\n\n* chore: various changes to convert_lightglue_to_hf.py\n\n- Added model saving\n- Added model reloading\n\n* chore: fixed imports in lightglue files\n\n* [run-slow] lightglue\n\n* chore: make style\n\n* [run-slow] lightglue\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* [run-slow] lightglue\n\n* chore: Applied some suggestions from review\n\n- Added missing typehints\n- Refactor \"cuda\" to device variable\n- Variable renaming\n- LightGlue output order changed\n- Make style\n\n* fix: added missing grayscale argument in image processor in case use of SuperPoint keypoint detector\n\n* fix: changed lightglue HF repo to lightglue_superpoint with grayscale default to True\n\n* refactor: make keypoints `(batch_size, num_keypoints, keypoint_dim)` through forward and unsqueeze only before attention layer\n\n* refactor: refactor do_layer_keypoint_pruning\n\n* tests: added tests with no early stop and keypoint pruning\n\n* refactor: various refactoring to modeling_lightglue.py\n\n- Removed unused functions\n- Renamed variables for consistency\n- Added comments for clarity\n- Set methods to private in LightGlueForKeypointMatching\n- Replaced tensor initialization to list then concatenation\n- Used more pythonic list comprehension for repetitive instructions\n\n* refactor: added comments and renamed filter_matches to get_matches_from_scores\n\n* tests: added copied from statement with superglue tests\n\n* docs: added comment to prepare_keypoint_matching_output function in tests\n\n* [run-slow] lightglue\n\n* refactor: reordered _concat_early_stopped_outputs in LightGlue class\n\n* [run-slow] lightglue\n\n* docs: added lightglue.md model doc\n\n* docs: added Optional typehint to LightGlueKeypointMatchingOutput\n\n* chore: removed pad_images function\n\n* chore: set do_grayscale default value to True in LightGlueImageProcessor\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* docs: added missing LightGlueConfig typehint in nn.Module __init__ methods\n\n* docs: removed unnecessary code in docs\n\n* docs: import SuperPointConfig only from a TYPE_CHECKING context\n\n* chore: use PretrainedConfig arguments `num_hidden_layers` and `num_attention_heads` instead of `num_layers` and `num_heads`\n\n* chore: added organization as arg in convert_lightglue_to_hf.py script\n\n* refactor: set device variable\n\n* chore: added \"gelu\" in LightGlueConfig as hidden_act parameter\n\n* docs: added comments to reshape.flip.reshape instruction to perform cross attention\n\n* refactor: used batched inference for keypoint detector forward pass\n\n* fix: added fix for SDPA tests\n\n* docs: fixed docstring for LightGlueImageProcessor\n\n* [run-slow] lightglue\n\n* refactor: removed unused line\n\n* refactor: added missing arguments in LightGlueConfig init method\n\n* docs: added missing LightGlueConfig typehint in init methods\n\n* refactor: added checkpoint url as default variable to verify models output only if it is the default url\n\n* fix: moved print message inside if statement\n\n* fix: added log assignment r removal in convert script\n\n* fix: got rid of confidence_thresholds as registered buffers\n\n* refactor: applied suggestions from SuperGlue PR\n\n* docs: changed copyright to 2025\n\n* refactor: modular LightGlue\n\n* fix: removed unnecessary import\n\n* feat: added plot_keypoint_matching method to LightGlueImageProcessor with matplotlib soft dependency\n\n* fix: added missing import error for matplotlib\n\n* Updated convert script to push on ETH org\n\n* fix: added missing licence\n\n* fix: make fix-copies\n\n* refactor: use cohere apply_rotary_pos_emb function\n\n* fix: update model references to use ETH-CVG/lightglue_superpoint\n\n* refactor: add and use intermediate_size attribute in config to inherit CLIPMLP for LightGlueMLP\n\n* refactor: explicit variables instead of slicing\n\n* refactor: use can_return_tuple decorator in LightGlue model\n\n* fix: make fix-copies\n\n* docs: Update model references in `lightglue.md` to use the correct pretrained model from ETH-CVG\n\n* Refactor LightGlue configuration and processing classes\n\n- Updated type hints for `keypoint_detector_config` in `LightGlueConfig` to use `SuperPointConfig` directly.\n- Changed `size` parameter in `LightGlueImageProcessor` to be optional.\n- Modified `position_embeddings` in `LightGlueAttention` and `LightGlueAttentionBlock` to be optional tuples.\n- Cleaned up import statements across multiple files for better readability and consistency.\n\n* refactor: Update LightGlue configuration to enforce eager attention implementation\n\n- Added `attn_implementation=\"eager\"` to `keypoint_detector_config` in `LightGlueConfig` and `LightGlueAttention` classes.\n- Removed unnecessary logging related to attention implementation fallback.\n- Cleaned up import statements for better readability.\n\n* refactor: renamed message into attention_output\n\n* fix: ensure device compatibility in LightGlueMatchAssignmentLayer descriptor normalization\n\n- Updated the normalization of `m_descriptors` to use the correct device for the tensor, ensuring compatibility across different hardware setups.\n\n* refactor: removed Conv layers from init_weights since LightGlue doesn't have any\n\n* refactor: replace add_start_docstrings with auto_docstring in LightGlue models\n\n- Updated LightGlue model classes to utilize the new auto_docstring utility for automatic documentation generation.\n- Removed legacy docstring handling to streamline the code and improve maintainability.\n\n* refactor: simplify LightGlue image processing tests by inheriting from SuperGlue\n\n- Refactored `LightGlueImageProcessingTester` and `LightGlueImageProcessingTest` to inherit from their SuperGlue counterparts, reducing code duplication.\n- Removed redundant methods and properties, streamlining the test setup and improving maintainability.\n\n* test: forced eager attention implementation to LightGlue model tests\n\n- Updated `LightGlueModelTester` to include `attn_implementation=\"eager\"` in the model configuration.\n- This change aligns the test setup with the recent updates in LightGlue configuration for eager attention.\n\n* refactor: update LightGlue model references\n\n* fix: import error\n\n* test: enhance LightGlue image processing tests with setup method\n\n- Added a setup method in `LightGlueImageProcessingTest` to initialize `LightGlueImageProcessingTester`.\n- Included a docstring for `LightGlueImageProcessingTester` to clarify its purpose.\n\n* refactor: added LightGlue image processing implementation to modular file\n\n* refactor: moved attention blocks into the transformer layer\n\n* fix: added missing import\n\n* fix: added missing import in __all__ variable\n\n* doc: added comment about enforcing eager attention because of SuperPoint\n\n* refactor: added SuperPoint eager attention comment and moved functions to the closest they are used\n\n---------\n\nCo-authored-by: Steven Bucaille <steven.bucaille@buawei.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
    "files": [
        {
            "sha": "fd9b69ebc17af8c0bf58558e04ca7968ee9cb014",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -743,6 +743,8 @@\n         title: ImageGPT\n       - local: model_doc/levit\n         title: LeViT\n+      - local: model_doc/lightglue\n+        title: LightGlue\n       - local: model_doc/mask2former\n         title: Mask2Former\n       - local: model_doc/maskformer"
        },
        {
            "sha": "3d9403c4206a253396a573d58411d09915b5bd32",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,104 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the MIT License; you may not use this file except in compliance with\n+the License.\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+\n+-->\n+\n+# LightGlue\n+\n+## Overview\n+\n+The LightGlue model was proposed in [LightGlue: Local Feature Matching at Light Speed](https://arxiv.org/abs/2306.13643)\n+by Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys.\n+\n+Similar to [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor), this model consists of matching\n+two sets of local features extracted from two images, its goal is to be faster than SuperGlue. Paired with the \n+[SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and \n+estimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\n+\n+The abstract from the paper is the following:\n+\n+*We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple\n+design decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. \n+Cumulatively, they make LightGlue more efficient - in terms of both memory and computation, more accurate, and much\n+easier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much\n+faster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited\n+appearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like\n+3D reconstruction. The code and trained models are publicly available at this [https URL](https://github.com/cvg/LightGlue)*\n+\n+## How to use\n+\n+Here is a quick example of using the model. Since this model is an image matching model, it requires pairs of images to be matched. \n+The raw outputs contain the list of keypoints detected by the keypoint detector as well as the list of matches with their corresponding \n+matching scores.\n+```python\n+from transformers import AutoImageProcessor, AutoModel\n+import torch\n+from PIL import Image\n+import requests\n+\n+url_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n+image1 = Image.open(requests.get(url_image1, stream=True).raw)\n+url_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n+image2 = Image.open(requests.get(url_image2, stream=True).raw)\n+\n+images = [image1, image2]\n+\n+processor = AutoImageProcessor.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n+model = AutoModel.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n+\n+inputs = processor(images, return_tensors=\"pt\")\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+```\n+\n+You can use the `post_process_keypoint_matching` method from the `LightGlueImageProcessor` to get the keypoints and matches in a readable format:\n+```python\n+image_sizes = [[(image.height, image.width) for image in images]]\n+outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+for i, output in enumerate(outputs):\n+    print(\"For the image pair\", i)\n+    for keypoint0, keypoint1, matching_score in zip(\n+            output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n+    ):\n+        print(\n+            f\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n+        )\n+```\n+\n+You can visualize the matches between the images by providing the original images as well as the outputs to this method:\n+```python\n+processor.plot_keypoint_matching(images, outputs)\n+```\n+\n+![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/duPp09ty8NRZlMZS18ccP.png)\n+\n+This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n+The original code can be found [here](https://github.com/cvg/LightGlue).\n+\n+## LightGlueConfig\n+\n+[[autodoc]] LightGlueConfig\n+\n+## LightGlueImageProcessor\n+\n+[[autodoc]] LightGlueImageProcessor\n+\n+- preprocess\n+- post_process_keypoint_matching\n+- plot_keypoint_matching\n+\n+## LightGlueForKeypointMatching\n+\n+[[autodoc]] LightGlueForKeypointMatching\n+\n+- forward"
        },
        {
            "sha": "5a277749f2915ed7185f19aadb8b179eb18992c9",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -231,6 +231,7 @@\n         \"is_faiss_available\",\n         \"is_flax_available\",\n         \"is_keras_nlp_available\",\n+        \"is_matplotlib_available\",\n         \"is_phonemizer_available\",\n         \"is_psutil_available\",\n         \"is_py3nvml_available\",\n@@ -728,6 +729,7 @@\n         is_faiss_available,\n         is_flax_available,\n         is_keras_nlp_available,\n+        is_matplotlib_available,\n         is_phonemizer_available,\n         is_psutil_available,\n         is_py3nvml_available,"
        },
        {
            "sha": "3520b79d695b2fe9f2d89c93e6c9d167efe10fb1",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -162,6 +162,7 @@\n     from .layoutxlm import *\n     from .led import *\n     from .levit import *\n+    from .lightglue import *\n     from .lilt import *\n     from .llama import *\n     from .llama4 import *"
        },
        {
            "sha": "5b46868f64badbe21c6fd0ac12633a4edb6439f4",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -185,6 +185,7 @@\n         (\"layoutlmv3\", \"LayoutLMv3Config\"),\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n+        (\"lightglue\", \"LightGlueConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n         (\"llama\", \"LlamaConfig\"),\n         (\"llama4\", \"Llama4Config\"),\n@@ -556,6 +557,7 @@\n         (\"layoutxlm\", \"LayoutXLM\"),\n         (\"led\", \"LED\"),\n         (\"levit\", \"LeViT\"),\n+        (\"lightglue\", \"LightGlue\"),\n         (\"lilt\", \"LiLT\"),\n         (\"llama\", \"LLaMA\"),\n         (\"llama2\", \"Llama2\"),"
        },
        {
            "sha": "5b7180246597515c5de2c00ed827709bfb3309fd",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -106,6 +106,7 @@\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"levit\", (\"LevitImageProcessor\", \"LevitImageProcessorFast\")),\n+            (\"lightglue\", (\"LightGlueImageProcessor\",)),\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),"
        },
        {
            "sha": "fbd0adfe4b108d2108dbc2a1fd6952e5be3564a6",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -175,6 +175,7 @@\n         (\"layoutlmv3\", \"LayoutLMv3Model\"),\n         (\"led\", \"LEDModel\"),\n         (\"levit\", \"LevitModel\"),\n+        (\"lightglue\", \"LightGlueForKeypointMatching\"),\n         (\"lilt\", \"LiltModel\"),\n         (\"llama\", \"LlamaModel\"),\n         (\"llama4\", \"Llama4ForConditionalGeneration\"),"
        },
        {
            "sha": "190e4e4329419f92da2141c2027f8212d6178e6f",
            "filename": "src/transformers/models/lightglue/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2F__init__.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lightglue import *\n+    from .image_processing_lightglue import *\n+    from .modeling_lightglue import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "f0962c0cc77a0eed1c830bf28acca2580b96e53f",
            "filename": "src/transformers/models/lightglue/configuration_lightglue.py",
            "status": "added",
            "additions": 143,
            "deletions": 0,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,143 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lightglue/modular_lightglue.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lightglue.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..superpoint import SuperPointConfig\n+\n+\n+class LightGlueConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LightGlueForKeypointMatching`]. It is used to\n+    instantiate a LightGlue model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the LightGlue\n+    [ETH-CVG/lightglue_superpoint](https://huggingface.co/ETH-CVG/lightglue_superpoint) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        keypoint_detector_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SuperPointConfig`):\n+            The config object or dictionary of the keypoint detector.\n+        descriptor_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the descriptors.\n+        num_hidden_layers (`int`, *optional*, defaults to 9):\n+            The number of self and cross attention layers.\n+        num_attention_heads (`int`, *optional*, defaults to 4):\n+            The number of heads in the multi-head attention.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        depth_confidence (`float`, *optional*, defaults to 0.95):\n+            The confidence threshold used to perform early stopping\n+        width_confidence (`float`, *optional*, defaults to 0.99):\n+            The confidence threshold used to prune points\n+        filter_threshold (`float`, *optional*, defaults to 0.1):\n+            The confidence threshold used to filter matches\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function to be used in the hidden layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+\n+    Examples:\n+        ```python\n+        >>> from transformers import LightGlueConfig, LightGlueForKeypointMatching\n+\n+        >>> # Initializing a LightGlue style configuration\n+        >>> configuration = LightGlueConfig()\n+\n+        >>> # Initializing a model from the LightGlue style configuration\n+        >>> model = LightGlueForKeypointMatching(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    \"\"\"\n+\n+    model_type = \"lightglue\"\n+    sub_configs = {\"keypoint_detector_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        keypoint_detector_config: SuperPointConfig = None,\n+        descriptor_dim: int = 256,\n+        num_hidden_layers: int = 9,\n+        num_attention_heads: int = 4,\n+        num_key_value_heads=None,\n+        depth_confidence: float = 0.95,\n+        width_confidence: float = 0.99,\n+        filter_threshold: float = 0.1,\n+        initializer_range: float = 0.02,\n+        hidden_act: str = \"gelu\",\n+        attention_dropout=0.0,\n+        attention_bias=True,\n+        **kwargs,\n+    ):\n+        if descriptor_dim % num_attention_heads != 0:\n+            raise ValueError(\"descriptor_dim % num_heads is different from zero\")\n+\n+        self.descriptor_dim = descriptor_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        self.depth_confidence = depth_confidence\n+        self.width_confidence = width_confidence\n+        self.filter_threshold = filter_threshold\n+        self.initializer_range = initializer_range\n+\n+        # Keypoint Detector is forced into eager attention mode because SuperPoint does not have Attention\n+        # See https://github.com/huggingface/transformers/pull/31718#discussion_r2109733153\n+        if isinstance(keypoint_detector_config, dict):\n+            keypoint_detector_config[\"model_type\"] = (\n+                keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n+            )\n+            keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n+                **keypoint_detector_config, attn_implementation=\"eager\"\n+            )\n+        if keypoint_detector_config is None:\n+            keypoint_detector_config = CONFIG_MAPPING[\"superpoint\"](attn_implementation=\"eager\")\n+\n+        self.keypoint_detector_config = keypoint_detector_config\n+\n+        self.hidden_size = descriptor_dim\n+        self.intermediate_size = descriptor_dim * 2\n+        self.hidden_act = hidden_act\n+        self.attention_dropout = attention_dropout\n+        self.attention_bias = attention_bias\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"LightGlueConfig\"]"
        },
        {
            "sha": "c1cb2ce5870705ce88c17f47a59ba1d3760ca97a",
            "filename": "src/transformers/models/lightglue/convert_lightglue_to_hf.py",
            "status": "added",
            "additions": 281,
            "deletions": 0,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,281 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import os\n+import re\n+from typing import List\n+\n+import torch\n+from datasets import load_dataset\n+\n+from transformers import (\n+    AutoModelForKeypointDetection,\n+    LightGlueForKeypointMatching,\n+    LightGlueImageProcessor,\n+)\n+from transformers.models.lightglue.configuration_lightglue import LightGlueConfig\n+\n+\n+DEFAULT_CHECKPOINT_URL = \"https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/superpoint_lightglue.pth\"\n+\n+\n+def prepare_imgs():\n+    dataset = load_dataset(\"hf-internal-testing/image-matching-test-dataset\", split=\"train\")\n+    image0 = dataset[0][\"image\"]\n+    image1 = dataset[1][\"image\"]\n+    image2 = dataset[2][\"image\"]\n+    # [image1, image1] on purpose to test the model early stopping\n+    return [[image2, image0], [image1, image1]]\n+\n+\n+def verify_model_outputs(model, device):\n+    images = prepare_imgs()\n+    preprocessor = LightGlueImageProcessor()\n+    inputs = preprocessor(images=images, return_tensors=\"pt\").to(device)\n+    model.to(device)\n+    with torch.no_grad():\n+        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+    predicted_matches_values = outputs.matches[0, 0, 20:30]\n+    predicted_matching_scores_values = outputs.matching_scores[0, 0, 20:30]\n+\n+    predicted_number_of_matches = torch.sum(outputs.matches[0][0] != -1).item()\n+\n+    expected_max_number_keypoints = 866\n+    expected_matches_shape = torch.Size((len(images), 2, expected_max_number_keypoints))\n+    expected_matching_scores_shape = torch.Size((len(images), 2, expected_max_number_keypoints))\n+\n+    expected_matches_values = torch.tensor([-1, -1, 5, -1, -1, 19, -1, 10, -1, 11], dtype=torch.int64).to(device)\n+    expected_matching_scores_values = torch.tensor([0, 0, 0.2997, 0, 0, 0.6762, 0, 0.8826, 0, 0.5583]).to(device)\n+\n+    expected_number_of_matches = 140\n+\n+    assert outputs.matches.shape == expected_matches_shape\n+    assert outputs.matching_scores.shape == expected_matching_scores_shape\n+\n+    assert torch.allclose(predicted_matches_values, expected_matches_values, atol=1e-2)\n+    assert torch.allclose(predicted_matching_scores_values, expected_matching_scores_values, atol=1e-2)\n+\n+    assert predicted_number_of_matches == expected_number_of_matches\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"posenc.Wr\": r\"positional_encoder.projector\",\n+    r\"self_attn.(\\d+).Wqkv\": r\"transformer_layers.\\1.self_attention.Wqkv\",\n+    r\"self_attn.(\\d+).out_proj\": r\"transformer_layers.\\1.self_attention.o_proj\",\n+    r\"self_attn.(\\d+).ffn.0\": r\"transformer_layers.\\1.self_mlp.fc1\",\n+    r\"self_attn.(\\d+).ffn.1\": r\"transformer_layers.\\1.self_mlp.layer_norm\",\n+    r\"self_attn.(\\d+).ffn.3\": r\"transformer_layers.\\1.self_mlp.fc2\",\n+    r\"cross_attn.(\\d+).to_qk\": r\"transformer_layers.\\1.cross_attention.to_qk\",\n+    r\"cross_attn.(\\d+).to_v\": r\"transformer_layers.\\1.cross_attention.v_proj\",\n+    r\"cross_attn.(\\d+).to_out\": r\"transformer_layers.\\1.cross_attention.o_proj\",\n+    r\"cross_attn.(\\d+).ffn.0\": r\"transformer_layers.\\1.cross_mlp.fc1\",\n+    r\"cross_attn.(\\d+).ffn.1\": r\"transformer_layers.\\1.cross_mlp.layer_norm\",\n+    r\"cross_attn.(\\d+).ffn.3\": r\"transformer_layers.\\1.cross_mlp.fc2\",\n+    r\"log_assignment.(\\d+).matchability\": r\"match_assignment_layers.\\1.matchability\",\n+    r\"log_assignment.(\\d+).final_proj\": r\"match_assignment_layers.\\1.final_projection\",\n+    r\"token_confidence.(\\d+).token.0\": r\"token_confidence.\\1.token\",\n+}\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: List[str]):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def add_keypoint_detector_state_dict(lightglue_state_dict):\n+    keypoint_detector = AutoModelForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n+    keypoint_detector_state_dict = keypoint_detector.state_dict()\n+    for k, v in keypoint_detector_state_dict.items():\n+        lightglue_state_dict[f\"keypoint_detector.{k}\"] = v\n+    return lightglue_state_dict\n+\n+\n+def split_weights(state_dict):\n+    for i in range(9):\n+        # Remove unused r values\n+        log_assignment_r_key = f\"log_assignment.{i}.r\"\n+        if state_dict.get(log_assignment_r_key, None) is not None:\n+            state_dict.pop(log_assignment_r_key)\n+\n+        Wqkv_weight = state_dict.pop(f\"transformer_layers.{i}.self_attention.Wqkv.weight\")\n+        Wqkv_bias = state_dict.pop(f\"transformer_layers.{i}.self_attention.Wqkv.bias\")\n+        Wqkv_weight = Wqkv_weight.reshape(256, 3, 256)\n+        Wqkv_bias = Wqkv_bias.reshape(256, 3)\n+        query_weight, key_weight, value_weight = Wqkv_weight[:, 0], Wqkv_weight[:, 1], Wqkv_weight[:, 2]\n+        query_bias, key_bias, value_bias = Wqkv_bias[:, 0], Wqkv_bias[:, 1], Wqkv_bias[:, 2]\n+        state_dict[f\"transformer_layers.{i}.self_attention.q_proj.weight\"] = query_weight\n+        state_dict[f\"transformer_layers.{i}.self_attention.k_proj.weight\"] = key_weight\n+        state_dict[f\"transformer_layers.{i}.self_attention.v_proj.weight\"] = value_weight\n+        state_dict[f\"transformer_layers.{i}.self_attention.q_proj.bias\"] = query_bias\n+        state_dict[f\"transformer_layers.{i}.self_attention.k_proj.bias\"] = key_bias\n+        state_dict[f\"transformer_layers.{i}.self_attention.v_proj.bias\"] = value_bias\n+\n+        to_qk_weight = state_dict.pop(f\"transformer_layers.{i}.cross_attention.to_qk.weight\")\n+        to_qk_bias = state_dict.pop(f\"transformer_layers.{i}.cross_attention.to_qk.bias\")\n+        state_dict[f\"transformer_layers.{i}.cross_attention.q_proj.weight\"] = to_qk_weight\n+        state_dict[f\"transformer_layers.{i}.cross_attention.q_proj.bias\"] = to_qk_bias\n+        state_dict[f\"transformer_layers.{i}.cross_attention.k_proj.weight\"] = to_qk_weight\n+        state_dict[f\"transformer_layers.{i}.cross_attention.k_proj.bias\"] = to_qk_bias\n+\n+    return state_dict\n+\n+\n+@torch.no_grad()\n+def write_model(\n+    model_path,\n+    checkpoint_url,\n+    organization,\n+    safe_serialization=True,\n+    push_to_hub=False,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+\n+    # ------------------------------------------------------------\n+    # LightGlue config\n+    # ------------------------------------------------------------\n+\n+    config = LightGlueConfig(\n+        descriptor_dim=256,\n+        num_hidden_layers=9,\n+        num_attention_heads=4,\n+    )\n+    config.architectures = [\"LightGlueForKeypointMatching\"]\n+    config.save_pretrained(model_path)\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(f\"Fetching all parameters from the checkpoint at {checkpoint_url}...\")\n+    original_state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n+\n+    print(\"Converting model...\")\n+    all_keys = list(original_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        state_dict[new_key] = original_state_dict.pop(key).contiguous().clone()\n+\n+    del original_state_dict\n+    gc.collect()\n+    state_dict = split_weights(state_dict)\n+    state_dict = add_keypoint_detector_state_dict(state_dict)\n+\n+    print(\"Loading the checkpoint in a LightGlue model...\")\n+    device = \"cuda\"\n+    with torch.device(device):\n+        model = LightGlueForKeypointMatching(config)\n+    model.load_state_dict(state_dict)\n+    print(\"Checkpoint loaded successfully...\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model...\")\n+    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = LightGlueForKeypointMatching.from_pretrained(model_path)\n+    print(\"Model reloaded successfully.\")\n+\n+    model_name = \"lightglue\"\n+    if \"superpoint\" in checkpoint_url:\n+        model_name += \"_superpoint\"\n+    if checkpoint_url == DEFAULT_CHECKPOINT_URL:\n+        print(\"Checking the model outputs...\")\n+        verify_model_outputs(model, device)\n+    print(\"Model outputs verified successfully.\")\n+\n+    if push_to_hub:\n+        print(\"Pushing model to the hub...\")\n+        model.push_to_hub(\n+            repo_id=f\"{organization}/{model_name}\",\n+            commit_message=\"Add model\",\n+        )\n+        config.push_to_hub(repo_id=f\"{organization}/{model_name}\", commit_message=\"Add config\")\n+\n+    write_image_processor(model_path, model_name, organization, push_to_hub=push_to_hub)\n+\n+\n+def write_image_processor(save_dir, model_name, organization, push_to_hub=False):\n+    if \"superpoint\" in model_name:\n+        image_processor = LightGlueImageProcessor(do_grayscale=True)\n+    else:\n+        image_processor = LightGlueImageProcessor()\n+    image_processor.save_pretrained(save_dir)\n+\n+    if push_to_hub:\n+        print(\"Pushing image processor to the hub...\")\n+        image_processor.push_to_hub(\n+            repo_id=f\"{organization}/{model_name}\",\n+            commit_message=\"Add image processor\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--checkpoint_url\",\n+        default=DEFAULT_CHECKPOINT_URL,\n+        type=str,\n+        help=\"URL of the original LightGlue checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=None,\n+        type=str,\n+        required=True,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\"--save_model\", action=\"store_true\", help=\"Save model to local\")\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Push model and image preprocessor to the hub\",\n+    )\n+    parser.add_argument(\n+        \"--organization\",\n+        default=\"ETH-CVG\",\n+        type=str,\n+        help=\"Hub organization in which you want the model to be uploaded.\",\n+    )\n+\n+    args = parser.parse_args()\n+    write_model(\n+        args.pytorch_dump_folder_path,\n+        args.checkpoint_url,\n+        args.organization,\n+        safe_serialization=True,\n+        push_to_hub=args.push_to_hub,\n+    )"
        },
        {
            "sha": "fea0b32df337484189ca3bbfab0e88111282fa4f",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "added",
            "additions": 452,
            "deletions": 0,
            "changes": 452,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,452 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lightglue/modular_lightglue.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lightglue.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import resize, to_channel_dimension_format\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    is_pil_image,\n+    is_scaled_image,\n+    is_valid_image,\n+    is_vision_available,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_matplotlib_available, logging, requires_backends\n+from ...utils.import_utils import requires\n+from .modeling_lightglue import LightGlueKeypointMatchingOutput\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def is_grayscale(\n+    image: ImageInput,\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+):\n+    if input_data_format == ChannelDimension.FIRST:\n+        if image.shape[0] == 1:\n+            return True\n+        return np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...])\n+    elif input_data_format == ChannelDimension.LAST:\n+        if image.shape[-1] == 1:\n+            return True\n+        return np.all(image[..., 0] == image[..., 1]) and np.all(image[..., 1] == image[..., 2])\n+\n+\n+def convert_to_grayscale(\n+    image: ImageInput,\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+) -> ImageInput:\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n+    and tensorflow grayscale conversion\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (Image):\n+            The image to convert.\n+        input_data_format (`ChannelDimension` or `str`, *optional*):\n+            The channel dimension format for the input image.\n+    \"\"\"\n+    requires_backends(convert_to_grayscale, [\"vision\"])\n+\n+    if isinstance(image, np.ndarray):\n+        if is_grayscale(image, input_data_format=input_data_format):\n+            return image\n+        if input_data_format == ChannelDimension.FIRST:\n+            gray_image = image[0, ...] * 0.2989 + image[1, ...] * 0.5870 + image[2, ...] * 0.1140\n+            gray_image = np.stack([gray_image] * 3, axis=0)\n+        elif input_data_format == ChannelDimension.LAST:\n+            gray_image = image[..., 0] * 0.2989 + image[..., 1] * 0.5870 + image[..., 2] * 0.1140\n+            gray_image = np.stack([gray_image] * 3, axis=-1)\n+        return gray_image\n+\n+    if not isinstance(image, PIL.Image.Image):\n+        return image\n+\n+    image = image.convert(\"L\")\n+    return image\n+\n+\n+def validate_and_format_image_pairs(images: ImageInput):\n+    error_message = (\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+    def _is_valid_image(image):\n+        \"\"\"images is a PIL Image or a 3D array.\"\"\"\n+        return is_pil_image(image) or (\n+            is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+        )\n+\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image)) for image in images):\n+            return images\n+        if all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            return [image for image_pair in images for image in image_pair]\n+    raise ValueError(error_message)\n+\n+\n+@requires(backends=(\"torch\",))\n+class LightGlueImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a LightGlue image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n+            by `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n+            Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n+            `True`. Can be overridden by `size` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_grayscale (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255,\n+        do_grayscale: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 480, \"width\": 640}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_grayscale = do_grayscale\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Resize an image.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dictionary of the form `{\"height\": int, \"width\": int}`, specifying the size of the output image.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the output image. If not provided, it will be inferred from the input\n+                image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        return resize(\n+            image,\n+            size=(size[\"height\"], size[\"width\"]),\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    def preprocess(\n+        self,\n+        images,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_grayscale: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to preprocess. Expects either a list of 2 images or a list of list of 2 images list with\n+                pixel values ranging from 0 to 255. If passing in images with pixel values between 0 and 1, set\n+                `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the output image after `resize` has been applied. If `size[\"shortest_edge\"]` >= 384, the image\n+                is resized to `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Otherwise, the smaller edge of the\n+                image will be matched to `int(size[\"shortest_edge\"]/ crop_pct)`, after which the image is cropped to\n+                `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Only has an effect if `do_resize` is set to `True`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of `PILImageResampling`, filters. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_grayscale (`bool`, *optional*, defaults to `self.do_grayscale`):\n+                Whether to convert the image to grayscale.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_grayscale = do_grayscale if do_grayscale is not None else self.do_grayscale\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        # Validate and convert the input images into a flattened list of images for all subsequent processing steps.\n+        images = validate_and_format_image_pairs(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+        )\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_grayscale:\n+                image = convert_to_grayscale(image, input_data_format=input_data_format)\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            all_images.append(image)\n+\n+        # Convert back the flattened list of images into a list of pairs of images.\n+        image_pairs = [all_images[i : i + 2] for i in range(0, len(all_images), 2)]\n+\n+        data = {\"pixel_values\": image_pairs}\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: LightGlueKeypointMatchingOutput,\n+        target_sizes: Union[TensorType, List[Tuple]],\n+        threshold: float = 0.0,\n+    ) -> List[Dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.mask.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, List):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for mask_pair, keypoints_pair, matches, scores in zip(\n+            outputs.mask, keypoints, outputs.matches[:, 0], outputs.matching_scores[:, 0]\n+        ):\n+            mask0 = mask_pair[0] > 0\n+            mask1 = mask_pair[1] > 0\n+            keypoints0 = keypoints_pair[0][mask0]\n+            keypoints1 = keypoints_pair[1][mask1]\n+            matches0 = matches[mask0]\n+            scores0 = scores[mask0]\n+\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores0 > threshold, matches0 > -1)\n+\n+            matched_keypoints0 = keypoints0[valid_matches]\n+            matched_keypoints1 = keypoints1[matches0[valid_matches]]\n+            matching_scores = scores0[valid_matches]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires\n+        matplotlib to be installed.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2 images or\n+                a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            outputs ([`LightGlueKeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+        \"\"\"\n+        if is_matplotlib_available():\n+            import matplotlib.pyplot as plt\n+        else:\n+            raise ImportError(\"Please install matplotlib to use `plot_keypoint_matching` method\")\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = np.zeros((max(height0, height1), width0 + width1, 3))\n+            plot_image[:height0, :width0] = image_pair[0] / 255.0\n+            plot_image[:height1, width0:] = image_pair[1] / 255.0\n+            plt.imshow(plot_image)\n+            plt.axis(\"off\")\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                plt.plot(\n+                    [keypoint0_x, keypoint1_x + width0],\n+                    [keypoint0_y, keypoint1_y],\n+                    color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n+                    alpha=0.9,\n+                    linewidth=0.5,\n+                )\n+                plt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\n+                plt.scatter(keypoint1_x + width0, keypoint1_y, c=\"black\", s=2)\n+            plt.show()\n+\n+\n+__all__ = [\"LightGlueImageProcessor\"]"
        },
        {
            "sha": "2cd8b0732f5fecfaaa1e0a3f3f026d87071c71bc",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "added",
            "additions": 926,
            "deletions": 0,
            "changes": 926,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,926 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lightglue/modular_lightglue.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lightglue.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+from torch import nn\n+from torch.nn.utils.rnn import pad_sequence\n+\n+from ...activations import ACT2FN\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, auto_docstring\n+from ...utils.generic import can_return_tuple\n+from ..auto.modeling_auto import AutoModelForKeypointDetection\n+from .configuration_lightglue import LightGlueConfig\n+\n+\n+@dataclass\n+class LightGlueKeypointMatchingOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of LightGlue keypoint matching models. Due to the nature of keypoint detection and matching,\n+    the number of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the\n+    batch of images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask\n+    tensor is used to indicate which values in the keypoints, matches, matching_scores and prune tensors are keypoint\n+    matching information.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n+            Loss computed during training.\n+        matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+            Index of keypoint matched in the other image.\n+        matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+            Scores of predicted matches.\n+        keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n+            Absolute (x, y) coordinates of predicted keypoints in a given image.\n+        prune (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n+            Pruning mask indicating which keypoints are removed and at which layer.\n+        mask (`torch.BoolTensor` of shape `(batch_size, num_keypoints)`):\n+            Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching\n+            information.\n+        hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n+            num_keypoints)` returned when `output_hidden_states=True` is passed or when\n+            `config.output_hidden_states=True`\n+        attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n+            num_keypoints)` returned when `output_attentions=True` is passed or when\n+            `config.output_attentions=True`\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    matches: Optional[torch.FloatTensor] = None\n+    matching_scores: Optional[torch.FloatTensor] = None\n+    keypoints: Optional[torch.FloatTensor] = None\n+    prune: Optional[torch.IntTensor] = None\n+    mask: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+class LightGluePositionalEncoder(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+        self.projector = nn.Linear(2, config.descriptor_dim // config.num_attention_heads // 2, bias=False)\n+\n+    def forward(\n+        self, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        projected_keypoints = self.projector(keypoints)\n+        embeddings = projected_keypoints.repeat_interleave(2, dim=-1)\n+        cosines = torch.cos(embeddings)\n+        sines = torch.sin(embeddings)\n+        embeddings = (cosines, sines)\n+        output = (embeddings, projected_keypoints) if output_hidden_states else (embeddings,)\n+        return output\n+\n+\n+def rotate_half(x):\n+    # Split and rotate. Note that this function is different from e.g. Llama.\n+    x1 = x[..., ::2]\n+    x2 = x[..., 1::2]\n+    rot_x = torch.stack([-x2, x1], dim=-1).flatten(-2)\n+    return rot_x\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    dtype = q.dtype\n+    q = q.float()\n+    k = k.float()\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class LightGlueAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: LightGlueConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        is_cross_attention = encoder_hidden_states is not None\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        current_attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        key_states = self.k_proj(current_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(current_states).view(hidden_shape).transpose(1, 2)\n+\n+        if position_embeddings is not None:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            current_attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class LightGlueMLP(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.intermediate_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.layer_norm = nn.LayerNorm(config.intermediate_size, elementwise_affine=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class LightGlueTransformerLayer(nn.Module):\n+    def __init__(self, config: LightGlueConfig, layer_idx: int):\n+        super().__init__()\n+        self.self_attention = LightGlueAttention(config, layer_idx)\n+        self.self_mlp = LightGlueMLP(config)\n+        self.cross_attention = LightGlueAttention(config, layer_idx)\n+        self.cross_mlp = LightGlueMLP(config)\n+\n+    def forward(\n+        self,\n+        descriptors: torch.Tensor,\n+        keypoints: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        output_hidden_states: Optional[bool] = False,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]], Optional[Tuple[torch.Tensor]]]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (descriptors,)\n+\n+        batch_size, num_keypoints, descriptor_dim = descriptors.shape\n+\n+        # Self attention block\n+        attention_output, self_attentions = self.self_attention(\n+            descriptors,\n+            position_embeddings=keypoints,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        intermediate_states = torch.cat([descriptors, attention_output], dim=-1)\n+        output_states = self.self_mlp(intermediate_states)\n+        self_attention_descriptors = descriptors + output_states\n+\n+        if output_hidden_states:\n+            self_attention_hidden_states = (intermediate_states, output_states)\n+\n+        # Reshape hidden_states to group by image_pairs :\n+        #   (batch_size, num_keypoints, descriptor_dim) -> (batch_size, 2, num_keypoints, descriptor_dim)\n+        # Flip dimension 1 to perform cross attention :\n+        #   (image0, image1) -> (image1, image0)\n+        # Reshape back to original shape :\n+        #   (batch_size, 2, num_keypoints, descriptor_dim) -> (batch_size, num_keypoints, descriptor_dim)\n+        encoder_hidden_states = (\n+            self_attention_descriptors.reshape(-1, 2, num_keypoints, descriptor_dim)\n+            .flip(1)\n+            .reshape(batch_size, num_keypoints, descriptor_dim)\n+        )\n+        # Same for mask\n+        encoder_attention_mask = (\n+            attention_mask.reshape(-1, 2, 1, 1, num_keypoints).flip(1).reshape(batch_size, 1, 1, num_keypoints)\n+            if attention_mask is not None\n+            else None\n+        )\n+\n+        # Cross attention block\n+        cross_attention_output, cross_attentions = self.cross_attention(\n+            self_attention_descriptors,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        cross_intermediate_states = torch.cat([self_attention_descriptors, cross_attention_output], dim=-1)\n+        cross_output_states = self.cross_mlp(cross_intermediate_states)\n+        descriptors = self_attention_descriptors + cross_output_states\n+\n+        if output_hidden_states:\n+            cross_attention_hidden_states = (cross_intermediate_states, cross_output_states)\n+            all_hidden_states = (\n+                all_hidden_states\n+                + (self_attention_descriptors.reshape(batch_size, num_keypoints, descriptor_dim),)\n+                + self_attention_hidden_states\n+                + (descriptors.reshape(batch_size, num_keypoints, descriptor_dim),)\n+                + cross_attention_hidden_states\n+            )\n+\n+        if output_attentions:\n+            all_attentions = all_attentions + (self_attentions,) + (cross_attentions,)\n+\n+        return descriptors, all_hidden_states, all_attentions\n+\n+\n+def sigmoid_log_double_softmax(\n+    similarity: torch.Tensor, matchability0: torch.Tensor, matchability1: torch.Tensor\n+) -> torch.Tensor:\n+    \"\"\"create the log assignment matrix from logits and similarity\"\"\"\n+    batch_size, num_keypoints_0, num_keypoints_1 = similarity.shape\n+    certainties = nn.functional.logsigmoid(matchability0) + nn.functional.logsigmoid(matchability1).transpose(1, 2)\n+    scores0 = nn.functional.log_softmax(similarity, 2)\n+    scores1 = nn.functional.log_softmax(similarity.transpose(-1, -2).contiguous(), 2).transpose(-1, -2)\n+    scores = similarity.new_full((batch_size, num_keypoints_0 + 1, num_keypoints_1 + 1), 0)\n+    scores[:, :num_keypoints_0, :num_keypoints_1] = scores0 + scores1 + certainties\n+    scores[:, :-1, -1] = nn.functional.logsigmoid(-matchability0.squeeze(-1))\n+    scores[:, -1, :-1] = nn.functional.logsigmoid(-matchability1.squeeze(-1))\n+    return scores\n+\n+\n+class LightGlueMatchAssignmentLayer(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+\n+        self.descriptor_dim = config.descriptor_dim\n+        self.final_projection = nn.Linear(self.descriptor_dim, self.descriptor_dim, bias=True)\n+        self.matchability = nn.Linear(self.descriptor_dim, 1, bias=True)\n+\n+    def forward(self, descriptors: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_keypoints, descriptor_dim = descriptors.shape\n+        # Final projection and similarity computation\n+        m_descriptors = self.final_projection(descriptors)\n+        m_descriptors = m_descriptors / torch.tensor(self.descriptor_dim, device=m_descriptors.device) ** 0.25\n+        m_descriptors = m_descriptors.reshape(batch_size // 2, 2, num_keypoints, descriptor_dim)\n+        m_descriptors0 = m_descriptors[:, 0]\n+        m_descriptors1 = m_descriptors[:, 1]\n+        similarity = m_descriptors0 @ m_descriptors1.transpose(-1, -2)\n+        if mask is not None:\n+            mask = mask.reshape(batch_size // 2, 2, num_keypoints)\n+            mask0 = mask[:, 0].unsqueeze(-1)\n+            mask1 = mask[:, 1].unsqueeze(-1).transpose(-1, -2)\n+            mask = mask0 * mask1\n+            similarity = similarity.masked_fill(mask == 0, torch.finfo(similarity.dtype).min)\n+\n+        # Compute matchability of descriptors\n+        matchability = self.matchability(descriptors)\n+        matchability = matchability.reshape(batch_size // 2, 2, num_keypoints, 1)\n+        matchability_0 = matchability[:, 0]\n+        matchability_1 = matchability[:, 1]\n+\n+        # Compute scores from similarity and matchability\n+        scores = sigmoid_log_double_softmax(similarity, matchability_0, matchability_1)\n+        return scores\n+\n+    def get_matchability(self, descriptors: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Get matchability of descriptors as a probability\"\"\"\n+        matchability = self.matchability(descriptors)\n+        matchability = nn.functional.sigmoid(matchability).squeeze(-1)\n+        return matchability\n+\n+\n+class LightGlueTokenConfidenceLayer(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+\n+        self.token = nn.Linear(config.descriptor_dim, 1)\n+\n+    def forward(self, descriptors: torch.Tensor) -> torch.Tensor:\n+        token = self.token(descriptors.detach())\n+        token = nn.functional.sigmoid(token).squeeze(-1)\n+        return token\n+\n+\n+@auto_docstring\n+class LightGluePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = LightGlueConfig\n+    base_model_prefix = \"lightglue\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = False\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"obtain matches from a score matrix [Bx M+1 x N+1]\"\"\"\n+    batch_size, _, _ = scores.shape\n+    # For each keypoint, get the best match\n+    max0 = scores[:, :-1, :-1].max(2)\n+    max1 = scores[:, :-1, :-1].max(1)\n+    matches0 = max0.indices\n+    matches1 = max1.indices\n+\n+    # Mutual check for matches\n+    indices0 = torch.arange(matches0.shape[1], device=matches0.device)[None]\n+    indices1 = torch.arange(matches1.shape[1], device=matches1.device)[None]\n+    mutual0 = indices0 == matches1.gather(1, matches0)\n+    mutual1 = indices1 == matches0.gather(1, matches1)\n+\n+    # Get matching scores and filter based on mutual check and thresholding\n+    max0 = max0.values.exp()\n+    zero = max0.new_tensor(0)\n+    matching_scores0 = torch.where(mutual0, max0, zero)\n+    matching_scores1 = torch.where(mutual1, matching_scores0.gather(1, matches1), zero)\n+    valid0 = mutual0 & (matching_scores0 > threshold)\n+    valid1 = mutual1 & valid0.gather(1, matches1)\n+\n+    # Filter matches based on mutual check and thresholding of scores\n+    matches0 = torch.where(valid0, matches0, -1)\n+    matches1 = torch.where(valid1, matches1, -1)\n+    matches = torch.stack([matches0, matches1]).transpose(0, 1).reshape(batch_size * 2, -1)\n+    matching_scores = torch.stack([matching_scores0, matching_scores1]).transpose(0, 1).reshape(batch_size * 2, -1)\n+\n+    return matches, matching_scores\n+\n+\n+def normalize_keypoints(keypoints: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+    \"\"\"\n+    Normalize keypoints locations based on image image_shape\n+\n+    Args:\n+        keypoints (`torch.Tensor` of shape `(batch_size, num_keypoints, 2)`):\n+            Keypoints locations in (x, y) format.\n+        height (`int`):\n+            Image height.\n+        width (`int`):\n+            Image width.\n+\n+    Returns:\n+        Normalized keypoints locations of shape (`torch.Tensor` of shape `(batch_size, num_keypoints, 2)`).\n+    \"\"\"\n+    size = torch.tensor([width, height], device=keypoints.device, dtype=keypoints.dtype)[None]\n+    shift = size / 2\n+    scale = size.max(-1).values / 2\n+    keypoints = (keypoints - shift[..., None, :]) / scale[..., None, None]\n+    return keypoints\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    LightGlue model taking images as inputs and outputting the matching of them.\n+    \"\"\"\n+)\n+class LightGlueForKeypointMatching(LightGluePreTrainedModel):\n+    \"\"\"\n+    LightGlue is a model matching keypoints in images by leveraging detections from a keypoint detector such as\n+    SuperPoint. It is based on the SuperGlue architecture and is designed to be lightweight and efficient.\n+    It consists of :\n+        1. Keypoint Encoder\n+        2. A Graph Neural Network with self and cross attention layers\n+        3. Matching Assignment layers\n+\n+    The correspondence ids use -1 to indicate non-matching points.\n+\n+    Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed.\n+    In ICCV 2023. https://arxiv.org/pdf/2306.13643.pdf\n+    \"\"\"\n+\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__(config)\n+\n+        self.keypoint_detector = AutoModelForKeypointDetection.from_config(config.keypoint_detector_config)\n+\n+        self.descriptor_dim = config.descriptor_dim\n+        self.num_layers = config.num_hidden_layers\n+        self.filter_threshold = config.filter_threshold\n+        self.depth_confidence = config.depth_confidence\n+        self.width_confidence = config.width_confidence\n+\n+        if self.descriptor_dim != config.keypoint_detector_config.descriptor_decoder_dim:\n+            self.input_projection = nn.Linear(\n+                config.keypoint_detector_config.descriptor_decoder_dim, self.descriptor_dim, bias=True\n+            )\n+        else:\n+            self.input_projection = nn.Identity()\n+\n+        self.positional_encoder = LightGluePositionalEncoder(config)\n+\n+        self.transformer_layers = nn.ModuleList(\n+            [LightGlueTransformerLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+        )\n+        self.match_assignment_layers = nn.ModuleList(\n+            [LightGlueMatchAssignmentLayer(config) for _ in range(config.num_hidden_layers)]\n+        )\n+        self.token_confidence = nn.ModuleList(\n+            [LightGlueTokenConfidenceLayer(config) for _ in range(config.num_hidden_layers - 1)]\n+        )\n+\n+        self.post_init()\n+\n+    def _get_confidence_threshold(self, layer_index: int) -> float:\n+        \"\"\"scaled confidence threshold for a given layer\"\"\"\n+        threshold = 0.8 + 0.1 * np.exp(-4.0 * layer_index / self.num_layers)\n+        return np.clip(threshold, 0, 1)\n+\n+    def _keypoint_processing(\n+        self, descriptors: torch.Tensor, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n+    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n+        descriptors = descriptors.detach().contiguous()\n+        projected_descriptors = self.input_projection(descriptors)\n+        keypoint_encoding_output = self.positional_encoder(keypoints, output_hidden_states=output_hidden_states)\n+        return projected_descriptors, keypoint_encoding_output\n+\n+    def _get_early_stopped_image_pairs(\n+        self, keypoint_confidences: torch.Tensor, layer_index: int, mask: torch.Tensor, num_points: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"evaluate whether we should stop inference based on the confidence of the keypoints\"\"\"\n+        batch_size, _ = mask.shape\n+        if layer_index < self.num_layers - 1:\n+            # If the current layer is not the last layer, we compute the confidence of the keypoints and check\n+            # if we should stop the forward pass through the transformer layers for each pair of images.\n+            keypoint_confidences = keypoint_confidences.masked_fill(mask == 0, 1)\n+            keypoint_confidences = keypoint_confidences.reshape(batch_size // 2, -1)\n+            threshold = self._get_confidence_threshold(layer_index)\n+            ratio_confident = 1.0 - (keypoint_confidences < threshold).float().sum(dim=1) / num_points\n+            early_stopped_pairs = ratio_confident > self.depth_confidence\n+        else:\n+            # If the current layer is the last layer, we stop the forward pass through the transformer layers for\n+            # all pairs of images.\n+            early_stopped_pairs = torch.ones(batch_size, dtype=torch.bool)\n+        return early_stopped_pairs\n+\n+    def _get_keypoint_matching(self, descriptors, mask, layer_index, early_stops=None):\n+        if early_stops is not None:\n+            descriptors = descriptors[early_stops]\n+            mask = mask[early_stops]\n+        scores = self.match_assignment_layers[layer_index](descriptors, mask)\n+        matches, matching_scores = get_matches_from_scores(scores, self.filter_threshold)\n+        return matches, matching_scores\n+\n+    def _get_pruning_mask(self, confidences: torch.Tensor, scores: torch.Tensor, layer_index: int) -> torch.Tensor:\n+        \"\"\"mask points which should be removed\"\"\"\n+        keep = scores > (1 - self.width_confidence)\n+        if confidences is not None:  # Low-confidence points are never pruned.\n+            keep |= confidences <= self._get_confidence_threshold(layer_index)\n+        return keep\n+\n+    def _do_layer_keypoint_pruning(\n+        self,\n+        descriptors: torch.Tensor,\n+        keypoints: torch.Tensor,\n+        mask: torch.Tensor,\n+        indices: torch.Tensor,\n+        prune_output: torch.Tensor,\n+        keypoint_confidences: torch.Tensor,\n+        layer_index: int,\n+    ):\n+        \"\"\"\n+        For a given layer, prune keypoints based on the confidence of the keypoints and the matchability of the\n+        descriptors.\n+        \"\"\"\n+        batch_size, _, _ = descriptors.shape\n+        descriptors_matchability = self.match_assignment_layers[layer_index].get_matchability(descriptors)\n+        pruned_keypoints_mask = self._get_pruning_mask(keypoint_confidences, descriptors_matchability, layer_index)\n+        pruned_keypoints_mask = pruned_keypoints_mask.masked_fill(mask == 0, torch.tensor(False))\n+\n+        # For each image, we extract the pruned indices and the corresponding descriptors and keypoints.\n+        pruned_descriptors, pruned_keypoints_0, pruned_keypoints_1, pruned_mask, pruned_indices = (\n+            [t[mask] for t, mask in zip(tensor, pruned_keypoints_mask)]\n+            for tensor in [descriptors, keypoints[0], keypoints[1], pruned_keypoints_mask, indices]\n+        )\n+        for i in range(batch_size):\n+            prune_output[i, pruned_indices[i]] += 1\n+\n+        # Pad the pruned descriptors, keypoints, indices and mask to have the same shape across the batch.\n+        pruned_descriptors, pruned_keypoints_0, pruned_keypoints_1, pruned_mask = (\n+            pad_sequence(pruned_tensor, batch_first=True)\n+            for pruned_tensor in [pruned_descriptors, pruned_keypoints_0, pruned_keypoints_1, pruned_mask]\n+        )\n+        pruned_keypoints = (pruned_keypoints_0, pruned_keypoints_1)\n+        pruned_indices = pad_sequence(pruned_indices, batch_first=True, padding_value=-1)\n+\n+        return pruned_descriptors, pruned_keypoints, pruned_indices, pruned_mask, prune_output\n+\n+    def _concat_early_stopped_outputs(\n+        self,\n+        early_stops_indices,\n+        final_pruned_keypoints_indices,\n+        final_pruned_keypoints_iterations,\n+        matches,\n+        matching_scores,\n+    ):\n+        early_stops_indices = torch.stack(early_stops_indices)\n+        matches, final_pruned_keypoints_indices = (\n+            pad_sequence(tensor, batch_first=True, padding_value=-1)\n+            for tensor in [matches, final_pruned_keypoints_indices]\n+        )\n+        matching_scores, final_pruned_keypoints_iterations = (\n+            pad_sequence(tensor, batch_first=True, padding_value=0)\n+            for tensor in [matching_scores, final_pruned_keypoints_iterations]\n+        )\n+        matches, matching_scores, final_pruned_keypoints_indices, final_pruned_keypoints_iterations = (\n+            tensor[early_stops_indices]\n+            for tensor in [\n+                matches,\n+                matching_scores,\n+                final_pruned_keypoints_indices,\n+                final_pruned_keypoints_iterations,\n+            ]\n+        )\n+        return final_pruned_keypoints_indices, final_pruned_keypoints_iterations, matches, matching_scores\n+\n+    def _do_final_keypoint_pruning(\n+        self,\n+        indices: torch.Tensor,\n+        matches: torch.Tensor,\n+        matching_scores: torch.Tensor,\n+        num_keypoints: torch.Tensor,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        # (batch_size, num_keypoints) -> (batch_size // 2, 2, num_keypoints) -> 2 * (batch_size // 2, num_keypoints) to\n+        # have tensors from\n+        batch_size, _ = indices.shape\n+        indices, matches, matching_scores = (\n+            tensor.reshape(batch_size // 2, 2, -1) for tensor in [indices, matches, matching_scores]\n+        )\n+        indices0 = indices[:, 0]\n+        indices1 = indices[:, 1]\n+        matches0 = matches[:, 0]\n+        matches1 = matches[:, 1]\n+        matching_scores0 = matching_scores[:, 0]\n+        matching_scores1 = matching_scores[:, 1]\n+\n+        # Prepare final matches and matching scores\n+        _matches = torch.full((batch_size // 2, 2, num_keypoints), -1, device=indices.device, dtype=matches.dtype)\n+        _matching_scores = torch.zeros(\n+            (batch_size // 2, 2, num_keypoints), device=indices.device, dtype=matching_scores.dtype\n+        )\n+        # Fill the matches and matching scores for each image pair\n+        for i in range(batch_size // 2):\n+            _matches[i, 0, indices0[i]] = torch.where(\n+                matches0[i] == -1, -1, indices1[i].gather(0, matches0[i].clamp(min=0))\n+            )\n+            _matches[i, 1, indices1[i]] = torch.where(\n+                matches1[i] == -1, -1, indices0[i].gather(0, matches1[i].clamp(min=0))\n+            )\n+            _matching_scores[i, 0, indices0[i]] = matching_scores0[i]\n+            _matching_scores[i, 1, indices1[i]] = matching_scores1[i]\n+        return _matches, _matching_scores\n+\n+    def _match_image_pair(\n+        self,\n+        keypoints: torch.Tensor,\n+        descriptors: torch.Tensor,\n+        height: int,\n+        width: int,\n+        mask: torch.Tensor = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple, Tuple]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        if keypoints.shape[2] == 0:  # no keypoints\n+            shape = keypoints.shape[:-1]\n+            return (\n+                keypoints.new_full(shape, -1, dtype=torch.int),\n+                keypoints.new_zeros(shape),\n+                keypoints.new_zeros(shape),\n+                all_hidden_states,\n+                all_attentions,\n+            )\n+\n+        device = keypoints.device\n+        batch_size, _, initial_num_keypoints, _ = keypoints.shape\n+        num_points_per_pair = torch.sum(mask.reshape(batch_size, -1), dim=1)\n+        # (batch_size, 2, num_keypoints, 2) -> (batch_size * 2, num_keypoints, 2)\n+        keypoints = keypoints.reshape(batch_size * 2, initial_num_keypoints, 2)\n+        mask = mask.reshape(batch_size * 2, initial_num_keypoints) if mask is not None else None\n+        descriptors = descriptors.reshape(batch_size * 2, initial_num_keypoints, self.descriptor_dim)\n+        image_indices = torch.arange(batch_size * 2, device=device)\n+        # Keypoint normalization\n+        keypoints = normalize_keypoints(keypoints, height, width)\n+\n+        descriptors, keypoint_encoding_output = self._keypoint_processing(\n+            descriptors, keypoints, output_hidden_states=output_hidden_states\n+        )\n+\n+        keypoints = keypoint_encoding_output[0]\n+\n+        # Early stop consists of stopping the forward pass through the transformer layers when the confidence of the\n+        # keypoints is above a certain threshold.\n+        do_early_stop = self.depth_confidence > 0\n+        # Keypoint pruning consists of removing keypoints from the input of the transformer layers when the confidence of\n+        # the keypoints is below a certain threshold.\n+        do_keypoint_pruning = self.width_confidence > 0\n+\n+        early_stops_indices = []\n+        matches = []\n+        matching_scores = []\n+        final_pruned_keypoints_indices = []\n+        final_pruned_keypoints_iterations = []\n+\n+        pruned_keypoints_indices = torch.arange(0, initial_num_keypoints, device=device).expand(batch_size * 2, -1)\n+        pruned_keypoints_iterations = torch.ones_like(pruned_keypoints_indices)\n+\n+        for layer_index in range(self.num_layers):\n+            input_shape = descriptors.size()\n+            if mask is not None:\n+                extended_attention_mask = self.get_extended_attention_mask(mask, input_shape)\n+            else:\n+                extended_attention_mask = torch.ones((batch_size, input_shape[-2]), device=keypoints.device)\n+            layer_output = self.transformer_layers[layer_index](\n+                descriptors,\n+                keypoints,\n+                attention_mask=extended_attention_mask,\n+                output_hidden_states=output_hidden_states,\n+                output_attentions=output_attentions,\n+            )\n+            descriptors, hidden_states, attention = layer_output\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + hidden_states\n+            if output_attentions:\n+                all_attentions = all_attentions + attention\n+\n+            if do_early_stop:\n+                if layer_index < self.num_layers - 1:\n+                    # Get the confidence of the keypoints for the current layer\n+                    keypoint_confidences = self.token_confidence[layer_index](descriptors)\n+\n+                    # Determine which pairs of images should be early stopped based on the confidence of the keypoints for\n+                    # the current layer.\n+                    early_stopped_pairs = self._get_early_stopped_image_pairs(\n+                        keypoint_confidences, layer_index, mask, num_points=num_points_per_pair\n+                    )\n+                else:\n+                    # Early stopping always occurs at the last layer\n+                    early_stopped_pairs = torch.ones(batch_size, dtype=torch.bool)\n+\n+                if torch.any(early_stopped_pairs):\n+                    # If a pair of images is considered early stopped, we compute the matches for the remaining\n+                    # keypoints and stop the forward pass through the transformer layers for this pair of images.\n+                    early_stops = early_stopped_pairs.repeat_interleave(2)\n+                    early_stopped_image_indices = image_indices[early_stops]\n+                    early_stopped_matches, early_stopped_matching_scores = self._get_keypoint_matching(\n+                        descriptors, mask, layer_index, early_stops=early_stops\n+                    )\n+                    early_stops_indices.extend(list(early_stopped_image_indices))\n+                    matches.extend(list(early_stopped_matches))\n+                    matching_scores.extend(list(early_stopped_matching_scores))\n+                    if do_keypoint_pruning:\n+                        final_pruned_keypoints_indices.extend(list(pruned_keypoints_indices[early_stops]))\n+                        final_pruned_keypoints_iterations.extend(list(pruned_keypoints_iterations[early_stops]))\n+\n+                    # Remove image pairs that have been early stopped from the forward pass\n+                    num_points_per_pair = num_points_per_pair[~early_stopped_pairs]\n+                    descriptors, keypoints_0, keypoint_1, mask, image_indices = tuple(\n+                        (\n+                            tensor[~early_stops]\n+                            for tensor in [descriptors, keypoints[0], keypoints[1], mask, image_indices]\n+                        )\n+                    )\n+                    keypoints = (keypoints_0, keypoint_1)\n+                    if do_keypoint_pruning:\n+                        pruned_keypoints_indices, pruned_keypoints_iterations, keypoint_confidences = tuple(\n+                            (\n+                                tensor[~early_stops]\n+                                for tensor in [\n+                                    pruned_keypoints_indices,\n+                                    pruned_keypoints_iterations,\n+                                    keypoint_confidences,\n+                                ]\n+                            )\n+                        )\n+                # If all pairs of images are early stopped, we stop the forward pass through the transformer\n+                # layers for all pairs of images.\n+                if torch.all(early_stopped_pairs):\n+                    break\n+\n+            if do_keypoint_pruning:\n+                # Prune keypoints from the input of the transformer layers for the next iterations if the confidence of\n+                # the keypoints is below a certain threshold.\n+                descriptors, keypoints, pruned_keypoints_indices, mask, pruned_keypoints_iterations = (\n+                    self._do_layer_keypoint_pruning(\n+                        descriptors,\n+                        keypoints,\n+                        mask,\n+                        pruned_keypoints_indices,\n+                        pruned_keypoints_iterations,\n+                        keypoint_confidences,\n+                        layer_index,\n+                    )\n+                )\n+\n+        if do_early_stop and do_keypoint_pruning:\n+            # Concatenate early stopped outputs together and perform final keypoint pruning\n+            final_pruned_keypoints_indices, final_pruned_keypoints_iterations, matches, matching_scores = (\n+                self._concat_early_stopped_outputs(\n+                    early_stops_indices,\n+                    final_pruned_keypoints_indices,\n+                    final_pruned_keypoints_iterations,\n+                    matches,\n+                    matching_scores,\n+                )\n+            )\n+            matches, matching_scores = self._do_final_keypoint_pruning(\n+                final_pruned_keypoints_indices,\n+                matches,\n+                matching_scores,\n+                initial_num_keypoints,\n+            )\n+        else:\n+            matches, matching_scores = self._get_keypoint_matching(descriptors, mask, self.num_layers - 1)\n+            final_pruned_keypoints_iterations = torch.ones_like(matching_scores) * self.num_layers\n+\n+        final_pruned_keypoints_iterations = final_pruned_keypoints_iterations.reshape(\n+            batch_size, 2, initial_num_keypoints\n+        )\n+\n+        return (\n+            matches,\n+            matching_scores,\n+            final_pruned_keypoints_iterations,\n+            all_hidden_states,\n+            all_attentions,\n+        )\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Union[Tuple, LightGlueKeypointMatchingOutput]:\n+        loss = None\n+        if labels is not None:\n+            raise ValueError(\"LightGlue is not trainable, no labels should be provided.\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if pixel_values.ndim != 5 or pixel_values.size(1) != 2:\n+            raise ValueError(\"Input must be a 5D tensor of shape (batch_size, 2, num_channels, height, width)\")\n+\n+        batch_size, _, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * 2, channels, height, width)\n+        keypoint_detections = self.keypoint_detector(pixel_values)\n+\n+        keypoints, _, descriptors, mask = keypoint_detections[:4]\n+        keypoints = keypoints.reshape(batch_size, 2, -1, 2).to(pixel_values)\n+        descriptors = descriptors.reshape(batch_size, 2, -1, self.descriptor_dim).to(pixel_values)\n+        mask = mask.reshape(batch_size, 2, -1)\n+\n+        absolute_keypoints = keypoints.clone()\n+        absolute_keypoints[:, :, :, 0] = absolute_keypoints[:, :, :, 0] * width\n+        absolute_keypoints[:, :, :, 1] = absolute_keypoints[:, :, :, 1] * height\n+\n+        matches, matching_scores, prune, hidden_states, attentions = self._match_image_pair(\n+            absolute_keypoints,\n+            descriptors,\n+            height,\n+            width,\n+            mask=mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        return LightGlueKeypointMatchingOutput(\n+            loss=loss,\n+            matches=matches,\n+            matching_scores=matching_scores,\n+            keypoints=keypoints,\n+            prune=prune,\n+            mask=mask,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\"LightGluePreTrainedModel\", \"LightGlueForKeypointMatching\"]"
        },
        {
            "sha": "482c230fb82fdb96ee32cbe04e8157dfa6e3b761",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "added",
            "additions": 1000,
            "deletions": 0,
            "changes": 1000,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,1000 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import Callable, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+from torch import nn\n+from torch.nn.utils.rnn import pad_sequence\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...image_utils import ImageInput, to_numpy_array\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TensorType, auto_docstring, is_matplotlib_available, logging\n+from ...utils.generic import can_return_tuple\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..auto.modeling_auto import AutoModelForKeypointDetection\n+from ..clip.modeling_clip import CLIPMLP\n+from ..cohere.modeling_cohere import apply_rotary_pos_emb\n+from ..llama.modeling_llama import LlamaAttention, eager_attention_forward\n+from ..superglue.image_processing_superglue import SuperGlueImageProcessor, validate_and_format_image_pairs\n+from ..superpoint import SuperPointConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC_ = \"LightGlueConfig\"\n+_CHECKPOINT_FOR_DOC_ = \"ETH-CVG/lightglue_superpoint\"\n+\n+\n+class LightGlueConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LightGlueForKeypointMatching`]. It is used to\n+    instantiate a LightGlue model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the LightGlue\n+    [ETH-CVG/lightglue_superpoint](https://huggingface.co/ETH-CVG/lightglue_superpoint) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        keypoint_detector_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SuperPointConfig`):\n+            The config object or dictionary of the keypoint detector.\n+        descriptor_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the descriptors.\n+        num_hidden_layers (`int`, *optional*, defaults to 9):\n+            The number of self and cross attention layers.\n+        num_attention_heads (`int`, *optional*, defaults to 4):\n+            The number of heads in the multi-head attention.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        depth_confidence (`float`, *optional*, defaults to 0.95):\n+            The confidence threshold used to perform early stopping\n+        width_confidence (`float`, *optional*, defaults to 0.99):\n+            The confidence threshold used to prune points\n+        filter_threshold (`float`, *optional*, defaults to 0.1):\n+            The confidence threshold used to filter matches\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function to be used in the hidden layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+\n+    Examples:\n+        ```python\n+        >>> from transformers import LightGlueConfig, LightGlueForKeypointMatching\n+\n+        >>> # Initializing a LightGlue style configuration\n+        >>> configuration = LightGlueConfig()\n+\n+        >>> # Initializing a model from the LightGlue style configuration\n+        >>> model = LightGlueForKeypointMatching(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    \"\"\"\n+\n+    model_type = \"lightglue\"\n+    sub_configs = {\"keypoint_detector_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        keypoint_detector_config: SuperPointConfig = None,\n+        descriptor_dim: int = 256,\n+        num_hidden_layers: int = 9,\n+        num_attention_heads: int = 4,\n+        num_key_value_heads=None,\n+        depth_confidence: float = 0.95,\n+        width_confidence: float = 0.99,\n+        filter_threshold: float = 0.1,\n+        initializer_range: float = 0.02,\n+        hidden_act: str = \"gelu\",\n+        attention_dropout=0.0,\n+        attention_bias=True,\n+        **kwargs,\n+    ):\n+        if descriptor_dim % num_attention_heads != 0:\n+            raise ValueError(\"descriptor_dim % num_heads is different from zero\")\n+\n+        self.descriptor_dim = descriptor_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        self.depth_confidence = depth_confidence\n+        self.width_confidence = width_confidence\n+        self.filter_threshold = filter_threshold\n+        self.initializer_range = initializer_range\n+\n+        # Keypoint Detector is forced into eager attention mode because SuperPoint does not have Attention\n+        # See https://github.com/huggingface/transformers/pull/31718#discussion_r2109733153\n+        if isinstance(keypoint_detector_config, dict):\n+            keypoint_detector_config[\"model_type\"] = (\n+                keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n+            )\n+            keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n+                **keypoint_detector_config, attn_implementation=\"eager\"\n+            )\n+        if keypoint_detector_config is None:\n+            keypoint_detector_config = CONFIG_MAPPING[\"superpoint\"](attn_implementation=\"eager\")\n+\n+        self.keypoint_detector_config = keypoint_detector_config\n+\n+        self.hidden_size = descriptor_dim\n+        self.intermediate_size = descriptor_dim * 2\n+        self.hidden_act = hidden_act\n+        self.attention_dropout = attention_dropout\n+        self.attention_bias = attention_bias\n+        super().__init__(**kwargs)\n+\n+\n+@dataclass\n+class LightGlueKeypointMatchingOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of LightGlue keypoint matching models. Due to the nature of keypoint detection and matching,\n+    the number of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the\n+    batch of images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask\n+    tensor is used to indicate which values in the keypoints, matches, matching_scores and prune tensors are keypoint\n+    matching information.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n+            Loss computed during training.\n+        matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+            Index of keypoint matched in the other image.\n+        matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+            Scores of predicted matches.\n+        keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n+            Absolute (x, y) coordinates of predicted keypoints in a given image.\n+        prune (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n+            Pruning mask indicating which keypoints are removed and at which layer.\n+        mask (`torch.BoolTensor` of shape `(batch_size, num_keypoints)`):\n+            Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching\n+            information.\n+        hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n+            num_keypoints)` returned when `output_hidden_states=True` is passed or when\n+            `config.output_hidden_states=True`\n+        attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n+            num_keypoints)` returned when `output_attentions=True` is passed or when\n+            `config.output_attentions=True`\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    matches: Optional[torch.FloatTensor] = None\n+    matching_scores: Optional[torch.FloatTensor] = None\n+    keypoints: Optional[torch.FloatTensor] = None\n+    prune: Optional[torch.IntTensor] = None\n+    mask: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+class LightGlueImageProcessor(SuperGlueImageProcessor):\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: LightGlueKeypointMatchingOutput,\n+        target_sizes: Union[TensorType, List[Tuple]],\n+        threshold: float = 0.0,\n+    ) -> List[Dict[str, torch.Tensor]]:\n+        return super().post_process_keypoint_matching(outputs, target_sizes, threshold)\n+\n+    def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires\n+        matplotlib to be installed.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2 images or\n+                a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            outputs ([`LightGlueKeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+        \"\"\"\n+        if is_matplotlib_available():\n+            import matplotlib.pyplot as plt\n+        else:\n+            raise ImportError(\"Please install matplotlib to use `plot_keypoint_matching` method\")\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = np.zeros((max(height0, height1), width0 + width1, 3))\n+            plot_image[:height0, :width0] = image_pair[0] / 255.0\n+            plot_image[:height1, width0:] = image_pair[1] / 255.0\n+            plt.imshow(plot_image)\n+            plt.axis(\"off\")\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                plt.plot(\n+                    [keypoint0_x, keypoint1_x + width0],\n+                    [keypoint0_y, keypoint1_y],\n+                    color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n+                    alpha=0.9,\n+                    linewidth=0.5,\n+                )\n+                plt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\n+                plt.scatter(keypoint1_x + width0, keypoint1_y, c=\"black\", s=2)\n+            plt.show()\n+\n+\n+class LightGluePositionalEncoder(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+        self.projector = nn.Linear(2, config.descriptor_dim // config.num_attention_heads // 2, bias=False)\n+\n+    def forward(\n+        self, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        projected_keypoints = self.projector(keypoints)\n+        embeddings = projected_keypoints.repeat_interleave(2, dim=-1)\n+        cosines = torch.cos(embeddings)\n+        sines = torch.sin(embeddings)\n+        embeddings = (cosines, sines)\n+        output = (embeddings, projected_keypoints) if output_hidden_states else (embeddings,)\n+        return output\n+\n+\n+class LightGlueAttention(LlamaAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        is_cross_attention = encoder_hidden_states is not None\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        current_attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        key_states = self.k_proj(current_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(current_states).view(hidden_shape).transpose(1, 2)\n+\n+        if position_embeddings is not None:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            current_attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class LightGlueMLP(CLIPMLP):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__(config)\n+        self.fc1 = nn.Linear(config.intermediate_size, config.intermediate_size)\n+        self.layer_norm = nn.LayerNorm(config.intermediate_size, elementwise_affine=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class LightGlueTransformerLayer(nn.Module):\n+    def __init__(self, config: LightGlueConfig, layer_idx: int):\n+        super().__init__()\n+        self.self_attention = LightGlueAttention(config, layer_idx)\n+        self.self_mlp = LightGlueMLP(config)\n+        self.cross_attention = LightGlueAttention(config, layer_idx)\n+        self.cross_mlp = LightGlueMLP(config)\n+\n+    def forward(\n+        self,\n+        descriptors: torch.Tensor,\n+        keypoints: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        output_hidden_states: Optional[bool] = False,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]], Optional[Tuple[torch.Tensor]]]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (descriptors,)\n+\n+        batch_size, num_keypoints, descriptor_dim = descriptors.shape\n+\n+        # Self attention block\n+        attention_output, self_attentions = self.self_attention(\n+            descriptors,\n+            position_embeddings=keypoints,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        intermediate_states = torch.cat([descriptors, attention_output], dim=-1)\n+        output_states = self.self_mlp(intermediate_states)\n+        self_attention_descriptors = descriptors + output_states\n+\n+        if output_hidden_states:\n+            self_attention_hidden_states = (intermediate_states, output_states)\n+\n+        # Reshape hidden_states to group by image_pairs :\n+        #   (batch_size, num_keypoints, descriptor_dim) -> (batch_size, 2, num_keypoints, descriptor_dim)\n+        # Flip dimension 1 to perform cross attention :\n+        #   (image0, image1) -> (image1, image0)\n+        # Reshape back to original shape :\n+        #   (batch_size, 2, num_keypoints, descriptor_dim) -> (batch_size, num_keypoints, descriptor_dim)\n+        encoder_hidden_states = (\n+            self_attention_descriptors.reshape(-1, 2, num_keypoints, descriptor_dim)\n+            .flip(1)\n+            .reshape(batch_size, num_keypoints, descriptor_dim)\n+        )\n+        # Same for mask\n+        encoder_attention_mask = (\n+            attention_mask.reshape(-1, 2, 1, 1, num_keypoints).flip(1).reshape(batch_size, 1, 1, num_keypoints)\n+            if attention_mask is not None\n+            else None\n+        )\n+\n+        # Cross attention block\n+        cross_attention_output, cross_attentions = self.cross_attention(\n+            self_attention_descriptors,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        cross_intermediate_states = torch.cat([self_attention_descriptors, cross_attention_output], dim=-1)\n+        cross_output_states = self.cross_mlp(cross_intermediate_states)\n+        descriptors = self_attention_descriptors + cross_output_states\n+\n+        if output_hidden_states:\n+            cross_attention_hidden_states = (cross_intermediate_states, cross_output_states)\n+            all_hidden_states = (\n+                all_hidden_states\n+                + (self_attention_descriptors.reshape(batch_size, num_keypoints, descriptor_dim),)\n+                + self_attention_hidden_states\n+                + (descriptors.reshape(batch_size, num_keypoints, descriptor_dim),)\n+                + cross_attention_hidden_states\n+            )\n+\n+        if output_attentions:\n+            all_attentions = all_attentions + (self_attentions,) + (cross_attentions,)\n+\n+        return descriptors, all_hidden_states, all_attentions\n+\n+\n+def sigmoid_log_double_softmax(\n+    similarity: torch.Tensor, matchability0: torch.Tensor, matchability1: torch.Tensor\n+) -> torch.Tensor:\n+    \"\"\"create the log assignment matrix from logits and similarity\"\"\"\n+    batch_size, num_keypoints_0, num_keypoints_1 = similarity.shape\n+    certainties = nn.functional.logsigmoid(matchability0) + nn.functional.logsigmoid(matchability1).transpose(1, 2)\n+    scores0 = nn.functional.log_softmax(similarity, 2)\n+    scores1 = nn.functional.log_softmax(similarity.transpose(-1, -2).contiguous(), 2).transpose(-1, -2)\n+    scores = similarity.new_full((batch_size, num_keypoints_0 + 1, num_keypoints_1 + 1), 0)\n+    scores[:, :num_keypoints_0, :num_keypoints_1] = scores0 + scores1 + certainties\n+    scores[:, :-1, -1] = nn.functional.logsigmoid(-matchability0.squeeze(-1))\n+    scores[:, -1, :-1] = nn.functional.logsigmoid(-matchability1.squeeze(-1))\n+    return scores\n+\n+\n+class LightGlueMatchAssignmentLayer(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+\n+        self.descriptor_dim = config.descriptor_dim\n+        self.final_projection = nn.Linear(self.descriptor_dim, self.descriptor_dim, bias=True)\n+        self.matchability = nn.Linear(self.descriptor_dim, 1, bias=True)\n+\n+    def forward(self, descriptors: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_keypoints, descriptor_dim = descriptors.shape\n+        # Final projection and similarity computation\n+        m_descriptors = self.final_projection(descriptors)\n+        m_descriptors = m_descriptors / torch.tensor(self.descriptor_dim, device=m_descriptors.device) ** 0.25\n+        m_descriptors = m_descriptors.reshape(batch_size // 2, 2, num_keypoints, descriptor_dim)\n+        m_descriptors0 = m_descriptors[:, 0]\n+        m_descriptors1 = m_descriptors[:, 1]\n+        similarity = m_descriptors0 @ m_descriptors1.transpose(-1, -2)\n+        if mask is not None:\n+            mask = mask.reshape(batch_size // 2, 2, num_keypoints)\n+            mask0 = mask[:, 0].unsqueeze(-1)\n+            mask1 = mask[:, 1].unsqueeze(-1).transpose(-1, -2)\n+            mask = mask0 * mask1\n+            similarity = similarity.masked_fill(mask == 0, torch.finfo(similarity.dtype).min)\n+\n+        # Compute matchability of descriptors\n+        matchability = self.matchability(descriptors)\n+        matchability = matchability.reshape(batch_size // 2, 2, num_keypoints, 1)\n+        matchability_0 = matchability[:, 0]\n+        matchability_1 = matchability[:, 1]\n+\n+        # Compute scores from similarity and matchability\n+        scores = sigmoid_log_double_softmax(similarity, matchability_0, matchability_1)\n+        return scores\n+\n+    def get_matchability(self, descriptors: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Get matchability of descriptors as a probability\"\"\"\n+        matchability = self.matchability(descriptors)\n+        matchability = nn.functional.sigmoid(matchability).squeeze(-1)\n+        return matchability\n+\n+\n+class LightGlueTokenConfidenceLayer(nn.Module):\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__()\n+\n+        self.token = nn.Linear(config.descriptor_dim, 1)\n+\n+    def forward(self, descriptors: torch.Tensor) -> torch.Tensor:\n+        token = self.token(descriptors.detach())\n+        token = nn.functional.sigmoid(token).squeeze(-1)\n+        return token\n+\n+\n+@auto_docstring\n+class LightGluePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = LightGlueConfig\n+    base_model_prefix = \"lightglue\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = False\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"obtain matches from a score matrix [Bx M+1 x N+1]\"\"\"\n+    batch_size, _, _ = scores.shape\n+    # For each keypoint, get the best match\n+    max0 = scores[:, :-1, :-1].max(2)\n+    max1 = scores[:, :-1, :-1].max(1)\n+    matches0 = max0.indices\n+    matches1 = max1.indices\n+\n+    # Mutual check for matches\n+    indices0 = torch.arange(matches0.shape[1], device=matches0.device)[None]\n+    indices1 = torch.arange(matches1.shape[1], device=matches1.device)[None]\n+    mutual0 = indices0 == matches1.gather(1, matches0)\n+    mutual1 = indices1 == matches0.gather(1, matches1)\n+\n+    # Get matching scores and filter based on mutual check and thresholding\n+    max0 = max0.values.exp()\n+    zero = max0.new_tensor(0)\n+    matching_scores0 = torch.where(mutual0, max0, zero)\n+    matching_scores1 = torch.where(mutual1, matching_scores0.gather(1, matches1), zero)\n+    valid0 = mutual0 & (matching_scores0 > threshold)\n+    valid1 = mutual1 & valid0.gather(1, matches1)\n+\n+    # Filter matches based on mutual check and thresholding of scores\n+    matches0 = torch.where(valid0, matches0, -1)\n+    matches1 = torch.where(valid1, matches1, -1)\n+    matches = torch.stack([matches0, matches1]).transpose(0, 1).reshape(batch_size * 2, -1)\n+    matching_scores = torch.stack([matching_scores0, matching_scores1]).transpose(0, 1).reshape(batch_size * 2, -1)\n+\n+    return matches, matching_scores\n+\n+\n+def normalize_keypoints(keypoints: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+    \"\"\"\n+    Normalize keypoints locations based on image image_shape\n+\n+    Args:\n+        keypoints (`torch.Tensor` of shape `(batch_size, num_keypoints, 2)`):\n+            Keypoints locations in (x, y) format.\n+        height (`int`):\n+            Image height.\n+        width (`int`):\n+            Image width.\n+\n+    Returns:\n+        Normalized keypoints locations of shape (`torch.Tensor` of shape `(batch_size, num_keypoints, 2)`).\n+    \"\"\"\n+    size = torch.tensor([width, height], device=keypoints.device, dtype=keypoints.dtype)[None]\n+    shift = size / 2\n+    scale = size.max(-1).values / 2\n+    keypoints = (keypoints - shift[..., None, :]) / scale[..., None, None]\n+    return keypoints\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    LightGlue model taking images as inputs and outputting the matching of them.\n+    \"\"\"\n+)\n+class LightGlueForKeypointMatching(LightGluePreTrainedModel):\n+    \"\"\"\n+    LightGlue is a model matching keypoints in images by leveraging detections from a keypoint detector such as\n+    SuperPoint. It is based on the SuperGlue architecture and is designed to be lightweight and efficient.\n+    It consists of :\n+        1. Keypoint Encoder\n+        2. A Graph Neural Network with self and cross attention layers\n+        3. Matching Assignment layers\n+\n+    The correspondence ids use -1 to indicate non-matching points.\n+\n+    Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed.\n+    In ICCV 2023. https://arxiv.org/pdf/2306.13643.pdf\n+    \"\"\"\n+\n+    def __init__(self, config: LightGlueConfig):\n+        super().__init__(config)\n+\n+        self.keypoint_detector = AutoModelForKeypointDetection.from_config(config.keypoint_detector_config)\n+\n+        self.descriptor_dim = config.descriptor_dim\n+        self.num_layers = config.num_hidden_layers\n+        self.filter_threshold = config.filter_threshold\n+        self.depth_confidence = config.depth_confidence\n+        self.width_confidence = config.width_confidence\n+\n+        if self.descriptor_dim != config.keypoint_detector_config.descriptor_decoder_dim:\n+            self.input_projection = nn.Linear(\n+                config.keypoint_detector_config.descriptor_decoder_dim, self.descriptor_dim, bias=True\n+            )\n+        else:\n+            self.input_projection = nn.Identity()\n+\n+        self.positional_encoder = LightGluePositionalEncoder(config)\n+\n+        self.transformer_layers = nn.ModuleList(\n+            [LightGlueTransformerLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+        )\n+        self.match_assignment_layers = nn.ModuleList(\n+            [LightGlueMatchAssignmentLayer(config) for _ in range(config.num_hidden_layers)]\n+        )\n+        self.token_confidence = nn.ModuleList(\n+            [LightGlueTokenConfidenceLayer(config) for _ in range(config.num_hidden_layers - 1)]\n+        )\n+\n+        self.post_init()\n+\n+    def _get_confidence_threshold(self, layer_index: int) -> float:\n+        \"\"\"scaled confidence threshold for a given layer\"\"\"\n+        threshold = 0.8 + 0.1 * np.exp(-4.0 * layer_index / self.num_layers)\n+        return np.clip(threshold, 0, 1)\n+\n+    def _keypoint_processing(\n+        self, descriptors: torch.Tensor, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n+    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n+        descriptors = descriptors.detach().contiguous()\n+        projected_descriptors = self.input_projection(descriptors)\n+        keypoint_encoding_output = self.positional_encoder(keypoints, output_hidden_states=output_hidden_states)\n+        return projected_descriptors, keypoint_encoding_output\n+\n+    def _get_early_stopped_image_pairs(\n+        self, keypoint_confidences: torch.Tensor, layer_index: int, mask: torch.Tensor, num_points: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"evaluate whether we should stop inference based on the confidence of the keypoints\"\"\"\n+        batch_size, _ = mask.shape\n+        if layer_index < self.num_layers - 1:\n+            # If the current layer is not the last layer, we compute the confidence of the keypoints and check\n+            # if we should stop the forward pass through the transformer layers for each pair of images.\n+            keypoint_confidences = keypoint_confidences.masked_fill(mask == 0, 1)\n+            keypoint_confidences = keypoint_confidences.reshape(batch_size // 2, -1)\n+            threshold = self._get_confidence_threshold(layer_index)\n+            ratio_confident = 1.0 - (keypoint_confidences < threshold).float().sum(dim=1) / num_points\n+            early_stopped_pairs = ratio_confident > self.depth_confidence\n+        else:\n+            # If the current layer is the last layer, we stop the forward pass through the transformer layers for\n+            # all pairs of images.\n+            early_stopped_pairs = torch.ones(batch_size, dtype=torch.bool)\n+        return early_stopped_pairs\n+\n+    def _get_keypoint_matching(self, descriptors, mask, layer_index, early_stops=None):\n+        if early_stops is not None:\n+            descriptors = descriptors[early_stops]\n+            mask = mask[early_stops]\n+        scores = self.match_assignment_layers[layer_index](descriptors, mask)\n+        matches, matching_scores = get_matches_from_scores(scores, self.filter_threshold)\n+        return matches, matching_scores\n+\n+    def _get_pruning_mask(self, confidences: torch.Tensor, scores: torch.Tensor, layer_index: int) -> torch.Tensor:\n+        \"\"\"mask points which should be removed\"\"\"\n+        keep = scores > (1 - self.width_confidence)\n+        if confidences is not None:  # Low-confidence points are never pruned.\n+            keep |= confidences <= self._get_confidence_threshold(layer_index)\n+        return keep\n+\n+    def _do_layer_keypoint_pruning(\n+        self,\n+        descriptors: torch.Tensor,\n+        keypoints: torch.Tensor,\n+        mask: torch.Tensor,\n+        indices: torch.Tensor,\n+        prune_output: torch.Tensor,\n+        keypoint_confidences: torch.Tensor,\n+        layer_index: int,\n+    ):\n+        \"\"\"\n+        For a given layer, prune keypoints based on the confidence of the keypoints and the matchability of the\n+        descriptors.\n+        \"\"\"\n+        batch_size, _, _ = descriptors.shape\n+        descriptors_matchability = self.match_assignment_layers[layer_index].get_matchability(descriptors)\n+        pruned_keypoints_mask = self._get_pruning_mask(keypoint_confidences, descriptors_matchability, layer_index)\n+        pruned_keypoints_mask = pruned_keypoints_mask.masked_fill(mask == 0, torch.tensor(False))\n+\n+        # For each image, we extract the pruned indices and the corresponding descriptors and keypoints.\n+        pruned_descriptors, pruned_keypoints_0, pruned_keypoints_1, pruned_mask, pruned_indices = (\n+            [t[mask] for t, mask in zip(tensor, pruned_keypoints_mask)]\n+            for tensor in [descriptors, keypoints[0], keypoints[1], pruned_keypoints_mask, indices]\n+        )\n+        for i in range(batch_size):\n+            prune_output[i, pruned_indices[i]] += 1\n+\n+        # Pad the pruned descriptors, keypoints, indices and mask to have the same shape across the batch.\n+        pruned_descriptors, pruned_keypoints_0, pruned_keypoints_1, pruned_mask = (\n+            pad_sequence(pruned_tensor, batch_first=True)\n+            for pruned_tensor in [pruned_descriptors, pruned_keypoints_0, pruned_keypoints_1, pruned_mask]\n+        )\n+        pruned_keypoints = (pruned_keypoints_0, pruned_keypoints_1)\n+        pruned_indices = pad_sequence(pruned_indices, batch_first=True, padding_value=-1)\n+\n+        return pruned_descriptors, pruned_keypoints, pruned_indices, pruned_mask, prune_output\n+\n+    def _concat_early_stopped_outputs(\n+        self,\n+        early_stops_indices,\n+        final_pruned_keypoints_indices,\n+        final_pruned_keypoints_iterations,\n+        matches,\n+        matching_scores,\n+    ):\n+        early_stops_indices = torch.stack(early_stops_indices)\n+        matches, final_pruned_keypoints_indices = (\n+            pad_sequence(tensor, batch_first=True, padding_value=-1)\n+            for tensor in [matches, final_pruned_keypoints_indices]\n+        )\n+        matching_scores, final_pruned_keypoints_iterations = (\n+            pad_sequence(tensor, batch_first=True, padding_value=0)\n+            for tensor in [matching_scores, final_pruned_keypoints_iterations]\n+        )\n+        matches, matching_scores, final_pruned_keypoints_indices, final_pruned_keypoints_iterations = (\n+            tensor[early_stops_indices]\n+            for tensor in [\n+                matches,\n+                matching_scores,\n+                final_pruned_keypoints_indices,\n+                final_pruned_keypoints_iterations,\n+            ]\n+        )\n+        return final_pruned_keypoints_indices, final_pruned_keypoints_iterations, matches, matching_scores\n+\n+    def _do_final_keypoint_pruning(\n+        self,\n+        indices: torch.Tensor,\n+        matches: torch.Tensor,\n+        matching_scores: torch.Tensor,\n+        num_keypoints: torch.Tensor,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        # (batch_size, num_keypoints) -> (batch_size // 2, 2, num_keypoints) -> 2 * (batch_size // 2, num_keypoints) to\n+        # have tensors from\n+        batch_size, _ = indices.shape\n+        indices, matches, matching_scores = (\n+            tensor.reshape(batch_size // 2, 2, -1) for tensor in [indices, matches, matching_scores]\n+        )\n+        indices0 = indices[:, 0]\n+        indices1 = indices[:, 1]\n+        matches0 = matches[:, 0]\n+        matches1 = matches[:, 1]\n+        matching_scores0 = matching_scores[:, 0]\n+        matching_scores1 = matching_scores[:, 1]\n+\n+        # Prepare final matches and matching scores\n+        _matches = torch.full((batch_size // 2, 2, num_keypoints), -1, device=indices.device, dtype=matches.dtype)\n+        _matching_scores = torch.zeros(\n+            (batch_size // 2, 2, num_keypoints), device=indices.device, dtype=matching_scores.dtype\n+        )\n+        # Fill the matches and matching scores for each image pair\n+        for i in range(batch_size // 2):\n+            _matches[i, 0, indices0[i]] = torch.where(\n+                matches0[i] == -1, -1, indices1[i].gather(0, matches0[i].clamp(min=0))\n+            )\n+            _matches[i, 1, indices1[i]] = torch.where(\n+                matches1[i] == -1, -1, indices0[i].gather(0, matches1[i].clamp(min=0))\n+            )\n+            _matching_scores[i, 0, indices0[i]] = matching_scores0[i]\n+            _matching_scores[i, 1, indices1[i]] = matching_scores1[i]\n+        return _matches, _matching_scores\n+\n+    def _match_image_pair(\n+        self,\n+        keypoints: torch.Tensor,\n+        descriptors: torch.Tensor,\n+        height: int,\n+        width: int,\n+        mask: torch.Tensor = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple, Tuple]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        if keypoints.shape[2] == 0:  # no keypoints\n+            shape = keypoints.shape[:-1]\n+            return (\n+                keypoints.new_full(shape, -1, dtype=torch.int),\n+                keypoints.new_zeros(shape),\n+                keypoints.new_zeros(shape),\n+                all_hidden_states,\n+                all_attentions,\n+            )\n+\n+        device = keypoints.device\n+        batch_size, _, initial_num_keypoints, _ = keypoints.shape\n+        num_points_per_pair = torch.sum(mask.reshape(batch_size, -1), dim=1)\n+        # (batch_size, 2, num_keypoints, 2) -> (batch_size * 2, num_keypoints, 2)\n+        keypoints = keypoints.reshape(batch_size * 2, initial_num_keypoints, 2)\n+        mask = mask.reshape(batch_size * 2, initial_num_keypoints) if mask is not None else None\n+        descriptors = descriptors.reshape(batch_size * 2, initial_num_keypoints, self.descriptor_dim)\n+        image_indices = torch.arange(batch_size * 2, device=device)\n+        # Keypoint normalization\n+        keypoints = normalize_keypoints(keypoints, height, width)\n+\n+        descriptors, keypoint_encoding_output = self._keypoint_processing(\n+            descriptors, keypoints, output_hidden_states=output_hidden_states\n+        )\n+\n+        keypoints = keypoint_encoding_output[0]\n+\n+        # Early stop consists of stopping the forward pass through the transformer layers when the confidence of the\n+        # keypoints is above a certain threshold.\n+        do_early_stop = self.depth_confidence > 0\n+        # Keypoint pruning consists of removing keypoints from the input of the transformer layers when the confidence of\n+        # the keypoints is below a certain threshold.\n+        do_keypoint_pruning = self.width_confidence > 0\n+\n+        early_stops_indices = []\n+        matches = []\n+        matching_scores = []\n+        final_pruned_keypoints_indices = []\n+        final_pruned_keypoints_iterations = []\n+\n+        pruned_keypoints_indices = torch.arange(0, initial_num_keypoints, device=device).expand(batch_size * 2, -1)\n+        pruned_keypoints_iterations = torch.ones_like(pruned_keypoints_indices)\n+\n+        for layer_index in range(self.num_layers):\n+            input_shape = descriptors.size()\n+            if mask is not None:\n+                extended_attention_mask = self.get_extended_attention_mask(mask, input_shape)\n+            else:\n+                extended_attention_mask = torch.ones((batch_size, input_shape[-2]), device=keypoints.device)\n+            layer_output = self.transformer_layers[layer_index](\n+                descriptors,\n+                keypoints,\n+                attention_mask=extended_attention_mask,\n+                output_hidden_states=output_hidden_states,\n+                output_attentions=output_attentions,\n+            )\n+            descriptors, hidden_states, attention = layer_output\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + hidden_states\n+            if output_attentions:\n+                all_attentions = all_attentions + attention\n+\n+            if do_early_stop:\n+                if layer_index < self.num_layers - 1:\n+                    # Get the confidence of the keypoints for the current layer\n+                    keypoint_confidences = self.token_confidence[layer_index](descriptors)\n+\n+                    # Determine which pairs of images should be early stopped based on the confidence of the keypoints for\n+                    # the current layer.\n+                    early_stopped_pairs = self._get_early_stopped_image_pairs(\n+                        keypoint_confidences, layer_index, mask, num_points=num_points_per_pair\n+                    )\n+                else:\n+                    # Early stopping always occurs at the last layer\n+                    early_stopped_pairs = torch.ones(batch_size, dtype=torch.bool)\n+\n+                if torch.any(early_stopped_pairs):\n+                    # If a pair of images is considered early stopped, we compute the matches for the remaining\n+                    # keypoints and stop the forward pass through the transformer layers for this pair of images.\n+                    early_stops = early_stopped_pairs.repeat_interleave(2)\n+                    early_stopped_image_indices = image_indices[early_stops]\n+                    early_stopped_matches, early_stopped_matching_scores = self._get_keypoint_matching(\n+                        descriptors, mask, layer_index, early_stops=early_stops\n+                    )\n+                    early_stops_indices.extend(list(early_stopped_image_indices))\n+                    matches.extend(list(early_stopped_matches))\n+                    matching_scores.extend(list(early_stopped_matching_scores))\n+                    if do_keypoint_pruning:\n+                        final_pruned_keypoints_indices.extend(list(pruned_keypoints_indices[early_stops]))\n+                        final_pruned_keypoints_iterations.extend(list(pruned_keypoints_iterations[early_stops]))\n+\n+                    # Remove image pairs that have been early stopped from the forward pass\n+                    num_points_per_pair = num_points_per_pair[~early_stopped_pairs]\n+                    descriptors, keypoints_0, keypoint_1, mask, image_indices = tuple(\n+                        (\n+                            tensor[~early_stops]\n+                            for tensor in [descriptors, keypoints[0], keypoints[1], mask, image_indices]\n+                        )\n+                    )\n+                    keypoints = (keypoints_0, keypoint_1)\n+                    if do_keypoint_pruning:\n+                        pruned_keypoints_indices, pruned_keypoints_iterations, keypoint_confidences = tuple(\n+                            (\n+                                tensor[~early_stops]\n+                                for tensor in [\n+                                    pruned_keypoints_indices,\n+                                    pruned_keypoints_iterations,\n+                                    keypoint_confidences,\n+                                ]\n+                            )\n+                        )\n+                # If all pairs of images are early stopped, we stop the forward pass through the transformer\n+                # layers for all pairs of images.\n+                if torch.all(early_stopped_pairs):\n+                    break\n+\n+            if do_keypoint_pruning:\n+                # Prune keypoints from the input of the transformer layers for the next iterations if the confidence of\n+                # the keypoints is below a certain threshold.\n+                descriptors, keypoints, pruned_keypoints_indices, mask, pruned_keypoints_iterations = (\n+                    self._do_layer_keypoint_pruning(\n+                        descriptors,\n+                        keypoints,\n+                        mask,\n+                        pruned_keypoints_indices,\n+                        pruned_keypoints_iterations,\n+                        keypoint_confidences,\n+                        layer_index,\n+                    )\n+                )\n+\n+        if do_early_stop and do_keypoint_pruning:\n+            # Concatenate early stopped outputs together and perform final keypoint pruning\n+            final_pruned_keypoints_indices, final_pruned_keypoints_iterations, matches, matching_scores = (\n+                self._concat_early_stopped_outputs(\n+                    early_stops_indices,\n+                    final_pruned_keypoints_indices,\n+                    final_pruned_keypoints_iterations,\n+                    matches,\n+                    matching_scores,\n+                )\n+            )\n+            matches, matching_scores = self._do_final_keypoint_pruning(\n+                final_pruned_keypoints_indices,\n+                matches,\n+                matching_scores,\n+                initial_num_keypoints,\n+            )\n+        else:\n+            matches, matching_scores = self._get_keypoint_matching(descriptors, mask, self.num_layers - 1)\n+            final_pruned_keypoints_iterations = torch.ones_like(matching_scores) * self.num_layers\n+\n+        final_pruned_keypoints_iterations = final_pruned_keypoints_iterations.reshape(\n+            batch_size, 2, initial_num_keypoints\n+        )\n+\n+        return (\n+            matches,\n+            matching_scores,\n+            final_pruned_keypoints_iterations,\n+            all_hidden_states,\n+            all_attentions,\n+        )\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Union[Tuple, LightGlueKeypointMatchingOutput]:\n+        loss = None\n+        if labels is not None:\n+            raise ValueError(\"LightGlue is not trainable, no labels should be provided.\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if pixel_values.ndim != 5 or pixel_values.size(1) != 2:\n+            raise ValueError(\"Input must be a 5D tensor of shape (batch_size, 2, num_channels, height, width)\")\n+\n+        batch_size, _, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * 2, channels, height, width)\n+        keypoint_detections = self.keypoint_detector(pixel_values)\n+\n+        keypoints, _, descriptors, mask = keypoint_detections[:4]\n+        keypoints = keypoints.reshape(batch_size, 2, -1, 2).to(pixel_values)\n+        descriptors = descriptors.reshape(batch_size, 2, -1, self.descriptor_dim).to(pixel_values)\n+        mask = mask.reshape(batch_size, 2, -1)\n+\n+        absolute_keypoints = keypoints.clone()\n+        absolute_keypoints[:, :, :, 0] = absolute_keypoints[:, :, :, 0] * width\n+        absolute_keypoints[:, :, :, 1] = absolute_keypoints[:, :, :, 1] * height\n+\n+        matches, matching_scores, prune, hidden_states, attentions = self._match_image_pair(\n+            absolute_keypoints,\n+            descriptors,\n+            height,\n+            width,\n+            mask=mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        return LightGlueKeypointMatchingOutput(\n+            loss=loss,\n+            matches=matches,\n+            matching_scores=matching_scores,\n+            keypoints=keypoints,\n+            prune=prune,\n+            mask=mask,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\"LightGluePreTrainedModel\", \"LightGlueForKeypointMatching\", \"LightGlueConfig\", \"LightGlueImageProcessor\"]"
        },
        {
            "sha": "e39c4f933bffb70ad6ff6f0ad4f24446d3180171",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -17,7 +17,6 @@\n \n import numpy as np\n \n-from ... import is_torch_available, is_vision_available\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import resize, to_channel_dimension_format\n from ...image_utils import (\n@@ -29,7 +28,9 @@\n     infer_channel_dimension_format,\n     is_pil_image,\n     is_scaled_image,\n+    is_torch_available,\n     is_valid_image,\n+    is_vision_available,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,"
        },
        {
            "sha": "63d717add70dafb2180e2e917a17bc422f5fe483",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -253,7 +253,7 @@ def _extract_keypoints(self, scores: torch.Tensor) -> Tuple[torch.Tensor, torch.\n             keypoints, scores = top_k_keypoints(keypoints, scores, self.max_keypoints)\n \n         # Convert (y, x) to (x, y)\n-        keypoints = torch.flip(keypoints, [1]).float()\n+        keypoints = torch.flip(keypoints, [1]).to(scores.dtype)\n \n         return keypoints, scores\n "
        },
        {
            "sha": "386a85228ffe93ec9c927e77c179d8d9d31d4a87",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -179,6 +179,7 @@\n     is_librosa_available,\n     is_liger_kernel_available,\n     is_lomo_available,\n+    is_matplotlib_available,\n     is_mlx_available,\n     is_natten_available,\n     is_ninja_available,"
        },
        {
            "sha": "420955e998e1c056a2ee4e2e97b2fc40d2f3d2c8",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -225,6 +225,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _spqr_available = _is_package_available(\"spqr_quant\")\n _rich_available = _is_package_available(\"rich\")\n _kernels_available = _is_package_available(\"kernels\")\n+_matplotlib_available = _is_package_available(\"matplotlib\")\n \n _torch_version = \"N/A\"\n _torch_available = False\n@@ -1443,6 +1444,10 @@ def is_rich_available():\n     return _rich_available\n \n \n+def is_matplotlib_available():\n+    return _matplotlib_available\n+\n+\n def check_torch_load_is_safe():\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError("
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/lightglue/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/tests%2Fmodels%2Flightglue%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/tests%2Fmodels%2Flightglue%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flightglue%2F__init__.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b"
        },
        {
            "sha": "3d01acf46960139ddd4afc93d109be14ba96c46b",
            "filename": "tests/models/lightglue/test_image_processing_lightglue.py",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,96 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import unittest\n+\n+from tests.models.superglue.test_image_processing_superglue import (\n+    SuperGlueImageProcessingTest,\n+    SuperGlueImageProcessingTester,\n+)\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+\n+if is_torch_available():\n+    import numpy as np\n+    import torch\n+\n+    from transformers.models.lightglue.modeling_lightglue import LightGlueKeypointMatchingOutput\n+\n+if is_vision_available():\n+    from transformers import LightGlueImageProcessor\n+\n+\n+def random_array(size):\n+    return np.random.randint(255, size=size)\n+\n+\n+def random_tensor(size):\n+    return torch.rand(size)\n+\n+\n+class LightGlueImageProcessingTester(SuperGlueImageProcessingTester):\n+    \"\"\"Tester for LightGlueImageProcessor\"\"\"\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=6,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_grayscale=True,\n+    ):\n+        super().__init__(\n+            parent, batch_size, num_channels, image_size, min_resolution, max_resolution, do_resize, size, do_grayscale\n+        )\n+\n+    def prepare_keypoint_matching_output(self, pixel_values):\n+        \"\"\"Prepare a fake output for the keypoint matching model with random matches between 50 keypoints per image.\"\"\"\n+        max_number_keypoints = 50\n+        batch_size = len(pixel_values)\n+        mask = torch.zeros((batch_size, 2, max_number_keypoints), dtype=torch.int)\n+        keypoints = torch.zeros((batch_size, 2, max_number_keypoints, 2))\n+        matches = torch.full((batch_size, 2, max_number_keypoints), -1, dtype=torch.int)\n+        scores = torch.zeros((batch_size, 2, max_number_keypoints))\n+        prune = torch.zeros((batch_size, 2, max_number_keypoints), dtype=torch.int)\n+        for i in range(batch_size):\n+            random_number_keypoints0 = np.random.randint(10, max_number_keypoints)\n+            random_number_keypoints1 = np.random.randint(10, max_number_keypoints)\n+            random_number_matches = np.random.randint(5, min(random_number_keypoints0, random_number_keypoints1))\n+            mask[i, 0, :random_number_keypoints0] = 1\n+            mask[i, 1, :random_number_keypoints1] = 1\n+            keypoints[i, 0, :random_number_keypoints0] = torch.rand((random_number_keypoints0, 2))\n+            keypoints[i, 1, :random_number_keypoints1] = torch.rand((random_number_keypoints1, 2))\n+            random_matches_indices0 = torch.randperm(random_number_keypoints1, dtype=torch.int)[:random_number_matches]\n+            random_matches_indices1 = torch.randperm(random_number_keypoints0, dtype=torch.int)[:random_number_matches]\n+            matches[i, 0, random_matches_indices1] = random_matches_indices0\n+            matches[i, 1, random_matches_indices0] = random_matches_indices1\n+            scores[i, 0, random_matches_indices1] = torch.rand((random_number_matches,))\n+            scores[i, 1, random_matches_indices0] = torch.rand((random_number_matches,))\n+        return LightGlueKeypointMatchingOutput(\n+            mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores, prune=prune\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class LightGlueImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n+    image_processing_class = LightGlueImageProcessor if is_vision_available() else None\n+\n+    def setUp(self) -> None:\n+        super().setUp()\n+        self.image_processor_tester = LightGlueImageProcessingTester(self)"
        },
        {
            "sha": "20d9f2ef61cbe3f140aae952a86c6cc402bd0058",
            "filename": "tests/models/lightglue/test_modeling_lightglue.py",
            "status": "added",
            "additions": 584,
            "deletions": 0,
            "changes": 584,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/tests%2Fmodels%2Flightglue%2Ftest_modeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a9ce48f711b1f26eef3f7047a13b8235e4a71b/tests%2Fmodels%2Flightglue%2Ftest_modeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flightglue%2Ftest_modeling_lightglue.py?ref=e5a9ce48f711b1f26eef3f7047a13b8235e4a71b",
            "patch": "@@ -0,0 +1,584 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import inspect\n+import unittest\n+\n+from datasets import load_dataset\n+\n+from transformers.models.lightglue.configuration_lightglue import LightGlueConfig\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import LightGlueForKeypointMatching\n+\n+if is_vision_available():\n+    from transformers import AutoImageProcessor\n+\n+\n+class LightGlueModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        image_width=80,\n+        image_height=60,\n+        keypoint_detector_config={\n+            \"encoder_hidden_sizes\": [32, 32, 64],\n+            \"decoder_hidden_size\": 64,\n+            \"keypoint_decoder_dim\": 65,\n+            \"descriptor_decoder_dim\": 64,\n+            \"keypoint_threshold\": 0.005,\n+            \"max_keypoints\": 256,\n+            \"nms_radius\": 4,\n+            \"border_removal_distance\": 4,\n+        },\n+        descriptor_dim: int = 64,\n+        num_layers: int = 2,\n+        num_heads: int = 4,\n+        depth_confidence: float = 1.0,\n+        width_confidence: float = 1.0,\n+        filter_threshold: float = 0.1,\n+        matching_threshold: float = 0.0,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_width = image_width\n+        self.image_height = image_height\n+\n+        self.keypoint_detector_config = keypoint_detector_config\n+        self.descriptor_dim = descriptor_dim\n+        self.num_layers = num_layers\n+        self.num_heads = num_heads\n+        self.depth_confidence = depth_confidence\n+        self.width_confidence = width_confidence\n+        self.filter_threshold = filter_threshold\n+        self.matching_threshold = matching_threshold\n+\n+    def prepare_config_and_inputs(self):\n+        # LightGlue expects a grayscale image as input\n+        pixel_values = floats_tensor([self.batch_size, 2, 3, self.image_height, self.image_width])\n+        config = self.get_config()\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return LightGlueConfig(\n+            keypoint_detector_config=self.keypoint_detector_config,\n+            descriptor_dim=self.descriptor_dim,\n+            num_hidden_layers=self.num_layers,\n+            num_attention_heads=self.num_heads,\n+            depth_confidence=self.depth_confidence,\n+            width_confidence=self.width_confidence,\n+            filter_threshold=self.filter_threshold,\n+            matching_threshold=self.matching_threshold,\n+            attn_implementation=\"eager\",\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = LightGlueForKeypointMatching(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        maximum_num_matches = result.mask.shape[-1]\n+        self.parent.assertEqual(\n+            result.keypoints.shape,\n+            (self.batch_size, 2, maximum_num_matches, 2),\n+        )\n+        self.parent.assertEqual(\n+            result.matches.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+        self.parent.assertEqual(\n+            result.matching_scores.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+        self.parent.assertEqual(\n+            result.prune.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class LightGlueModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (LightGlueForKeypointMatching,) if is_torch_available() else ()\n+    all_generative_model_classes = () if is_torch_available() else ()\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    has_attentions = True\n+\n+    def setUp(self):\n+        self.model_tester = LightGlueModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=LightGlueConfig, has_text_modality=False, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.create_and_test_config_to_json_string()\n+        self.config_tester.create_and_test_config_to_json_file()\n+        self.config_tester.create_and_test_config_from_and_save_pretrained()\n+        self.config_tester.create_and_test_config_with_num_labels()\n+        self.config_tester.check_config_can_be_init_without_params()\n+        self.config_tester.check_config_arguments_init()\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching does not use feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching is not trainable\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlueForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"LightGlue does not output any loss term in the forward pass\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.hidden_states\n+            maximum_num_matches = outputs.mask.shape[-1]\n+\n+            hidden_states_sizes = [\n+                self.model_tester.descriptor_dim,\n+                self.model_tester.descriptor_dim,\n+                self.model_tester.descriptor_dim * 2,\n+                self.model_tester.descriptor_dim,\n+                self.model_tester.descriptor_dim,\n+                self.model_tester.descriptor_dim * 2,\n+                self.model_tester.descriptor_dim,\n+            ] * self.model_tester.num_layers\n+\n+            for i, hidden_states_size in enumerate(hidden_states_sizes):\n+                self.assertListEqual(\n+                    list(hidden_states[i].shape[-2:]),\n+                    [maximum_num_matches, hidden_states_size],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_attention_outputs(self):\n+        def check_attention_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.attentions\n+            maximum_num_matches = outputs.mask.shape[-1]\n+\n+            expected_attention_shape = [self.model_tester.num_heads, maximum_num_matches, maximum_num_matches]\n+\n+            for i, attention in enumerate(attentions):\n+                self.assertListEqual(\n+                    list(attention.shape[-3:]),\n+                    expected_attention_shape,\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            check_attention_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+\n+            check_attention_output(inputs_dict, config, model_class)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        from_pretrained_ids = [\"ETH-CVG/lightglue_superpoint\"]\n+        for model_name in from_pretrained_ids:\n+            model = LightGlueForKeypointMatching.from_pretrained(model_name)\n+            self.assertIsNotNone(model)\n+\n+    # Copied from tests.models.superglue.test_modeling_superglue.SuperGlueModelTest.test_forward_labels_should_be_none\n+    def test_forward_labels_should_be_none(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                model_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                # Provide an arbitrary sized Tensor as labels to model inputs\n+                model_inputs[\"labels\"] = torch.rand((128, 128))\n+\n+                with self.assertRaises(ValueError) as cm:\n+                    model(**model_inputs)\n+                self.assertEqual(ValueError, cm.exception.__class__)\n+\n+\n+def prepare_imgs():\n+    dataset = load_dataset(\"hf-internal-testing/image-matching-test-dataset\", split=\"train\")\n+    image0 = dataset[0][\"image\"]\n+    image1 = dataset[1][\"image\"]\n+    image2 = dataset[2][\"image\"]\n+    # [image1, image1] on purpose to test the model early stopping\n+    return [[image2, image0], [image1, image1]]\n+\n+\n+@require_torch\n+@require_vision\n+class LightGlueModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return AutoImageProcessor.from_pretrained(\"ETH-CVG/lightglue_superpoint\") if is_vision_available() else None\n+\n+    @slow\n+    def test_inference(self):\n+        model = LightGlueForKeypointMatching.from_pretrained(\n+            \"ETH-CVG/lightglue_superpoint\", attn_implementation=\"eager\"\n+        ).to(torch_device)\n+        preprocessor = self.default_image_processor\n+        images = prepare_imgs()\n+        inputs = preprocessor(images=images, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+        predicted_number_of_matches0 = torch.sum(outputs.matches[0][0] != -1).item()\n+        predicted_matches_values0 = outputs.matches[0, 0, 10:30]\n+        predicted_matching_scores_values0 = outputs.matching_scores[0, 0, 10:30]\n+\n+        predicted_number_of_matches1 = torch.sum(outputs.matches[1][0] != -1).item()\n+        predicted_matches_values1 = outputs.matches[1, 0, 10:30]\n+        predicted_matching_scores_values1 = outputs.matching_scores[1, 0, 10:30]\n+\n+        expected_number_of_matches0 = 140\n+        expected_matches_values0 = torch.tensor(\n+            [14, -1, -1, 15, 17, 13, -1, -1, -1, -1, -1, -1, 5, -1, -1, 19, -1, 10, -1, 11],\n+            dtype=torch.int64,\n+            device=torch_device,\n+        )\n+        expected_matching_scores_values0 = torch.tensor(\n+            [0.3796, 0, 0, 0.3772, 0.4439, 0.2411, 0, 0, 0.0032, 0, 0, 0, 0.2997, 0, 0, 0.6762, 0, 0.8826, 0, 0.5583],\n+            device=torch_device,\n+        )\n+\n+        expected_number_of_matches1 = 866\n+        expected_matches_values1 = torch.tensor(\n+            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n+            dtype=torch.int64,\n+            device=torch_device,\n+        )\n+        expected_matching_scores_values1 = torch.tensor(\n+            [\n+                0.6188,0.7817,0.5686,0.9353,0.9801,0.9193,0.8632,0.9111,0.9821,0.5496,\n+                0.9906,0.8682,0.9679,0.9914,0.9318,0.1910,0.9669,0.3240,0.9971,0.9923,\n+            ],\n+            device=torch_device\n+        )  # fmt:skip\n+\n+        # expected_early_stopping_layer = 2\n+        # predicted_early_stopping_layer = torch.max(outputs.prune[1]).item()\n+        # self.assertEqual(predicted_early_stopping_layer, expected_early_stopping_layer)\n+        # self.assertEqual(predicted_number_of_matches, expected_second_number_of_matches)\n+\n+        \"\"\"\n+        Because of inconsistencies introduced between CUDA versions, the checks here are less strict. SuperGlue relies\n+        on SuperPoint, which may, depending on CUDA version, return different number of keypoints (866 or 867 in this\n+        specific test example). The consequence of having different number of keypoints is that the number of matches\n+        will also be different. In the 20 first matches being checked, having one keypoint less will result in 1 less\n+        match. The matching scores will also be different, as the keypoints are different. The checks here are less\n+        strict to account for these inconsistencies.\n+        Therefore, the test checks that the predicted number of matches, matches and matching scores are close to the\n+        expected values, individually. Here, the tolerance of the number of values changing is set to 2.\n+\n+        This was discussed [here](https://github.com/huggingface/transformers/pull/29886#issuecomment-2482752787)\n+        Such CUDA inconsistencies can be found\n+        [here](https://github.com/huggingface/transformers/pull/33200/files#r1785980300)\n+        \"\"\"\n+\n+        self.assertTrue(abs(predicted_number_of_matches0 - expected_number_of_matches0) < 4)\n+        self.assertTrue(abs(predicted_number_of_matches1 - expected_number_of_matches1) < 4)\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values0, expected_matching_scores_values0, atol=1e-2))\n+            < 4\n+        )\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values1, expected_matching_scores_values1, atol=1e-2))\n+            < 4\n+        )\n+        self.assertTrue(torch.sum(predicted_matches_values0 != expected_matches_values0) < 4)\n+        self.assertTrue(torch.sum(predicted_matches_values1 != expected_matches_values1) < 4)\n+\n+    @slow\n+    def test_inference_without_early_stop(self):\n+        model = LightGlueForKeypointMatching.from_pretrained(\n+            \"ETH-CVG/lightglue_superpoint\", attn_implementation=\"eager\", depth_confidence=1.0\n+        ).to(torch_device)\n+        preprocessor = self.default_image_processor\n+        images = prepare_imgs()\n+        inputs = preprocessor(images=images, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+        predicted_number_of_matches0 = torch.sum(outputs.matches[0][0] != -1).item()\n+        predicted_matches_values0 = outputs.matches[0, 0, 10:30]\n+        predicted_matching_scores_values0 = outputs.matching_scores[0, 0, 10:30]\n+\n+        predicted_number_of_matches1 = torch.sum(outputs.matches[1][0] != -1).item()\n+        predicted_matches_values1 = outputs.matches[1, 0, 10:30]\n+        predicted_matching_scores_values1 = outputs.matching_scores[1, 0, 10:30]\n+\n+        expected_number_of_matches0 = 134\n+        expected_matches_values0 = torch.tensor(\n+            [-1, -1, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 19, -1, 10, -1, 11], dtype=torch.int64\n+        ).to(torch_device)\n+        expected_matching_scores_values0 = torch.tensor(\n+            [0.0083, 0, 0.2022, 0.0621, 0, 0.0828, 0, 0, 0.0003, 0, 0, 0, 0.0960, 0, 0, 0.6940, 0, 0.7167, 0, 0.1512]\n+        ).to(torch_device)\n+\n+        expected_number_of_matches1 = 862\n+        expected_matches_values1 = torch.tensor(\n+            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], dtype=torch.int64\n+        ).to(torch_device)\n+        expected_matching_scores_values1 = torch.tensor(\n+            [\n+                0.4772,\n+                0.3781,\n+                0.0631,\n+                0.9559,\n+                0.8746,\n+                0.9271,\n+                0.4882,\n+                0.5406,\n+                0.9439,\n+                0.1526,\n+                0.5028,\n+                0.4107,\n+                0.5591,\n+                0.9130,\n+                0.7572,\n+                0.0302,\n+                0.4532,\n+                0.0893,\n+                0.9490,\n+                0.4880,\n+            ]\n+        ).to(torch_device)\n+\n+        # expected_early_stopping_layer = 2\n+        # predicted_early_stopping_layer = torch.max(outputs.prune[1]).item()\n+        # self.assertEqual(predicted_early_stopping_layer, expected_early_stopping_layer)\n+        # self.assertEqual(predicted_number_of_matches, expected_second_number_of_matches)\n+\n+        \"\"\"\n+        Because of inconsistencies introduced between CUDA versions, the checks here are less strict. SuperGlue relies\n+        on SuperPoint, which may, depending on CUDA version, return different number of keypoints (866 or 867 in this\n+        specific test example). The consequence of having different number of keypoints is that the number of matches\n+        will also be different. In the 20 first matches being checked, having one keypoint less will result in 1 less\n+        match. The matching scores will also be different, as the keypoints are different. The checks here are less\n+        strict to account for these inconsistencies.\n+        Therefore, the test checks that the predicted number of matches, matches and matching scores are close to the\n+        expected values, individually. Here, the tolerance of the number of values changing is set to 2.\n+\n+        This was discussed [here](https://github.com/huggingface/transformers/pull/29886#issuecomment-2482752787)\n+        Such CUDA inconsistencies can be found\n+        [here](https://github.com/huggingface/transformers/pull/33200/files#r1785980300)\n+        \"\"\"\n+\n+        self.assertTrue(abs(predicted_number_of_matches0 - expected_number_of_matches0) < 4)\n+        self.assertTrue(abs(predicted_number_of_matches1 - expected_number_of_matches1) < 4)\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values0, expected_matching_scores_values0, atol=1e-2))\n+            < 4\n+        )\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values1, expected_matching_scores_values1, atol=1e-2))\n+            < 4\n+        )\n+        self.assertTrue(torch.sum(predicted_matches_values0 != expected_matches_values0) < 4)\n+        self.assertTrue(torch.sum(predicted_matches_values1 != expected_matches_values1) < 4)\n+\n+    @slow\n+    def test_inference_without_early_stop_and_keypoint_pruning(self):\n+        model = LightGlueForKeypointMatching.from_pretrained(\n+            \"ETH-CVG/lightglue_superpoint\",\n+            attn_implementation=\"eager\",\n+            depth_confidence=1.0,\n+            width_confidence=1.0,\n+        ).to(torch_device)\n+        preprocessor = self.default_image_processor\n+        images = prepare_imgs()\n+        inputs = preprocessor(images=images, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+        predicted_number_of_matches0 = torch.sum(outputs.matches[0][0] != -1).item()\n+        predicted_matches_values0 = outputs.matches[0, 0, 10:30]\n+        predicted_matching_scores_values0 = outputs.matching_scores[0, 0, 10:30]\n+\n+        predicted_number_of_matches1 = torch.sum(outputs.matches[1][0] != -1).item()\n+        predicted_matches_values1 = outputs.matches[1, 0, 10:30]\n+        predicted_matching_scores_values1 = outputs.matching_scores[1, 0, 10:30]\n+\n+        expected_number_of_matches0 = 144\n+        expected_matches_values0 = torch.tensor(\n+            [-1, -1, 17, -1, -1, 13, -1, -1, -1, -1, -1, -1, 5, -1, -1, 19, -1, 10, -1, 11], dtype=torch.int64\n+        ).to(torch_device)\n+        expected_matching_scores_values0 = torch.tensor(\n+            [\n+                0.0699,\n+                0.0302,\n+                0.3356,\n+                0.0820,\n+                0,\n+                0.2266,\n+                0,\n+                0,\n+                0.0241,\n+                0,\n+                0,\n+                0,\n+                0.1674,\n+                0,\n+                0,\n+                0.8114,\n+                0,\n+                0.8120,\n+                0,\n+                0.2936,\n+            ]\n+        ).to(torch_device)\n+\n+        expected_number_of_matches1 = 862\n+        expected_matches_values1 = torch.tensor(\n+            [10, 11, -1, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, -1, 26, -1, 28, 29], dtype=torch.int64\n+        ).to(torch_device)\n+        expected_matching_scores_values1 = torch.tensor(\n+            [\n+                0.4772,\n+                0.3781,\n+                0.0631,\n+                0.9559,\n+                0.8746,\n+                0.9271,\n+                0.4882,\n+                0.5406,\n+                0.9439,\n+                0.1526,\n+                0.5028,\n+                0.4107,\n+                0.5591,\n+                0.9130,\n+                0.7572,\n+                0.0302,\n+                0.4532,\n+                0.0893,\n+                0.9490,\n+                0.4880,\n+            ]\n+        ).to(torch_device)\n+\n+        # expected_early_stopping_layer = 2\n+        # predicted_early_stopping_layer = torch.max(outputs.prune[1]).item()\n+        # self.assertEqual(predicted_early_stopping_layer, expected_early_stopping_layer)\n+        # self.assertEqual(predicted_number_of_matches, expected_second_number_of_matches)\n+\n+        \"\"\"\n+        Because of inconsistencies introduced between CUDA versions, the checks here are less strict. SuperGlue relies\n+        on SuperPoint, which may, depending on CUDA version, return different number of keypoints (866 or 867 in this\n+        specific test example). The consequence of having different number of keypoints is that the number of matches\n+        will also be different. In the 20 first matches being checked, having one keypoint less will result in 1 less\n+        match. The matching scores will also be different, as the keypoints are different. The checks here are less\n+        strict to account for these inconsistencies.\n+        Therefore, the test checks that the predicted number of matches, matches and matching scores are close to the\n+        expected values, individually. Here, the tolerance of the number of values changing is set to 2.\n+\n+        This was discussed [here](https://github.com/huggingface/transformers/pull/29886#issuecomment-2482752787)\n+        Such CUDA inconsistencies can be found\n+        [here](https://github.com/huggingface/transformers/pull/33200/files#r1785980300)\n+        \"\"\"\n+\n+        self.assertTrue(abs(predicted_number_of_matches0 - expected_number_of_matches0) < 4)\n+        self.assertTrue(abs(predicted_number_of_matches1 - expected_number_of_matches1) < 4)\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values0, expected_matching_scores_values0, atol=1e-2))\n+            < 4\n+        )\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values1, expected_matching_scores_values1, atol=1e-2))\n+            < 4\n+        )\n+        self.assertTrue(torch.sum(predicted_matches_values0 != expected_matches_values0) < 4)\n+        self.assertTrue(torch.sum(predicted_matches_values1 != expected_matches_values1) < 4)"
        }
    ],
    "stats": {
        "total": 3634,
        "additions": 3632,
        "deletions": 2
    }
}