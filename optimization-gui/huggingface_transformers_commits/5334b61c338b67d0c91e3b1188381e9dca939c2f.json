{
    "author": "ydshieh",
    "message": "Revive AMD scheduled CI (#33448)\n\nRevive AMD scheduled CI\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "5334b61c338b67d0c91e3b1188381e9dca939c2f",
    "files": [
        {
            "sha": "a7e6c7b1ccd5761c9aa8af818e2a54a6755e1e7d",
            "filename": ".github/workflows/model_jobs_amd.yml",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fmodel_jobs_amd.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fmodel_jobs_amd.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs_amd.yml?ref=5334b61c338b67d0c91e3b1188381e9dca939c2f",
            "patch": "@@ -0,0 +1,129 @@\n+name: model jobs\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      folder_slices:\n+        required: true\n+        type: string\n+      machine_type:\n+        required: true\n+        type: string\n+      slice_id:\n+        required: true\n+        type: number\n+      runner:\n+        required: true\n+        type: string\n+      docker:\n+        required: true\n+        type: string\n+\n+env:\n+  HF_HOME: /mnt/cache\n+  TRANSFORMERS_IS_CI: yes\n+  OMP_NUM_THREADS: 8\n+  MKL_NUM_THREADS: 8\n+  RUN_SLOW: yes\n+  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n+  # This token is created under the bot `hf-transformers-bot`.\n+  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n+  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n+  TF_FORCE_GPU_ALLOW_GROWTH: true\n+  RUN_PT_TF_CROSS_TESTS: 1\n+  CUDA_VISIBLE_DEVICES: 0,1\n+\n+jobs:\n+  run_models_gpu:\n+    name: \" \"\n+    strategy:\n+      max-parallel: 1  # For now, not to parallelize. Can change later if it works well.\n+      fail-fast: false\n+      matrix:\n+        folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}\n+    runs-on: ['${{ inputs.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n+    container:\n+      image: ${{ inputs.docker }}\n+      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    steps:\n+      - name: Echo input and matrix info\n+        shell: bash\n+        run: |\n+          echo \"${{ inputs.folder_slices }}\"\n+          echo \"${{ matrix.folders }}\"\n+          echo \"${{ toJson(fromJson(inputs.folder_slices)[inputs.slice_id]) }}\"\n+\n+      - name: Echo folder ${{ matrix.folders }}\n+        shell: bash\n+        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n+        # set the artifact folder names (because the character `/` is not allowed).\n+        run: |\n+          echo \"${{ matrix.folders }}\"\n+          matrix_folders=${{ matrix.folders }}\n+          matrix_folders=${matrix_folders/'models/'/'models_'}\n+          echo \"$matrix_folders\"\n+          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n+\n+      - name: Update clone\n+        working-directory: /transformers\n+        run: git fetch && git checkout ${{ github.sha }}\n+\n+      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n+        working-directory: /transformers\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n+\n+      - name: Update / Install some packages (for Past CI)\n+        if: ${{ contains(inputs.docker, '-past-') }}\n+        working-directory: /transformers\n+        run: |\n+          python3 -m pip install -U datasets\n+\n+      - name: Update / Install some packages (for Past CI)\n+        if: ${{ contains(inputs.docker, '-past-') && contains(inputs.docker, '-pytorch-') }}\n+        working-directory: /transformers\n+        run: |\n+          python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate\n+\n+      - name: ROCM-SMI\n+        run: |\n+          rocm-smi\n+\n+      - name: ROCM-INFO\n+        run: |\n+          rocminfo  | grep \"Agent\" -A 14\n+\n+      - name: Show ROCR environment\n+        run: |\n+          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n+\n+      - name: Environment\n+        working-directory: /transformers\n+        run: |\n+          python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        working-directory: /transformers\n+        run: pip freeze\n+\n+      - name: Run all tests on GPU\n+        working-directory: /transformers\n+        run: python3 -m pytest -rsfE -v --make-reports=${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}  -m \"not not_device_test\"\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: cat /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n+\n+      - name: Run test\n+        shell: bash\n+        run: |\n+          mkdir -p /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          echo \"hello\" > /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n+          echo \"${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n+\n+      - name: \"Test suite reports artifacts: ${{ inputs.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ inputs.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports"
        },
        {
            "sha": "1c79b38a314e0bdf57520b4e765fe66c574ae4fb",
            "filename": ".github/workflows/self-scheduled-amd-mi210-caller.yml",
            "status": "modified",
            "additions": 39,
            "deletions": 4,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled-amd-mi210-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled-amd-mi210-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd-mi210-caller.yml?ref=5334b61c338b67d0c91e3b1188381e9dca939c2f",
            "patch": "@@ -10,11 +10,46 @@ on:\n       - run_amd_scheduled_ci_caller*\r\n \r\n jobs:\r\n-  run_amd_ci:\r\n-    name: AMD mi210\r\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_scheduled_ci_caller')))\r\n+  model-ci:\r\n+    name: Model CI\r\n     uses: ./.github/workflows/self-scheduled-amd.yml\r\n     with:\r\n-      gpu_flavor: mi210\r\n+      job: run_models_gpu\r\n       slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi210\r\n+      docker: huggingface/transformers-pytorch-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi210\r\n+    secrets: inherit\r\n+\r\n+  torch-pipeline:\r\n+    name: Torch pipeline CI\r\n+    uses: ./.github/workflows/self-scheduled-amd.yml\r\n+    with:\r\n+      job: run_pipelines_torch_gpu\r\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi210\r\n+      docker: huggingface/transformers-pytorch-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi210\r\n+    secrets: inherit\r\n+\r\n+  example-ci:\r\n+    name: Example CI\r\n+    uses: ./.github/workflows/self-scheduled-amd.yml\r\n+    with:\r\n+      job: run_examples_gpu\r\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi210\r\n+      docker: huggingface/transformers-pytorch-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi210\r\n+    secrets: inherit\r\n+\r\n+  deepspeed-ci:\r\n+    name: DeepSpeed CI\r\n+    uses: ./.github/workflows/self-scheduled-amd.yml\r\n+    with:\r\n+      job: run_torch_cuda_extensions_gpu\r\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi210\r\n+      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi210\r\n     secrets: inherit\r"
        },
        {
            "sha": "fd151305716396ce8808c65fbde8e863d23f0568",
            "filename": ".github/workflows/self-scheduled-amd-mi250-caller.yml",
            "status": "modified",
            "additions": 39,
            "deletions": 4,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled-amd-mi250-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled-amd-mi250-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd-mi250-caller.yml?ref=5334b61c338b67d0c91e3b1188381e9dca939c2f",
            "patch": "@@ -10,11 +10,46 @@ on:\n       - run_amd_scheduled_ci_caller*\r\n \r\n jobs:\r\n-  run_amd_ci:\r\n-    name: AMD mi250\r\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_scheduled_ci_caller')))\r\n+  model-ci:\r\n+    name: Model CI\r\n     uses: ./.github/workflows/self-scheduled-amd.yml\r\n     with:\r\n-      gpu_flavor: mi250\r\n+      job: run_models_gpu\r\n       slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi250\r\n+      docker: huggingface/transformers-pytorch-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi250\r\n+    secrets: inherit\r\n+\r\n+  torch-pipeline:\r\n+    name: Torch pipeline CI\r\n+    uses: ./.github/workflows/self-scheduled-amd.yml\r\n+    with:\r\n+      job: run_pipelines_torch_gpu\r\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi250\r\n+      docker: huggingface/transformers-pytorch-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi250\r\n+    secrets: inherit\r\n+\r\n+  example-ci:\r\n+    name: Example CI\r\n+    uses: ./.github/workflows/self-scheduled-amd.yml\r\n+    with:\r\n+      job: run_examples_gpu\r\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi250\r\n+      docker: huggingface/transformers-pytorch-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi250\r\n+    secrets: inherit\r\n+\r\n+  deepspeed-ci:\r\n+    name: DeepSpeed CI\r\n+    uses: ./.github/workflows/self-scheduled-amd.yml\r\n+    with:\r\n+      job: run_torch_cuda_extensions_gpu\r\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n+      runner: mi250\r\n+      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu\r\n+      ci_event: Scheduled CI (AMD) - mi250\r\n     secrets: inherit\r"
        },
        {
            "sha": "a9e7b934c34b772307a6abaeb31cd5b008e44e46",
            "filename": ".github/workflows/self-scheduled-amd-mi300-caller.yml",
            "status": "removed",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/d71d6cbdadd08ee05064855308cb8bdd04717fbf/.github%2Fworkflows%2Fself-scheduled-amd-mi300-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d71d6cbdadd08ee05064855308cb8bdd04717fbf/.github%2Fworkflows%2Fself-scheduled-amd-mi300-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd-mi300-caller.yml?ref=d71d6cbdadd08ee05064855308cb8bdd04717fbf",
            "patch": "@@ -1,21 +0,0 @@\n-name: Self-hosted runner (AMD mi300 scheduled CI caller)\n-\n-on:\n-  workflow_run:\n-    workflows: [\"Self-hosted runner (AMD scheduled CI caller)\"]\n-    branches: [\"main\"]\n-    types: [completed]\n-  push:\n-    branches:\n-      - run_amd_scheduled_ci_caller*\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi300\n-    needs: build-docker-containers\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && (startsWith(github.ref_name, 'run_amd_push_ci_caller') || startsWith(github.ref_name, 'mi300-ci'))))\n-    uses: ./.github/workflows/self-scheduled-amd.yml\n-    with:\n-      gpu_flavor: mi300\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\n-    secrets: inherit"
        },
        {
            "sha": "47f92cd6a2b086c5f7d71cdf1484f512b82caf5d",
            "filename": ".github/workflows/self-scheduled-amd.yml",
            "status": "modified",
            "additions": 83,
            "deletions": 254,
            "changes": 337,
            "blob_url": "https://github.com/huggingface/transformers/blob/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled-amd.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled-amd.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd.yml?ref=5334b61c338b67d0c91e3b1188381e9dca939c2f",
            "patch": "@@ -3,10 +3,23 @@ name: Self-hosted runner (scheduled-amd)\n # Note: For the AMD CI, we rely on a caller workflow and on the workflow_call event to trigger the\n # CI in order to run it on both MI210 and MI250, without having to use matrix here which pushes\n # us towards the limit of allowed jobs on GitHub Actions.\n+\n on:\n   workflow_call:\n     inputs:\n-      gpu_flavor:\n+      job:\n+        required: true\n+        type: string\n+      slack_report_channel:\n+        required: true\n+        type: string\n+      runner:\n+        required: true\n+        type: string\n+      docker:\n+        required: true\n+        type: string\n+      ci_event:\n         required: true\n         type: string\n \n@@ -18,7 +31,7 @@ env:\n   RUN_SLOW: yes\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n   SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n-\n+  NUM_SLICES: 2\n \n # Important note: each job (run_tests_single_gpu, run_tests_multi_gpu, run_examples_gpu, run_pipelines_torch_gpu) requires all the previous jobs before running.\n # This is done so that we avoid parallelizing the scheduled tests, to leave available\n@@ -42,33 +55,37 @@ jobs:\n     strategy:\n       matrix:\n         machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n+    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n     container:\n       image: huggingface/transformers-pytorch-amd-gpu\n       options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: ROCM-SMI\n         run: |\n           rocm-smi\n+\n       - name: ROCM-INFO\n         run: |\n           rocminfo  | grep \"Agent\" -A 14\n+\n       - name: Show ROCR environment\n         run: |\n           echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n \n   setup:\n+    if: contains(fromJSON('[\"run_models_gpu\"]'), inputs.job)\n     name: Setup\n     needs: check_runners\n     strategy:\n       matrix:\n         machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n+    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n     container:\n       image: huggingface/transformers-pytorch-amd-gpu\n       options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+      folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}\n+      slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}\n     steps:\n       - name: Update clone\n         working-directory: /transformers\n@@ -90,7 +107,8 @@ jobs:\n         name: Identify models to test\n         working-directory: /transformers/tests\n         run: |\n-          echo \"matrix=$(python3 -c 'import os; tests = os.getcwd(); model_tests = os.listdir(os.path.join(tests, \"models\")); d1 = sorted(list(filter(os.path.isdir, os.listdir(tests)))); d2 = sorted(list(filter(os.path.isdir, [f\"models/{x}\" for x in model_tests]))); d1.remove(\"models\"); d = d2 + d1; print(d)')\" >> $GITHUB_OUTPUT\n+          echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n+          echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n \n       - name: ROCM-SMI\n         run: |\n@@ -99,6 +117,7 @@ jobs:\n       - name: ROCM-INFO\n         run: |\n           rocminfo  | grep \"Agent\" -A 14\n+\n       - name: Show ROCR environment\n         run: |\n           echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n@@ -108,99 +127,38 @@ jobs:\n         run: |\n           python3 utils/print_env.py\n \n-  run_models_gpu_single_gpu:\n+  run_models_gpu:\n+    if: ${{ inputs.job == 'run_models_gpu' }}\n     name: Single GPU tests\n+    needs: setup\n     strategy:\n       max-parallel: 1  # For now, not to parallelize. Can change later if it works well.\n       fail-fast: false\n       matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [single-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    needs: setup\n-    steps:\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all tests on GPU\n-        working-directory: /transformers\n-        run: python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }} -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+        machine_type: [single-gpu, multi-gpu]\n+        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n+    uses: ./.github/workflows/model_jobs_amd.yml\n+    with:\n+      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n+      machine_type: ${{ matrix.machine_type }}\n+      slice_id: ${{ matrix.slice_id }}\n+      runner: ${{ inputs.runner }}\n+      docker: ${{ inputs.docker }}\n+    secrets: inherit\n \n-  run_models_gpu_multi_gpu:\n-    name: Multi GPU tests\n+  run_pipelines_torch_gpu:\n+    if: ${{ inputs.job == 'run_pipelines_torch_gpu' }}\n+    name: PyTorch pipelines\n+    needs: check_runners\n     strategy:\n-      max-parallel: 1\n       fail-fast: false\n       matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n+        machine_type: [single-gpu, multi-gpu]\n+    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n     container:\n-      image: huggingface/transformers-pytorch-amd-gpu\n+      image: ${{ inputs.docker }}\n       options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    needs: setup\n     steps:\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n       - name: Update clone\n         working-directory: /transformers\n         run: git fetch && git checkout ${{ github.sha }}\n@@ -212,9 +170,11 @@ jobs:\n       - name: ROCM-SMI\n         run: |\n           rocm-smi\n+\n       - name: ROCM-INFO\n         run: |\n           rocminfo  | grep \"Agent\" -A 14\n+\n       - name: Show ROCR environment\n         run: |\n           echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n@@ -228,33 +188,35 @@ jobs:\n         working-directory: /transformers\n         run: pip freeze\n \n-      - name: Run all tests on GPU\n+      - name: Run all pipeline tests on GPU\n         working-directory: /transformers\n-        run: python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }} -m \"not not_device_test\"\n+        run: |\n+          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports tests/pipelines -m \"not not_device_test\"\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          name: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n+          path: /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n \n   run_examples_gpu:\n-    name: Examples tests\n+    if: ${{ inputs.job == 'run_examples_gpu' }}\n+    name: Examples directory\n+    needs: check_runners\n     strategy:\n       fail-fast: false\n       matrix:\n         machine_type: [single-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n+    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n     container:\n-      image: huggingface/transformers-pytorch-amd-gpu\n+      image: ${{ inputs.docker }}\n       options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    needs: setup\n     steps:\n       - name: Update clone\n         working-directory: /transformers\n@@ -267,9 +229,11 @@ jobs:\n       - name: ROCM-SMI\n         run: |\n           rocm-smi\n+\n       - name: ROCM-INFO\n         run: |\n           rocminfo  | grep \"Agent\" -A 14\n+\n       - name: Show ROCR environment\n         run: |\n           echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n@@ -301,73 +265,17 @@ jobs:\n           name: ${{ matrix.machine_type }}_run_examples_gpu_test_reports\n           path: /transformers/reports/${{ matrix.machine_type }}_run_examples_gpu_test_reports\n \n-  run_pipelines_torch_gpu:\n-    name: PyTorch pipelines tests\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    needs: setup\n-    steps:\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all pipeline tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports tests/pipelines -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n-\n   run_torch_cuda_extensions_gpu:\n+    if: ${{ inputs.job == 'run_torch_cuda_extensions_gpu' }}\n     name: Torch ROCm deepspeed tests\n+    needs: check_runners\n     strategy:\n       fail-fast: false\n       matrix:\n         machine_type: [single-gpu, multi-gpu]\n-\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    needs: setup\n+    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n     container:\n-      image: huggingface/transformers-pytorch-deepspeed-amd-gpu\n+      image: ${{ inputs.docker }}\n       options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: Update clone\n@@ -381,6 +289,7 @@ jobs:\n       - name: ROCM-SMI\n         run: |\n           rocm-smi\n+\n       - name: ROCM-INFO\n         run: |\n           rocminfo  | grep \"Agent\" -A 14\n@@ -414,107 +323,27 @@ jobs:\n           name: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n           path: /transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n \n-  run_extract_warnings:\n-    name: Extract warnings in CI artifacts\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-      check_runner_status,\n-      check_runners,\n-      setup,\n-      run_models_gpu_single_gpu,\n-      run_models_gpu_multi_gpu,\n-      run_examples_gpu,\n-      run_pipelines_torch_gpu,\n-      run_torch_cuda_extensions_gpu\n-    ]\n-    steps:\n-      - name: Checkout transformers\n-        uses: actions/checkout@v4\n-        with:\n-          fetch-depth: 2\n-\n-      - name: Install transformers\n-        run: pip install transformers\n-\n-      - name: Show installed libraries and their versions\n-        run: pip freeze\n-\n-      - name: Create output directory\n-        run: mkdir warnings_in_ci\n-\n-      - uses: actions/download-artifact@v4\n-        with:\n-          path: warnings_in_ci\n-\n-      - name: Show artifacts\n-        run: echo \"$(python3 -c 'import os; d = os.listdir(); print(d)')\"\n-        working-directory: warnings_in_ci\n-\n-      - name: Extract warnings in CI artifacts\n-        run: |\n-          python3 utils/extract_warnings.py --workflow_run_id ${{ github.run_id }} --output_dir warnings_in_ci --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }} --from_gh\n-          echo \"$(python3 -c 'import os; import json; fp = open(\"warnings_in_ci/selected_warnings.json\"); d = json.load(fp); d = \"\\n\".join(d) ;print(d)')\"\n-\n-      - name: Upload artifact\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: warnings_in_ci\n-          path: warnings_in_ci/selected_warnings.json\n-\n   send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n+    name: Slack Report\n     needs: [\n       check_runner_status,\n       check_runners,\n       setup,\n-      run_models_gpu_single_gpu,\n-      run_models_gpu_multi_gpu,\n-      run_examples_gpu,\n+      run_models_gpu,\n       run_pipelines_torch_gpu,\n-      run_torch_cuda_extensions_gpu,\n-      run_extract_warnings\n+      run_examples_gpu,\n+      run_torch_cuda_extensions_gpu\n     ]\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Runner availability: ${{ needs.check_runner_status.result }}\"\n-          echo \"Runner status: ${{ needs.check_runners.result }}\"\n-          echo \"Setup status: ${{ needs.setup.result }}\"\n-\n-      - uses: actions/checkout@v4\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID_DAILY_AMD: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY_AMD }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY_AMD }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: Scheduled CI (AMD) - ${{ inputs.gpu_flavor }}\n-          CI_SHA: ${{ github.sha }}\n-          CI_WORKFLOW_REF: ${{ github.workflow_ref }}\n-          RUNNER_STATUS: ${{ needs.check_runner_status.result }}\n-          RUNNER_ENV_STATUS: ${{ needs.check_runners.result }}\n-          SETUP_STATUS: ${{ needs.setup.result }}\n-        # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          sudo apt-get install -y curl\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\"\n-\n-      # Upload complete failure tables, as they might be big and only truncated versions could be sent to Slack.\n-      - name: Failure table artifacts\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_failure_tables\n-          path: test_failure_tables\n+    if: ${{ always() }}\n+    uses: ./.github/workflows/slack-report.yml\n+    with:\n+      job: ${{ inputs.job }}\n+      # This would be `skipped` if `setup` is skipped.\n+      setup_status: ${{ needs.setup.result }}\n+      slack_report_channel: ${{ inputs.slack_report_channel }}\n+      # This would be an empty string if `setup` is skipped.\n+      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n+      quantization_matrix: ${{ needs.setup.outputs.quantization_matrix }}\n+      ci_event: ${{ inputs.ci_event }}\n+\n+    secrets: inherit"
        },
        {
            "sha": "f29799b730fc00c602759efc723c3c4b9088d476",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5334b61c338b67d0c91e3b1188381e9dca939c2f/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=5334b61c338b67d0c91e3b1188381e9dca939c2f",
            "patch": "@@ -83,7 +83,7 @@ jobs:\n         run: |\n           echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n           echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n-      \n+\n       - id: set-matrix-quantization\n         if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n         name: Identify quantization method to test"
        }
    ],
    "stats": {
        "total": 575,
        "additions": 291,
        "deletions": 284
    }
}