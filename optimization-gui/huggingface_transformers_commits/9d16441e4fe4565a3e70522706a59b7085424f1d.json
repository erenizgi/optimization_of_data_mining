{
    "author": "kjohew",
    "message": "Fix the memory usage issue of logits in generate() (#34813)",
    "sha": "9d16441e4fe4565a3e70522706a59b7085424f1d",
    "files": [
        {
            "sha": "e3657550d0e7de7ed7199c68103e5101b1298b8c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d16441e4fe4565a3e70522706a59b7085424f1d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d16441e4fe4565a3e70522706a59b7085424f1d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9d16441e4fe4565a3e70522706a59b7085424f1d",
            "patch": "@@ -3246,7 +3246,7 @@ def _sample(\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n-            next_token_logits = outputs.logits.clone()[:, -1, :].float()\n+            next_token_logits = outputs.logits[:, -1, :].clone().float()\n             next_token_logits = next_token_logits.to(input_ids.device)\n \n             # pre-process distribution"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}