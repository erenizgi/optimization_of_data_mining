{
    "author": "cyyever",
    "message": "Change Qwen2RMSNorm to RMSNorm from PyTorch (#40066)\n\n* Unify Qwen2RMSNorm definitions and use RMSNorm from PyTorch\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* subclass RMSNorm\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
    "files": [
        {
            "sha": "2641bc22e442297dcc4ae1824148b3d1c92d2655",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -160,6 +160,11 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n \n [[autodoc]] Qwen2TokenizerFast\n \n+## Qwen2RMSNorm\n+\n+[[autodoc]] Qwen2RMSNorm\n+    - forward\n+\n ## Qwen2Model\n \n [[autodoc]] Qwen2Model"
        },
        {
            "sha": "ab3909a5c0b1b7ce83676ff17326e0bf9b360720",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -43,15 +43,15 @@\n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n class Dots1RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n+    def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n         \"\"\"\n         Dots1RMSNorm is equivalent to T5LayerNorm\n         \"\"\"\n         super().__init__()\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n         self.variance_epsilon = eps\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         input_dtype = hidden_states.dtype\n         hidden_states = hidden_states.to(torch.float32)\n         variance = hidden_states.pow(2).mean(-1, keepdim=True)"
        },
        {
            "sha": "c5b0ff67b41b6334d92d6453c7336aa5d71eb006",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -185,15 +185,15 @@ def forward(\n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n+    def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n         \"\"\"\n         Qwen2RMSNorm is equivalent to T5LayerNorm\n         \"\"\"\n         super().__init__()\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n         self.variance_epsilon = eps\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         input_dtype = hidden_states.dtype\n         hidden_states = hidden_states.to(torch.float32)\n         variance = hidden_states.pow(2).mean(-1, keepdim=True)\n@@ -497,6 +497,7 @@ class Qwen2ForQuestionAnswering(GenericForQuestionAnswering, Qwen2PreTrainedMode\n     \"Qwen2PreTrainedModel\",\n     \"Qwen2Model\",\n     \"Qwen2ForCausalLM\",\n+    \"Qwen2RMSNorm\",\n     \"Qwen2ForSequenceClassification\",\n     \"Qwen2ForTokenClassification\",\n     \"Qwen2ForQuestionAnswering\","
        },
        {
            "sha": "0ae5e00eb5b9041654d36286b8e2568269b09ac5",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -2,9 +2,11 @@\n \n import torch\n import torch.utils.checkpoint\n+from packaging import version\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n+from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -15,6 +17,7 @@\n from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n+from ...utils.import_utils import get_torch_version\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -97,6 +100,35 @@ def forward(\n         return attn_output, attn_weights\n \n \n+if version.parse(get_torch_version()) >= version.parse(\"2.3.0\"):\n+\n+    class Qwen2RMSNorm(nn.RMSNorm):\n+        def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n+            super().__init__(normalized_shape=hidden_size, eps=eps, elementwise_affine=True)\n+\n+else:\n+\n+    @use_kernel_forward_from_hub(\"RMSNorm\")\n+    class Qwen2RMSNorm(nn.Module):\n+        def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n+            \"\"\"\n+            Qwen2RMSNorm is equivalent to T5LayerNorm\n+            \"\"\"\n+            super().__init__()\n+            self.weight = nn.Parameter(torch.ones(hidden_size))\n+            self.variance_epsilon = eps\n+\n+        def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+            input_dtype = hidden_states.dtype\n+            hidden_states = hidden_states.to(torch.float32)\n+            variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+            hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+            return self.weight * hidden_states.to(input_dtype)\n+\n+        def extra_repr(self):\n+            return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n class Qwen2DecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: Qwen2Config, layer_idx: int):\n         super().__init__()\n@@ -206,6 +238,7 @@ class Qwen2ForQuestionAnswering(LlamaForQuestionAnswering):\n     \"Qwen2PreTrainedModel\",\n     \"Qwen2Model\",\n     \"Qwen2ForCausalLM\",\n+    \"Qwen2RMSNorm\",\n     \"Qwen2ForSequenceClassification\",\n     \"Qwen2ForTokenClassification\",\n     \"Qwen2ForQuestionAnswering\","
        },
        {
            "sha": "da5e9e5a1f91231ba32c7ae693ad1d99facbee78",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -43,6 +43,7 @@\n from ...utils import TransformersKwargs, auto_docstring, check_torch_load_is_safe, logging\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.hub import cached_file\n+from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_omni import (\n     Qwen2_5OmniAudioEncoderConfig,\n     Qwen2_5OmniBigVGANConfig,\n@@ -986,26 +987,6 @@ def forward(self, hidden_state):\n         return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n \n \n-class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Qwen2RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n class Qwen2_5OmniVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig) -> None:\n         super().__init__()"
        },
        {
            "sha": "ff7b5a0f88bf273aa7ac45d61975b27221d60108",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -43,6 +43,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n \n@@ -103,26 +104,6 @@ def forward(self, seqlen: int) -> torch.Tensor:\n         return freqs\n \n \n-class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Qwen2RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n class Qwen2_5_VLPatchMerger(nn.Module):\n     def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> None:\n         super().__init__()"
        },
        {
            "sha": "4eef04856220c5f37d508821baa0f0c133bd5fdf",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -46,6 +46,9 @@\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from ..qwen2.modeling_qwen2 import (\n+    Qwen2RMSNorm,\n+)\n from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig\n \n \n@@ -441,27 +444,6 @@ def forward(\n         return hidden_states\n \n \n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm\n-class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Qwen2RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n # Copied from transformers.models.qwen2.modeling_qwen2.Qwen2MLP\n class Qwen2MLP(nn.Module):\n     def __init__(self, config):"
        },
        {
            "sha": "f538202aab22f3130010f7496b60bdef0eebeb23",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e2e28f3c804db6c697ebcb37dc48093bd7d7190/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=1e2e28f3c804db6c697ebcb37dc48093bd7d7190",
            "patch": "@@ -48,15 +48,15 @@\n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen3RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n+    def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n         \"\"\"\n         Qwen3RMSNorm is equivalent to T5LayerNorm\n         \"\"\"\n         super().__init__()\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n         self.variance_epsilon = eps\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         input_dtype = hidden_states.dtype\n         hidden_states = hidden_states.to(torch.float32)\n         variance = hidden_states.pow(2).mean(-1, keepdim=True)"
        }
    ],
    "stats": {
        "total": 117,
        "additions": 50,
        "deletions": 67
    }
}