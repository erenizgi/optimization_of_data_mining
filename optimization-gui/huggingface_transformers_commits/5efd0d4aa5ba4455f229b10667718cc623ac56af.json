{
    "author": "SunMarc",
    "message": "Fix eetq quanto quant methods (#42557)\n\n* fix\n\n* eetq dep removed\n\n* maybe ?\n\n* Fix !\n\n* eveyrthing is passing !\n\n* Apply style fixes\n\n* move to nn.paramters\n\n* grad false\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "5efd0d4aa5ba4455f229b10667718cc623ac56af",
    "files": [
        {
            "sha": "c1cf2921395659ad66afe18bc920c2f83e3937c0",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 83,
            "deletions": 69,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -12,11 +12,13 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ..utils import is_accelerate_available, is_eetq_available, logging\n+from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import should_convert_module\n+from ..utils import is_accelerate_available, is_torch_available, logging\n \n \n-if is_eetq_available():\n-    import eetq\n+if is_torch_available():\n+    import torch\n     import torch.nn as nn\n \n if is_accelerate_available():\n@@ -25,91 +27,103 @@\n logger = logging.get_logger(__name__)\n \n \n-def _replace_with_eetq_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    has_been_replaced=False,\n-    pre_quantized=False,\n-):\n-    \"\"\"\n-    Private method that wraps the recursion for module replacement.\n+class EetqQuantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n-    \"\"\"\n-    if current_key_name is None:\n-        current_key_name = []\n-\n-    for name, module in model.named_children():\n-        current_key_name.append(name)\n-\n-        if (isinstance(module, nn.Linear)) and name not in modules_to_not_convert:\n-            # Check if the current key is not in the `modules_to_not_convert`\n-            current_key_name_str = \".\".join(current_key_name)\n-            if not any(\n-                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n-            ):\n-                with init_empty_weights():\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n-                    model._modules[name] = eetq.EetqLinear(\n-                        in_features, out_features, module.bias is not None, module.weight.device\n-                    )\n-                    if pre_quantized:\n-                        model._modules[name].register_scale(module.weight.device)\n-                    has_been_replaced = True\n-\n-                    # Force requires grad to False to avoid unexpected errors\n-                    model._modules[name].requires_grad_(False)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _replace_with_eetq_linear(\n-                module,\n-                modules_to_not_convert,\n-                current_key_name,\n-                quantization_config,\n-                has_been_replaced=has_been_replaced,\n-                pre_quantized=pre_quantized,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n+    def convert(\n+        self, input_dict: dict[str, list[torch.Tensor]], full_layer_name: str | None = None, **kwargs\n+    ) -> dict[str, torch.Tensor]:\n+        _, value = tuple(input_dict.items())[0]\n+        value = value[0]\n+\n+        value_device = value.device\n+        int8_weight = torch.t(value).contiguous().cpu()\n+        int8_weight, scales = eetq_kernels_hub.quant_weights(int8_weight, torch.int8, False)\n+\n+        int8_weight = int8_weight.to(value_device)\n+        scales = scales.to(value_device)\n+\n+        return {full_layer_name: int8_weight, f\"{full_layer_name}_scales\": scales}\n+\n+\n+class EetqLinearMMFunction(torch.autograd.Function):\n+    @staticmethod\n+    def forward(ctx, x, weight, scales, bias=None):\n+        # The forward pass can use ctx.\n+        ctx.save_for_backward(x, weight, scales, bias)\n+        output = eetq_kernels_hub.w8_a16_gemm(x, weight, scales)\n+        output = output + bias if bias is not None else output\n+        return output\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        input, weight, scales, bias = ctx.saved_tensors\n+        identity = torch.eye(weight.shape[0]).to(weight.device).to(input.dtype)\n+\n+        # Dequantize the weight\n+        weight = eetq_kernels_hub.w8_a16_gemm(identity, weight, scales)\n+\n+        if ctx.needs_input_grad[0]:\n+            # 2D matrix multiplication, unsqueeze to 3D\n+            grad_input = grad_output.squeeze(0).matmul(weight.transpose(0, 1)).unsqueeze(0)\n+\n+        return grad_input, None, None, None\n+\n+\n+class EetqLinear(nn.Module):\n+    def __init__(self, in_features, out_features, dtype=torch.int8, bias=False):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.empty((in_features, out_features), dtype=dtype), requires_grad=False)\n+        self.weight_scales = nn.Parameter(torch.empty((out_features), dtype=torch.float16))\n+        if bias:\n+            self.bias = nn.Parameter(torch.empty((out_features), dtype=torch.float16))\n+        else:\n+            self.bias = None\n+\n+    def forward(self, input):\n+        output = EetqLinearMMFunction.apply(input, self.weight, self.weight_scales, self.bias)\n+        return output\n \n \n def replace_with_eetq_linear(\n-    model, modules_to_not_convert=None, current_key_name=None, quantization_config=None, pre_quantized=False\n+    model: torch.nn.Module, modules_to_not_convert: list[str] | None = None, pre_quantized=False\n ):\n     \"\"\"\n-    A helper function to replace all `torch.nn.Linear` modules by `eetq.EetqLinear` modules from the `eetq`\n-    library. This will enable running your models using high performance int8 weight-only gemm kerner from\n-    FasterTransformer and TensorRT-LLM. Make sure `eetq` compiled with the correct CUDA\n-    version of your hardware is installed before running this function. EETQ shall be installed via the source\n-    'https://github.com/NetEase-FuXi/EETQ'\n-\n-    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n+    A helper function to replace all `torch.nn.Linear` modules by `EetqLinear` modules.\n+    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `modules_to_not_convert` that should\n     be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n     CPU/GPU memory is required to run this function. Each weight will be quantized along the channel.\n \n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `None`):\n             Names of the modules to not convert in `EetqLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n         current_key_name (`list[`str`]`, *optional*):\n             An array to track the current key of the recursion. This is used to check whether the current key (part of\n             it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n             `disk`).\n     \"\"\"\n-\n-    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n-\n-    if quantization_config.modules_to_not_convert is not None:\n-        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n-    modules_to_not_convert = list(set(modules_to_not_convert))\n-    model, has_been_replaced = _replace_with_eetq_linear(\n-        model, modules_to_not_convert, current_key_name, quantization_config, pre_quantized=pre_quantized\n-    )\n+    from kernels import get_kernel\n+\n+    global eetq_kernels_hub\n+    eetq_kernels_hub = get_kernel(\"kernels-community/quantization-eetq\")\n+\n+    has_been_replaced = False\n+    # we need this to correctly materialize the weights during quantization\n+    module_kwargs = {} if pre_quantized else {\"dtype\": None}\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                new_module = EetqLinear(\n+                    module.in_features, module.out_features, bias=module.bias is not None, **module_kwargs\n+                )\n+                model.set_submodule(module_name, new_module)\n+                has_been_replaced = True\n \n     if not has_been_replaced:\n         logger.warning("
        },
        {
            "sha": "75d43da2319436412f4a862612ebffb13a957176",
            "filename": "src/transformers/integrations/quanto.py",
            "status": "modified",
            "additions": 71,
            "deletions": 54,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquanto.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -12,21 +12,52 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ..utils import is_optimum_quanto_available, is_torch_available, logging\n+from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n+from ..utils import is_torch_available, logging\n \n \n if is_torch_available():\n     import torch\n+    import torch.nn as nn\n \n logger = logging.get_logger(__name__)\n \n \n+class QuantoQuantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        model: torch.nn.Module | None = None,\n+        full_layer_name: str | None = None,\n+        missing_keys: list[str] | None = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        _, value = tuple(input_dict.items())[0]\n+        value = value[0]\n+\n+        from ..modeling_utils import _load_parameter_into_model\n+\n+        _load_parameter_into_model(model, full_layer_name, value)\n+        module, _ = get_module_from_name(model, full_layer_name)\n+        module.freeze()\n+        module.weight.requires_grad = False\n+        module._is_hf_initialized = True\n+\n+        # need to discard some missing keys we already updated the module in freeze.\n+        module_name = full_layer_name.rsplit(\".\", 1)[0]\n+        missing_keys.discard(f\"{module_name}.input_scale\")\n+        missing_keys.discard(f\"{module_name}.output_scale\")\n+        return {}\n+\n+\n def replace_with_quanto_layers(\n     model,\n     quantization_config=None,\n     modules_to_not_convert=None,\n-    current_key_name=None,\n-    has_been_replaced=False,\n ):\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with Quanto quantized layers.\n@@ -35,64 +66,50 @@ def replace_with_quanto_layers(\n     Args:\n         model (`torch.nn.Module`):\n             The model to convert, can be any `torch.nn.Module` instance.\n-        quantization_config (`AqlmConfig`, defaults to `None`):\n+        quantization_config (`QuantoConfig`, defaults to `None`):\n             The quantization config object that contains the quantization parameters.\n         modules_to_not_convert (`list`, *optional*, defaults to `None`):\n             A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\n             converted.\n-        current_key_name (`list`, *optional*, defaults to `None`):\n-            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n-        has_been_replaced (`bool`, *optional*, defaults to `None`):\n-            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n-            should not be passed by the user.\n     \"\"\"\n     from accelerate import init_empty_weights\n-\n-    if is_optimum_quanto_available():\n-        from optimum.quanto import QLayerNorm, QLinear, qfloat8, qint2, qint4, qint8\n+    from optimum.quanto import QLayerNorm, QLinear, qfloat8, qint2, qint4, qint8\n \n     w_mapping = {\"float8\": qfloat8, \"int8\": qint8, \"int4\": qint4, \"int2\": qint2}\n     a_mapping = {None: None, \"float8\": qfloat8, \"int8\": qint8}\n \n-    if modules_to_not_convert is None:\n-        modules_to_not_convert = []\n-\n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n-            with init_empty_weights():\n-                if isinstance(module, torch.nn.Linear):\n-                    model._modules[name] = QLinear(\n-                        in_features=module.in_features,\n-                        out_features=module.out_features,\n-                        bias=module.bias is not None,\n-                        dtype=module.weight.dtype,\n-                        weights=w_mapping[quantization_config.weights],\n-                        activations=a_mapping[quantization_config.activations],\n-                    )\n-                    model._modules[name].requires_grad_(False)\n-                    has_been_replaced = True\n-                elif isinstance(module, torch.nn.LayerNorm):\n-                    if quantization_config.activations is not None:\n-                        model._modules[name] = QLayerNorm(\n-                            module.normalized_shape,\n-                            module.eps,\n-                            module.elementwise_affine,\n-                            module.bias is not None,\n-                            activations=a_mapping[quantization_config.activations],\n-                        )\n-                        has_been_replaced = True\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = replace_with_quanto_layers(\n-                module,\n-                quantization_config=quantization_config,\n-                modules_to_not_convert=modules_to_not_convert,\n-                current_key_name=current_key_name,\n-                has_been_replaced=has_been_replaced,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n+    has_been_replaced = False\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            new_module = None\n+            if isinstance(module, nn.Linear):\n+                new_module = QLinear(\n+                    in_features=module.in_features,\n+                    out_features=module.out_features,\n+                    bias=module.bias is not None,\n+                    dtype=module.weight.dtype,\n+                    weights=w_mapping[quantization_config.weights],\n+                    activations=a_mapping[quantization_config.activations],\n+                )\n+            elif isinstance(module, torch.nn.LayerNorm) and quantization_config.activations is not None:\n+                new_module = QLayerNorm(\n+                    module.normalized_shape,\n+                    module.eps,\n+                    module.elementwise_affine,\n+                    module.bias is not None,\n+                    activations=a_mapping[quantization_config.activations],\n+                )\n+            if new_module is not None:\n+                has_been_replaced = True\n+                model.set_submodule(module_name, new_module)\n+\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using quanto but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n+\n+    return model"
        },
        {
            "sha": "52efea9d00ee79b46566385ce5876b1a28f2b67d",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 10,
            "deletions": 50,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -19,7 +19,7 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_accelerate_available, is_eetq_available, is_torch_available, logging\n+from ..utils import is_accelerate_available, is_kernels_available, is_torch_available, logging\n from .quantizers_utils import get_module_from_name\n \n \n@@ -47,25 +47,8 @@ def __init__(self, quantization_config, **kwargs):\n         self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n-        if not is_eetq_available():\n-            raise ImportError(\n-                \"Using `eetq` 8-bit quantization requires eetq.\"\n-                \"Please install the latest version of eetq from : https://github.com/NetEase-FuXi/EETQ\"\n-            )\n-\n-        try:\n-            import eetq  # noqa: F401\n-        except ImportError as exc:\n-            if \"shard_checkpoint\" in str(exc):\n-                # EETQ 1.0.0 is currently broken with the latest transformers because it tries to import the removed\n-                # shard_checkpoint function, see https://github.com/NetEase-FuXi/EETQ/issues/34.\n-                # TODO: Update message once eetq releases a fix\n-                raise ImportError(\n-                    \"You are using a version of EETQ that is incompatible with the current transformers version. \"\n-                    \"Either downgrade transformers to <= v4.46.3 or, if available, upgrade EETQ to > v1.0.0.\"\n-                ) from exc\n-            else:\n-                raise\n+        if not is_kernels_available():\n+            raise ImportError(\"Loading an EETQ quantized model requires kernels (`pip install kernels`)\")\n \n         if not is_accelerate_available():\n             raise ImportError(\"Loading an EETQ quantized model requires accelerate (`pip install accelerate`)\")\n@@ -101,7 +84,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         return dtype\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n-        from eetq import EetqLinear\n+        from ..integrations.eetq import EetqLinear\n \n         module, tensor_name = get_module_from_name(model, param_name)\n \n@@ -112,31 +95,6 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n                 return True\n         return False\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        from eetq import EetqLinear, quantize_and_preprocess_weights\n-\n-        module, tensor_name = get_module_from_name(model, param_name)\n-        new_value, weight_scale = quantize_and_preprocess_weights(param_value)\n-\n-        # Samity check\n-        if isinstance(module, EetqLinear):\n-            if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n-            else:\n-                if tensor_name == \"weight_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n-\n-        module._buffers[tensor_name] = new_value.to(target_device)\n-        module.register(\"weight_scales\", weight_scale.to(target_device))\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -150,10 +108,7 @@ def _process_model_before_weight_loading(\n         )\n \n         model = replace_with_eetq_linear(\n-            model,\n-            modules_to_not_convert=self.modules_to_not_convert,\n-            quantization_config=self.quantization_config,\n-            pre_quantized=self.pre_quantized,\n+            model, modules_to_not_convert=self.modules_to_not_convert, pre_quantized=self.pre_quantized\n         )\n \n         model.config.quantization_config = self.quantization_config\n@@ -164,3 +119,8 @@ def is_serializable(self, safe_serialization=None):\n     @property\n     def is_trainable(self) -> bool:\n         return True\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.eetq import EetqQuantize\n+\n+        return EetqQuantize(self)"
        },
        {
            "sha": "25af8d2874c67524c19a32d39984505b84081161",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 24,
            "deletions": 61,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -46,17 +46,6 @@ class QuantoHfQuantizer(HfQuantizer):\n \n     def __init__(self, quantization_config: QuantoConfig, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.post_init()\n-\n-    def post_init(self):\n-        r\"\"\"\n-        Safety checker\n-        \"\"\"\n-        if self.quantization_config.activations is not None and not self.pre_quantized:\n-            raise ValueError(\n-                \"We don't support quantizing the activations with transformers library.\"\n-                \"Use quanto library for more complex use cases such as activations quantization, calibration and quantization aware training.\"\n-            )\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_optimum_quanto_available():\n@@ -67,42 +56,26 @@ def validate_environment(self, *args, **kwargs):\n             raise ImportError(\n                 \"Loading an optimum-quanto quantized model requires accelerate library (`pip install accelerate`)\"\n             )\n-\n-    def update_device_map(self, device_map):\n-        if device_map is None:\n-            device_map = {\"\": \"cpu\"}\n-            logger.info(\n-                \"The device_map was not initialized. \"\n-                \"Setting device_map to {'':'cpu'}. \"\n-                \"If you want to use the model for inference, please set device_map ='auto'\"\n+        device_map = kwargs.get(\"device_map\")\n+        if device_map is not None:\n+            if (\n+                isinstance(device_map, dict)\n+                and len(device_map) >= 2\n+                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n+            ):\n+                raise ValueError(\n+                    \"You are attempting to load an model with a device_map that contains a CPU or disk device.\"\n+                    \"This is not supported with quanto when the model is quantized on the fly. \"\n+                    \"Please remove the CPU or disk device from the device_map.\"\n+                )\n+        if self.quantization_config.activations is not None:\n+            raise ValueError(\n+                \"We don't support quantizing the activations with transformers library.\"\n+                \"Use quanto library for more complex use cases such as activations quantization, calibration and quantization aware training.\"\n             )\n-        return device_map\n-\n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            logger.info(\"You did not specify `dtype` in `from_pretrained`. Setting it to `torch.float32`.\")\n-            dtype = torch.float32\n-        return dtype\n-\n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        if is_optimum_quanto_available():\n-            from optimum.quanto import QModuleMixin\n-\n-        not_missing_keys = []\n-        for name, module in model.named_modules():\n-            if isinstance(module, QModuleMixin):\n-                for missing in missing_keys:\n-                    if (\n-                        (name in missing or name in f\"{prefix}.{missing}\")\n-                        and not missing.endswith(\".weight\")\n-                        and not missing.endswith(\".bias\")\n-                    ):\n-                        not_missing_keys.append(missing)\n-        return [k for k in missing_keys if k not in not_missing_keys]\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n-        if is_optimum_quanto_available():\n-            from optimum.quanto import QModuleMixin\n+        from optimum.quanto import QModuleMixin\n \n         module, tensor_name = get_module_from_name(model, param_name)\n         # We only quantize the weights and the bias is not quantized.\n@@ -116,21 +89,6 @@ def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int |\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        from ..modeling_utils import _load_parameter_into_model\n-\n-        _load_parameter_into_model(model, param_name, param_value.to(target_device))\n-        module, _ = get_module_from_name(model, param_name)\n-        module.freeze()\n-        module.weight.requires_grad = False\n-\n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         from accelerate.utils import CustomDtype\n \n@@ -152,7 +110,7 @@ def _process_model_before_weight_loading(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n \n-        model, _ = replace_with_quanto_layers(\n+        model = replace_with_quanto_layers(\n             model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n         model.config.quantization_config = self.quantization_config\n@@ -161,5 +119,10 @@ def _process_model_before_weight_loading(\n     def is_trainable(self) -> bool:\n         return True\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return False\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.quanto import QuantoQuantize\n+\n+        return QuantoQuantize(self)"
        },
        {
            "sha": "0e90e238ec4a6ed4a392b170c95d1c6895025a80",
            "filename": "src/transformers/quantizers/quantizers_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fquantizers%2Fquantizers_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Fquantizers%2Fquantizers_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizers_utils.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import re\n-from typing import Any, Optional\n+from typing import Any\n \n \n def get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n@@ -22,7 +22,7 @@ def get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n     return module, tensor_name\n \n \n-def should_convert_module(full_name, patterns: Optional[list] = None):\n+def should_convert_module(full_name, patterns: list[str] | None = None):\n     if patterns is None:\n         return True\n "
        },
        {
            "sha": "2bfbf3f5781cc076bffff1b6afe73d72b8bf7d34",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -88,7 +88,6 @@\n     is_cython_available,\n     is_decord_available,\n     is_detectron2_available,\n-    is_eetq_available,\n     is_essentia_available,\n     is_faiss_available,\n     is_fbgemm_gpu_available,\n@@ -1239,23 +1238,6 @@ def require_spqr(test_case):\n     return unittest.skipUnless(is_spqr_available(), \"test requires spqr\")(test_case)\n \n \n-def require_eetq(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires eetq\n-    \"\"\"\n-    eetq_available = is_eetq_available()\n-    if eetq_available:\n-        try:\n-            import eetq  # noqa: F401\n-        except ImportError as exc:\n-            if \"shard_checkpoint\" in str(exc):\n-                # EETQ 1.0.0 is currently broken with the latest transformers because it tries to import the removed\n-                # shard_checkpoint function, see https://github.com/NetEase-FuXi/EETQ/issues/34.\n-                # TODO: Remove once eetq releases a fix and this release is used in CI\n-                eetq_available = False\n-    return unittest.skipUnless(eetq_available, \"test requires eetq\")(test_case)\n-\n-\n def require_av(test_case):\n     \"\"\"\n     Decorator marking a test that requires av"
        },
        {
            "sha": "b32488d0da04e2e44cedfa2e0ced1da2bba0e351",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -129,7 +129,6 @@\n     is_datasets_available,\n     is_decord_available,\n     is_detectron2_available,\n-    is_eetq_available,\n     is_essentia_available,\n     is_faiss_available,\n     is_fbgemm_gpu_available,"
        },
        {
            "sha": "0268fb72c9be1abdf8325382d05821bf97467675",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -1025,11 +1025,6 @@ def is_gptqmodel_available() -> bool:\n     return _is_package_available(\"gptqmodel\")\n \n \n-@lru_cache\n-def is_eetq_available() -> bool:\n-    return _is_package_available(\"eetq\")\n-\n-\n @lru_cache\n def is_fbgemm_gpu_available() -> bool:\n     return _is_package_available(\"fbgemm_gpu\")"
        },
        {
            "sha": "791cb7f5e7c8b1a73a593708ae3e8117eb5198ab",
            "filename": "tests/quantization/eetq_integration/test_eetq.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -20,7 +20,7 @@\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_accelerate,\n-    require_eetq,\n+    require_kernels,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -62,8 +62,8 @@ def test_from_dict(self):\n \n @slow\n @require_torch_gpu\n-@require_eetq\n @require_accelerate\n+@require_kernels\n class EetqTest(unittest.TestCase):\n     model_name = \"facebook/opt-350m\"\n \n@@ -95,13 +95,11 @@ def test_quantized_model_conversion(self):\n         \"\"\"\n         Simple test that checks if the quantized model has been converted properly\n         \"\"\"\n-        from eetq import EetqLinear\n-\n         from transformers.integrations import replace_with_eetq_linear\n+        from transformers.integrations.eetq import EetqLinear\n \n         model_id = \"facebook/opt-350m\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n-        quantization_config = EetqConfig(weights=\"int8\")\n \n         with init_empty_weights():\n             model = OPTForCausalLM(config)\n@@ -111,25 +109,24 @@ def test_quantized_model_conversion(self):\n             if isinstance(module, torch.nn.Linear):\n                 nb_linears += 1\n \n-        model = replace_with_eetq_linear(model, quantization_config=quantization_config)\n+        model = replace_with_eetq_linear(model)\n         nb_eetq_linear = 0\n         for module in model.modules():\n             if isinstance(module, EetqLinear):\n                 nb_eetq_linear += 1\n \n-        self.assertEqual(nb_linears - 1, nb_eetq_linear)\n+        self.assertEqual(nb_linears, nb_eetq_linear)\n \n         # Try with `modules_to_not_convert`\n         with init_empty_weights():\n             model = OPTForCausalLM(config)\n-        quantization_config = EetqConfig(modules_to_not_convert=[\"fc1\"])\n-        model = replace_with_eetq_linear(model, quantization_config=quantization_config)\n+        model = replace_with_eetq_linear(model, modules_to_not_convert=[\"fc1\"])\n         nb_eetq_linear = 0\n         for module in model.modules():\n             if isinstance(module, EetqLinear):\n                 nb_eetq_linear += 1\n         # 25 corresponds to the lm_head along with 24 fc1 layers.\n-        self.assertEqual(nb_linears - 25, nb_eetq_linear)\n+        self.assertEqual(nb_linears - 24, nb_eetq_linear)\n \n     def test_quantized_model(self):\n         \"\"\""
        },
        {
            "sha": "c8872546b12c332b1f54b53641ad5a0b8dce9ed1",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 18,
            "deletions": 200,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/5efd0d4aa5ba4455f229b10667718cc623ac56af/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5efd0d4aa5ba4455f229b10667718cc623ac56af/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=5efd0d4aa5ba4455f229b10667718cc623ac56af",
            "patch": "@@ -30,8 +30,6 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import LlamaForCausalLM\n-\n if is_accelerate_available():\n     from accelerate import init_empty_weights\n \n@@ -41,15 +39,10 @@\n     from transformers.integrations.quanto import replace_with_quanto_layers\n \n \n-class QuantoConfigTest(unittest.TestCase):\n-    def test_attributes(self):\n-        pass\n-\n-\n @require_optimum_quanto\n @require_accelerate\n class QuantoTestIntegration(unittest.TestCase):\n-    model_id = \"facebook/opt-350m\"\n+    model_id = \"HuggingFaceTB/SmolLM3-3B\"\n \n     def setUp(self):\n         config = AutoConfig.from_pretrained(self.model_id)\n@@ -70,7 +63,7 @@ def test_weight_only_quantization_conversion(self):\n \n         # Try with weight only quantization\n         quantization_config = QuantoConfig(weights=\"int8\", activations=None)\n-        self.model, _ = replace_with_quanto_layers(self.model, quantization_config=quantization_config)\n+        self.model = replace_with_quanto_layers(self.model, quantization_config=quantization_config)\n \n         nb_qlinear = 0\n         for module in self.model.modules():\n@@ -86,7 +79,7 @@ def test_weight_and_activation_quantization_conversion(self):\n \n         # Try with weight + activation quantization\n         quantization_config = QuantoConfig(weights=\"int8\", activations=\"int8\")\n-        self.model, _ = replace_with_quanto_layers(self.model, quantization_config=quantization_config)\n+        self.model = replace_with_quanto_layers(self.model, quantization_config=quantization_config)\n \n         nb_qlinear = 0\n         nb_qlayernorm = 0\n@@ -106,7 +99,7 @@ def test_conversion_with_modules_to_not_convert(self):\n \n         # Try with weight + activatioin quantization\n         quantization_config = QuantoConfig(weights=\"int8\", activations=\"int8\")\n-        self.model, _ = replace_with_quanto_layers(\n+        self.model = replace_with_quanto_layers(\n             self.model, quantization_config=quantization_config, modules_to_not_convert=[\"lm_head\"]\n         )\n \n@@ -130,14 +123,14 @@ class QuantoQuantizationTest(unittest.TestCase):\n     Test 8-bit weights only quantization\n     \"\"\"\n \n-    model_name = \"bigscience/bloom-560m\"\n+    model_name = \"HuggingFaceTB/SmolLM2-135M\"\n \n     weights = \"int8\"\n     activations = None\n     device_map = \"cpu\"\n \n     input_text = \"Hello my name is\"\n-    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer and I\"\n+    EXPECTED_OUTPUTS = \"Hello my name is John. I am a student of the University of\"\n \n     def setUp(self):\n         \"\"\"\n@@ -192,14 +185,10 @@ def test_quantized_model_layers(self):\n         Suite of simple test to check if the layers are quantized and are working properly\n         \"\"\"\n         # Test the type of the quantized layer\n-        self.assertTrue(isinstance(self.quantized_model.transformer.h[0].self_attention.query_key_value, QModuleMixin))\n-        self.assertTrue(\n-            isinstance(self.quantized_model.transformer.h[0].self_attention.query_key_value.weight, QTensor)\n-        )\n+        self.assertTrue(isinstance(self.quantized_model.model.layers[0].self_attn.k_proj, QModuleMixin))\n+        self.assertTrue(isinstance(self.quantized_model.model.layers[0].self_attn.k_proj.weight, QTensor))\n         if self.weights == \"int4\":\n-            self.assertTrue(\n-                isinstance(self.quantized_model.transformer.h[0].self_attention.query_key_value.weight, QBitsTensor)\n-            )\n+            self.assertTrue(isinstance(self.quantized_model.model.layers[0].self_attn.k_proj.weight, QBitsTensor))\n \n         # check that the lm_head was indeed not quantized, just like bnb\n         self.assertTrue(\n@@ -208,13 +197,11 @@ def test_quantized_model_layers(self):\n         )\n         if self.device_map in [\"cpu\", \"cuda\"]:\n             self.assertEqual(\n-                self.quantized_model.transformer.h[0].self_attention.query_key_value.weight._data.device.type,\n+                self.quantized_model.model.layers[0].self_attn.k_proj.weight._data.device.type,\n                 self.device_map,\n             )\n             self.quantized_model.to(0)\n-        self.assertEqual(\n-            self.quantized_model.transformer.h[0].self_attention.query_key_value.weight._data.device.type, torch_device\n-        )\n+        self.assertEqual(self.quantized_model.model.layers[0].self_attn.k_proj.weight._data.device.type, torch_device)\n \n     def test_serialization_bin(self):\n         \"\"\"\n@@ -223,14 +210,7 @@ def test_serialization_bin(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             with self.assertRaises(ValueError) as e:\n                 self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n-            self.assertIn(\n-                \"The model is quantized with QuantizationMethod.QUANTO and is not serializable\", str(e.exception)\n-            )\n-            # TODO: replace by the following when it works\n-            # quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-            #     tmpdirname, dtype=torch.float32, device_map=\"cpu\"\n-            # )\n-            # self.check_inference_correctness(quantized_model_from_saved, device=\"cuda\")\n+            self.assertIn(\"The model is quantized with quanto and is not serializable\", str(e.exception))\n \n     def test_serialization_safetensors(self):\n         \"\"\"\n@@ -239,13 +219,7 @@ def test_serialization_safetensors(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             with self.assertRaises(ValueError) as e:\n                 self.quantized_model.save_pretrained(tmpdirname)\n-            self.assertIn(\n-                \"The model is quantized with QuantizationMethod.QUANTO and is not serializable\", str(e.exception)\n-            )\n-            # quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-            #     tmpdirname, dtype=torch.float32, device_map=\"cpu\"\n-            # )\n-            # self.check_inference_correctness(quantized_model_from_saved, device=\"cuda\")\n+            self.assertIn(\"The model is quantized with quanto and is not serializable\", str(e.exception))\n \n     def check_same_model(self, model1, model2):\n         d0 = dict(model1.named_parameters())\n@@ -268,170 +242,14 @@ def test_compare_with_quanto(self):\n             dtype=torch.float32,\n         )\n         # we do not quantize the lm_head since we don't do that in transformers\n-        quantize(model.transformer, weights=w_mapping[self.weights])\n-        freeze(model.transformer)\n+        quantize(model.model, weights=w_mapping[self.weights])\n+        freeze(model.model)\n         self.check_same_model(model, self.quantized_model)\n         self.check_inference_correctness(model, device=torch_device)\n \n-    @unittest.skip\n-    def test_load_from_quanto_saved(self):\n-        from optimum.quanto import freeze, qint4, qint8, quantize\n-\n-        from transformers import QuantoConfig\n-\n-        w_mapping = {\"int8\": qint8, \"int4\": qint4}\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name,\n-            device_map=self.device_map,\n-            dtype=torch.float32,\n-        )\n-        # we do not quantize the lm_head since we don't do that in transformers\n-        quantize(model.transformer, weights=w_mapping[self.weights])\n-        freeze(model.transformer)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            model.config.quantization_config = QuantoConfig(\n-                weights=self.weights, activations=self.activations, modules_to_not_convert=[\"lm_head\"]\n-            )\n-            model.save_pretrained(tmpdirname, safe_serialization=False)\n-            quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-                tmpdirname,\n-                device_map=self.device_map,\n-                dtype=torch.float32,\n-            )\n-        self.check_same_model(model, quantized_model_from_saved)\n-        self.check_inference_correctness(quantized_model_from_saved, device=\"cuda\")\n-\n-\n-class QuantoQuantizationOffloadTest(QuantoQuantizationTest):\n-    device_map = {\n-        \"transformer.word_embeddings\": 0,\n-        \"transformer.word_embeddings_layernorm\": 0,\n-        \"transformer.ln_f\": 0,\n-        \"transformer.h.0\": 0,\n-        \"transformer.h.1\": 0,\n-        \"transformer.h.2\": 0,\n-        \"transformer.h.3\": 0,\n-        \"transformer.h.4\": 0,\n-        \"transformer.h.5\": 0,\n-        \"transformer.h.6\": 0,\n-        \"transformer.h.7\": 0,\n-        \"transformer.h.8\": 0,\n-        \"transformer.h.9\": 0,\n-        \"transformer.h.10\": 0,\n-        \"transformer.h.11\": 0,\n-        \"transformer.h.12\": 0,\n-        \"transformer.h.13\": 0,\n-        \"transformer.h.14\": 0,\n-        \"transformer.h.15\": 0,\n-        \"transformer.h.16\": 0,\n-        \"transformer.h.17\": 0,\n-        \"transformer.h.18\": 0,\n-        \"transformer.h.19\": 0,\n-        \"transformer.h.20\": 0,\n-        \"transformer.h.21\": 0,\n-        \"transformer.h.22\": \"cpu\",\n-        \"transformer.h.23\": \"disk\",\n-        \"lm_head\": 0,\n-    }\n-\n-    @unittest.skip(reason=\"The execution device is a gpu\")\n-    def test_generate_quality_cpu(self):\n-        pass\n-\n-    @unittest.skip(reason=\"We can't save offloaded values\")\n-    def test_serialization_bin(self):\n-        pass\n-\n-    @unittest.skip\n-    def test_serialization_safetensors(self):\n-        pass\n-\n-    @unittest.skip\n-    def test_compare_with_quanto(self):\n-        pass\n-\n-    @unittest.skip\n-    def test_load_from_quanto_saved(self):\n-        pass\n-\n-    def test_check_offload_quantized(self):\n-        \"\"\"\n-        We check that we have unquantized value in the cpu and in the disk\n-        \"\"\"\n-        from optimum.quanto import QBitsTensor, QTensor\n-\n-        cpu_weights = self.quantized_model.transformer.h[22].self_attention.query_key_value._hf_hook.weights_map[\n-            \"weight\"\n-        ]\n-        disk_weights = self.quantized_model.transformer.h[23].self_attention.query_key_value._hf_hook.weights_map[\n-            \"weight\"\n-        ]\n-        self.assertTrue(isinstance(cpu_weights, torch.Tensor) and not isinstance(cpu_weights, QTensor))\n-        self.assertTrue(isinstance(disk_weights, torch.Tensor) and not isinstance(disk_weights, QTensor))\n-        if self.weights == \"int4\":\n-            self.assertTrue(isinstance(cpu_weights, torch.Tensor) and not isinstance(disk_weights, QBitsTensor))\n-            self.assertTrue(isinstance(disk_weights, torch.Tensor) and not isinstance(disk_weights, QBitsTensor))\n-\n-\n-@unittest.skip(reason=\"Skipping test class because serialization is not supported yet\")\n-class QuantoQuantizationSerializationTest(QuantoQuantizationTest):\n-    \"\"\"\n-    Perform the same tests as in QuantoQuantizationTest but with a serialized model.\n-    \"\"\"\n-\n-    def setUp(self):\n-        \"\"\"\n-        Setup quantized model\n-        \"\"\"\n-        quantization_config = QuantoConfig(\n-            weights=self.weights,\n-            activations=self.activations,\n-        )\n-        quantized_model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name,\n-            device_map=self.device_map,\n-            quantization_config=quantization_config,\n-            dtype=torch.float32,\n-        )\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n-            self.quantized_model = AutoModelForCausalLM.from_pretrained(\n-                tmpdirname, dtype=torch.float32, device_map=self.device_map\n-            )\n-\n-        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n-\n-        self.have_accelerate_hooks = (\n-            getattr(self.quantized_model, \"hf_device_map\", False) and len(self.quantized_model.hf_device_map) > 1\n-        )\n-\n-\n-@unittest.skip(reason=\"Skipping test class because serialization is not supported yet\")\n-class QuantoQuantizationSerializationCudaTest(QuantoQuantizationTest):\n-    \"\"\"\n-    Perform the same tests as in QuantoQuantizationTest but with model on cuda\n-    \"\"\"\n-\n-    device_map = \"cuda:0\"\n-\n \n class QuantoQuantizationQBitsTensorTest(QuantoQuantizationTest):\n-    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer, I\"\n-    weights = \"int4\"\n-\n-\n-class QuantoQuantizationQBitsTensorOffloadTest(QuantoQuantizationOffloadTest):\n-    EXPECTED_OUTPUTS = [\n-        \"Hello my name is John, I am a professional photographer, I\",  # CUDA output\n-        \"Hello my name is Nils, I am a student of the University\",  # XPU output\n-    ]\n-    weights = \"int4\"\n-\n-\n-@unittest.skip(reason=\"Skipping test class because serialization is not supported yet\")\n-class QuantoQuantizationQBitsTensorSerializationTest(QuantoQuantizationSerializationTest):\n-    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer, I\"\n+    EXPECTED_OUTPUTS = \"Hello my name is joe and i am a little girl\\n\\n\"\n     weights = \"int4\"\n \n \n@@ -443,7 +261,7 @@ def test_quantize_activation(self):\n             activations=\"int8\",\n         )\n         with self.assertRaises(ValueError) as e:\n-            AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", quantization_config=quantization_config)\n+            AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", quantization_config=quantization_config)\n         self.assertIn(\"We don't support quantizing the activations with transformers library\", str(e.exception))\n \n \n@@ -465,7 +283,7 @@ def test_quantized_cache(self):\n         tokenizer = AutoTokenizer.from_pretrained(\n             \"unsloth/Llama-3.2-1B-Instruct\", pad_token=\"</s>\", padding_side=\"left\"\n         )\n-        model = LlamaForCausalLM.from_pretrained(\n+        model = AutoModelForCausalLM.from_pretrained(\n             \"unsloth/Llama-3.2-1B-Instruct\", device_map=\"sequential\", dtype=torch.float16\n         )\n         inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(torch_device)"
        }
    ],
    "stats": {
        "total": 685,
        "additions": 215,
        "deletions": 470
    }
}