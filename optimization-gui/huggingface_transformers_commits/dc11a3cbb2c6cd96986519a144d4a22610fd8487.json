{
    "author": "Cyrilvallez",
    "message": "[core] Refactor the Cache logic to make it simpler and more general (#39797)\n\n* Simplify the logic quite a bit\n\n* Update cache_utils.py\n\n* continue work\n\n* continue simplifying a lot\n\n* style\n\n* Update cache_utils.py\n\n* offloading much simpler\n\n* style\n\n* Update cache_utils.py\n\n* update inits\n\n* Update cache_utils.py\n\n* consistemncy\n\n* Update cache_utils.py\n\n* update generate\n\n* style\n\n* fix\n\n* fix\n\n* add early_initialization\n\n* fix\n\n* fix mamba caches\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n* fix tests\n\n* fix configs\n\n* revert\n\n* fix tests\n\n* alright\n\n* Update modeling_gptj.py\n\n* fix the constructors\n\n* cache tests\n\n* Update test_cache_utils.py\n\n* fix\n\n* simplify\n\n* back to before -> avoid compile bug\n\n* doc\n\n* mistral test\n\n* llama4 test dtype\n\n* Update test_modeling_llama4.py\n\n* CIs\n\n* Finally find a nice impl\n\n* Update cache_utils.py\n\n* Update cache_utils.py\n\n* add lazy methods in autodoc\n\n* typo\n\n* better doc\n\n* Add detailed docstring for lazy init\n\n* CIs\n\n* style\n\n* fix",
    "sha": "dc11a3cbb2c6cd96986519a144d4a22610fd8487",
    "files": [
        {
            "sha": "b19e724e06d02a624502da7170aaeefd9e1e44d3",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 11,
            "deletions": 18,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -363,37 +363,34 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n     - get_max_cache_shape\n     - reset\n     - reorder_cache\n+    - lazy_initialization\n \n [[autodoc]] DynamicLayer\n     - update\n+    - lazy_initialization\n     - crop\n     - batch_repeat_interleave\n     - batch_select_indices\n \n [[autodoc]] StaticLayer\n     - update\n+    - lazy_initialization\n \n [[autodoc]] SlidingWindowLayer\n     - update\n+    - lazy_initialization\n \n-[[autodoc]] CacheProcessor\n-    - pre_update\n-    - post_update\n-\n-[[autodoc]] OffloadedCacheProcessor\n-    - pre_update\n-\n-[[autodoc]] QuantizedCacheProcessor\n-    - post_update\n-\n-[[autodoc]] QuantoQuantizedCacheProcessor\n-    - post_update\n+[[autodoc]] QuantoQuantizedLayer\n+    - update\n+    - lazy_initialization\n \n-[[autodoc]] HQQQuantizedCacheProcessor\n-    - post_update\n+[[autodoc]] HQQQuantizedLayer\n+    - update\n+    - lazy_initialization\n \n [[autodoc]] Cache\n     - update\n+    - early_initialization\n     - get_seq_length\n     - get_mask_sizes\n     - get_max_cache_shape\n@@ -411,12 +408,8 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n [[autodoc]] QuantoQuantizedCache\n \n-[[autodoc]] QuantoQuantizedCacheProcessor\n-\n [[autodoc]] HQQQuantizedCache\n \n-[[autodoc]] HQQQuantizedCacheProcessor\n-\n [[autodoc]] OffloadedCache\n \n [[autodoc]] StaticCache"
        },
        {
            "sha": "256bba7c76258b70a04f03a21c19ff4579ed998e",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -312,7 +312,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n # Init StaticCache with big enough max-length (1024 tokens for the below example)\n # You can also init a DynamicCache, if that suits you better\n-prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=model.device.type, dtype=torch.bfloat16)\n+prompt_cache = StaticCache(config=model.config, max_cache_len=1024)\n \n INITIAL_PROMPT = \"You are a helpful assistant. \"\n inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(model.device.type)"
        },
        {
            "sha": "9b6bdb8b614fd251f343d5b8ed2737e7c4f322af",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -93,11 +93,8 @@ model.generation_config.max_new_tokens = 16\n \n past_key_values = StaticCache(\n     config=model.config,\n-    max_batch_size=1,\n     # If you plan to reuse the cache, make sure the cache length is large enough for all cases\n     max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\n-    device=model.device,\n-    dtype=model.dtype\n )\n outputs = model.generate(**input_ids, past_key_values=past_key_values)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n@@ -159,7 +156,7 @@ from torch.nn.attention import SDPBackend, sdpa_kernel\n batch_size, seq_length = inputs[\"input_ids\"].shape\n with torch.no_grad():\n     past_key_values = StaticCache(\n-        config=model.config, max_batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype\n+        config=model.config, max_cache_len=4096\n     )\n     cache_position = torch.arange(seq_length, device=torch_device)\n     generated_ids = torch.zeros("
        },
        {
            "sha": "08ff2359f4c16ff12f0eab5cea80897ec6d1b3b1",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -138,8 +138,7 @@ visualizer(\"You are an assistant. Make sure you print me\")\n \n     inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n     max_generated_length = inputs.input_ids.shape[1] + 10\n-    past_key_values = HybridCache(config=model.config, max_batch_size=1,\n-    max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+    past_key_values = HybridCache(config=model.config, max_cache_len=max_generated_length)\n     outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     ```\n "
        },
        {
            "sha": "cedc34bd74f78ddc3aaa1f2976c6e852c510ef2b",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -362,21 +362,11 @@ generation_output[:2]\n [[autodoc]] SlidingWindowLayer\n     - update\n \n-[[autodoc]] CacheProcessor\n-    - pre_update\n-    - post_update\n-\n-[[autodoc]] OffloadedCacheProcessor\n-    - pre_update\n-\n-[[autodoc]] QuantizedCacheProcessor\n-    - post_update\n-\n-[[autodoc]] QuantoQuantizedCacheProcessor\n-    - post_update\n+[[autodoc]] QuantoQuantizedLayer\n+    - update\n \n-[[autodoc]] HQQQuantizedCacheProcessor\n-    - post_update\n+[[autodoc]] HQQQuantizedLayer\n+    - update\n \n [[autodoc]] Cache\n     - update\n@@ -397,12 +387,8 @@ generation_output[:2]\n \n [[autodoc]] QuantoQuantizedCache\n \n-[[autodoc]] QuantoQuantizedCacheProcessor\n-\n [[autodoc]] HQQQuantizedCache\n \n-[[autodoc]] HQQQuantizedCacheProcessor\n-\n [[autodoc]] OffloadedCache\n \n [[autodoc]] StaticCache"
        },
        {
            "sha": "2a631721b88d7d5d5c71b0dad9d394583ed64f57",
            "filename": "docs/source/ko/llm_optims.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fko%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/docs%2Fsource%2Fko%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_optims.md?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -99,11 +99,8 @@ model.generation_config.max_new_tokens = 16\n \n past_key_values = StaticCache(\n     config=model.config,\n-    max_batch_size=1,\n     # 캐시를 재사용할 계획이 있는 경우, 모든 경우에 충분한 캐시 길이를 설정해야 합니다\n     max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\n-    device=model.device,\n-    dtype=model.dtype\n )\n outputs = model.generate(**input_ids, past_key_values=past_key_values)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n@@ -161,7 +158,7 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n batch_size, seq_length = inputs[\"input_ids\"].shape\n with torch.no_grad():\n     past_key_values = StaticCache(\n-        config=model.config, max_batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype\n+        config=model.config, max_cache_len=4096\n     )\n     cache_position = torch.arange(seq_length, device=torch_device)\n     generated_ids = torch.zeros("
        },
        {
            "sha": "f99eca0e0bbfad7d77840f447a020ebf17cc0f75",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -377,23 +377,18 @@\n         \"StaticLayer\",\n         \"SlidingWindowLayer\",\n         \"ChunkedSlidingLayer\",\n-        \"CacheProcessor\",\n-        \"OffloadedCacheProcessor\",\n-        \"QuantizedCacheProcessor\",\n-        \"QuantoQuantizedCacheProcessor\",\n-        \"HQQQuantizedCacheProcessor\",\n+        \"QuantoQuantizedLayer\",\n+        \"HQQQuantizedLayer\",\n         \"Cache\",\n         \"CacheConfig\",\n         \"DynamicCache\",\n         \"EncoderDecoderCache\",\n         \"HQQQuantizedCache\",\n-        \"HQQQuantizedCacheProcessor\",\n         \"HybridCache\",\n         \"HybridChunkedCache\",\n         \"OffloadedCache\",\n         \"OffloadedStaticCache\",\n         \"QuantizedCache\",\n-        \"QuantoQuantizedCacheProcessor\",\n         \"QuantizedCacheConfig\",\n         \"QuantoQuantizedCache\",\n         \"SinkCache\",\n@@ -586,19 +581,25 @@\n     # All modeling imports\n     from .cache_utils import Cache as Cache\n     from .cache_utils import CacheConfig as CacheConfig\n+    from .cache_utils import ChunkedSlidingLayer as ChunkedSlidingLayer\n     from .cache_utils import DynamicCache as DynamicCache\n+    from .cache_utils import DynamicLayer as DynamicLayer\n     from .cache_utils import EncoderDecoderCache as EncoderDecoderCache\n     from .cache_utils import HQQQuantizedCache as HQQQuantizedCache\n+    from .cache_utils import HQQQuantizedLayer as HQQQuantizedLayer\n     from .cache_utils import HybridCache as HybridCache\n     from .cache_utils import MambaCache as MambaCache\n     from .cache_utils import OffloadedCache as OffloadedCache\n     from .cache_utils import OffloadedStaticCache as OffloadedStaticCache\n     from .cache_utils import QuantizedCache as QuantizedCache\n     from .cache_utils import QuantizedCacheConfig as QuantizedCacheConfig\n     from .cache_utils import QuantoQuantizedCache as QuantoQuantizedCache\n+    from .cache_utils import QuantoQuantizedLayer as QuantoQuantizedLayer\n     from .cache_utils import SinkCache as SinkCache\n     from .cache_utils import SlidingWindowCache as SlidingWindowCache\n+    from .cache_utils import SlidingWindowLayer as SlidingWindowLayer\n     from .cache_utils import StaticCache as StaticCache\n+    from .cache_utils import StaticLayer as StaticLayer\n     from .configuration_utils import PretrainedConfig as PretrainedConfig\n     from .convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS as SLOW_TO_FAST_CONVERTERS\n     from .convert_slow_tokenizer import convert_slow_tokenizer as convert_slow_tokenizer"
        },
        {
            "sha": "3fcfecbf911e20bfbc0d89f94a210968715c34e1",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 521,
            "deletions": 1013,
            "changes": 1534,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -1,26 +1,33 @@\n import copy\n-import functools\n-import importlib.metadata\n-import inspect\n import json\n import os\n from abc import ABC, abstractmethod\n from collections.abc import Iterable\n from dataclasses import dataclass\n-from typing import Any, Callable, Optional, Union\n+from typing import Any, Optional, Union\n \n import torch\n-from packaging import version\n \n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_6\n \n from .configuration_utils import PretrainedConfig\n-from .utils import is_hqq_available, is_optimum_quanto_available, is_torch_greater_or_equal, logging\n+from .utils import (\n+    is_hqq_available,\n+    is_quanto_greater,\n+    is_torch_greater_or_equal,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n \n \n+if _is_quanto_greater_than_0_2_5 := is_quanto_greater(\"0.2.5\", accept_dev=True):\n+    from optimum.quanto import MaxOptimizer, qint2, qint4, quantize_weight\n+\n if is_hqq_available():\n     from hqq.core.quantize import Quantizer as HQQQuantizer\n \n+_is_torch_greater_or_equal_than_2_7 = is_torch_greater_or_equal(\"2.7\", accept_dev=True)\n+\n \n logger = logging.get_logger(__name__)\n \n@@ -35,12 +42,12 @@ def __init__(self):\n \n     @abstractmethod\n     def update(\n-        self,\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = None\n     ) -> tuple[torch.Tensor, torch.Tensor]: ...\n \n+    @abstractmethod\n+    def lazy_initialization(self, key_states: torch.Tensor): ...\n+\n     @abstractmethod\n     def get_seq_length(self, cache_position=None) -> int: ...\n \n@@ -50,10 +57,23 @@ def get_max_cache_shape(self) -> int: ...\n     @abstractmethod\n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...\n \n+    def offload(self):\n+        \"\"\"Offload this layer's data to CPU device.\"\"\"\n+        if self.keys is not None:\n+            self.keys = self.keys.to(\"cpu\", non_blocking=True)\n+            self.values = self.values.to(\"cpu\", non_blocking=True)\n+\n+    def prefetch(self):\n+        \"\"\"In case of layer offloading, this allows to move the data back to the layer's device ahead of time.\"\"\"\n+        if self.keys is not None and self.keys.device != self.device:\n+            self.keys = self.keys.to(self.device, non_blocking=True)\n+            self.values = self.values.to(self.device, non_blocking=True)\n+\n     def reset(self) -> None:\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n-        self.keys.zero_()\n-        self.values.zero_()\n+        if self.keys is not None:\n+            self.keys.zero_()\n+            self.values.zero_()\n \n     def reorder_cache(self, beam_idx: torch.LongTensor) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Reorders this layer's cache for beam search.\"\"\"\n@@ -75,6 +95,11 @@ class DynamicLayer(CacheLayerMixin):\n \n     is_sliding = False\n \n+    def lazy_initialization(self, key_states: torch.Tensor):\n+        self.dtype, self.device = key_states.dtype, key_states.device\n+        self.keys = torch.tensor([], dtype=self.dtype, device=self.device)\n+        self.values = torch.tensor([], dtype=self.dtype, device=self.device)\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -95,12 +120,12 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n+        # Lazy initialization\n         if self.keys is None:\n-            self.keys = key_states\n-            self.values = value_states\n-        else:\n-            self.keys = torch.cat([self.keys, key_states], dim=-2)\n-            self.values = torch.cat([self.values, value_states], dim=-2)\n+            self.lazy_initialization(key_states)\n+\n+        self.keys = torch.cat([self.keys, key_states], dim=-2)\n+        self.values = torch.cat([self.values, value_states], dim=-2)\n         return self.keys, self.values\n \n     def get_seq_length(self, cache_position=None) -> int:\n@@ -170,6 +195,7 @@ def from_tensors(cls, keys: torch.Tensor, values: torch.Tensor) -> \"DynamicLayer\n             the supplied tensors.\n         \"\"\"\n         layer = cls()\n+        layer.dtype, layer.device = keys.dtype, keys.device\n         layer.keys = keys\n         layer.values = values\n         return layer\n@@ -186,61 +212,49 @@ class StaticLayer(CacheLayerMixin):\n     is_compileable = True\n     is_sliding = False\n \n-    def __init__(\n-        self,\n-        max_cache_len: int,\n-        batch_size: int,\n-        num_heads: int,\n-        head_dim: int,\n-        dtype: torch.dtype = torch.float32,\n-        device: str = \"cpu\",\n-        sliding_window: Optional[int] = None,\n-    ):\n+    def __init__(self, max_cache_len: int):\n         \"\"\"\n         Args:\n             max_cache_len (`int`):\n                 Maximum number of tokens that can be stored, used for tensor preallocation.\n-            batch_size (`int`):\n-                Maximum batch size the cache is pre-allocated for.\n-            num_heads (`int`):\n-                Number of attention heads.\n-            head_dim (`int`):\n-                Per-head hidden dimension.\n-            dtype (`torch.dtype`, defaults to `torch.float32`):\n-                Data type of the cache tensors.\n-            device (`str` or `torch.device`, defaults to `\"cpu\"`):\n-                Device on which the cache tensors will be materialised.\n-\n-        Notes:\n-            Static layers allocate their full backing tensors up-front and mutate them\n-            in-place. See the documentation of `Cache` for shared helper methods that\n-            operate uniformly across all layer types.\n         \"\"\"\n+        super().__init__()\n         self.max_cache_len = max_cache_len\n-        self.max_batch_size = batch_size\n-        self.num_heads = num_heads\n-        self.head_dim = head_dim\n-        self.dtype = dtype\n-        self.device = device\n+\n+    def lazy_initialization(self, key_states: torch.Tensor):\n+        \"\"\"\n+        Lazy initialization of the keys and values tensors. This allows to get all properties (dtype, device,\n+        num_heads in case of TP etc...) at runtime directly, which is extremely practical as it avoids moving\n+        devices, dtypes etc later on for each `update` (which could break the static dynamo addresses as well).\n+\n+        If this is unwanted, one can call `early_initialization(...)` on the Cache directly, which will call this\n+        function ahead-of-time (this is required for `torch.export` for example). Note that for `compile`, as we\n+        internally don't compile the prefill, this is guaranteed to have been called already when compiling.\n+        If compiling the prefill as well, e.g. calling `model.compile(...)` before `generate` with a static cache,\n+        it is still supported in general, but without guarantees depending on the compilation options (e.g. cuda graphs,\n+        i.e. `mode=\"reduce-overhead\"` is known to fail). But it will in general work correctly, and prefill should\n+        not be compiled anyway for performances!\n+        \"\"\"\n+        self.max_batch_size, self.num_heads, _, self.head_dim = key_states.shape\n+        self.dtype, self.device = key_states.dtype, key_states.device\n \n         self.keys = torch.zeros(\n-            (batch_size, num_heads, self.max_cache_len, head_dim),\n-            dtype=dtype,\n-            device=device,\n+            (self.max_batch_size, self.num_heads, self.max_cache_len, self.head_dim),\n+            dtype=self.dtype,\n+            device=self.device,\n         )\n         self.values = torch.zeros(\n-            (batch_size, num_heads, self.max_cache_len, head_dim),\n-            dtype=dtype,\n-            device=device,\n+            (self.max_batch_size, self.num_heads, self.max_cache_len, self.head_dim),\n+            dtype=self.dtype,\n+            device=self.device,\n         )\n-        # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n-        # preventing compiled graph breaks when updating the cache.\n-        torch._dynamo.mark_static_address(self.keys)\n-        torch._dynamo.mark_static_address(self.values)\n-\n-    def get_max_cache_shape(self) -> int:\n-        \"\"\"Return the maximum cache shape of the cache\"\"\"\n-        return self.max_cache_len\n+        # Note: `mark_static_address` is used to tag the cache as a fixed data pointer, preventing compiled graph\n+        # breaks when updating the cache. However, it is not supported when tracing the graph, so we skip it in this case.\n+        # As prefill should never be compiled, this is not an issue and it will still be run (except when users compile\n+        # prefill explicitly, but this should be avoided!)\n+        if not is_torchdynamo_compiling():\n+            torch._dynamo.mark_static_address(self.keys)\n+            torch._dynamo.mark_static_address(self.values)\n \n     def update(\n         self,\n@@ -259,34 +273,31 @@ def update(\n         Returns:\n             tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value states.\n         \"\"\"\n-        cache_position = cache_kwargs.get(\"cache_position\") if cache_kwargs else None\n-        key_states = key_states.to(self.keys.dtype)\n-        value_states = value_states.to(self.values.dtype)\n-\n-        # This may be needed if the Layer was not created with the right device in the beginning, i.e. if it did not respect\n-        # the device_map. However, even if it is the case, this will only run once, because then the new states received\n-        # will always have the same device\n-        if self.device != key_states.device:\n-            self.device = key_states.device\n-            self.keys = self.keys.to(self.device)\n-            self.values = self.values.to(self.device)\n-\n-        if cache_position is None:\n-            # Prefill phase where seq_len potentially equals max_cache_len. Directly copy.\n-            self.keys.copy_(key_states)\n-            self.values.copy_(value_states)\n-        else:\n-            # Generation phase. Update specific positions.\n-            # Use index_copy_ for in-place update (compile-friendly).\n-            try:\n-                self.keys.index_copy_(2, cache_position, key_states)\n-                self.values.index_copy_(2, cache_position, value_states)\n-            except NotImplementedError:\n-                # Fallback for devices like MPS where index_copy_ might not be supported.\n-                self.keys[:, :, cache_position] = key_states\n-                self.values[:, :, cache_position] = value_states\n+        # Lazy initialization\n+        if self.keys is None:\n+            self.lazy_initialization(key_states)\n+\n+        # Some old models give None for `cache_position` or even omit passing `cache_kwargs` when used as cross-attention,\n+        # in which case we should copy the whole Layer (key_states.shape[-2] == self.max_cache_len)\n+        cache_position = cache_kwargs.get(\"cache_position\") if cache_kwargs is not None else None\n+        cache_position = (\n+            cache_position if cache_position is not None else torch.arange(key_states.shape[-2], device=self.device)\n+        )\n+\n+        # Update the cache\n+        try:\n+            self.keys.index_copy_(2, cache_position, key_states)\n+            self.values.index_copy_(2, cache_position, value_states)\n+        except NotImplementedError:\n+            # Fallback for devices like MPS where index_copy_ might not be supported.\n+            self.keys[:, :, cache_position] = key_states\n+            self.values[:, :, cache_position] = value_states\n         return self.keys, self.values\n \n+    def get_max_cache_shape(self) -> int:\n+        \"\"\"Return the maximum cache shape of the cache\"\"\"\n+        return self.max_cache_len\n+\n     def get_seq_length(self, cache_position=None) -> int:\n         \"\"\"Returns the sequence length of the cached states.\"\"\"\n         if cache_position is not None:\n@@ -319,15 +330,17 @@ class SlidingWindowLayer(StaticLayer):\n \n     is_sliding = True\n \n-    def __init__(self, sliding_window, *args, **kwargs):\n+    def __init__(self, max_cache_len: int, sliding_window: int):\n         \"\"\"\n         Args:\n+            max_cache_len (`int`):\n+                Maximum number of tokens that can be stored, used for tensor preallocation.\n             sliding_window (`int`):\n-                Effective window size: number of tokens that are kept on each update call.\n+                The size of the sliding window.\n         \"\"\"\n-        max_cache_len = kwargs.pop(\"max_cache_len\", None)\n-        max_cache_len = min(sliding_window, max_cache_len) if max_cache_len is not None else sliding_window\n-        super().__init__(*args, max_cache_len=max_cache_len, *args, **kwargs)\n+        effective_max_cache_len = min(sliding_window, max_cache_len)\n+        super().__init__(max_cache_len=effective_max_cache_len)\n+        self.cumulative_length = 0\n \n     def update(\n         self,\n@@ -346,54 +359,46 @@ def update(\n         Returns:\n             tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value states.\n         \"\"\"\n-        cache_position = cache_kwargs.get(\"cache_position\") if cache_kwargs else None\n-        if cache_position is None:\n-            raise ValueError(\"`cache_position` must be provided for SlidingWindowLayer.\")\n+        # Lazy initialization\n+        if self.keys is None:\n+            self.lazy_initialization(key_states)\n \n-        # This may be needed if the Layer was not created with the right device in the beginning, i.e. if it did not respect\n-        # the device_map. However, even if it is the case, this will only run once, because then the new states received\n-        # will always have the same device\n-        if self.device != key_states.device:\n-            self.device = key_states.device\n-            self.keys = self.keys.to(self.device)\n-            self.values = self.values.to(self.device)\n+        cache_position = cache_kwargs.get(\"cache_position\")\n \n-        key_states = key_states.to(self.keys.dtype)\n-        value_states = value_states.to(self.values.dtype)\n+        is_full = self.cumulative_length >= self.max_cache_len\n+        # Update it now that we saved the value above\n+        self.cumulative_length += key_states.shape[-2]\n \n         # Handle prefill phase when prompt length > sliding_window_size.\n         # Note that we store cropped key/value states in the cache but return the full key/value states.\n         if cache_position.shape[0] > self.max_cache_len:\n-            new_k = key_states[:, :, -self.max_cache_len :, :]\n-            new_v = value_states[:, :, -self.max_cache_len :, :]\n-            self.keys.copy_(new_k)\n-            self.values.copy_(new_v)\n+            self.keys.copy_(key_states[:, :, -self.max_cache_len :, :])\n+            self.values.copy_(value_states[:, :, -self.max_cache_len :, :])\n+            # Return the full states here\n             return key_states, value_states\n \n-        # Sliding window logic for generation phase or prefill < window\n-        slicing = torch.arange(self.max_cache_len, device=self.device)\n-        current_seq_len = cache_position[-1] + 1  # Use last position to determine current length\n-        to_shift = current_seq_len > self.max_cache_len\n-        indices = (slicing + to_shift.sum()) % self.max_cache_len\n-\n-        k_out_shifted = self.keys[:, :, indices]\n-        v_out_shifted = self.values[:, :, indices]\n-\n-        # Clamp cache_position to determine the *target index* within the shifted cache view\n-        update_position = cache_position.clamp(min=0, max=self.max_cache_len - 1)\n+        # Here we only assume decoding stage, i.e. 1 token at a time\n+        if is_full:\n+            # Roll all values to the left by 1 position\n+            new_keys = self.keys.roll(-1, dims=-2)\n+            new_values = self.values.roll(-1, dims=-2)\n+            # Overwrite the last position with new states\n+            # (note: very important to use a tensor to index here, see https://github.com/pytorch/pytorch/issues/159855)\n+            index = torch.tensor([-1], dtype=int, device=self.device)\n+            new_keys[:, :, index] = key_states\n+            new_values[:, :, index] = value_states\n+\n+            # Copy back into `self` (do not just assign again) in order to keep the static dynamo address\n+            self.keys.copy_(new_keys)\n+            self.values.copy_(new_values)\n+        else:\n+            try:\n+                self.keys.index_copy_(2, cache_position, key_states)\n+                self.values.index_copy_(2, cache_position, value_states)\n+            except NotImplementedError:\n+                self.keys[:, :, cache_position] = key_states\n+                self.values[:, :, cache_position] = value_states\n \n-        try:\n-            k_out_updated = k_out_shifted.index_copy(2, update_position, key_states)\n-            v_out_updated = v_out_shifted.index_copy(2, update_position, value_states)\n-        except NotImplementedError:\n-            # Fallback for MPS: clone and modify the clone\n-            k_out_updated = k_out_shifted.clone()\n-            v_out_updated = v_out_shifted.clone()\n-            k_out_updated[:, :, update_position] = key_states\n-            v_out_updated[:, :, update_position] = value_states\n-\n-        self.keys.copy_(k_out_updated)\n-        self.values.copy_(v_out_updated)\n         return self.keys, self.values\n \n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n@@ -406,6 +411,14 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         kv_length = max(query_length, self.max_cache_len)\n         return kv_length, kv_offset\n \n+    def reset(self) -> None:\n+        super().reset()\n+        self.cumulative_length = 0\n+\n+    def get_seq_length(self, cache_position=None) -> int:\n+        \"\"\"Returns the sequence length of the cached states.\"\"\"\n+        return self.cumulative_length\n+\n \n class ChunkedSlidingLayer(SlidingWindowLayer):\n     \"\"\"\n@@ -414,31 +427,22 @@ class ChunkedSlidingLayer(SlidingWindowLayer):\n     See `SlidingWindowLayer` for details on common methods that are implemented by all cache layers.\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self.cumulative_length = 0\n-\n     def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n-        cache_position = cache_kwargs.get(\"cache_position\") if cache_kwargs else None\n-        if cache_position is None:\n-            raise ValueError(\"`cache_position` must be provided for ChunkedSlidingLayer.\")\n-\n-        # This may be needed if the Layer was not created with the right device in the beginning, i.e. if it did not respect\n-        # the device_map. However, even if it is the case, this will only run once, because then the new states received\n-        # will always have the same device\n-        if self.device != key_states.device:\n-            self.device = key_states.device\n-            self.keys = self.keys.to(self.device)\n-            self.values = self.values.to(self.device)\n+        # Lazy initialization\n+        if self.keys is None:\n+            self.lazy_initialization(key_states)\n+\n+        cache_position = cache_kwargs.get(\"cache_position\")\n \n         cumulative_length = self.cumulative_length\n-        self.cumulative_length += key_states.shape[-2]\n         is_full = cumulative_length >= self.max_cache_len\n+        # Update it now that we saved the value above\n+        self.cumulative_length += key_states.shape[-2]\n \n         if is_full:\n             full_key_states = torch.cat((self.keys[:, :, 1:, :], key_states), dim=-2)\n@@ -451,6 +455,7 @@ def update(\n                 self.values.copy_(full_value_states)\n                 return self.keys, self.values\n         elif not is_full and cumulative_length + key_states.shape[2] > self.max_cache_len:\n+            # Fast prefill path, no need to cat() in this case, as the cache is currently empty\n             if cumulative_length == 0:\n                 full_key_states = key_states\n                 full_value_states = value_states\n@@ -468,12 +473,10 @@ def update(\n \n         self.keys.copy_(full_key_states[:, :, -self.max_cache_len :, :])\n         self.values.copy_(full_value_states[:, :, -self.max_cache_len :, :])\n+        # we should return the whole states instead of `self.keys/values` here, as otherwise we lose some context\n+        # which is outside the window\n         return full_key_states, full_value_states\n \n-    def reset(self) -> None:\n-        super().reset()\n-        self.cumulative_length = 0\n-\n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         query_length = cache_position.shape[0]\n         first_cache_position = cache_position[0]\n@@ -493,392 +496,111 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         return kv_length, kv_offset\n \n \n-class CacheProcessor:\n-    \"\"\"\n-    Base class for cache processors. It defines a pre-update and post-update methods that are called before and after the cache update.\n-    This class should be subclassed.\n-    \"\"\"\n-\n-    def __init__(self, cache: \"Cache\", **kwargs) -> None:\n-        \"\"\"\n-        Initialize the processor and perform compatibility checks with the cache.\n-\n-        Args:\n-            cache (`Cache`): The cache instance this processor will be applied to.\n-            **kwargs: Additional arguments that may be needed for initialization.\n-        \"\"\"\n-        raise NotImplementedError(f\"Make sure to implement `init` in {self.__class__.__name__}.\")\n-\n-    def pre_update(\n-        self,\n-        cache: \"Cache\",\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        layer_idx: int,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Function called before the cache update. Can modify the key/value states.\n-\n-        Args:\n-            cache (`Cache`): The cache instance.\n-            key_states (`torch.Tensor`): The new key states to cache.\n-            value_states (`torch.Tensor`): The new value states to cache.\n-            layer_idx (`int`): The index of the layer to cache the states for.\n-            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n-\n-        Returns:\n-            The modified key and value states.\n-        \"\"\"\n-        return key_states, value_states\n-\n-    def post_update(\n-        self,\n-        cache: \"Cache\",\n-        key_tensors: torch.Tensor,\n-        value_tensors: torch.Tensor,\n-        layer_idx: int,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Function called after the cache update. Can process the cached data.\n-\n-        Args:\n-            cache (`Cache`): The cache instance.\n-            key_states (`torch.Tensor`): The key states that were cached.\n-            value_states (`torch.Tensor`): The value states that were cached.\n-            layer_idx (`int`): The index of the layer that was updated.\n-            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n-\n-        Returns:\n-            The final key and value states to return to the model.\n-        \"\"\"\n-        return key_tensors, value_tensors\n-\n-\n-class OffloadedCacheProcessor(CacheProcessor):\n+class QuantizedLayer(DynamicLayer):\n     \"\"\"\n-    A cache processor that offloads cache tensors to conserve accelerator memory.\n-\n-    This processor manages moving cache tensors between accelerator and CPU memory,\n-    using asynchronous prefetching to minimize performance impact. Works with both\n-    dynamic and static layers.\n-    \"\"\"\n-\n-    def __init__(self, cache: \"Cache\", offload_device: Union[str, torch.device] = \"cpu\", **kwargs):\n-        \"\"\"Initialize the offload processor and check device compatibility.\"\"\"\n-        self.offload_device = torch.device(offload_device)\n-        self.original_device = []\n-        self.prefetch_stream = None\n-        self.beam_idx = None\n-\n-        if not (\n-            torch.cuda.is_available()\n-            or (is_torch_greater_or_equal(\"2.7\", accept_dev=True) and torch.xpu.is_available())\n-        ):\n-            raise RuntimeError(\n-                \"OffloadedCacheProcessor can only be used with a GPU\"\n-                + (\" or XPU\" if is_torch_greater_or_equal(\"2.7\", accept_dev=True) else \"\")\n-            )\n-\n-        self.is_static = any(isinstance(layer, StaticLayer) for layer in cache.layers)\n-        if self.is_static:\n-            for i, layer in enumerate(cache.layers):\n-                device = cache.layer_init_kwargs[\"device\"] if i == 0 else self.offload_device\n-                layer.keys = layer.keys.to(device)\n-                layer.values = layer.values.to(device)\n-                self.original_device.append(cache.layer_init_kwargs[\"device\"])\n-            if len(cache) != cache.num_hidden_layers:\n-                raise ValueError(\"If static layers are used, all cache layers must be initialized\")\n-\n-        self.prefetch_stream = (\n-            torch.Stream() if is_torch_greater_or_equal(\"2.7\", accept_dev=True) else torch.cuda.Stream()\n-        )\n-\n-    def pre_update(\n-        self,\n-        cache: \"Cache\",\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        layer_idx: int,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"Handles prefetching and eviction before cache update.\"\"\"\n-        # Update the cache\n-        if len(cache) < layer_idx:\n-            raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n-        elif len(cache) == layer_idx:\n-            self.original_device.append(key_states.device)\n-            self._evict_previous_layer(cache, layer_idx)\n-        else:\n-            # Wait for the previous layer to be evicted (on default stream)\n-            if is_torch_greater_or_equal(\"2.7\", accept_dev=True):\n-                torch.accelerator.current_stream().synchronize()\n-            else:\n-                torch.cuda.current_stream().synchronize()\n-            self._evict_previous_layer(cache, layer_idx)\n-            self._ensure_layer_on_device(cache, layer_idx)\n-\n-            # Prefetch the next layer\n-            self._prefetch_layer(cache, (layer_idx + 1) % len(cache))\n-        return key_states, value_states\n-\n-    def _prefetch_layer(self, cache: \"Cache\", layer_idx: int):\n-        \"\"\"Starts prefetching the next layer cache.\"\"\"\n-        if layer_idx < len(cache):\n-            with (\n-                self.prefetch_stream\n-                if is_torch_greater_or_equal(\"2.7\", accept_dev=True)\n-                else torch.cuda.stream(self.prefetch_stream)\n-            ):\n-                # Prefetch next layer tensors to GPU\n-                device = self.original_device[layer_idx]\n-                cache.layers[layer_idx].keys = cache.layers[layer_idx].keys.to(device, non_blocking=True)\n-                cache.layers[layer_idx].values = cache.layers[layer_idx].values.to(device, non_blocking=True)\n-\n-    def _evict_previous_layer(self, cache: \"Cache\", layer_idx: int):\n-        \"\"\"Moves the previous layer cache to the CPU.\"\"\"\n-        if len(cache) >= 2:  # Layer 0 stays on device to be on-device after all layers are created\n-            # We do it on the default stream so it occurs after all earlier computations on these tensors are done\n-            prev_layer_idx = (layer_idx - 1) % len(cache)\n-            cache.layers[prev_layer_idx].keys = cache.layers[prev_layer_idx].keys.to(\n-                self.offload_device, non_blocking=True\n-            )\n-            cache.layers[prev_layer_idx].values = cache.layers[prev_layer_idx].values.to(\n-                self.offload_device, non_blocking=True\n-            )\n-\n-    def _ensure_layer_on_device(self, cache: \"Cache\", layer_idx: int):\n-        \"\"\"Ensures the current layer is on the original device.\"\"\"\n-        if layer_idx < len(cache):\n-            # Wait for the previous prefetch to be done\n-            self.prefetch_stream.synchronize()\n-\n-            # Handle delayed beam search operations\n-            if self.beam_idx is not None:\n-                self.beam_idx = self.beam_idx.to(self.original_device[layer_idx])\n-                cache.layers[layer_idx].keys = cache.layers[layer_idx].keys.index_select(0, self.beam_idx)\n-                cache.layers[layer_idx].values = cache.layers[layer_idx].values.index_select(0, self.beam_idx)\n-\n-\n-class QuantizedCacheProcessor(CacheProcessor):\n-    \"\"\"\n-    A cache processor that applies quantization to cache tensors to reduce memory usage.\n-\n-    This processor quantizes cache tensors after they are stored, maintaining a residual\n-    length in original precision and quantizing older tokens.\n+    A quantized layer similar to what is described in the [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://huggingface.co/papers/2402.02750).\n+    It allows the model to generate longer sequence length without allocating too much memory for Key and Value cache by\n+    applying quantization.\n+\n+    The cache has two types of storage, one for original precision and one for the quantized cache. A `residual length`\n+    is set as a maximum capacity for the original precision cache. When the length goes beyond maximum capacity, the original\n+    precision cache is discarded and moved into the quantized cache. The quantization is done per-channel with a set `q_group_size`\n+    for both Keys and Values, in contrast to what was described in the paper.\n     \"\"\"\n \n     def __init__(\n         self,\n-        cache: \"Cache\",\n-        backend: str = \"quanto\",\n         nbits: int = 4,\n         axis_key: int = 0,\n         axis_value: int = 0,\n         q_group_size: int = 64,\n         residual_length: int = 128,\n-        compute_dtype: torch.dtype = torch.float16,\n-        device: str = \"cpu\",\n     ):\n-        \"\"\"\n-        Parameters:\n-            backend (`str`, defaults to `\"quanto\"`):\n-                Backend to use when performing quantization, Can be one of [`quanto`, `HQQ`]\n-            nbits (`int`, defaults to 4):\n-                Number of bits, can be 2 or 4 for the `quanto` backend and one of [1, 2, 3, 4, 8] for the `HQQ` backend. Defaults to 2.\n-            axis_key (`int`, defaults to 0):\n-                Axis over which to perform grouping for the key tensors. Can be [0, -1] for `quanto` backend and [0, 1] for `HQQ` backend.\n-            axis_value (`int`, defaults to 0):\n-                Axis over which to perform grouping for the value tensors. Can be [0, -1] for `quanto` backend and [0, 1] for `HQQ` backend.\n-            q_group_size (`int`, defaults to 64):\n-                Size of the quantization group, should be a divisor of the model's hidden dimension.\n-                Defaults to 64.\n-            residual_length (`int`, defaults to 128):\n-                Length of the residual cache which will always be stored in original precision.\n-                Defaults to 128.\n-            compute_dtype (`torch.dtype`, defaults to `torch.float16`):\n-                The default dtype used for computations in the model. Keys and Values will be cast to this dtype after dequantization.\n-            device (`str`, defaults to `\"cpu\"`):\n-                Device on which to perform computations, should be same as the model's device.\n-        \"\"\"\n-        self.backend = backend\n+        super().__init__(self)\n         self.nbits = nbits\n         self.axis_key = axis_key\n         self.axis_value = axis_value\n         self.q_group_size = q_group_size\n         self.residual_length = residual_length\n-        self.compute_dtype = compute_dtype\n-        self.device = device\n-        self._quantized_keys: list[torch.Tensor] = []\n-        self._quantized_values: list[torch.Tensor] = []\n-\n-        self.validate()\n-        self.erased_length = 0\n-\n-        # Only compatible with DynamicCache\n-        if not isinstance(cache.layers[0], DynamicLayer):\n-            raise ValueError(\"QuantizedCacheProcessor is only compatible with DynamicCache\")\n-\n-    def validate(self):\n-        \"\"\"Validates if the arguments passed are correct\"\"\"\n-\n-        incorrect_arg_msg = (\n-            \"Some of the keys in `cache_config` are defined incorrectly. `{key}` should be {correct_value}` \"\n-            \"but found {found_value}\"\n-        )\n-        # Check that the values are reasonable in general (nbits, axis)\n-        # Later in QuantizedCache init we check if they are supported for that particular backend\n-        if self.nbits not in [1, 2, 3, 4, 8]:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"nbits\",\n-                    correct_value=\"2 or 4 or 8\",\n-                    found_value=self.nbits,\n-                ),\n-            )\n-        if self.q_group_size <= 0:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"q_group_size\",\n-                    correct_value=\"a positive integer\",\n-                    found_value=self.q_group_size,\n-                ),\n-            )\n-        if self.residual_length < 0:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"residual_length\",\n-                    correct_value=\"a positive integer\",\n-                    found_value=self.residual_length,\n-                ),\n-            )\n-\n-        if self.axis_key not in [0, 1, -1]:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"axis_key\",\n-                    correct_value=\"`1` or `0`, `-1`\",\n-                    found_value=self.axis_key,\n-                ),\n-            )\n-\n-        if self.axis_value not in [0, 1, -1]:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"axis_value\",\n-                    correct_value=\"`1` or `0` or `-1`\",\n-                    found_value=self.axis_value,\n-                ),\n-            )\n+        self.cumulative_length = 0\n \n-    def post_update(\n+    def update(\n         self,\n-        cache: \"Cache\",\n-        key_tensors: torch.Tensor,\n-        value_tensors: torch.Tensor,\n-        layer_idx: int,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"Apply quantization after cache update.\"\"\"\n-\n-        if len(cache) < layer_idx:\n-            raise ValueError(\"QuantizedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n-\n-        # `key_tensors` is the content of the residual cache, after having been updated by DynamicLayer\n-        # On the first forward pass, we quantize the whole prompt (prefill, quantize_length=0)\n-        # On subsequent passes, we accumulate the tokens in the residual cache and quantize when it is full.\n-        if self._is_quantized_length_zero(layer_idx):\n-            self._quantized_keys.append(self._quantize(key_tensors.contiguous(), axis=self.axis_key))\n-            self._quantized_values.append(self._quantize(value_tensors.contiguous(), axis=self.axis_value))\n-\n-            # Clear the residual cache\n-            self.erased_length = key_tensors.shape[-2]\n-            cache.layers[layer_idx].keys = torch.zeros(\n-                0,\n-                dtype=key_tensors.dtype,\n-                device=key_tensors.device,\n-            )\n-            cache.layers[layer_idx].values = torch.zeros(\n-                0,\n-                dtype=value_tensors.dtype,\n-                device=value_tensors.device,\n-            )\n-            # On prefill, we return the original prompt\n-            keys_to_return, values_to_return = key_tensors, value_tensors\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            cache_kwargs (`dict[str, Any]`, *optional*):\n+                Additional arguments for the cache subclass. No additional arguments are used in `DynamicLayer`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        self.cumulative_length += key_states.shape[-2]\n+\n+        # Lazy initialization\n+        if self.keys is None:\n+            self.lazy_initialization(key_states)\n+            self._quantized_keys = self._quantize(key_states.contiguous(), axis=self.axis_key)\n+            self._quantized_values = self._quantize(value_states.contiguous(), axis=self.axis_value)\n+            return key_states, value_states\n \n+        dequant_keys = self._dequantize(self._quantized_keys)\n+        dequant_values = self._dequantize(self._quantized_values)\n+        keys_to_return = torch.cat([dequant_keys, self.keys, key_states], dim=-2)\n+        values_to_return = torch.cat([dequant_values, self.values, value_states], dim=-2)\n+        if self.keys.dim() == 4 and self.keys.shape[-2] + 1 >= self.residual_length:\n+            self._quantized_keys = self._quantize(keys_to_return.contiguous(), axis=self.axis_key)\n+            self._quantized_values = self._quantize(values_to_return.contiguous(), axis=self.axis_value)\n+            self.keys = torch.tensor([], dtype=key_states.dtype, device=key_states.device)\n+            self.values = torch.tensor([], dtype=key_states.dtype, device=key_states.device)\n         else:\n-            # Prepend the previously quantized cache\n-            dequant_key = self._dequantize(self._quantized_keys[layer_idx])\n-            dequant_value = self._dequantize(self._quantized_values[layer_idx])\n-            keys_to_return = torch.cat([dequant_key, key_tensors], dim=-2)\n-            values_to_return = torch.cat([dequant_value, value_tensors], dim=-2)\n-            if key_tensors.shape[-2] >= self.residual_length:\n-                # Quantize and store\n-                self._quantized_keys[layer_idx] = self._quantize(keys_to_return.contiguous(), axis=self.axis_key)\n-                self._quantized_values[layer_idx] = self._quantize(values_to_return.contiguous(), axis=self.axis_value)\n-\n-                # Clear the residual cache\n-                self.erased_length += key_tensors.shape[-2]\n-                cache.layers[layer_idx].keys = torch.zeros(\n-                    0,\n-                    dtype=key_tensors.dtype,\n-                    device=key_tensors.device,\n-                )\n-                cache.layers[layer_idx].values = torch.zeros(\n-                    0,\n-                    dtype=value_tensors.dtype,\n-                    device=value_tensors.device,\n-                )\n+            self.keys = torch.cat([self.keys, key_states], dim=-2)\n+            self.values = torch.cat([self.values, value_states], dim=-2)\n \n         return keys_to_return, values_to_return\n \n-    def _quantize(self, tensor: torch.Tensor, axis: int) -> torch.Tensor:\n-        \"\"\"Quantize a tensor - to be implemented by specific quantization backends.\"\"\"\n-        raise NotImplementedError(\"Quantization backend must implement _quantize method\")\n-\n-    def _dequantize(self, tensor: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Dequantize a tensor - to be implemented by specific quantization backends.\"\"\"\n-        raise NotImplementedError(\"Quantization backend must implement _dequantize method\")\n+    def get_seq_length(self, cache_position=None) -> int:\n+        \"\"\"Returns the sequence length of the cached states.\"\"\"\n+        return self.cumulative_length\n \n-    def _is_quantized_length_zero(self, layer_idx: int) -> bool:\n-        \"\"\"Check if quantized cache is empty for layer. Note: shape[-2] is unreliable since quantized tensors are bit-packed and flattened.\"\"\"\n-        return layer_idx >= len(self._quantized_keys)\n+    @abstractmethod\n+    def _quantize(self, tensor, axis): ...\n \n+    @abstractmethod\n+    def _dequantize(self, q_tensor): ...\n \n-class QuantoQuantizedCacheProcessor(QuantizedCacheProcessor):\n-    \"\"\"\n-    Quantized cache processor that uses `quanto` as a backend to perform quantization.\n-    Current implementation supports `int2` and `int4` dtypes only.\n-    \"\"\"\n \n+class QuantoQuantizedLayer(QuantizedLayer):\n     def __init__(\n         self,\n-        cache: \"Cache\",\n-        backend: str = \"quanto\",\n         nbits: int = 4,\n         axis_key: int = 0,\n         axis_value: int = 0,\n         q_group_size: int = 64,\n         residual_length: int = 128,\n-        compute_dtype: torch.dtype = torch.float16,\n-        device: str = \"cpu\",\n-    ) -> None:\n-        \"\"\"Initialize the quanto quantization processor.\"\"\"\n+    ):\n         super().__init__(\n-            cache, backend, nbits, axis_key, axis_value, q_group_size, residual_length, compute_dtype, device\n+            nbits=nbits,\n+            axis_key=axis_key,\n+            axis_value=axis_value,\n+            q_group_size=q_group_size,\n+            residual_length=residual_length,\n         )\n \n-        if backend != \"quanto\":\n-            raise ValueError(f\"QuantoQuantizedCacheProcessor only supports `quanto` backend, but got {backend}\")\n-\n-        if is_optimum_quanto_available():\n-            optimum_quanto_version = version.parse(importlib.metadata.version(\"optimum-quanto\"))\n-            if optimum_quanto_version <= version.parse(\"0.2.5\"):\n-                raise ImportError(\n-                    f\"You need optimum-quanto package version to be greater or equal than 0.2.5 to use `QuantoQuantizedCacheProcessor`. Detected version {optimum_quanto_version}.\"\n-                )\n-            from optimum.quanto import MaxOptimizer, qint2, qint4\n+        if not _is_quanto_greater_than_0_2_5:\n+            raise ImportError(\n+                \"You need optimum-quanto package version to be greater or equal than 0.2.5 to use `QuantoQuantizedCache`. \"\n+                \"Detected version {optimum_quanto_version}.\"\n+            )\n \n         if self.nbits not in [2, 4]:\n             raise ValueError(f\"`nbits` for `quanto` backend has to be one of [`2`, `4`] but got {self.nbits}\")\n@@ -892,47 +614,36 @@ def __init__(\n             )\n \n         self.qtype = qint4 if self.nbits == 4 else qint2\n-        self.optimizer = MaxOptimizer()\n-\n-    def _quantize(self, tensor: torch.Tensor, axis: int) -> torch.Tensor:\n-        \"\"\"Quantize tensor using quanto backend.\"\"\"\n-        if is_optimum_quanto_available():\n-            from optimum.quanto import quantize_weight\n+        self.optimizer = MaxOptimizer()  # hardcode as it's the only one for per-channel quantization\n \n-            scale, zeropoint = self.optimizer(tensor, self.qtype, axis, self.q_group_size)\n-            qtensor = quantize_weight(tensor, self.qtype, axis, scale, zeropoint, self.q_group_size)\n-            return qtensor\n+    def _quantize(self, tensor, axis):\n+        scale, zeropoint = self.optimizer(tensor, self.qtype, axis, self.q_group_size)\n+        qtensor = quantize_weight(tensor, self.qtype, axis, scale, zeropoint, self.q_group_size)\n+        return qtensor\n \n-    def _dequantize(self, qtensor: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Dequantize tensor using quanto backend.\"\"\"\n+    def _dequantize(self, qtensor):\n         return qtensor.dequantize()\n \n \n-class HQQQuantizedCacheProcessor(QuantizedCacheProcessor):\n-    \"\"\"\n-    Quantized cache processor that uses `HQQ` as a backend to perform quantization.\n-    Current implementation supports `int2`, `int4`, `int8` dtypes.\n-    \"\"\"\n-\n+class HQQQuantizedLayer(QuantizedLayer):\n     def __init__(\n         self,\n-        cache: \"Cache\",\n-        backend: str = \"quanto\",\n         nbits: int = 4,\n         axis_key: int = 0,\n         axis_value: int = 0,\n         q_group_size: int = 64,\n         residual_length: int = 128,\n-        compute_dtype: torch.dtype = torch.float16,\n-        device: str = \"cpu\",\n-    ) -> None:\n-        \"\"\"Initialize the HQQ quantization processor.\"\"\"\n+    ):\n         super().__init__(\n-            cache, backend, nbits, axis_key, axis_value, q_group_size, residual_length, compute_dtype, device\n+            nbits=nbits,\n+            axis_key=axis_key,\n+            axis_value=axis_value,\n+            q_group_size=q_group_size,\n+            residual_length=residual_length,\n         )\n \n-        if backend != \"quanto\":\n-            raise ValueError(f\"HQQQuantizedCacheProcessor only supports `quanto` backend, but got {backend}\")\n+        if not is_hqq_available():\n+            raise ImportError(\"You need to install `hqq` to use `HQQQuantizedLayer`\")\n \n         if self.nbits not in [1, 2, 3, 4, 8]:\n             raise ValueError(\n@@ -947,58 +658,32 @@ def __init__(\n \n         self.quantizer = HQQQuantizer\n \n-    def _quantize(self, tensor: torch.Tensor, axis: int) -> tuple[torch.Tensor, dict]:\n-        \"\"\"Quantize tensor using HQQ backend.\"\"\"\n+    def _quantize(self, tensor, axis):\n         qtensor, meta = self.quantizer.quantize(\n             tensor,\n             axis=axis,\n-            device=self.device,\n-            compute_dtype=self.compute_dtype,\n+            device=self.keys.device,\n+            compute_dtype=self.keys.dtype,\n             nbits=self.nbits,\n             group_size=self.q_group_size,\n         )\n-        meta[\"compute_dtype\"] = self.compute_dtype\n-        self.quantizer.cuda(qtensor, meta=meta, device=self.device)  # Move to device and cast to dtype\n+        meta[\"compute_dtype\"] = self.keys.dtype\n+        self.quantizer.cuda(qtensor, meta=meta, device=self.keys.device)  # Move to device and cast to dtype\n         meta[\"scale\"] = meta[\"scale\"].to(qtensor.device)\n         meta[\"zero\"] = meta[\"zero\"].to(qtensor.device)\n         return qtensor, meta\n \n-    def _dequantize(self, qtensor_and_meta: tuple[torch.Tensor, dict]) -> torch.Tensor:\n-        \"\"\"Dequantize tensor using HQQ backend.\"\"\"\n-        quant_tensor, meta = qtensor_and_meta\n+    def _dequantize(self, qtensor):\n+        quant_tensor, meta = qtensor\n         tensor = self.quantizer.dequantize(quant_tensor, meta)\n         return tensor\n \n \n-def apply_processors(\n-    fn: Callable[..., tuple[torch.Tensor, torch.Tensor]],\n-) -> Callable[..., tuple[torch.Tensor, torch.Tensor]]:\n-    @functools.wraps(fn)\n-    def _wrapped_update(\n-        self,\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        layer_idx: int,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Wrapper around the update method to apply cache processors.\n-        \"\"\"\n-        if self.cache_processor is not None:\n-            key_states, value_states = self.cache_processor.pre_update(\n-                self, key_states, value_states, layer_idx, cache_kwargs\n-            )\n-\n-        key_tensors, value_tensors = fn(self, key_states, value_states, layer_idx, cache_kwargs)\n-\n-        if self.cache_processor is not None:\n-            key_tensors, value_tensors = self.cache_processor.post_update(\n-                self, key_tensors, value_tensors, layer_idx, cache_kwargs\n-            )\n-\n-        return key_tensors, value_tensors\n-\n-    return _wrapped_update\n+LAYER_CLASS_MAP: dict[str, type[CacheLayerMixin]] = {\n+    \"full_attention\": StaticLayer,\n+    \"sliding_attention\": SlidingWindowLayer,\n+    \"chunked_attention\": ChunkedSlidingLayer,\n+}\n \n \n class KeyValuesWrapper:\n@@ -1035,144 +720,80 @@ def __bool__(self):\n \n class Cache:\n     \"\"\"\n-    Base container for per-layer key/value caches.\n-\n-    A `Cache` behaves like a list of `CacheLayerMixin` objects, one per model layer.\n-    Sub-classes such as `DynamicCache`, `StaticCache`, or `SlidingWindowCache`\n-    simply pre-select which `CacheLayerMixin` class to use and may attach a\n-    `CacheProcessor` (off-loading, quantization).\n-\n-    Example\n-    -------\n-    ```python\n-    from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n-\n-    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-    tok   = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-    inputs = tok(\"Hello\", return_tensors=\"pt\")\n-\n-    cache = DynamicCache()\n-    outputs = model(**inputs, past_key_values=cache, use_cache=True)\n-    ```\n+    A `Cache` is mostly a list of `CacheLayerMixin` objects, one per model layer. It serves as a container for\n+    the Cache of each layer.\n \n     Parameters:\n-        layer_classes (`type[CacheLayerMixin]` or `list[type[CacheLayerMixin]]`):\n-            A list of `CacheLayerMixin` classes to instantiate for the cache. If only a `CacheLayerMixin` class is\n-            provided, then it is used for all layers.\n-        config (`PretrainedConfig`, *optional*):\n-            Model configuration used to infer number of layers, head sizes, default\n-            device/dtype, etc.\n-        cache_processor (`CacheProcessor` or `str`, *optional*):\n-            Cache processor to apply (e.g., \"offloaded\", \"quanto_quantized\", \"hqq_quantized\")\n-            or a CacheProcessor class.\n-        max_batch_size (`int`, *optional*): Maximum batch size for static caches.\n-        max_cache_len (`int`, *optional*): Maximum sequence length. For hybrid caches, SlidingWindowLayers are\n-            clamped to `min(sliding_window, max_cache_len)`, StaticLayers use full `max_cache_len`.\n-        device (`torch.device`, *optional*): Device for cache tensors.\n-        dtype (`torch.dtype`, *optional*): Data type for cache tensors.\n-        layer_device_map (`dict[int, Union[str, torch.device]]`, *optional*): Per-layer device mapping.\n-        tp_size (`int`, *optional*): Tensor parallel size to adjust the number of key/value heads.\n-\n-    Additional keyword arguments are forwarded to the chosen layers constructor(s) and CacheProcessors. See the\n-    documentation of the relevant `CacheLayerMixin` class and `CacheProcessor` class for more details.\n+        layers (`Optional`, *optional*):\n+            A list of pre-created `CacheLayerMixin`. If omitted (`None`), then `layer_class_to_replicate` will\n+            be used.\n+        layer_class_to_replicate (`type[CacheLayerMixin]`, *optional*):\n+            Only used if `layers` is omitted (`None`), in which case it will be used as the base class for each layer,\n+            and the layers will be added lazily as soon as `update` is called with a `layer_idx` greater than the current\n+            list of layers.\n+        offloading (`bool`, *optional*, defaults to `False`):\n+            Whether to perform offloading of the layers to `cpu`, to save GPU memory.\n+        offload_only_non_sliding (`bool`, *optional*, defaults to `True`):\n+            If `offloading` is `True`, this further decides if only the non-sliding layers will be offloaded (because\n+            usually the sliding layers are small in size, so there is no need to offload them, and skipping it is faster).\n     \"\"\"\n \n     def __init__(\n         self,\n-        layer_classes: Union[list[type[CacheLayerMixin]], type[CacheLayerMixin]],\n-        config: Optional[PretrainedConfig] = None,\n-        cache_processor: Optional[Union[str, type[CacheProcessor]]] = None,\n-        max_batch_size: Optional[int] = None,\n-        max_cache_len: Optional[int] = None,\n-        device: Union[torch.device, str, None] = None,\n-        dtype: Optional[torch.dtype] = None,\n-        layer_device_map: Optional[dict[int, torch.device]] = None,\n-        tp_size: Optional[int] = None,\n-        **kwargs,\n+        layers: Optional[list[CacheLayerMixin]] = None,\n+        layer_class_to_replicate: Optional[type[CacheLayerMixin]] = None,\n+        offloading: bool = False,\n+        offload_only_non_sliding: bool = True,\n     ):\n-        self.layers: list[CacheLayerMixin] = []\n-        self.layer_classes = layer_classes\n-\n-        processor_class = PROCESSOR_CLASS_MAP[cache_processor] if isinstance(cache_processor, str) else cache_processor\n-        kwargs.update(\n-            max_batch_size=max_batch_size,\n-            max_cache_len=max_cache_len,\n-            device=device,\n-            dtype=dtype,\n-            layer_device_map=layer_device_map,\n-            tp_size=tp_size,\n-        )\n-        processor_kwargs, kwargs = parse_processor_args(processor_class, kwargs)\n-\n-        self.layer_init_kwargs = parse_layer_args_from_model_config(config, **kwargs)\n-        self.num_hidden_layers = getattr(config, \"num_hidden_layers\", 1)\n-\n-        self.append_new_layers(self.num_hidden_layers - 1)\n-        self.cache_processor = processor_class(self, **processor_kwargs) if processor_class is not None else None\n-\n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n-        sequence length.\n-        \"\"\"\n-        if layer_idx < len(self.layers):\n-            return self.layers[layer_idx].keys, self.layers[layer_idx].values\n-        else:\n-            raise KeyError(\n-                f\"Cache only has {len(self.layers)} layers, attempted to access layer with index {layer_idx}\"\n+        if layers is not None and layer_class_to_replicate is not None:\n+            raise ValueError(\n+                \"You can construct a Cache either from a list `layers` of all the predefined `CacheLayer`, or from a \"\n+                \"`layer_class_to_replicate`, in which case the Cache will append a new layer corresponding to \"\n+                \"`layer_class_to_replicate` for each new call to `update` with an idx not already in the Cache.\"\n+            )\n+        if layers is None and layer_class_to_replicate is None:\n+            raise ValueError(\n+                \"You should provide exactly one of `layers` or `layer_class_to_replicate` to initialize a Cache.\"\n             )\n+        self.layers = layers if layers is not None else []\n+        self.layer_class_to_replicate = layer_class_to_replicate\n+        self.offloading = offloading\n+        if self.offloading:\n+            self.only_non_sliding = offload_only_non_sliding\n+            self.prefetch_stream = torch.Stream() if _is_torch_greater_or_equal_than_2_7 else torch.cuda.Stream()\n \n-    def __iter__(self):\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n-        keys and values\n-        \"\"\"\n-        for layer_idx in range(len(self)):\n-            yield (self.layers[layer_idx].keys, self.layers[layer_idx].values)\n+    def __repr__(self):\n+        return f\"{self.__class__.__name__}(layers={self.layers})\"\n \n-    def __len__(self):\n+    def prefetch(self, layer_idx: int, only_non_sliding: bool = True):\n         \"\"\"\n-        Support for backwards-compatible `past_key_values` length, e.g. `len(past_key_values)`. This value corresponds\n-        to the number of layers in the model.\n+        Prefetch a given layer on its device. If `only_non_sliding` is True, it will try to prefetch only the layers\n+        which are non-sliding. If the `layer_idx` is outside the range, this will circle back to the first layers.\n+        Note that we use a non-default stream for this, to avoid blocking.\n         \"\"\"\n-        # Best effort BC support for old-style caches like Mambas, Falcon, HybridChunked that rely on __len__\n-        if getattr(self, \"layers\", None) is None:\n-            if getattr(self, \"key_cache\", None) is not None:\n-                return len(self.key_cache)\n-            return 0\n-        # Empty dynamic caches initialize an empty layer to be ready for first update\n-        dynamic_empty = (\n-            getattr(self, \"layers\", None) is not None\n-            and len(self.layers) == 1\n-            and isinstance(self.layers[0], DynamicLayer)\n-            and self.layers[0].keys is None\n-        )\n-        return len(self.layers) if not dynamic_empty else 0\n+        if only_non_sliding:\n+            # Try to find next non-sliding, starting at `layer_idx`\n+            try:\n+                layer_idx = layer_idx + self.is_sliding[layer_idx:].index(False)\n+            # In this case, we need to circle back to the begining\n+            except ValueError:\n+                layer_idx = self.is_sliding.index(False)\n+        else:\n+            layer_idx = layer_idx if layer_idx < len(self.layers) else 0\n \n-    def __repr__(self):\n-        return f\"{self.__class__.__name__}(layers={self.layers})\"\n+        # Prefetch\n+        with self.prefetch_stream if _is_torch_greater_or_equal_than_2_7 else torch.cuda.stream(self.prefetch_stream):\n+            self.layers[layer_idx].prefetch()\n \n-    def append_new_layers(self, layer_idx: int) -> None:\n+    def offload(self, layer_idx: int, only_non_sliding: bool = True):\n         \"\"\"\n-        Appends layers to the cache until the layer `layer_idx` is reached.\n-        Used for preallocation in static caches and on the fly in dynamic caches.\n-\n-        Args:\n-            layer_idx (`int`):\n-                The index of the layer to append.\n+        Offload a given `layer_idx`. If `only_non_sliding` is True, it will offload `layer_idx` only if it is a\n+        non-sliding layer. Note that we do it on the default stream, so that we ensure all earlier\n+        computation in the layer's `update` methods are finished.\n         \"\"\"\n-        while len(self.layers) <= layer_idx:\n-            kwargs = self.layer_init_kwargs.copy()\n-            if self.layer_init_kwargs.get(\"layer_device_map\", None) is not None:\n-                kwargs[\"device\"] = kwargs.pop(\"layer_device_map\")[len(self.layers)]\n+        if not (only_non_sliding and self.is_sliding[layer_idx]):\n+            self.layers[layer_idx].offload()\n \n-            new_layer_class = (\n-                self.layer_classes[len(self.layers)] if isinstance(self.layer_classes, list) else self.layer_classes\n-            )\n-            new_layer = new_layer_class(**kwargs)\n-            self.layers.append(new_layer)\n-\n-    @apply_processors\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -1197,48 +818,62 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        self.append_new_layers(layer_idx)\n-        return self.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n+        # In this case, the `layers` were not provided, and we must append as much as `layer_idx`\n+        if self.layer_class_to_replicate is not None:\n+            while len(self.layers) <= layer_idx:\n+                self.layers.append(self.layer_class_to_replicate())\n+\n+        if self.offloading:\n+            # Wait for the stream to finish if needed, and start prefetching the next layer\n+            torch.cuda.default_stream(key_states.device).wait_stream(self.prefetch_stream)\n+            self.prefetch(layer_idx + 1, self.only_non_sliding)\n+\n+        keys, values = self.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n+\n+        if self.offloading:\n+            self.offload(layer_idx, self.only_non_sliding)\n+\n+        return keys, values\n+\n+    def early_initialization(\n+        self, batch_size: int, num_heads: int, head_dim: int, dtype: torch.dtype, device: torch.device\n+    ):\n+        \"\"\"\n+        Initialize all the layers in advance (it's otherwise lazily initialized on the first `update` call).\n+        This is useful for our `export` recipes, as `export` needs everything in advance.\n+        \"\"\"\n+        # Note that the initialization needs all dimensions (except -2), as well as device and dtype, so we use\n+        # this fake tensor approach. It has size 0 on the -2 dimension, so it does not allocate any data (it only\n+        # creates an empty tensor with correct shape, dtype and device), which is very efficient and practical\n+        fake_keys_tensor = torch.zeros((batch_size, num_heads, 0, head_dim), dtype=dtype, device=device)\n+        # Init all layers\n+        for layer in self.layers:\n+            layer.lazy_initialization(fake_keys_tensor)\n \n     def get_seq_length(self, layer_idx: int = 0, cache_position=None) -> int:\n-        \"\"\"Returns the sequence length of the cache for the given layer. TODO: deprecate in favor of cache_position\"\"\"\n+        \"\"\"Returns the sequence length of the cache for the given layer.\"\"\"\n         if layer_idx >= len(self.layers):\n             return 0\n-        # Hack since QuantizedCache messes with keys shape as it becomes the residual cache\n-        if self.cache_processor is not None and isinstance(self.cache_processor, QuantizedCacheProcessor):\n-            return self.cache_processor.erased_length + self.layers[layer_idx].get_seq_length(cache_position)\n         return self.layers[layer_idx].get_seq_length(cache_position)\n \n     def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n         \"\"\"\n         Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n         the given layer at `layer_idx`.\n-        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n-        for each layer.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns for each layer.\n         \"\"\"\n-        kv_length, kv_offset = self.layers[layer_idx].get_mask_sizes(cache_position)\n-        return kv_length, kv_offset\n-\n-    @property\n-    def key_cache(self) -> KeyValuesWrapper:\n-        \"\"\"List-like object of key cache tensors indexed by layer. Deprecated in favor of `cache.layers[idx].keys`\"\"\"\n-        logger.warning_once(\n-            \"`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\"\n-        )\n-        return KeyValuesWrapper(self.layers, \"keys\")\n-\n-    @property\n-    def value_cache(self) -> KeyValuesWrapper:\n-        \"\"\"List-like object of value cache tensors indexed by layer. Deprecated in favor of `cache.layers[idx].values`\"\"\"\n-        logger.warning_once(\n-            \"`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\"\n-        )\n-        return KeyValuesWrapper(self.layers, \"values\")\n-\n-    ### Wrappers for layer operations and properties ###\n+        # For DynamicCache, where the layers are created at runtime -> if it was not yet created, the size is\n+        # simply the shape of `cache_position`\n+        if layer_idx >= len(self.layers):\n+            return cache_position.shape[0], 0\n+        return self.layers[layer_idx].get_mask_sizes(cache_position)\n \n     def get_max_cache_shape(self, layer_idx: int = 0) -> int:\n         \"\"\"Returns maximum sequence length of the cache object. Dynamic caches do not have a maximum length.\"\"\"\n+        # For DynamicCache, where the layers are created at runtime -> if it was not yet created, return -1\n+        # as DynamicLayer does\n+        if layer_idx >= len(self.layers):\n+            return -1\n         return self.layers[layer_idx].get_max_cache_shape()\n \n     def reset(self):\n@@ -1283,13 +918,60 @@ def max_cache_len(self) -> int:\n     @property\n     def is_compileable(self) -> bool:\n         \"\"\"Return whether the cache is compileable\"\"\"\n+        # For DynamicCache dispatching the layers lazily (otherwise, all([]) is True)\n+        if len(self.layers) == 0:\n+            return False\n         return all(layer.is_compileable for layer in self.layers)\n \n     @property\n     def is_sliding(self) -> list[bool]:\n         \"\"\"Return whether the layers of the cache are sliding window\"\"\"\n         return [getattr(layer, \"is_sliding\", False) for layer in self.layers]\n \n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n+        sequence length.\n+        \"\"\"\n+        if layer_idx < len(self.layers):\n+            return self.layers[layer_idx].keys, self.layers[layer_idx].values\n+        else:\n+            raise KeyError(\n+                f\"Cache only has {len(self.layers)} layers, attempted to access layer with index {layer_idx}\"\n+            )\n+\n+    def __iter__(self):\n+        \"\"\"\n+        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n+        keys and values\n+        \"\"\"\n+        for layer_idx in range(len(self)):\n+            yield (self.layers[layer_idx].keys, self.layers[layer_idx].values)\n+\n+    def __len__(self):\n+        \"\"\"\n+        This value corresponds to the number of layers in the model.\n+        \"\"\"\n+        # Note: for DynamicCache, layers are initialized lazily, so this will not be accurate before the first\n+        # forward through all the layers\n+        return len(self.layers)\n+\n+    @property\n+    def key_cache(self) -> KeyValuesWrapper:\n+        \"\"\"List-like object of key cache tensors indexed by layer. Deprecated in favor of `cache.layers[idx].keys`\"\"\"\n+        logger.warning_once(\n+            \"`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\"\n+        )\n+        return KeyValuesWrapper(self.layers, \"keys\")\n+\n+    @property\n+    def value_cache(self) -> KeyValuesWrapper:\n+        \"\"\"List-like object of value cache tensors indexed by layer. Deprecated in favor of `cache.layers[idx].values`\"\"\"\n+        logger.warning_once(\n+            \"`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\"\n+        )\n+        return KeyValuesWrapper(self.layers, \"values\")\n+\n \n class DynamicCache(Cache):\n     \"\"\"\n@@ -1319,17 +1001,18 @@ class DynamicCache(Cache):\n     \"\"\"\n \n     # Specialized constructor for DDP cache data, needed for BC\n-    def __init__(self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None, *args, **kwargs):\n-        super().__init__(layer_classes=DynamicLayer, *args, **kwargs)\n+    def __init__(self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None):\n         # `ddp_cache_data` was originally added for compatibility with `torch.distributed` (DDP). See #36212\n         # and #36373 for more information. In a nutshell, it is `map(gather_map, zip(*caches))`, i.e. each item in the\n         # iterable contains the key and value states for a layer gathered across replicas by torch.distributed\n         # (shape=[global batch size, num_heads, seq_len, head_dim]).\n-        # WARNING: `ddp_cache_data` must be the first argument in `__init__`, otherwise we'll break\n-        # compatibility. The name of the argument doesn't matter.\n         if ddp_cache_data is not None:\n+            layers = []\n             for key_states, value_states in ddp_cache_data:\n-                self.layers.append(DynamicLayer.from_tensors(key_states, value_states))\n+                layers.append(DynamicLayer.from_tensors(key_states, value_states))\n+            super().__init__(layers=layers)\n+        else:\n+            super().__init__(layer_class_to_replicate=DynamicLayer)\n \n     def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n         \"\"\"\n@@ -1403,22 +1086,16 @@ def _unflatten_dynamic_cache(\n     )\n \n \n-class OffloadedCache(DynamicCache):\n+class OffloadedCache(Cache):\n     \"\"\"\n-    A drop-in replacement for DynamicCache that conserves accelerator(GPU, XPU) memory at the expense of more CPU memory.\n+    A drop-in replacement for DynamicCache that conserves accelerator (GPU, XPU) memory at the expense of more CPU memory.\n     Useful for generating from models with very long context.\n \n-    In addition to the default accelerator stream, where all forward() computations happen,\n-    this class uses another stream, the prefetch stream, which it creates itself.\n-    Since scheduling of operations on separate streams happens independently, this class uses\n-    the prefetch stream to asynchronously prefetch the KV cache of layer k+1 when layer k is executing.\n-    The movement of the layer k-1 cache to the CPU is handled by the default stream as a simple way to\n-    ensure the eviction is scheduled after all computations on that cache are finished.\n+    See `Cache` for details on common methods that are implemented by all cache classes.\n     \"\"\"\n \n     def __init__(self) -> None:\n-        # Create the underlying cache with offload processor\n-        super().__init__(cache_processor=OffloadedCacheProcessor)\n+        super().__init__(layer_class_to_replicate=DynamicLayer, offloading=True)\n \n \n class StaticCache(Cache):\n@@ -1440,18 +1117,20 @@ class StaticCache(Cache):\n         >>> # Prepare a cache class and pass it to model's forward\n         >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = StaticCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = StaticCache(max_cache_len=max_generated_length, config=model.config)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         StaticCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(layer_classes=StaticLayer, *args, **kwargs)\n+    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+        layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n+        super().__init__(layers=layers)\n \n \n-class OffloadedStaticCache(StaticCache):\n+class OffloadedStaticCache(Cache):\n     \"\"\"\n     A drop-in replacement for StaticCache that conserves accelerator memory by offloading\n     cache tensors to CPU when not actively being used.\n@@ -1472,40 +1151,22 @@ class OffloadedStaticCache(StaticCache):\n \n         >>> # Prepare a cache class with offloading\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = OffloadedStaticCache(\n-        ...     config=model.config,\n-        ...     max_batch_size=1,\n-        ...     max_cache_len=max_generated_length,\n-        ...     device=model.device,\n-        ...     dtype=model.dtype\n-        ... )\n+        >>> past_key_values = OffloadedStaticCache(max_cache_len=max_generated_length, config=model.config)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache with offloaded layers\n         OffloadedStaticCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs) -> None:\n-        super().__init__(*args, cache_processor=OffloadedCacheProcessor, **kwargs)\n+    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+        layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n+        super().__init__(layers=layers, offloading=True)\n \n \n class SlidingWindowCache(Cache):\n     \"\"\"\n     Sliding Window Cache class to be used with `torch.compile` for models like Mistral that support sliding window attention.\n-    Every time when we try to update the cache, we compute the `indices` based on `cache_position >= self.sliding_window - 1`,\n-    if true(which means the cache can not hold all the old key value states and new states together because of the sliding window constraint),\n-    we need to do a cycle shift based on `indices` to replace the oldest states by the new key value states passed in.\n-\n-    The `to_shift` is only true once we are above sliding_window. Thus with `sliding_window==64`:\n-\n-    indices = (slicing + to_shift[-1].sum()-1) % self.sliding_window\n-    tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n-        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n-        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n-        55, 56, 57, 58, 59, 60, 61, 62, 63,  0])\n-\n-    We overwrite the cache using these, then we always write at cache_position (clamped to `sliding_window`)\n-\n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Example:\n@@ -1521,22 +1182,24 @@ class SlidingWindowCache(Cache):\n         >>> # Prepare a cache class and pass it to model's forward\n         >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = SlidingWindowCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = SlidingWindowCache(max_cache_len=max_generated_length, config=model.config)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         SlidingWindowCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(layer_classes=SlidingWindowLayer, *args, **kwargs)\n+    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+        layers = [SlidingWindowLayer(max_cache_len, config.sliding_window) for _ in range(config.num_hidden_layers)]\n+        super().__init__(layers=layers)\n \n \n class HybridCache(Cache):\n     \"\"\"\n     Hybrid Cache class to be used with `torch.compile` for models that alternate between a local sliding window\n     attention and global attention in every other layer (originally implemented for Gemma2).\n-    Under the hood, Hybrid Cache leverages [\"SlidingWindowCache\"] for sliding window attention and [\"StaticCache\"]\n+    Under the hood, Hybrid Cache leverages [\"SlidingWindowLayer\"] for sliding window attention and [\"StaticLayer\"]\n     for global attention. For more information, see the documentation of those layer types.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n@@ -1554,27 +1217,35 @@ class HybridCache(Cache):\n         >>> # Prepare a cache class and pass it to model's forward\n         >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = HybridCache(max_cache_len=max_generated_length, config=model.config)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         HybridCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, config: PretrainedConfig, *args, **kwargs):\n+    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n         if hasattr(config, \"layer_types\"):\n-            layer_classes = [LAYER_CLASS_MAP[layer_type] for layer_type in config.layer_types]\n+            layers = []\n+            for layer_type in config.layer_types:\n+                init_kwargs = {\"max_cache_len\": max_cache_len}\n+                if layer_type == \"sliding_attention\":\n+                    init_kwargs[\"sliding_window\"] = config.sliding_window\n+                elif layer_type == \"chunked_attention\":\n+                    init_kwargs[\"sliding_window\"] = config.attention_chunk_size\n+                layers.append(LAYER_CLASS_MAP[layer_type](**init_kwargs))\n         else:\n             # In this case, fall back to StaticCache\n-            layer_classes = [StaticLayer] * config.num_hidden_layers\n-        super().__init__(config=config, layer_classes=layer_classes, *args, **kwargs)\n+            layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n+        super().__init__(layers=layers)\n \n \n # The mapping already handles dispatching the correct layers in Hybrid, this is only used for BC\n class HybridChunkedCache(HybridCache): ...\n \n \n-class OffloadedHybridCache(HybridChunkedCache):\n+class OffloadedHybridCache(Cache):\n     \"\"\"\n     A drop-in replacement for HybridChunkedCache that conserves accelerator memory by offloading\n     cache tensors to CPU when not actively being used.\n@@ -1585,118 +1256,150 @@ class OffloadedHybridCache(HybridChunkedCache):\n     See `Cache` for details on common methods that are implemented by all cache classes.\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs) -> None:\n-        super().__init__(*args, cache_processor=OffloadedCacheProcessor, **kwargs)\n+    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+        if hasattr(config, \"layer_types\"):\n+            layers = []\n+            for layer_type in config.layer_types:\n+                init_kwargs = {\"max_cache_len\": max_cache_len}\n+                if layer_type == \"sliding_attention\":\n+                    init_kwargs[\"sliding_window\"] = config.sliding_window\n+                elif layer_type == \"chunked_attention\":\n+                    init_kwargs[\"sliding_window\"] = config.attention_chunk_size\n+                layers.append(LAYER_CLASS_MAP[layer_type](**init_kwargs))\n+        else:\n+            # In this case, fall back to StaticCache\n+            layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n+        super().__init__(layers=layers, offloading=True)\n \n \n-class QuantizedCache(DynamicCache):\n+class QuantizedCache(Cache):\n     \"\"\"\n-    A quantizer cache similar to what is described in the [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n-    It allows the model to generate longer sequence length without allocating too much memory for Key and Value cache by applying quantization.\n-\n-    The cache has two types of storage, one for original precision and one for the quantized cache. A `residual length` is set as a maximum capacity for the\n-    original precision cache. When the length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache. The\n-    quantization is done per-channel with a set `q_group_size` for both Keys and Values, in contrast to what was described in the paper.\n-\n-    It stores Keys and Values a list of quantized tensors (tuples in case we need to store metadata), one for each layer. Additionally, it stores the Key and\n-    Value in original precision states as a list of tensors, one for each layer. The size of each tensor\n-    is `[batch_size, num_heads, seq_len - residual_length, head_dim]`.\n+    A quantizer cache similar to what is described in the\n+    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n+    It allows the model to generate longer sequence length without allocating too much memory for keys and values\n+    by applying quantization.\n+    The cache has two types of storage, one for original precision and one for the\n+    quantized cache. A `residual length` is set as a maximum capacity for the original precision cache. When the\n+    length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache.\n+    The quantization is done per-channel with a set `q_group_size` for both keys and values, in contrast to what was\n+    described in the paper.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n     \"\"\"\n \n-    def __init__(self, backend, **kwargs) -> None:\n+    def __init__(\n+        self,\n+        backend: str,\n+        config: PretrainedConfig,\n+        nbits: int = 4,\n+        axis_key: int = 0,\n+        axis_value: int = 0,\n+        q_group_size: int = 64,\n+        residual_length: int = 128,\n+    ):\n         if backend == \"quanto\":\n-            processor = QuantoQuantizedCacheProcessor\n+            layer_class = QuantoQuantizedLayer\n         elif backend == \"hqq\":\n-            processor = HQQQuantizedCacheProcessor\n+            layer_class = HQQQuantizedLayer\n         else:\n             raise ValueError(f\"Unknown quantization backend `{backend}`\")\n \n-        super().__init__(cache_processor=processor, **kwargs)\n+        layers = [\n+            layer_class(nbits, axis_key, axis_value, q_group_size, residual_length)\n+            for _ in range(config.num_hidden_layers)\n+        ]\n+        super().__init__(layers=layers)\n \n \n class QuantoQuantizedCache(QuantizedCache):\n     \"\"\"\n-    A quantizer cache similar to what is described in the [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://huggingface.co/papers/2402.02750).\n-    It allows the model to generate longer sequence length without allocating too much memory for Key and Value cache by applying quantization.\n-\n-    The cache has two types of storage, one for original precision and one for the quantized cache. A `residual length` is set as a maximum capacity for the\n-    original precision cache. When the length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache. The\n-    quantization is done per-channel with a set `q_group_size` for both Keys and Values, in contrast to what was described in the paper.\n-\n-    It stores Keys and Values a list of quantized tensors (tuples in case we need to store metadata), one for each layer. Additionally, it stores the Key and\n-    Value in original precision states as a list of tensors, one for each layer. The size of each tensor\n-    is `[batch_size, num_heads, seq_len - residual_length, head_dim]`\n-\n-    Uses `quanto` as a backend to perform quantization. Current implementation supports `int2` and `int4` dtypes only.\n+    A quantizer cache similar to what is described in the\n+    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n+    It allows the model to generate longer sequence length without allocating too much memory for keys and values\n+    by applying quantization.\n+    The cache has two types of storage, one for original precision and one for the\n+    quantized cache. A `residual length` is set as a maximum capacity for the original precision cache. When the\n+    length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache.\n+    The quantization is done per-channel with a set `q_group_size` for both keys and values, in contrast to what was\n+    described in the paper.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Example:\n \n         ```python\n         >>> # Run pip install quanto first if you don't have it yet\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache, QuantizedCacheConfig\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n \n         >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n \n         >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n \n         >>> # Prepare a cache class and pass it to model's forward\n-        >>> cache_config = QuantizedCacheConfig(nbits=4)\n-        >>> past_key_values = QuantoQuantizedCache(cache_config=cache_config)\n+        >>> past_key_values = QuantoQuantizedCache(config=model.config, nbits=4)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         QuantoQuantizedCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, **kwargs) -> None:\n-        DynamicCache.__init__(self, cache_processor=QuantoQuantizedCacheProcessor, **kwargs)\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        nbits: int = 4,\n+        axis_key: int = 0,\n+        axis_value: int = 0,\n+        q_group_size: int = 64,\n+        residual_length: int = 128,\n+    ):\n+        super().__init__(\"quanto\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n \n \n class HQQQuantizedCache(QuantizedCache):\n     \"\"\"\n-    A quantizer cache similar to what is described in the [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n-    It allows the model to generate longer sequence length without allocating too much memory for Key and Value cache by applying quantization.\n-\n-    The cache has two types of storage, one for original precision and one for the quantized cache. A `residual length` is set as a maximum capacity for the\n-    original precision cache. When the length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache. The\n-    quantization is done per-channel with a set `q_group_size` for both Keys and Values, in contrast to what was described in the paper.\n-\n-    It stores Keys and Values a list of quantized tensors (tuples in case we need to store metadata), one for each layer. Additionally, it stores the Key and\n-    Value in original precision states as a list of tensors, one for each layer. The size of each tensor\n-    is `[batch_size, num_heads, seq_len - residual_length, head_dim]`\n-\n-    Uses `HQQ` as a backend to perform quantization. Current implementation supports `int2`, `int4`, `int8` dtypes.\n+    A quantizer cache similar to what is described in the\n+    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n+    It allows the model to generate longer sequence length without allocating too much memory for keys and values\n+    by applying quantization.\n+    The cache has two types of storage, one for original precision and one for the\n+    quantized cache. A `residual length` is set as a maximum capacity for the original precision cache. When the\n+    length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache.\n+    The quantization is done per-channel with a set `q_group_size` for both keys and values, in contrast to what was\n+    described in the paper.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Example:\n \n         ```python\n         >>> # Run pip install hqq first if you don't have it yet\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache, QuantizedCacheConfig\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n \n         >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n \n         >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n \n         >>> # Prepare a cache class and pass it to model's forward\n-        >>> cache_config = QuantizedCacheConfig(nbits=4, axis_key=1, axis_value=1)\n-        >>> past_key_values = HQQQuantizedCache(cache_config=cache_config)\n+        >>> past_key_values = HQQQuantizedCache(config=model.config, nbits=4, axis_key=1, axis_value=1)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         HQQQuantizedCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, backend=\"HQQ\", **kwargs) -> None:\n-        assert backend == \"HQQ\"\n-        DynamicCache.__init__(self, cache_processor=HQQQuantizedCacheProcessor, **kwargs)\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        nbits: int = 4,\n+        axis_key: int = 0,\n+        axis_value: int = 0,\n+        q_group_size: int = 64,\n+        residual_length: int = 128,\n+    ):\n+        super().__init__(\"hqq\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n \n \n class EncoderDecoderCache(Cache):\n@@ -1724,16 +1427,15 @@ class EncoderDecoderCache(Cache):\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         EncoderDecoderCache()\n         ```\n-\n     \"\"\"\n \n-    # Override @property from Cache\n-    is_compileable = None\n+    # Override @property from Cache -> this will be set in __init__ on the instances\n+    is_compileable = False\n \n     def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n-        super().__init__(layer_classes=DynamicLayer)\n         self.self_attention_cache = self_attention_cache\n         self.cross_attention_cache = cross_attention_cache\n+        # Override @property from Cache\n         self.is_compileable = getattr(self.self_attention_cache, \"is_compileable\", False)\n \n         self.is_updated = {}\n@@ -1884,115 +1586,6 @@ def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[\n         return self.self_attention_cache.get_mask_sizes(cache_position, layer_idx)\n \n \n-def parse_processor_args(processor_class: Optional[type[\"CacheProcessor\"]], kwargs: dict) -> tuple[dict, dict]:\n-    \"\"\"\n-    Parse processor arguments from kwargs based on the processor class init signature.\n-\n-    Args:\n-        processor_class: The processor class to inspect, or None\n-        kwargs: Dictionary of keyword arguments\n-\n-    Returns:\n-        tuple: (processor_kwargs, remaining_kwargs)\n-    \"\"\"\n-    try:\n-        params = list(inspect.signature(processor_class.__init__).parameters)[2:]\n-    except Exception:\n-        return {}, kwargs\n-\n-    processor_kwargs = {k: kwargs[k] for k in params if k in kwargs}\n-    remaining_kwargs = {k: v for k, v in kwargs.items() if k not in processor_kwargs}\n-    return processor_kwargs, remaining_kwargs\n-\n-\n-def parse_layer_args_from_model_config(\n-    config: Optional[PretrainedConfig],\n-    batch_size: Optional[int] = None,\n-    max_cache_len: Optional[int] = None,\n-    device: Union[torch.device, str, None] = None,\n-    dtype: Optional[torch.dtype] = None,\n-    layer_device_map: Optional[dict[int, torch.device]] = None,\n-    tp_size: Optional[int] = None,\n-    max_batch_size: Optional[int] = None,\n-) -> dict:\n-    \"\"\"\n-    Parse layer arguments from model configuration for cache initialization.\n-\n-    Args:\n-        config (`Optional[PretrainedConfig]`): Model configuration containing shape/device info.\n-        batch_size (`Optional[int]`): Batch size for cache initialization.\n-        max_cache_len (`Optional[int]`): Maximum sequence length for cache.\n-        device (`Union[torch.device, str, None]`): Device for cache tensors.\n-        dtype (`Optional[torch.dtype]`): Data type for cache tensors.\n-        layer_device_map: Per-layer device mapping.\n-        tp_size (`Optional[int]`): Tensor parallel size to adjust number of key/value heads.\n-        max_batch_size (`Optional[int]`): Maximum batch size for cache initialization.\n-\n-    Returns:\n-        `dict`: Dictionary containing parsed layer arguments for cache initialization.\n-    \"\"\"\n-    # No model config -> must be a dynamic cache, return bare dict\n-    if config is None:\n-        return {}\n-    # Build the args dict for hybrid, sliding or static\n-    else:\n-        # Hybrid/Sliding caches require a config that supports sliding_window (max_cache_len already used)\n-        if (\n-            getattr(config, \"layer_types\", None) is not None\n-            and \"sliding_attention\" in config.layer_types\n-            and \"full_attention\" in config.layer_types\n-        ):\n-            if getattr(config, \"sliding_window\", None) is None:\n-                raise ValueError(\n-                    \"Setting up a hybrid or sliding window KVCache requires the model config supporting \"\n-                    \"sliding window attention, please check if there is a `sliding_window` field in the model \"\n-                    \"config and it's not set to None.\"\n-                )\n-        # Adjust max_cache_len for sliding window layers (they can't be larger than sliding window)\n-        max_cache_len = max_cache_len or config.max_position_embeddings\n-        # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads:\n-        head_dim = (\n-            config.head_dim\n-            if getattr(config, \"head_dim\", None) is not None\n-            else config.hidden_size // config.num_attention_heads\n-        )\n-        num_heads = (\n-            config.num_attention_heads\n-            if getattr(config, \"num_key_value_heads\", None) is None\n-            else config.num_key_value_heads\n-        )\n-        if tp_size is not None and tp_size > 1:\n-            if num_heads % tp_size != 0:\n-                raise ValueError(\n-                    f\"Number of key value heads {num_heads} must be divisible by tensor parallel size {tp_size}.\"\n-                )\n-            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n-            num_heads //= tp_size\n-        layer_args = {\n-            \"batch_size\": max_batch_size if max_batch_size is not None else batch_size,\n-            \"max_cache_len\": max_cache_len,\n-            \"device\": torch.device(device) if device is not None else None,\n-            \"dtype\": dtype,\n-            \"layer_device_map\": layer_device_map,\n-            \"head_dim\": head_dim,\n-            \"num_heads\": num_heads,\n-            \"sliding_window\": getattr(config, \"sliding_window\", None),\n-        }\n-        return {k: v for k, v in layer_args.items() if v is not None}\n-\n-\n-LAYER_CLASS_MAP: dict[str, type[\"CacheLayerMixin\"]] = {\n-    \"full_attention\": StaticLayer,\n-    \"sliding_attention\": SlidingWindowLayer,\n-    \"chunked_attention\": ChunkedSlidingLayer,\n-}\n-PROCESSOR_CLASS_MAP: dict[str, type[\"CacheProcessor\"]] = {\n-    \"offloaded\": OffloadedCacheProcessor,\n-    \"quanto_quantized\": QuantizedCacheProcessor,\n-    \"hqq_quantized\": HQQQuantizedCacheProcessor,\n-}\n-\n-\n ### Deprecated classes\n \n \n@@ -2234,91 +1827,6 @@ def __init__(self, batch_size: int, max_cache_len: int, device=\"cpu\"):\n         self.max_cache_len = max_cache_len\n         self.device = device\n \n-    def initialise_cache_layer(self, layer_idx, key_states):\n-        \"\"\"Overridden to use the correct device if offloaded layer (and pin memory).\"\"\"\n-        if len(self.key_cache) > layer_idx:\n-            return\n-\n-        num_key_value_heads = key_states.shape[1]\n-        device = key_states.device if self.is_sliding[layer_idx] else self.offload_device\n-        pin_memory = not self.is_sliding[layer_idx]\n-        global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n-        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self.sliding_window, self.head_dim)\n-        # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n-        # breaks when updating the cache.\n-        cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape\n-        new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=device, pin_memory=pin_memory)\n-        new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=device, pin_memory=pin_memory)\n-        torch._dynamo.mark_static_address(new_layer_key_cache)\n-        torch._dynamo.mark_static_address(new_layer_value_cache)\n-        self.key_cache.append(new_layer_key_cache)\n-        self.value_cache.append(new_layer_value_cache)\n-\n-        # Make sure to initialize the on-device layer if it does not already exist\n-        if self.device_key_cache is None and not self.is_sliding[layer_idx]:\n-            self.device_key_cache = []\n-            self.device_value_cache = []\n-            # We need 2 layers to avoid race conditions when prefetching the next one\n-            for _ in range(2):\n-                device_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=key_states.device)\n-                device_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=key_states.device)\n-                torch._dynamo.mark_static_address(new_layer_key_cache)\n-                torch._dynamo.mark_static_address(new_layer_value_cache)\n-                self.device_key_cache.append(device_layer_key_cache)\n-                self.device_value_cache.append(device_layer_value_cache)\n-\n-    def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-        # Wait for prefetch stream if needed\n-        if self._prefetch_stream is not None:\n-            torch.cuda.default_stream(key_states.device).wait_stream(self._prefetch_stream)\n-\n-        # Get correct on-device layer\n-        k_out = self.device_key_cache[self.active_device_layer]\n-        v_out = self.device_value_cache[self.active_device_layer]\n-\n-        # Let's prefetch the next layer as soon as possible\n-        self._prefetch_next_layer(layer_idx)\n-\n-        # Copy to on-device layer\n-        k_out[:, :, cache_position] = key_states\n-        v_out[:, :, cache_position] = value_states\n-\n-        # Copy to offloaded device\n-        self.key_cache[layer_idx][:, :, cache_position] = key_states.to(self.offload_device)\n-        self.value_cache[layer_idx][:, :, cache_position] = value_states.to(self.offload_device)\n-\n-        return k_out, v_out\n-\n-    def _prefetch_next_layer(self, layer_idx: int) -> None:\n-        \"\"\"Based on current layer_idx, prefetch next full layer to the device.\"\"\"\n-\n-        # Switch the active layer\n-        self.active_device_layer = 0 if self.active_device_layer == 1 else 1\n-\n-        # Find the next non-sliding layer\n-        try:\n-            next_layer = layer_idx + 1 + self.is_sliding[layer_idx + 1 :].index(False)\n-        # In this case, we are at the last layer, and we go back to prefect the first one\n-        except ValueError:\n-            next_layer = self.is_sliding.index(False)\n-\n-        # Alternate between two on-device caches.\n-        if self._prefetch_stream is not None:\n-            with torch.cuda.stream(self._prefetch_stream):\n-                self._prefetch_layer_in_context(next_layer)\n-        else:\n-            self._prefetch_layer_in_context(next_layer)\n-\n-    def _prefetch_layer_in_context(self, layer_idx: int) -> None:\n-        \"\"\"Performs the actual copy of the layer to device cache.\"\"\"\n-        if len(self.key_cache) > layer_idx:\n-            self.device_key_cache[self.active_device_layer].copy_(self.key_cache[layer_idx], non_blocking=True)\n-            self.device_value_cache[self.active_device_layer].copy_(self.value_cache[layer_idx], non_blocking=True)\n-        # The layer was not yet initialized\n-        else:\n-            self.device_key_cache[self.active_device_layer].fill_(0.0)\n-            self.device_value_cache[self.active_device_layer].fill_(0.0)\n-\n \n # TODO (manuel, joao): remove this class, it is here only for backwards compatibility\n # PEP 562: Lazy loading for deprecated location of MambaCache"
        },
        {
            "sha": "7b40178023da9015738589a6936561339153b751",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 99,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -1798,7 +1798,8 @@ def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         if model_kwargs.get(\"past_key_values\") is not None:\n             cache = model_kwargs[\"past_key_values\"]\n             past_length = 0\n-            if not isinstance(cache, Cache):\n+            # Support for BC tuple cache format\n+            if isinstance(cache, tuple):\n                 past_length = cache[0][0].shape[2]\n             elif hasattr(cache, \"get_seq_length\") and cache.get_seq_length() is not None:\n                 past_length = cache.get_seq_length()\n@@ -1808,87 +1809,7 @@ def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         model_kwargs[\"cache_position\"] = cache_position\n         return model_kwargs\n \n-    def _get_layer_device_map_for_cache_init(self) -> Optional[dict[int, Union[str, int]]]:\n-        \"\"\"\n-        Returns the device map for each decoder layer, to allocate the cache on the right device.\n-        Inspired from `dispatch_model` in accelerate.\n-        \"\"\"\n-        execution_device_map = None\n-\n-        if hasattr(self, \"hf_device_map\"):\n-            if set(self.hf_device_map.values()) == {\"cpu\"} or set(self.hf_device_map.values()) == {\"cpu\", \"disk\"}:\n-                main_device = \"cpu\"\n-            else:\n-                main_device = [d for d in self.hf_device_map.values() if d not in [\"cpu\", \"disk\"]][0]\n-            execution_device_map = {\n-                name: main_device if device in [\"cpu\", \"disk\"] else device\n-                for name, device in self.hf_device_map.items()\n-            }\n-\n-        # No `execution_device_map` -> rely on `self.device` to allocate the cache\n-        if execution_device_map is None:\n-            return None\n-\n-        # Single device for all layers\n-        num_hidden_layers = self.config.get_text_config().num_hidden_layers\n-        if len(execution_device_map) == 1 and \"\" in execution_device_map:\n-            return dict.fromkeys(range(num_hidden_layers), execution_device_map[\"\"])\n-\n-        # Multiple devices in `execution_device_map` -> we need to map decoder layers to the correct device.\n-        layer_device_map = {}\n-        # Case 1: The model has a `get_decoder` method, we can use it to find the decoder name.\n-        if hasattr(self, \"get_decoder\"):\n-            decoder_name = None\n-            for name, module in self.named_modules():\n-                if module is self.get_decoder():\n-                    decoder_name = name\n-                    break\n-            if decoder_name is None:\n-                raise RuntimeError(\n-                    \"`model.get_decoder()` is not returning a named module of the model. This is unexpected, please \"\n-                    \"open an issue on GitHub.\"\n-                )\n-\n-            decoder_mapped_modules = [\n-                module_name for module_name in execution_device_map if decoder_name in module_name\n-            ]\n-            # The decoder name may be present in `execution_device_map` in two forms:\n-            # a) each layer has a device mapping\n-            if len(decoder_mapped_modules) >= num_hidden_layers:\n-                for idx in range(num_hidden_layers):\n-                    for module_name in decoder_mapped_modules:\n-                        if f\".{idx}.\" in f\"{module_name}.\":\n-                            layer_device_map[idx] = execution_device_map[module_name]\n-                            break\n-\n-            # b) the whole module is mapped to a single device. If the decoder name is NOT present in the device map,\n-            # then the mapping is done in a parent module\n-            else:\n-                while True:\n-                    if decoder_name in execution_device_map:\n-                        layer_device_map = dict.fromkeys(range(num_hidden_layers), execution_device_map[decoder_name])\n-                        break\n-                    elif \".\" in decoder_name:\n-                        decoder_name = decoder_name.rsplit(\".\", 1)[0]  # gets the name of the parent module\n-                    else:\n-                        raise RuntimeError(f\"Decoder name {decoder_name} not found in execution device map\")\n-\n-        # Case 2: Legacy code path: assume the decoder layers are named as `(...).X` (X being the layer index)\n-        else:\n-            for layer in execution_device_map:\n-                for idx in range(num_hidden_layers):\n-                    if f\".{idx}.\" in f\"{layer}.\":\n-                        layer_device_map[idx] = execution_device_map[layer]\n-                        break\n-\n-        for idx in range(num_hidden_layers):\n-            if idx not in layer_device_map:\n-                raise RuntimeError(f\"layer {idx} has not been mapped to a device.\")\n-        return layer_device_map\n-\n-    def _get_cache(\n-        self, cache_implementation: str, batch_size: int, max_cache_len: int, device: torch.device, model_kwargs\n-    ) -> Cache:\n+    def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len: int, model_kwargs) -> Cache:\n         \"\"\"\n         Sets a cache for `generate`, that will persist across calls. A new cache will only be initialized a\n         new `generate` call requires a larger cache or uses a different batch size.\n@@ -1926,23 +1847,10 @@ def _get_cache(\n             )\n \n         if need_new_cache:\n-            if hasattr(self.config, \"_pre_quantization_dtype\"):\n-                cache_dtype = self.config._pre_quantization_dtype\n-            else:\n-                cache_dtype = self.dtype\n-\n-            layer_device_map = self._get_layer_device_map_for_cache_init()\n             cache_kwargs = {\n-                \"config\": self.config.get_text_config(),\n-                \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n-                \"dtype\": cache_dtype,\n-                \"device\": device,\n-                \"layer_device_map\": layer_device_map,\n+                \"config\": self.config.get_text_config(),\n             }\n-            if cache_implementation in [\"static\", \"hybrid\", \"offloaded_static\"]:\n-                cache_kwargs.update({\"tp_size\": self.tp_size})\n-\n             self._cache = cache_cls(**cache_kwargs)\n             if requires_cross_attention_cache:\n                 encoder_kwargs = cache_kwargs.copy()\n@@ -1978,7 +1886,6 @@ def _prepare_cache_for_generation(\n         assistant_model: \"PreTrainedModel\",\n         batch_size: int,\n         max_cache_length: int,\n-        device: torch.device,\n     ) -> bool:\n         \"\"\"\n         Prepares the cache for generation (if applicable), given `generate`'s parameterization. If a cache is\n@@ -2051,7 +1958,6 @@ def _prepare_cache_for_generation(\n                     cache_implementation=generation_config.cache_implementation,\n                     batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,\n                     max_cache_len=max_cache_length,\n-                    device=device,\n                     model_kwargs=model_kwargs,\n                 )\n             elif generation_config.cache_implementation == \"quantized\":\n@@ -2473,7 +2379,7 @@ def generate(\n         ):\n             max_cache_length += inputs_tensor.shape[1]\n         self._prepare_cache_for_generation(\n-            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n+            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length\n         )\n \n         # 8. determine generation mode"
        },
        {
            "sha": "0afe61ca78f59a8365dd2e0d29931be8232f4ba1",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -514,13 +514,16 @@ def __init__(\n \n         self.model = model\n         self.static_cache = StaticCache(\n-            config=config,\n-            max_batch_size=generation_config.cache_config.get(\"batch_size\"),\n             max_cache_len=generation_config.cache_config.get(\"max_cache_len\"),\n-            device=generation_config.cache_config.get(\"device\"),\n-            dtype=self.model.dtype,\n+            config=config,\n         )\n-\n+        batch_size = generation_config.cache_config.get(\"batch_size\")\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        device = generation_config.cache_config.get(\"device\")\n+        dtype = self.model.dtype\n+        # We need this call to initialize all the layers (otherwise it's done lazily, which is not exportable)\n+        self.static_cache.early_initialization(batch_size, num_heads, head_dim, dtype, device)\n         for i in range(len(self.static_cache)):\n             self.register_buffer(f\"key_cache_{i}\", self.static_cache.layers[i].keys, persistent=False)\n             self.register_buffer(f\"value_cache_{i}\", self.static_cache.layers[i].values, persistent=False)\n@@ -667,13 +670,14 @@ def __init__(\n             raise AssertionError(\"Model must have caching enabled.\")\n \n         # Initialize the HybridCache\n-        self.cache = HybridCache(\n-            config=config,\n-            max_batch_size=generation_config.cache_config.get(\"batch_size\"),\n-            max_cache_len=generation_config.cache_config.get(\"max_cache_len\"),\n-            device=generation_config.cache_config.get(\"device\"),\n-            dtype=self.model.dtype,\n-        )\n+        self.cache = HybridCache(max_cache_len=generation_config.cache_config.get(\"max_cache_len\"), config=config)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        max_batch_size = generation_config.cache_config.get(\"batch_size\")\n+        device = generation_config.cache_config.get(\"device\")\n+        dtype = self.model.dtype\n+        # We need this call to initialize all the layers (otherwise it's done lazily, which is not exportable)\n+        self.cache.early_initialization(max_batch_size, num_heads, head_dim, dtype, device)\n \n         # Register all key and value cache tensors as buffers\n         for i in range(len(self.cache)):\n@@ -814,13 +818,10 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         self.config = model.config\n \n         # Initialize static cache for decoder and DynamicCache for encoder\n-        self.static_cache = StaticCache(\n-            config=self.config,\n-            max_batch_size=batch_size,\n-            max_cache_len=max_static_cache_length,\n-            device=\"cpu\",\n-            dtype=torch.float32,\n-        )\n+        self.static_cache = StaticCache(max_cache_len=max_static_cache_length, config=self.config)\n+        head_dim = getattr(self.config, \"head_dim\", self.config.hidden_size // self.config.num_attention_heads)\n+        num_heads = getattr(self.config, \"num_key_value_heads\", self.config.num_attention_heads)\n+        self.static_cache.early_initialization(batch_size, num_heads, head_dim, torch.float32, \"cpu\")\n         self.cache = EncoderDecoderCache(self.static_cache, DynamicCache())\n \n         # Register cache buffers to make them exportable"
        },
        {
            "sha": "fd3e8c94bd98a0f363b45d56186259b0eb8dea8d",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -31,7 +31,7 @@\n \n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache, DynamicCache, DynamicLayer\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -86,7 +86,7 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n     seq_idx: torch.IntTensor\n \n \n-class HybridMambaAttentionDynamicCache(Cache):\n+class HybridMambaAttentionDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -100,12 +100,9 @@ class HybridMambaAttentionDynamicCache(Cache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n-    key_cache = None\n-    value_cache = None\n     is_compileable = False\n \n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__(layer_classes=DynamicLayer)\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv\n@@ -181,13 +178,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n \n class BambaRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`"
        },
        {
            "sha": "c75ca632a8830306f8c4107754dc542c1a127316",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -42,7 +42,6 @@\n     segment_sum,\n )\n \n-from ...cache_utils import DynamicLayer\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -115,7 +114,6 @@ class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache):\n     \"\"\"\n \n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        HybridMambaAttentionDynamicCache.__init__(layer_classes=DynamicLayer)\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv"
        },
        {
            "sha": "7cac22f0d4838af70d2a18241ed14d5d90c925f9",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -347,7 +347,7 @@ def _main_generate_loop(\n         ):\n             max_cache_length += inputs_tensor.shape[1]\n         self._prepare_cache_for_generation(\n-            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n+            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length\n         )\n \n         # 8. determine generation mode"
        },
        {
            "sha": "bdbdced722aa297b15e000e23205087f7545b92c",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -32,7 +32,7 @@\n \n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -63,7 +63,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class FalconHybridMambaAttentionDynamicCache(Cache):\n+class FalconHybridMambaAttentionDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -77,8 +77,6 @@ class FalconHybridMambaAttentionDynamicCache(Cache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n-    key_cache = None\n-    value_cache = None\n     is_compileable = False\n \n     def __init__(\n@@ -187,13 +185,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"FalconHybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"FalconHybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n     def update_conv_state(\n         self,\n         layer_idx: int,"
        },
        {
            "sha": "3c16f4a297e0d7eeea9c7043a8d5a8c623dee9ca",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -216,7 +216,7 @@ def forward(\n             embed_positions = self._get_embed_positions(position_ids)\n \n         repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])\n-        sincos = torch.gather(embed_positions, 1, repeated_position_ids)\n+        sincos = torch.gather(embed_positions, 1, repeated_position_ids).to(key.dtype)\n         sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)\n \n         if self.rotary_dim is not None:\n@@ -302,7 +302,7 @@ def forward(\n             embed_positions = self._get_embed_positions(position_ids)\n \n         repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])\n-        sincos = torch.gather(embed_positions, 1, repeated_position_ids)\n+        sincos = torch.gather(embed_positions, 1, repeated_position_ids).to(key.dtype)\n         sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)\n \n         if self.rotary_dim is not None:"
        },
        {
            "sha": "496ac56804ab412ed5baf42181b2ec073919e144",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -27,7 +27,7 @@\n \n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache, DynamicCache, DynamicLayer\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -224,7 +224,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class HybridMambaAttentionDynamicCache(Cache):\n+class HybridMambaAttentionDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -238,12 +238,9 @@ class HybridMambaAttentionDynamicCache(Cache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n-    key_cache = None\n-    value_cache = None\n     is_compileable = False\n \n     def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__(layer_classes=DynamicLayer)\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv\n@@ -319,13 +316,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n \n # Helper methods for segment sum computation\n "
        },
        {
            "sha": "f412d589c27bfc59659a42e8bea3e148ddc13957",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -28,7 +28,6 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, DynamicLayer\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -191,7 +190,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-class HybridMambaAttentionDynamicCache(Cache):\n+class HybridMambaAttentionDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -205,12 +204,9 @@ class HybridMambaAttentionDynamicCache(Cache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n-    key_cache = None\n-    value_cache = None\n     is_compileable = False\n \n     def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n-        super().__init__(layer_classes=DynamicLayer)\n         self.dtype = dtype\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n@@ -274,13 +270,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n-\n \n # Adapted from transformers.models.mistral.modeling_mistral.MistralAttention with Mistral->Jamba\n class JambaAttention(nn.Module):"
        },
        {
            "sha": "f2659b56935ad18299a6f326399ef12b93a1e30b",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -1220,7 +1220,6 @@ def _prepare_model_inputs(\n         cache_methods = [\n             \"_prepare_cache_for_generation\",\n             \"_get_cache\",\n-            \"_get_layer_device_map_for_cache_init\",\n         ]\n         for method in cache_methods:\n             setattr(self.codec_model, method, types.MethodType(getattr(self, method).__func__, self.codec_model))\n@@ -1229,13 +1228,13 @@ def _prepare_model_inputs(\n             self.codec_model, \"_supports_default_dynamic_cache\", types.MethodType(lambda x: True, self.codec_model)\n         )\n \n+        self.codec_model.generation_config.cache_implementation = \"dynamic\"\n         self.codec_model._prepare_cache_for_generation(\n             generation_config=self.codec_model.generation_config,\n             model_kwargs=temporary_model_kwargs,\n             assistant_model=None,\n             batch_size=batch_size,\n             max_cache_length=self.config.codec_config.sliding_window,\n-            device=device,\n         )\n \n         if \"past_key_values\" in temporary_model_kwargs:"
        },
        {
            "sha": "c1612ba435da11b5b0d9acef75096804f5754126",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -344,7 +344,6 @@ def _prepare_model_inputs(\n         cache_methods = [\n             \"_prepare_cache_for_generation\",\n             \"_get_cache\",\n-            \"_get_layer_device_map_for_cache_init\",\n         ]\n         for method in cache_methods:\n             setattr(self.codec_model, method, types.MethodType(getattr(self, method).__func__, self.codec_model))\n@@ -353,13 +352,13 @@ def _prepare_model_inputs(\n             self.codec_model, \"_supports_default_dynamic_cache\", types.MethodType(lambda x: True, self.codec_model)\n         )\n \n+        self.codec_model.generation_config.cache_implementation = \"dynamic\"\n         self.codec_model._prepare_cache_for_generation(\n             generation_config=self.codec_model.generation_config,\n             model_kwargs=temporary_model_kwargs,\n             assistant_model=None,\n             batch_size=batch_size,\n             max_cache_length=self.config.codec_config.sliding_window,\n-            device=device,\n         )\n \n         if \"past_key_values\" in temporary_model_kwargs:"
        },
        {
            "sha": "092a8b3caa82ea1a45b1b25d11bcb9253b963bc0",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -23,7 +23,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n@@ -122,7 +122,7 @@ def forward(self, x):\n         return self.w2(F.silu(self.w1(x)) * self.w3(x))\n \n \n-class Lfm2HybridConvCache(DynamicCache):\n+class Lfm2HybridConvCache:\n     \"\"\"\n     Attention and conv cache for Lfm2.\n \n@@ -254,16 +254,12 @@ def crop(self, max_length: int):\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n                 self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n \n+    def __len__(self) -> int:\n+        return len(self.key_cache)\n+\n     def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         return self.key_cache[layer_idx], self.value_cache[layer_idx]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n-\n     def reset(self):\n         for layer_idx in range(len(self.conv_cache)):\n             # In-place ops prevent breaking the static address"
        },
        {
            "sha": "5d3791cbe3b1f0f15afb9a9bd1c3534780c3e6c3",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -17,7 +17,6 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from ...cache_utils import DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n@@ -81,7 +80,7 @@ def forward(self, x):\n         return self.w2(F.silu(self.w1(x)) * self.w3(x))\n \n \n-class Lfm2HybridConvCache(DynamicCache):\n+class Lfm2HybridConvCache:\n     \"\"\"\n     Attention and conv cache for Lfm2.\n \n@@ -213,16 +212,12 @@ def crop(self, max_length: int):\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n                 self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n \n+    def __len__(self) -> int:\n+        return len(self.key_cache)\n+\n     def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         return self.key_cache[layer_idx], self.value_cache[layer_idx]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n-\n     def reset(self):\n         for layer_idx in range(len(self.conv_cache)):\n             # In-place ops prevent breaking the static address"
        },
        {
            "sha": "74f61e79029ef5ec05cc3d8b506ebb6bf7d22c25",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -1266,7 +1266,6 @@ def generate(\n             assistant_model=None,\n             batch_size=batch_size,\n             max_cache_length=max_cache_length,\n-            device=input_ids_length.device,\n         )\n \n         # 7. Prepare `input_ids` which will be used for auto-regressive generation"
        },
        {
            "sha": "b0afcf6da7efa2a4ac10b2a7baf9df9c00a38839",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -2184,7 +2184,6 @@ def generate(\n             assistant_model=None,\n             batch_size=batch_size,\n             max_cache_length=max_cache_length,\n-            device=inputs_tensor.device,\n         )\n \n         # build the delay pattern mask for offsetting each codebook prediction by 1 (this behaviour is specific to MusicGen)"
        },
        {
            "sha": "5f1c592d3230201e6cf19c97183d55e80570ce95",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -1569,7 +1569,6 @@ def extend_enc_output(tensor, num_beams=None):\n             assistant_model=None,\n             batch_size=input_ids.shape[0],\n             max_cache_length=generation_config.max_length - 1,\n-            device=input_ids.device,\n         )\n \n         if generation_config.num_beams == 1:"
        },
        {
            "sha": "dfe5b9bdd58904e833b2db88b0ad71dceadc0cab",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -28,7 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -94,7 +94,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-class ZambaHybridDynamicCache(Cache):\n+class ZambaHybridDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -108,8 +108,6 @@ class ZambaHybridDynamicCache(Cache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n-    key_cache = None\n-    value_cache = None\n     is_compileable = False\n \n     def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n@@ -191,13 +189,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"ZambaHybridDynamicCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"ZambaHybridDynamicCache does not have a legacy cache equivalent.\")\n-\n \n def eager_attention_forward(\n     module: nn.Module,"
        },
        {
            "sha": "dcc88def1002595a37848ab46807428bc4a87603",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -29,7 +29,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -98,7 +98,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Zamba2HybridDynamicCache(Cache):\n+class Zamba2HybridDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -112,8 +112,6 @@ class Zamba2HybridDynamicCache(Cache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n-    key_cache = None\n-    value_cache = None\n     is_compileable = False\n \n     def __init__(\n@@ -192,13 +190,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"Zamba2HybridDynamicCache does not have a legacy cache equivalent.\")\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"Zamba2HybridDynamicCache does not have a legacy cache equivalent.\")\n-\n     def update_conv_state(\n         self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n     ) -> torch.Tensor:"
        },
        {
            "sha": "97798ff9ed147a17a9e51f4a5181c12a3a32aac5",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -206,6 +206,7 @@\n     is_pytesseract_available,\n     is_pytest_available,\n     is_pytorch_quantization_available,\n+    is_quanto_greater,\n     is_quark_available,\n     is_qutlass_available,\n     is_rich_available,"
        },
        {
            "sha": "0ce888db6f99b2eb20a1e71069888666b9c205bf",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -1278,6 +1278,24 @@ def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool =\n         return version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse(library_version)\n \n \n+@lru_cache\n+def is_quanto_greater(library_version: str, accept_dev: bool = False):\n+    \"\"\"\n+    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n+    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n+    2.7.0).\n+    \"\"\"\n+    if not _is_package_available(\"optimum-quanto\"):\n+        return False\n+\n+    if accept_dev:\n+        return version.parse(version.parse(importlib.metadata.version(\"optimum-quanto\")).base_version) > version.parse(\n+            library_version\n+        )\n+    else:\n+        return version.parse(importlib.metadata.version(\"optimum-quanto\")) > version.parse(library_version)\n+\n+\n def is_torchdistx_available():\n     return _torchdistx_available\n "
        },
        {
            "sha": "c6376617341aff516fe3c407a4fdfa26bf7f0b18",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -4083,16 +4083,7 @@ def test_init_static_cache_multi_accelerator(self):\n         #     )\n         #     results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n \n-        # deduced from the device_map : layer 0 on device 0 and layer 1 on device 1\n-        layer_device_map = {0: 0, 1: 1}\n-        past_key_values = StaticCache(\n-            config=model.config,\n-            max_batch_size=1,\n-            max_cache_len=30,\n-            device=torch_device,\n-            dtype=model.dtype,\n-            layer_device_map=layer_device_map,\n-        )\n+        past_key_values = StaticCache(config=model.config, max_cache_len=30)\n         results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n \n         # check device of each layer\n@@ -4287,13 +4278,7 @@ def test_prepare_inputs_for_generation_decoder_llm(self):\n         max_cache_len = 10\n         batch_size = 2\n         query_length = input_ids.shape[-1] - init_input_ids.shape[-1]\n-        static_cache = StaticCache(\n-            config=config,\n-            max_batch_size=batch_size,\n-            max_cache_len=max_cache_len,\n-            device=torch_device,\n-            dtype=torch.float32,\n-        )\n+        static_cache = StaticCache(config=config, max_cache_len=max_cache_len)\n         static_cache = model(init_input_ids, past_key_values=static_cache).past_key_values\n         model_inputs = model.prepare_inputs_for_generation(\n             input_ids, past_key_values=static_cache, cache_position=cache_position, attention_mask=attention_mask"
        },
        {
            "sha": "653a8254616baeef751313288494efe63d45daa7",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -297,6 +297,26 @@ class BambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, HybridMambaAttentionDynamicCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        self.assertListEqual(\n+            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+            [expected_shape] * len(decoder_past_key_values.key_cache),\n+        )\n+        self.assertListEqual(\n+            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n+            [expected_shape] * len(decoder_past_key_values.value_cache),\n+        )\n+\n     def setUp(self):\n         self.model_tester = self.model_tester_class(self)\n         self.config_tester = ConfigTester(self, config_class=self.model_tester.config_class, hidden_size=64)"
        },
        {
            "sha": "c14cc8b1d4b7498dcf0644035f9898066bcefcd2",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -666,6 +666,7 @@ def prepare_config_and_inputs(self):\n             vocab_size=self.vocab_size,\n             d_model=self.d_model,\n             decoder_layers=self.decoder_layers,\n+            num_hidden_layers=self.decoder_layers,\n             decoder_ffn_dim=self.decoder_ffn_dim,\n             encoder_attention_heads=self.encoder_attention_heads,\n             decoder_attention_heads=self.decoder_attention_heads,"
        },
        {
            "sha": "5a05fd57468491a42441c0565ab16cebe1c61ca8",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -419,6 +419,7 @@ def prepare_config_and_inputs(self):\n             vocab_size=self.vocab_size,\n             d_model=self.d_model,\n             decoder_layers=self.decoder_layers,\n+            num_hidden_layers=self.decoder_layers,\n             decoder_ffn_dim=self.decoder_ffn_dim,\n             encoder_attention_heads=self.encoder_attention_heads,\n             decoder_attention_heads=self.decoder_attention_heads,"
        },
        {
            "sha": "f376fab87e148a0bf1aa71e2a9c6866bd46cefd9",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -764,13 +764,7 @@ def test_stacked_causal_mask_static_cache(self):\n \n         # upgrade the model with StaticCache\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n-        past_key_values = StaticCache(\n-            config=self.model.config,\n-            max_batch_size=1,\n-            max_cache_len=max_cache_len,\n-            device=torch_device,\n-            dtype=self.model.dtype,\n-        )\n+        past_key_values = StaticCache(config=self.model.config, max_cache_len=max_cache_len)\n \n         padded_attention_mask = torch.nn.functional.pad(\n             input=mask_shared_prefix,\n@@ -812,13 +806,7 @@ def test_partial_stacked_causal_mask_static_cache(self):\n \n         # upgrade the model with StaticCache\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n-        past_key_values = StaticCache(\n-            config=self.model.config,\n-            max_batch_size=1,\n-            max_cache_len=max_cache_len,\n-            device=torch_device,\n-            dtype=self.model.dtype,\n-        )\n+        past_key_values = StaticCache(config=self.model.config, max_cache_len=max_cache_len)\n \n         # forward run for the first part of input\n         part_a = 3  # split point"
        },
        {
            "sha": "efcf00798de072eaf734a087f409d22f7c6f2ab0",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 10,
            "deletions": 27,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -38,7 +38,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import AutoTokenizer, Cache, FalconH1ForCausalLM, FalconH1Model\n+    from transformers import AutoTokenizer, FalconH1ForCausalLM, FalconH1Model\n     from transformers.models.falcon_h1.modeling_falcon_h1 import (\n         FalconHybridMambaAttentionDynamicCache,\n     )\n@@ -273,7 +273,7 @@ class FalconH1ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n     )\n \n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n+        self.assertIsInstance(decoder_past_key_values, FalconHybridMambaAttentionDynamicCache)\n \n         # (batch, head, seq_length, head_features)\n         expected_shape = (\n@@ -283,31 +283,14 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             config.hidden_size // config.num_attention_heads,\n         )\n \n-        if isinstance(decoder_past_key_values, Cache):\n-            self.assertListEqual(\n-                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-                [expected_shape] * len(decoder_past_key_values.key_cache),\n-            )\n-            self.assertListEqual(\n-                [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n-                [expected_shape] * len(decoder_past_key_values.value_cache),\n-            )\n-\n-        # Legacy cache format checks. This branch should be removed when all models use `Cache` by default\n-        else:\n-            self.assertListEqual(\n-                [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n-                [True] * len(decoder_past_key_values),\n-            )\n-            # check shape key, value\n-            self.assertListEqual(\n-                [layer_past_key_values[0].shape for layer_past_key_values in decoder_past_key_values],\n-                [expected_shape] * len(decoder_past_key_values),\n-            )\n-            self.assertListEqual(\n-                [layer_past_key_values[1].shape for layer_past_key_values in decoder_past_key_values],\n-                [expected_shape] * len(decoder_past_key_values),\n-            )\n+        self.assertListEqual(\n+            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+            [expected_shape] * len(decoder_past_key_values.key_cache),\n+        )\n+        self.assertListEqual(\n+            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n+            [expected_shape] * len(decoder_past_key_values.value_cache),\n+        )\n \n     def setUp(self):\n         self.model_tester = FalconH1ModelTester(self)"
        },
        {
            "sha": "c1627fc59f2f66765cb018dc385048027d491823",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -342,6 +342,26 @@ class JambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     test_headmasking = False\n     test_pruning = False\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, HybridMambaAttentionDynamicCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        self.assertListEqual(\n+            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+            [expected_shape] * len(decoder_past_key_values.key_cache),\n+        )\n+        self.assertListEqual(\n+            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n+            [expected_shape] * len(decoder_past_key_values.value_cache),\n+        )\n+\n     def setUp(self):\n         self.model_tester = JambaModelTester(self)\n         self.config_tester = JambaConfigTester(self, config_class=JambaConfig, hidden_size=37)"
        },
        {
            "sha": "d58837cc0fbd75db9b0f27d9971b1349b02d5577",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -504,13 +504,7 @@ def test_stacked_causal_mask_static_cache(self):\n \n         # upgrade the model with StaticCache\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n-        past_key_values = StaticCache(\n-            config=self.model.config,\n-            max_batch_size=1,\n-            max_cache_len=max_cache_len,\n-            device=torch_device,\n-            dtype=self.model.dtype,\n-        )\n+        past_key_values = StaticCache(max_cache_len=max_cache_len, config=self.model.config)\n \n         padded_attention_mask = torch.nn.functional.pad(\n             input=mask_shared_prefix,\n@@ -552,13 +546,7 @@ def test_partial_stacked_causal_mask_static_cache(self):\n \n         # upgrade the model with StaticCache\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n-        past_key_values = StaticCache(\n-            config=self.model.config,\n-            max_batch_size=1,\n-            max_cache_len=max_cache_len,\n-            device=torch_device,\n-            dtype=self.model.dtype,\n-        )\n+        past_key_values = StaticCache(max_cache_len=max_cache_len, config=self.model.config)\n \n         # forward run for the first part of input\n         part_a = 3  # split point"
        },
        {
            "sha": "a0113dcb8eb71e7e9583de18d58d8fa24fbd128f",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -83,7 +83,7 @@ def setUp(self):\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n-    def test_model_17b_16e_fp16(self):\n+    def test_model_17b_16e_fp32(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): ['system\\n\\nYou are a helpful assistant.user\\n\\nWhat is shown in this image?assistant\\n\\nThe image shows a cow standing on a beach with a blue sky and a body of water in the background. The cow is brown with a white face'],"
        },
        {
            "sha": "99afab0843b26b0917b92e5ef30431728a65b964",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -694,6 +694,7 @@ def prepare_config_and_inputs(self):\n             vocab_size=self.vocab_size,\n             d_model=self.d_model,\n             decoder_layers=self.decoder_layers,\n+            num_hidden_layers=self.decoder_layers,\n             decoder_ffn_dim=self.decoder_ffn_dim,\n             encoder_attention_heads=self.encoder_attention_heads,\n             decoder_attention_heads=self.decoder_attention_heads,"
        },
        {
            "sha": "0a69d0ad062fe4359b22546c9fddc10c7b5a1439",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -591,6 +591,7 @@ def prepare_config_and_inputs(self):\n             vocab_size=self.vocab_size,\n             d_model=self.d_model,\n             decoder_layers=self.decoder_layers,\n+            num_hidden_layers=self.decoder_layers,\n             decoder_ffn_dim=self.decoder_ffn_dim,\n             encoder_attention_heads=self.encoder_attention_heads,\n             decoder_attention_heads=self.decoder_attention_heads,"
        },
        {
            "sha": "f56f03565b0e71ac17b577badfb24a13a802b029",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -320,8 +320,8 @@ def test_compile_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_text)\n \n         # Static Cache + compile\n-        forward_function = model.forward\n-        model.forward = torch.compile(forward_function, mode=\"reduce-overhead\", fullgraph=True)\n+        forward_function = model.__call__\n+        model.__call__ = torch.compile(forward_function, mode=\"reduce-overhead\", fullgraph=True)\n         generated_ids = model.generate(\n             **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n         )\n@@ -330,7 +330,7 @@ def test_compile_static_cache(self):\n \n         # Sliding Window Cache + compile\n         torch._dynamo.reset()\n-        model.forward = torch.compile(forward_function, mode=\"reduce-overhead\", fullgraph=True)\n+        model.__call__ = torch.compile(forward_function, mode=\"reduce-overhead\", fullgraph=True)\n         generated_ids = model.generate(\n             **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"sliding_window\"\n         )"
        },
        {
            "sha": "e1dd7676b348490e5daf9327ec84478b3adfae80",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -452,6 +452,7 @@ def prepare_config_and_inputs(self):\n             vocab_size=self.vocab_size,\n             d_model=self.d_model,\n             decoder_layers=self.decoder_layers,\n+            num_hidden_layers=self.decoder_layers,\n             decoder_ffn_dim=self.decoder_ffn_dim,\n             encoder_attention_heads=self.encoder_attention_heads,\n             decoder_attention_heads=self.decoder_attention_heads,"
        },
        {
            "sha": "f80015eeeb56703099b462a4971a432347a3cb3b",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -46,13 +46,7 @@ class Phi3MiniWithStaticCache(torch.nn.Module):\n         def __init__(self, model: Phi3ForCausalLM, batch_size: int, max_seq_len: int):\n             super().__init__()\n             self.model = model\n-            self.cache = StaticCache(\n-                config=model.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=max_seq_len,\n-                device=self.model.device,\n-                dtype=self.model.dtype,\n-            )\n+            self.cache = StaticCache(config=model.config, max_cache_len=max_seq_len)\n \n         def forward(\n             self,"
        },
        {
            "sha": "d53ca61733951fa8d0ba3fe21dbf535020876945",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -42,13 +42,7 @@ class PhimoeMiniWithStaticCache(torch.nn.Module):\n         def __init__(self, model: PhimoeForCausalLM, batch_size: int, max_seq_len: int):\n             super().__init__()\n             self.model = model\n-            self.cache = StaticCache(\n-                config=model.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=max_seq_len,\n-                device=self.model.device,\n-                dtype=self.model.dtype,\n-            )\n+            self.cache = StaticCache(config=model.config, max_cache_len=max_seq_len)\n \n         def forward(\n             self,"
        },
        {
            "sha": "431417f4c18bfa55f00ce762a24813273991b6fb",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -304,6 +304,26 @@ class ZambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     test_headmasking = False\n     test_pruning = False\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, ZambaHybridDynamicCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        self.assertListEqual(\n+            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+            [expected_shape] * len(decoder_past_key_values.key_cache),\n+        )\n+        self.assertListEqual(\n+            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n+            [expected_shape] * len(decoder_past_key_values.value_cache),\n+        )\n+\n     def setUp(self):\n         self.model_tester = ZambaModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=ZambaConfig, hidden_size=37)"
        },
        {
            "sha": "cb742707d713953155d1790729e44d45bec1a8d6",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -315,6 +315,26 @@ class Zamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     test_headmasking = False\n     test_pruning = False\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, Zamba2HybridDynamicCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        self.assertListEqual(\n+            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+            [expected_shape] * len(decoder_past_key_values.key_cache),\n+        )\n+        self.assertListEqual(\n+            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n+            [expected_shape] * len(decoder_past_key_values.value_cache),\n+        )\n+\n     def setUp(self):\n         self.model_tester = Zamba2ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Zamba2Config, hidden_size=37)"
        },
        {
            "sha": "2fbc4595f302fce070a587f10d20892bb85063f8",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -223,11 +223,7 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n \n         # Setup static KV cache for generation\n         past_key_values = StaticCache(\n-            config=self.quantized_model.config,\n-            max_batch_size=1,\n-            max_cache_len=seq_length + self.max_new_tokens + 1,\n-            device=torch_device,\n-            dtype=self.quantized_model.config._pre_quantization_dtype,\n+            config=self.quantized_model.config, max_cache_len=seq_length + self.max_new_tokens + 1\n         )\n \n         # Allocate token ids to be generated and copy prefix ids"
        },
        {
            "sha": "443b687d54a821ea5ae43de89fa9e563bd4d93f7",
            "filename": "tests/quantization/spqr_integration/test_spqr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -204,11 +204,7 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n \n         # Setup static KV cache for generation\n         past_key_values = StaticCache(\n-            config=self.quantized_model.config,\n-            max_batch_size=1,\n-            max_cache_len=seq_length + self.max_new_tokens + 1,\n-            device=torch_device,\n-            dtype=self.quantized_model.config._pre_quantization_dtype,\n+            config=self.quantized_model.config, max_cache_len=seq_length + self.max_new_tokens + 1\n         )\n \n         # Allocate token ids to be generated and copy prefix ids"
        },
        {
            "sha": "37d15452c7ed16315370f17bd5dfc843a55615ef",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 53,
            "deletions": 65,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc11a3cbb2c6cd96986519a144d4a22610fd8487/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=dc11a3cbb2c6cd96986519a144d4a22610fd8487",
            "patch": "@@ -49,12 +49,10 @@\n         DynamicCache,\n         Gemma2Config,\n         GenerationConfig,\n-        HQQQuantizedCacheProcessor,\n         HybridCache,\n         HybridChunkedCache,\n         LlamaConfig,\n         QuantizedCache,\n-        QuantoQuantizedCacheProcessor,\n         SlidingWindowCache,\n         StaticCache,\n         convert_and_export_with_cache,\n@@ -142,23 +140,23 @@ def _random_kvs(config):\n             return random_keys, random_values\n \n         mha_config = LlamaConfig(num_attention_heads=32)\n-        mha_static_cache = StaticCache(config=mha_config, max_batch_size=1, max_cache_len=10, device=torch_device)\n+        mha_static_cache = StaticCache(config=mha_config, max_cache_len=10)\n         cached_keys, cached_values = mha_static_cache.update(\n             *_random_kvs(mha_config), 0, cache_kwargs={\"cache_position\": torch.arange(1).to(torch_device)}\n         )\n         self.assertTrue(cached_keys.shape == (1, 32, 10, 128))\n         self.assertTrue(cached_values.shape == (1, 32, 10, 128))\n \n         gqa_config = LlamaConfig(num_attention_heads=32, num_key_value_heads=4)\n-        gqa_static_cache = StaticCache(config=gqa_config, max_batch_size=1, max_cache_len=10, device=torch_device)\n+        gqa_static_cache = StaticCache(config=gqa_config, max_cache_len=10)\n         cached_keys, cached_values = gqa_static_cache.update(\n             *_random_kvs(gqa_config), 0, cache_kwargs={\"cache_position\": torch.arange(1).to(torch_device)}\n         )\n         self.assertTrue(cached_keys.shape == (1, 4, 10, 128))\n         self.assertTrue(cached_values.shape == (1, 4, 10, 128))\n \n         mqa_config = LlamaConfig(num_attention_heads=32, num_key_value_heads=1)\n-        mqa_static_cache = StaticCache(config=mqa_config, max_batch_size=1, max_cache_len=10, device=torch_device)\n+        mqa_static_cache = StaticCache(config=mqa_config, max_cache_len=10)\n         cached_keys, cached_values = mqa_static_cache.update(\n             *_random_kvs(mqa_config), 0, cache_kwargs={\"cache_position\": torch.arange(1).to(torch_device)}\n         )\n@@ -294,20 +292,11 @@ def test_quantized_cache_generation(self, backend):\n         )\n \n         self.assertIsInstance(gen_out.past_key_values, QuantizedCache)\n-        processor = gen_out.past_key_values.cache_processor\n-        if backend == \"quanto\":\n-            self.assertIsInstance(processor, QuantoQuantizedCacheProcessor)\n-        elif backend == \"hqq\":\n-            self.assertIsInstance(processor, HQQQuantizedCacheProcessor)\n \n         decoded = self.tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         self.assertListEqual(decoded, expected_generation)\n \n-        self.assertTrue(len(processor._quantized_keys) > 0)\n-\n         # Check that something is actually quantized\n-        has_been_quantized = any((q[0] if isinstance(q, tuple) else q).numel() > 0 for q in processor._quantized_keys)\n-        self.assertTrue(has_been_quantized)\n \n     @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n     def test_cache_extra_left_padding(self, cache_implementation):\n@@ -360,7 +349,7 @@ def test_dynamic_cache_hard(self):\n \n         set_seed(0)\n         gen_out = model.generate(\n-            **inputs, do_sample=True, max_new_tokens=256, return_dict_in_generate=True, output_scores=True\n+            **inputs, do_sample=True, top_k=5, max_new_tokens=256, return_dict_in_generate=True, output_scores=True\n         )\n         decoded = tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         # sum of the scores for the generated tokens\n@@ -371,21 +360,21 @@ def test_dynamic_cache_hard(self):\n \n         EXPECTED_GENERATION = (\n             \"Here's everything I know about cats. Cats are mammals, they have four legs, they have a tail, they have \"\n-            \"a face with a nose, eyes, and mouth. They have fur, they have claws, and they have a body that is \"\n-            \"covered in fur. They are carnivores, so they eat meat. They are also very clean animals, they groom \"\n-            \"themselves. They have a lot of different breeds. Some are small, some are large. Some are friendly, \"\n-            \"some are not. They have a lot of different personalities. They can be very independent, or they can be \"\n-            \"very affectionate. They can be very playful, or they can be very lazy. They can be very intelligent, or \"\n-            \"they can be very silly. They have a lot of different behaviors. They can be very curious, or they can \"\n-            \"be very cautious. They can be very vocal, or they can be very quiet. They can be very social, or they \"\n-            \"can be very solitary. They can be very active, or they can be very inactive. They can be very \"\n-            \"affectionate, or they can be very aloof. They can be very playful, or they can be very lazy. They can \"\n-            \"be very intelligent, or they can be very silly. They have a lot of different behaviors. They can be \"\n-            \"very curious, or they can\"\n-        )\n-        EXPECTED_SCORE_SUM = 11017.4971\n+            \"a face with a nose, eyes, and mouth. They have fur, they have claws, and they have whiskers. They are \"\n+            \"usually small, but some are big. They are usually gray or black or white, but they can be many colors. \"\n+            \"They have a soft body, they are usually quiet, but they can be loud. They are good at catching mice, \"\n+            \"and they are good at climbing trees. They are often kept as pets, and they are often seen in homes. \"\n+            \"They are independent, but they can be affectionate with their owners. They have a keen sense of smell, \"\n+            \"and they can hear sounds that humans cannot hear. They have a good sense of balance, which helps them \"\n+            \"to jump and climb. They are also good at hunting, and they can be trained to do tricks. They are often \"\n+            \"used as pets, and they are also used in some jobs, like hunting or as service animals for people with \"\n+            \"disabilities. They have a long life span, and they can live for many years. They are also known for \"\n+            \"their agility and gracefulness. They are often associated with mystery and independence. They are also \"\n+            \"known for their ability to land on their feet when they fall. They\"\n+        )\n+        EXPECTED_SCORE_SUM = 10834.7919921875\n         self.assertEqual(decoded[0], EXPECTED_GENERATION)\n-        self.assertAlmostEqual(score_sum, EXPECTED_SCORE_SUM, places=2)\n+        self.assertAlmostEqual(score_sum.item(), EXPECTED_SCORE_SUM, places=2)\n         self.assertIsInstance(gen_out.past_key_values, DynamicCache)  # sanity check\n \n     @parameterized.expand([(\"eager\"), (\"sdpa\")])\n@@ -476,9 +465,7 @@ def test_cache_copy(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device, torch_dtype=torch.bfloat16)\n \n-        prompt_cache = StaticCache(\n-            config=model.config, max_batch_size=1, max_cache_len=1024, device=torch_device, dtype=torch.bfloat16\n-        )\n+        prompt_cache = StaticCache(config=model.config, max_cache_len=1024)\n \n         INITIAL_PROMPT = \"You are a helpful assistant. \"\n         inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(torch_device)\n@@ -498,11 +485,11 @@ def test_cache_copy(self):\n             responses.append(response)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTraveling is an \"\n-            \"enriching experience that broadens our horizons and allows us to explore the world beyond our comfort \"\n-            \"zones. Whether it's a short weekend getaway\",\n-            \"You are a helpful assistant. What is the capital of France?\\n\\n\\n## Response:Paris is the capital \"\n-            \"of France.\\n\\n\\n\\n\\n\\n\\n<|endoftext|>\",\n+            \"You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTraveling is a \"\n+            \"wonderful way to explore the world, learn about different cultures, and create unforgettable \"\n+            \"memories. Whether you're a seasoned traveler or someone\",\n+            \"You are a helpful assistant. What is the capital of France?\\n\\n\\n## Response:Paris is the capital\"\n+            \" of France.\\n\\n\\n\\nAs an AI, I am not a human being.\\n\\n\\n\\nThe Great Wall of China is\",\n         ]\n \n         self.assertEqual(responses, EXPECTED_DECODED_TEXT)\n@@ -899,12 +886,13 @@ def setUp(self):\n             head_dim=1,\n             hidden_size=1,\n             sliding_window=self.window_size,\n+            attention_chunk_size=self.window_size,\n             layer_types=[\"full_attention\"] * 1,  # Static cache by default\n         )\n \n     def test_static_cache_out_of_bounds(self):\n         \"\"\"Test StaticCache raises IndexError for out-of-bounds positions.\"\"\"\n-        static_cache = StaticCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        static_cache = StaticCache(config=self.config, max_cache_len=self.max_cache_len)\n         pos_out_of_bounds = torch.tensor([self.max_cache_len])  # Position >= max_cache_len\n \n         with self.assertRaises(IndexError):\n@@ -926,7 +914,7 @@ def test_static_cache(self):\n         update pos 3:  [1.0, 2.0, 3.0, 4.0]\n         \"\"\"\n         # Scenario 1: Fill up to near capacity\n-        static_cache = StaticCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        static_cache = StaticCache(config=self.config, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n         static_cache.update(key_states=prefill, value_states=prefill, layer_idx=0, cache_kwargs=None)\n         static_cache.update(\n@@ -968,19 +956,19 @@ def test_sliding_window_cache(self):\n         # Scenario 1: Update within window, no slide yet\n         config = copy.deepcopy(self.config)\n         config.layer_types = [\"sliding_attention\"] * config.num_hidden_layers\n-        sliding_cache = SlidingWindowCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n-        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        sliding_cache = SlidingWindowCache(config=config, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=prefill,\n             value_states=prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.arange(2)},\n         )\n         sliding_cache.update(\n             key_states=torch.tensor(3.0)[None, None, None, None],\n             value_states=torch.tensor(3.0)[None, None, None, None],\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([2]), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.tensor([2])},\n         )\n         self.assertEqual(\n             sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -989,19 +977,19 @@ def test_sliding_window_cache(self):\n         )\n \n         # Scenario 2: Update causing slide\n-        sliding_cache = SlidingWindowCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        sliding_cache = SlidingWindowCache(config=config, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=prefill,\n             value_states=prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.arange(4)},\n         )\n         sliding_cache.update(\n             key_states=torch.tensor(5.0)[None, None, None, None],\n             value_states=torch.tensor(5.0)[None, None, None, None],\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([4]), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.tensor([4])},\n         )\n         self.assertEqual(\n             sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -1010,13 +998,13 @@ def test_sliding_window_cache(self):\n         )\n \n         # Scenario 3: Long prompt handling\n-        sliding_cache = SlidingWindowCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        sliding_cache = SlidingWindowCache(config=config, max_cache_len=self.max_cache_len)\n         long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=long_prefill,\n             value_states=long_prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(6), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.arange(6)},\n         )\n         self.assertEqual(\n             sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -1038,13 +1026,13 @@ def test_hybrid_cache_static_mode(self):\n         config.layer_types = [\"full_attention\"] * config.num_hidden_layers\n \n         # Scenario 1\n-        hybrid_cache_static_mode = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n-        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        hybrid_cache_static_mode = HybridCache(config=config, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n         hybrid_cache_static_mode.update(\n             key_states=prefill,\n             value_states=prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(4)},\n+            cache_kwargs={\"cache_position\": torch.arange(2)},\n         )\n         hybrid_cache_static_mode.update(\n             key_states=torch.tensor(3.0)[None, None, None, None],\n@@ -1092,19 +1080,19 @@ def test_hybrid_cache_sliding_mode(self):\n         config = copy.deepcopy(self.config)\n         config.layer_types = [\"sliding_attention\"] * config.num_hidden_layers\n         # Scenario 1: Update within window, no slide yet\n-        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n-        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n         hybrid_cache.update(\n             key_states=prefill,\n             value_states=prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.arange(2)},\n         )\n         hybrid_cache.update(\n             key_states=torch.tensor(3.0)[None, None, None, None],\n             value_states=torch.tensor(3.0)[None, None, None, None],\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([2]), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.tensor([2])},\n         )\n         self.assertEqual(\n             hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -1113,19 +1101,19 @@ def test_hybrid_cache_sliding_mode(self):\n         )\n \n         # Scenario 2: Update causing first slide\n-        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n         hybrid_cache.update(\n             key_states=prefill,\n             value_states=prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.arange(4)},\n         )\n         hybrid_cache.update(\n             key_states=torch.tensor(5.0)[None, None, None, None],\n             value_states=torch.tensor(5.0)[None, None, None, None],\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([4]), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.tensor([4])},\n         )\n         self.assertEqual(\n             hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -1138,7 +1126,7 @@ def test_hybrid_cache_sliding_mode(self):\n             key_states=torch.tensor(6.0)[None, None, None, None],\n             value_states=torch.tensor(6.0)[None, None, None, None],\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([5]), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.tensor([5])},\n         )\n         self.assertEqual(\n             hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -1147,13 +1135,13 @@ def test_hybrid_cache_sliding_mode(self):\n         )\n \n         # Scenario 4: Long prompt handling\n-        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n         long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n         hybrid_cache.update(\n             key_states=long_prefill,\n             value_states=long_prefill,\n             layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(6), \"sliding_window\": self.window_size},\n+            cache_kwargs={\"cache_position\": torch.arange(6)},\n         )\n         self.assertEqual(\n             hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n@@ -1222,7 +1210,7 @@ def test_hybrid_cache(self):\n         config.num_hidden_layers = 2\n         config.layer_types = [\"full_attention\", \"sliding_attention\"]\n         config.sliding_window = 2\n-        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n \n         # Prefill both layers up to cache capacity\n         prefill_static = torch.tensor([1.0, 2.0, 3.0])[None, None, :, None]\n@@ -1324,9 +1312,9 @@ def test_hybrid_chunked_cache(self):\n         config = copy.deepcopy(self.config)\n         config.num_hidden_layers = 2\n         config.layer_types = [\"full_attention\", \"chunked_attention\"]\n-        config.sliding_window = 2\n+        config.attention_chunk_size = 2\n         max_cache_len = 4\n-        chunked_cache = HybridChunkedCache(config=config, max_batch_size=1, max_cache_len=max_cache_len)\n+        chunked_cache = HybridChunkedCache(config=config, max_cache_len=max_cache_len)\n \n         # 1) PREFILL (3 tokens > sliding_window)\n         prefill_static = torch.tensor([1.0, 2.0, 3.0])[None, None, :, None]\n@@ -1405,7 +1393,7 @@ def test_hybrid_chunked_cache_extra_cases(self):\n         config.num_hidden_layers = 1\n         config.layer_types = [\"chunked_attention\"]\n         config.sliding_window = 3\n-        cache = HybridChunkedCache(config, max_batch_size=1, max_cache_len=3)\n+        cache = HybridChunkedCache(config=config, max_cache_len=3)\n \n         # Step 0 : multi-token prefill\n         first_chunk = torch.tensor([10.0, 20.0])[None, None, :, None]  # L = 2"
        }
    ],
    "stats": {
        "total": 2228,
        "additions": 779,
        "deletions": 1449
    }
}