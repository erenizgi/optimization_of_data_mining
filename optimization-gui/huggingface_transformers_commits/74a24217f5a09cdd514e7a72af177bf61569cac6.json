{
    "author": "andrerom",
    "message": "Add bfloat16 support detection for MPS in is_torch_bf16_gpu_available() (#40458)\n\n* Add bfloat16 support detection for MPS (Apple Silicon) in is_torch_bf16_gpu_available\n\nbfloat16 seems to have been supported for a few years now in Metal and torch.mps.\r\n\r\nMake sure to allow it and not throw on bf16 usage with \"Your setup doesn't support bf16/gpu.\" from TrainingArguments.\n\n* Check bf16 support for MPS using torch method\n\nActually seems method exists: https://github.com/pytorch/pytorch/blob/5859edf1130cec5a021ace5d5b0e18144808f757/torch/_dynamo/device_interface.py#L519\r\n\r\nIt simply checks if you are on MacOs 14 or higher.\n\n* Document Metal emulation for bf16 support\n\nAdd note about Metal emulation for bf16 support on M1/M2.\n\n* Update bf16 support check for MPS backend\n\nis_bf16_supported() not exposed even if defined on MPSInterface, use same approach as in accelerate pr.\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "74a24217f5a09cdd514e7a72af177bf61569cac6",
    "files": [
        {
            "sha": "11c84566e3e8172f5390d37f42f0160dde2429f4",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/74a24217f5a09cdd514e7a72af177bf61569cac6/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/74a24217f5a09cdd514e7a72af177bf61569cac6/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=74a24217f5a09cdd514e7a72af177bf61569cac6",
            "patch": "@@ -636,6 +636,9 @@ def is_torch_bf16_gpu_available() -> bool:\n         return True\n     if is_torch_npu_available():\n         return torch.npu.is_bf16_supported()\n+    if is_torch_mps_available():\n+        # Note: Emulated in software by Metal using fp32 for hardware without native support (like M1/M2)\n+        return torch.backends.mps.is_macos_or_newer(14, 0)\n     return False\n \n "
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}