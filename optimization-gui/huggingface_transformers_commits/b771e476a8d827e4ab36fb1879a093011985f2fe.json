{
    "author": "gante",
    "message": "[CI] post-`GptOss` fixes for green CI (#39929)",
    "sha": "b771e476a8d827e4ab36fb1879a093011985f2fe",
    "files": [
        {
            "sha": "556b19f0114d9cec03d618109fd2a31306d84e4e",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -511,6 +511,8 @@\n         title: GPT2\n       - local: model_doc/gpt_bigcode\n         title: GPTBigCode\n+      - local: model_doc/gpt_oss\n+        title: GptOss\n       - local: model_doc/gptsan-japanese\n         title: GPTSAN Japanese\n       - local: model_doc/gpt-sw3\n@@ -617,8 +619,6 @@\n         title: OLMoE\n       - local: model_doc/open-llama\n         title: Open-Llama\n-      - local: model_doc/openai_moe\n-        title: OpenAIMoe\n       - local: model_doc/opt\n         title: OPT\n       - local: model_doc/pegasus"
        },
        {
            "sha": "e1f4940103c20a1ae3ed34fb8882246711b408e1",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -65,6 +65,10 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n \n [[autodoc]] HqqConfig\n \n+## Mxfp4Config\n+\n+[[autodoc]] Mxfp4Config\n+\n ## FbgemmFp8Config\n \n [[autodoc]] FbgemmFp8Config"
        },
        {
            "sha": "9b368bdc9ebe20cb807125246f7ed02fdc06887c",
            "filename": "docs/source/en/model_doc/gpt_oss.md",
            "status": "renamed",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -24,11 +24,11 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-# OpenAIMoE\n+# GptOss\n \n ## Overview\n \n-The OpenAIMoE model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.\n+The GptOss model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.\n <INSERT SHORT SUMMARY HERE>\n \n The abstract from the paper is the following:\n@@ -43,16 +43,16 @@ This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface\n The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).\n \n \n-## OpenAIMoeConfig\n+## GptOssConfig\n \n-[[autodoc]] OpenAIMoeConfig\n+[[autodoc]] GptOssConfig\n \n-## OpenAIMoeModel\n+## GptOssModel\n \n-[[autodoc]] OpenAIMoeModel\n+[[autodoc]] GptOssModel\n     - forward\n \n-## OpenAIMoeForCausalLM\n+## GptOssForCausalLM\n \n-[[autodoc]] OpenAIMoeForCausalLM\n+[[autodoc]] GptOssForCausalLM\n     - forward",
            "previous_filename": "docs/source/en/model_doc/openai_moe.md"
        },
        {
            "sha": "8fe6d2f1dc687a89eff090e29048609e71e1173e",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -40,7 +40,7 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.jetmoe.modeling_jetmoe.load_balancing_loss_func\n+# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n def load_balancing_loss_func(\n     gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n     num_experts: Optional[int] = None,"
        },
        {
            "sha": "34af2f2f2e54af2c25d8f404f019c0a5ac5caa0f",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -67,7 +67,7 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func with gate->router\n+# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func with gate->router\n def load_balancing_loss_func(\n     router_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n     num_experts: Optional[int] = None,"
        },
        {
            "sha": "9e0784152c4f0aac856c99f38d47d0cb16312766",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -50,7 +50,7 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n+# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n def load_balancing_loss_func(\n     gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n     num_experts: Optional[int] = None,"
        },
        {
            "sha": "d3b57883ef8218fe1bb221a3d7b5d63d9e0b3201",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -39,7 +39,7 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n+# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n def load_balancing_loss_func(\n     gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n     num_experts: Optional[int] = None,"
        },
        {
            "sha": "8510cd3e8aa891f98ff46b6ddbf3164243cfcd36",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -55,7 +55,7 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n+# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n def load_balancing_loss_func(\n     gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n     num_experts: Optional[int] = None,"
        },
        {
            "sha": "8a46795b47d630062f5a86076fc43b02e9c3b831",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -59,7 +59,6 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n def load_balancing_loss_func(\n     gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n     num_experts: Optional[int] = None,"
        },
        {
            "sha": "82d2b7000e2ab3a851828908aa4a7bda9de10ec2",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b771e476a8d827e4ab36fb1879a093011985f2fe/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b771e476a8d827e4ab36fb1879a093011985f2fe/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=b771e476a8d827e4ab36fb1879a093011985f2fe",
            "patch": "@@ -345,6 +345,8 @@\n         \"IdeficsConfig\": True,\n         \"IdeficsVisionConfig\": True,\n         \"IdeficsPerceiverConfig\": True,\n+        # TODO: @Arthur/Joao (`hidden_act` unused)\n+        \"GptOssConfig\": True,\n     }\n )\n "
        }
    ],
    "stats": {
        "total": 37,
        "additions": 21,
        "deletions": 16
    }
}