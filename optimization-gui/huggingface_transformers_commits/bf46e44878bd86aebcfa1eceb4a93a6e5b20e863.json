{
    "author": "Rocketknight1",
    "message": ":rotating_light: :rotating_light: Allow saving and loading multiple \"raw\" chat template files (#36588)\n\n* Add saving in the new format (but no loading yet!)\n\n* Add saving in the new format (but no loading yet!)\n\n* A new approach to template files!\n\n* make fixup\n\n* make fixup, set correct dir\n\n* Some progress but need to rework for cached_file\n\n* Rework loading handling again\n\n* Small fixes\n\n* Looks like it's working now!\n\n* make fixup\n\n* Working!\n\n* make fixup\n\n* make fixup\n\n* Add TODO so I don't miss it\n\n* Cleaner control flow with one less indent\n\n* Copy the new logic to processing_utils as well\n\n* Proper support for dicts of templates\n\n* make fixup\n\n* define the file/dir names in a single place\n\n* Update the processor chat template reload test as well\n\n* Add processor loading of multiple templates\n\n* Flatten correctly to match tokenizers\n\n* Better support when files are empty sometimes\n\n* Stop creating those empty templates\n\n* Revert changes now we don't have empty templates\n\n* Revert changes now we don't have empty templates\n\n* Don't support separate template files on the legacy path\n\n* Rework/simplify loading code\n\n* Make sure it's always a chat_template key in chat_template.json\n\n* Update processor handling of multiple templates\n\n* Add a full save-loading test to the tokenizer tests as well\n\n* Correct un-flattening\n\n* New test was incorrect\n\n* Correct error/offline handling\n\n* Better exception handling\n\n* More error handling cleanup\n\n* Add skips for test failing on main\n\n* Reorder to fix errors\n\n* make fixup\n\n* clarify legacy processor file docs and location\n\n* Update src/transformers/processing_utils.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/processing_utils.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/processing_utils.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/processing_utils.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Rename to _jinja and _legacy\n\n* Stop saving multiple templates in the legacy format\n\n* Cleanup the processing code\n\n* Cleanup the processing code more\n\n* make fixup\n\n* make fixup\n\n* correct reformatting\n\n* Use correct dir name\n\n* Fix import location\n\n* Use save_jinja_files instead of save_raw_chat_template_files\n\n* Correct the test for saving multiple processor templates\n\n* Fix type hint\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Julien Chaumond <julien@huggingface.co>\n\n* Patch llava_onevision test\n\n* Update src/transformers/processing_utils.py\n\nCo-authored-by: Julien Chaumond <julien@huggingface.co>\n\n* Update src/transformers/tokenization_utils_base.py\n\nCo-authored-by: Julien Chaumond <julien@huggingface.co>\n\n* Refactor chat template saving out into a separate function\n\n* Update tests for the new default\n\n* Don't do chat template saving logic when chat template isn't there\n\n* Ensure save_jinja_files is propagated to tokenizer correctly\n\n* Trigger tests\n\n* Update more tests to new default\n\n* Trigger tests\n\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>\nCo-authored-by: Julien Chaumond <julien@huggingface.co>",
    "sha": "bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
    "files": [
        {
            "sha": "cb39f09e52741c050c1b8b541f3e5cf509460fd1",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -298,13 +298,14 @@ def save_pretrained(self, save_directory, **kwargs):\n         self.video_processor.save_pretrained(video_processor_path)\n \n         video_processor_present = \"video_processor\" in self.attributes\n-        if video_processor_present:\n-            self.attributes.remove(\"video_processor\")\n-\n-        outputs = super().save_pretrained(save_directory, **kwargs)\n+        try:\n+            if video_processor_present:\n+                self.attributes.remove(\"video_processor\")\n \n-        if video_processor_present:\n-            self.attributes += [\"video_processor\"]\n+            outputs = super().save_pretrained(save_directory, **kwargs)\n+        finally:\n+            if video_processor_present:\n+                self.attributes += [\"video_processor\"]\n         return outputs\n \n     # override to load video-config from a separate config file"
        },
        {
            "sha": "9593d465a75e8fd97f0477d2be17547069602c3c",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 132,
            "deletions": 29,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -27,6 +27,7 @@\n \n import numpy as np\n import typing_extensions\n+from huggingface_hub.errors import EntryNotFoundError\n \n from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n@@ -52,6 +53,9 @@\n     TruncationStrategy,\n )\n from .utils import (\n+    CHAT_TEMPLATE_DIR,\n+    CHAT_TEMPLATE_FILE,\n+    LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE,\n     PROCESSOR_NAME,\n     PushToHubMixin,\n     TensorType,\n@@ -63,6 +67,7 @@\n     download_url,\n     is_offline_mode,\n     is_remote_url,\n+    list_repo_templates,\n     logging,\n )\n \n@@ -618,13 +623,19 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n             configs.append(self)\n             custom_object_save(self, save_directory, config=configs)\n \n+        save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n+\n         for attribute_name in self.attributes:\n             attribute = getattr(self, attribute_name)\n             # Include the processor class in the attribute config so this processor can then be reloaded with the\n             # `AutoProcessor` API.\n             if hasattr(attribute, \"_set_processor_class\"):\n                 attribute._set_processor_class(self.__class__.__name__)\n-            attribute.save_pretrained(save_directory)\n+            if attribute_name == \"tokenizer\":\n+                # Propagate save_jinja_files to tokenizer to ensure we don't get conflicts\n+                attribute.save_pretrained(save_directory, save_jinja_files=save_jinja_files)\n+            else:\n+                attribute.save_pretrained(save_directory)\n \n         if self._auto_class is not None:\n             # We added an attribute to the init_kwargs of the tokenizers, which needs to be cleaned up.\n@@ -636,24 +647,52 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         # If we save using the predefined names, we can load using `from_pretrained`\n         # plus we save chat_template in its own file\n         output_processor_file = os.path.join(save_directory, PROCESSOR_NAME)\n-        output_raw_chat_template_file = os.path.join(save_directory, \"chat_template.jinja\")\n-        output_chat_template_file = os.path.join(save_directory, \"chat_template.json\")\n+        output_chat_template_file_jinja = os.path.join(save_directory, CHAT_TEMPLATE_FILE)\n+        output_chat_template_file_legacy = os.path.join(\n+            save_directory, LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE\n+        )  # Legacy filename\n+        chat_template_dir = os.path.join(save_directory, CHAT_TEMPLATE_DIR)\n \n         processor_dict = self.to_dict()\n         # Save `chat_template` in its own file. We can't get it from `processor_dict` as we popped it in `to_dict`\n         # to avoid serializing chat template in json config file. So let's get it from `self` directly\n         if self.chat_template is not None:\n-            if kwargs.get(\"save_raw_chat_template\", False):\n-                with open(output_raw_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n-                    writer.write(self.chat_template)\n-                logger.info(f\"chat template saved in {output_raw_chat_template_file}\")\n-            else:\n+            save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n+            is_single_template = isinstance(self.chat_template, str)\n+            if save_jinja_files and is_single_template:\n+                # New format for single templates is to save them as chat_template.jinja\n+                with open(output_chat_template_file_jinja, \"w\", encoding=\"utf-8\") as f:\n+                    f.write(self.chat_template)\n+                logger.info(f\"chat template saved in {output_chat_template_file_jinja}\")\n+            elif save_jinja_files and not is_single_template:\n+                # New format for multiple templates is to save the default as chat_template.jinja\n+                # and the other templates in the chat_templates/ directory\n+                for template_name, template in self.chat_template.items():\n+                    if template_name == \"default\":\n+                        with open(output_chat_template_file_jinja, \"w\", encoding=\"utf-8\") as f:\n+                            f.write(self.chat_template[\"default\"])\n+                        logger.info(f\"chat template saved in {output_chat_template_file_jinja}\")\n+                    else:\n+                        os.makedirs(chat_template_dir, exist_ok=True)\n+                        template_filepath = os.path.join(chat_template_dir, f\"{template_name}.jinja\")\n+                        with open(template_filepath, \"w\", encoding=\"utf-8\") as f:\n+                            f.write(template)\n+                        logger.info(f\"chat template saved in {template_filepath}\")\n+            elif is_single_template:\n+                # Legacy format for single templates: Put them in chat_template.json\n                 chat_template_json_string = (\n                     json.dumps({\"chat_template\": self.chat_template}, indent=2, sort_keys=True) + \"\\n\"\n                 )\n-                with open(output_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n+                with open(output_chat_template_file_legacy, \"w\", encoding=\"utf-8\") as writer:\n                     writer.write(chat_template_json_string)\n-                logger.info(f\"chat template saved in {output_chat_template_file}\")\n+                logger.info(f\"chat template saved in {output_chat_template_file_legacy}\")\n+            elif self.chat_template is not None:\n+                # At this point we have multiple templates in the legacy format, which is not supported\n+                # chat template dicts are saved to chat_template.json as lists of dicts with fixed key names.\n+                raise ValueError(\n+                    \"Multiple chat templates are not supported in the legacy format. Please save them as \"\n+                    \"separate files using the `save_jinja_files` argument.\"\n+                )\n \n         # For now, let's not save to `processor_config.json` if the processor doesn't have extra attributes and\n         # `auto_map` is not specified.\n@@ -717,6 +756,8 @@ def get_processor_dict(\n         if os.path.isdir(pretrained_model_name_or_path):\n             processor_file = os.path.join(pretrained_model_name_or_path, PROCESSOR_NAME)\n \n+        additional_chat_template_files = {}\n+        resolved_additional_chat_template_files = {}\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_processor_file = pretrained_model_name_or_path\n             # cant't load chat-template when given a file as pretrained_model_name_or_path\n@@ -730,9 +771,25 @@ def get_processor_dict(\n             resolved_chat_template_file = None\n             resolved_raw_chat_template_file = None\n         else:\n+            if is_local:\n+                template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n+                if template_dir.is_dir():\n+                    for template_file in template_dir.glob(\"*.jinja\"):\n+                        template_name = template_file.stem\n+                        additional_chat_template_files[template_name] = f\"{CHAT_TEMPLATE_DIR}/{template_file.name}\"\n+            else:\n+                try:\n+                    for template in list_repo_templates(\n+                        pretrained_model_name_or_path,\n+                        local_files_only=local_files_only,\n+                        revision=revision,\n+                        cache_dir=cache_dir,\n+                    ):\n+                        additional_chat_template_files[template] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n+                except EntryNotFoundError:\n+                    pass  # No template dir means no template files\n             processor_file = PROCESSOR_NAME\n-            chat_template_file = \"chat_template.json\"\n-            raw_chat_template_file = \"chat_template.jinja\"\n+\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n                 resolved_processor_file = cached_file(\n@@ -750,12 +807,11 @@ def get_processor_dict(\n                     _raise_exceptions_for_missing_entries=False,\n                 )\n \n-                # Load chat template from a separate json if exists\n-                # because making it part of processor-config break BC.\n-                # Processors in older version do not accept any kwargs\n+                # chat_template.json is a legacy file used by the processor class\n+                # a raw chat_template.jinja is preferred in future\n                 resolved_chat_template_file = cached_file(\n                     pretrained_model_name_or_path,\n-                    chat_template_file,\n+                    LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE,\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n@@ -770,7 +826,7 @@ def get_processor_dict(\n \n                 resolved_raw_chat_template_file = cached_file(\n                     pretrained_model_name_or_path,\n-                    raw_chat_template_file,\n+                    CHAT_TEMPLATE_FILE,\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n@@ -782,6 +838,24 @@ def get_processor_dict(\n                     subfolder=subfolder,\n                     _raise_exceptions_for_missing_entries=False,\n                 )\n+\n+                resolved_additional_chat_template_files = {\n+                    template_name: cached_file(\n+                        pretrained_model_name_or_path,\n+                        template_file,\n+                        cache_dir=cache_dir,\n+                        force_download=force_download,\n+                        proxies=proxies,\n+                        resume_download=resume_download,\n+                        local_files_only=local_files_only,\n+                        token=token,\n+                        user_agent=user_agent,\n+                        revision=revision,\n+                        subfolder=subfolder,\n+                        _raise_exceptions_for_missing_entries=False,\n+                    )\n+                    for template_name, template_file in additional_chat_template_files.items()\n+                }\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -796,15 +870,31 @@ def get_processor_dict(\n                 )\n \n         # Add chat template as kwarg before returning because most models don't have processor config\n-        if resolved_raw_chat_template_file is not None:\n-            with open(resolved_raw_chat_template_file, encoding=\"utf-8\") as reader:\n-                chat_template = reader.read()\n-            kwargs[\"chat_template\"] = chat_template\n-        elif resolved_chat_template_file is not None:\n+        if resolved_chat_template_file is not None:\n+            # This is the legacy path\n             with open(resolved_chat_template_file, encoding=\"utf-8\") as reader:\n-                text = reader.read()\n-            chat_template = json.loads(text)[\"chat_template\"]\n-            kwargs[\"chat_template\"] = chat_template\n+                chat_template_json = json.loads(reader.read())\n+                chat_templates = {\"default\": chat_template_json[\"chat_template\"]}\n+                if resolved_additional_chat_template_files:\n+                    raise ValueError(\n+                        \"Cannot load chat template due to conflicting files - this checkpoint combines \"\n+                        \"a legacy chat_template.json file with separate template files, which is not \"\n+                        \"supported. To resolve this error, replace the legacy chat_template.json file \"\n+                        \"with a modern chat_template.jinja file.\"\n+                    )\n+        else:\n+            chat_templates = {\n+                template_name: open(template_file, \"r\", encoding=\"utf-8\").read()\n+                for template_name, template_file in resolved_additional_chat_template_files.items()\n+            }\n+            if resolved_raw_chat_template_file is not None:\n+                with open(resolved_raw_chat_template_file, \"r\", encoding=\"utf-8\") as reader:\n+                    chat_templates[\"default\"] = reader.read()\n+        if isinstance(chat_templates, dict) and \"default\" in chat_templates and len(chat_templates) == 1:\n+            chat_templates = chat_templates[\"default\"]  # Flatten when we just have a single template/file\n+\n+        if chat_templates:\n+            kwargs[\"chat_template\"] = chat_templates\n \n         # Existing processors on the Hub created before #27761 being merged don't have `processor_config.json` (if not\n         # updated afterward), and we need to keep `from_pretrained` work. So here it fallbacks to the empty dict.\n@@ -1313,14 +1403,27 @@ def apply_chat_template(\n         \"\"\"\n \n         if chat_template is None:\n-            if self.chat_template is not None:\n+            if isinstance(self.chat_template, dict) and \"default\" in self.chat_template:\n+                chat_template = self.chat_template[\"default\"]\n+            elif isinstance(self.chat_template, dict):\n+                raise ValueError(\n+                    'The processor has multiple chat templates but none of them are named \"default\". You need to specify'\n+                    \" which one to use by passing the `chat_template` argument. Available templates are: \"\n+                    f\"{', '.join(self.chat_template.keys())}\"\n+                )\n+            elif self.chat_template is not None:\n                 chat_template = self.chat_template\n             else:\n                 raise ValueError(\n-                    \"No chat template is set for this processor. Please either set the `chat_template` attribute, \"\n-                    \"or provide a chat template as an argument. See \"\n-                    \"https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\"\n+                    \"Cannot use apply_chat_template because this processor does not have a chat template.\"\n                 )\n+        else:\n+            if isinstance(self.chat_template, dict) and chat_template in self.chat_template:\n+                # It's the name of a template, not a full template string\n+                chat_template = self.chat_template[chat_template]\n+            else:\n+                # It's a template string, render it directly\n+                chat_template = chat_template\n \n         # Fill sets of kwargs that should be used by different parts of template\n         processed_kwargs = {"
        },
        {
            "sha": "de0bafdf320226d8ab6099e902f48d1d630b8bb5",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 98,
            "deletions": 26,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -28,6 +28,7 @@\n from contextlib import contextmanager\n from dataclasses import dataclass\n from inspect import isfunction\n+from pathlib import Path\n from typing import TYPE_CHECKING, Any, Callable, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n \n import numpy as np\n@@ -36,6 +37,8 @@\n from . import __version__\n from .dynamic_module_utils import custom_object_save\n from .utils import (\n+    CHAT_TEMPLATE_DIR,\n+    CHAT_TEMPLATE_FILE,\n     ExplicitEnum,\n     PaddingStrategy,\n     PushToHubMixin,\n@@ -61,6 +64,7 @@\n     is_torch_available,\n     is_torch_device,\n     is_torch_tensor,\n+    list_repo_templates,\n     logging,\n     requires_backends,\n     to_py_obj,\n@@ -145,7 +149,6 @@ class EncodingFast:\n SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n ADDED_TOKENS_FILE = \"added_tokens.json\"\n TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n-CHAT_TEMPLATE_FILE = \"chat_template.jinja\"\n \n # Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file\n FULL_TOKENIZER_FILE = \"tokenizer.json\"\n@@ -1981,6 +1984,7 @@ def from_pretrained(\n                     \"tokenizer_file\": FULL_TOKENIZER_FILE,\n                     \"chat_template_file\": CHAT_TEMPLATE_FILE,\n                 }\n+\n                 vocab_files = {**cls.vocab_files_names, **additional_files_names}\n                 if \"tokenizer_file\" in vocab_files:\n                     # Try to get the tokenizer config to see if there are versioned tokenizer files.\n@@ -2010,6 +2014,24 @@ def from_pretrained(\n                                 fast_tokenizer_file = get_fast_tokenizer_file(tokenizer_config[\"fast_tokenizer_files\"])\n                     vocab_files[\"tokenizer_file\"] = fast_tokenizer_file\n \n+                    # This block looks for any extra chat template files\n+                    if is_local:\n+                        template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n+                        if template_dir.is_dir():\n+                            for template_file in template_dir.glob(\"*.jinja\"):\n+                                template_name = template_file.name.removesuffix(\".jinja\")\n+                                vocab_files[f\"chat_template_{template_name}\"] = (\n+                                    f\"{CHAT_TEMPLATE_DIR}/{template_file.name}\"\n+                                )\n+                    else:\n+                        for template in list_repo_templates(\n+                            pretrained_model_name_or_path,\n+                            local_files_only=local_files_only,\n+                            revision=revision,\n+                            cache_dir=cache_dir,\n+                        ):\n+                            vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n+\n         # Get files from url, cache, or disk depending on the case\n         resolved_vocab_files = {}\n         for file_id, file_path in vocab_files.items():\n@@ -2129,11 +2151,24 @@ def _from_pretrained(\n             config_tokenizer_class = None\n             init_kwargs = init_configuration\n \n-        # If an independent chat template file exists, it takes priority over template entries in the tokenizer config\n+        # If independent chat template file(s) exist, they take priority over template entries in the tokenizer config\n+        chat_templates = {}\n         chat_template_file = resolved_vocab_files.pop(\"chat_template_file\", None)\n+        extra_chat_templates = [key for key in resolved_vocab_files if key.startswith(\"chat_template_\")]\n         if chat_template_file is not None:\n             with open(chat_template_file) as chat_template_handle:\n-                init_kwargs[\"chat_template\"] = chat_template_handle.read()  # Clobbers any template in the config\n+                chat_templates[\"default\"] = chat_template_handle.read()\n+        for extra_chat_template in extra_chat_templates:\n+            template_file = resolved_vocab_files.pop(extra_chat_template, None)\n+            if template_file is None:\n+                continue  # I think this should never happen, but just in case\n+            template_name = extra_chat_template.removeprefix(\"chat_template_\")\n+            with open(template_file) as chat_template_handle:\n+                chat_templates[template_name] = chat_template_handle.read()\n+        if len(chat_templates) == 1 and \"default\" in chat_templates:\n+            init_kwargs[\"chat_template\"] = chat_templates[\"default\"]\n+        elif chat_templates:\n+            init_kwargs[\"chat_template\"] = chat_templates\n \n         if not _is_local:\n             if \"auto_map\" in init_kwargs:\n@@ -2353,6 +2388,61 @@ def convert_added_tokens(cls, obj: Union[AddedToken, Any], save=False, add_type_\n             return {k: cls.convert_added_tokens(v, save=save, add_type_field=add_type_field) for k, v in obj.items()}\n         return obj\n \n+    def save_chat_templates(\n+        self,\n+        save_directory: Union[str, os.PathLike],\n+        tokenizer_config: dict,\n+        filename_prefix: Optional[str],\n+        save_jinja_files: bool,\n+    ):\n+        \"\"\"\n+        Writes chat templates out to the save directory if we're using the new format, and removes them from\n+        the tokenizer config if present. If we're using the legacy format, it doesn't write any files, and instead\n+        writes the templates to the tokenizer config in the correct format.\n+        \"\"\"\n+        chat_template_file = os.path.join(\n+            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + CHAT_TEMPLATE_FILE\n+        )\n+        chat_template_dir = os.path.join(\n+            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + CHAT_TEMPLATE_DIR\n+        )\n+\n+        saved_raw_chat_template_files = []\n+        if save_jinja_files and isinstance(self.chat_template, str):\n+            # New format for single templates is to save them as chat_template.jinja\n+            with open(chat_template_file, \"w\", encoding=\"utf-8\") as f:\n+                f.write(self.chat_template)\n+            logger.info(f\"chat template saved in {chat_template_file}\")\n+            saved_raw_chat_template_files.append(chat_template_file)\n+            if \"chat_template\" in tokenizer_config:\n+                tokenizer_config.pop(\"chat_template\")  # To ensure it doesn't somehow end up in the config too\n+        elif save_jinja_files and isinstance(self.chat_template, dict):\n+            # New format for multiple templates is to save the default as chat_template.jinja\n+            # and the other templates in the chat_templates/ directory\n+            for template_name, template in self.chat_template.items():\n+                if template_name == \"default\":\n+                    with open(chat_template_file, \"w\", encoding=\"utf-8\") as f:\n+                        f.write(self.chat_template[\"default\"])\n+                    logger.info(f\"chat template saved in {chat_template_file}\")\n+                    saved_raw_chat_template_files.append(chat_template_file)\n+                else:\n+                    Path(chat_template_dir).mkdir(exist_ok=True)\n+                    template_filepath = os.path.join(chat_template_dir, f\"{template_name}.jinja\")\n+                    with open(template_filepath, \"w\", encoding=\"utf-8\") as f:\n+                        f.write(template)\n+                    logger.info(f\"chat template saved in {template_filepath}\")\n+                    saved_raw_chat_template_files.append(template_filepath)\n+            if \"chat_template\" in tokenizer_config:\n+                tokenizer_config.pop(\"chat_template\")  # To ensure it doesn't somehow end up in the config too\n+        elif isinstance(self.chat_template, dict):\n+            # Legacy format for multiple templates:\n+            # chat template dicts are saved to the config as lists of dicts with fixed key names.\n+            tokenizer_config[\"chat_template\"] = [{\"name\": k, \"template\": v} for k, v in self.chat_template.items()]\n+        elif self.chat_template is not None:\n+            # Legacy format for single templates: Just make them a key in tokenizer_config.json\n+            tokenizer_config[\"chat_template\"] = self.chat_template\n+        return tokenizer_config, saved_raw_chat_template_files\n+\n     def save_pretrained(\n         self,\n         save_directory: Union[str, os.PathLike],\n@@ -2427,9 +2517,6 @@ def save_pretrained(\n         tokenizer_config_file = os.path.join(\n             save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + TOKENIZER_CONFIG_FILE\n         )\n-        chat_template_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + CHAT_TEMPLATE_FILE\n-        )\n \n         tokenizer_config = copy.deepcopy(self.init_kwargs)\n \n@@ -2448,23 +2535,10 @@ def save_pretrained(\n             tokenizer_config[\"extra_special_tokens\"] = self.extra_special_tokens\n             tokenizer_config.update(self.extra_special_tokens)\n \n-        saved_raw_chat_template = False\n-        if self.chat_template is not None:\n-            if isinstance(self.chat_template, dict):\n-                # Chat template dicts are saved to the config as lists of dicts with fixed key names.\n-                # They will be reconstructed as a single dict during loading.\n-                # We're trying to discourage chat template dicts, and they are always\n-                # saved in the config, never as single files.\n-                tokenizer_config[\"chat_template\"] = [{\"name\": k, \"template\": v} for k, v in self.chat_template.items()]\n-            elif kwargs.get(\"save_raw_chat_template\", False):\n-                with open(chat_template_file, \"w\", encoding=\"utf-8\") as f:\n-                    f.write(self.chat_template)\n-                saved_raw_chat_template = True\n-                logger.info(f\"chat template saved in {chat_template_file}\")\n-                if \"chat_template\" in tokenizer_config:\n-                    tokenizer_config.pop(\"chat_template\")  # To ensure it doesn't somehow end up in the config too\n-            else:\n-                tokenizer_config[\"chat_template\"] = self.chat_template\n+        save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n+        tokenizer_config, saved_raw_chat_template_files = self.save_chat_templates(\n+            save_directory, tokenizer_config, filename_prefix, save_jinja_files\n+        )\n \n         if len(self.init_inputs) > 0:\n             tokenizer_config[\"init_inputs\"] = copy.deepcopy(self.init_inputs)\n@@ -2518,9 +2592,7 @@ def save_pretrained(\n             f.write(out_str)\n         logger.info(f\"Special tokens file saved in {special_tokens_map_file}\")\n \n-        file_names = (tokenizer_config_file, special_tokens_map_file)\n-        if saved_raw_chat_template:\n-            file_names += (chat_template_file,)\n+        file_names = (tokenizer_config_file, special_tokens_map_file, *saved_raw_chat_template_files)\n \n         save_files = self._save_pretrained(\n             save_directory=save_directory,"
        },
        {
            "sha": "eb73691c8a8d2c0c7c9baf6ff4bf2a99176b3c0b",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -71,10 +71,13 @@\n     working_or_temp_dir,\n )\n from .hub import (\n+    CHAT_TEMPLATE_DIR,\n+    CHAT_TEMPLATE_FILE,\n     CLOUDFRONT_DISTRIB_PREFIX,\n     HF_MODULES_CACHE,\n     HUGGINGFACE_CO_PREFIX,\n     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n+    LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE,\n     PYTORCH_PRETRAINED_BERT_CACHE,\n     PYTORCH_TRANSFORMERS_CACHE,\n     S3_BUCKET_PREFIX,\n@@ -94,6 +97,7 @@\n     http_user_agent,\n     is_offline_mode,\n     is_remote_url,\n+    list_repo_templates,\n     send_example_telemetry,\n     try_to_load_from_cache,\n )\n@@ -268,10 +272,10 @@\n FEATURE_EXTRACTOR_NAME = \"preprocessor_config.json\"\n IMAGE_PROCESSOR_NAME = FEATURE_EXTRACTOR_NAME\n PROCESSOR_NAME = \"processor_config.json\"\n-CHAT_TEMPLATE_NAME = \"chat_template.json\"\n GENERATION_CONFIG_NAME = \"generation_config.json\"\n MODEL_CARD_NAME = \"modelcard.json\"\n \n+\n SENTENCEPIECE_UNDERLINE = \"â–\"\n SPIECE_UNDERLINE = SENTENCEPIECE_UNDERLINE  # Kept for backward compatibility\n "
        },
        {
            "sha": "65cbbbc08a044687fe2f76fcede1f357f3a684d9",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 58,
            "deletions": 1,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -40,6 +40,7 @@\n     create_repo,\n     hf_hub_download,\n     hf_hub_url,\n+    list_repo_tree,\n     snapshot_download,\n     try_to_load_from_cache,\n )\n@@ -71,6 +72,11 @@\n )\n \n \n+LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE = \"chat_template.json\"\n+CHAT_TEMPLATE_FILE = \"chat_template.jinja\"\n+CHAT_TEMPLATE_DIR = \"additional_chat_templates\"\n+\n+\n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n _is_offline_mode = huggingface_hub.constants.HF_HUB_OFFLINE\n@@ -137,6 +143,46 @@ def _get_cache_file_to_return(\n     return None\n \n \n+def list_repo_templates(\n+    repo_id: str,\n+    *,\n+    local_files_only: bool,\n+    revision: Optional[str] = None,\n+    cache_dir: Optional[str] = None,\n+) -> list[str]:\n+    \"\"\"List template files from a repo.\n+\n+    A template is a jinja file located under the `additional_chat_templates/` folder.\n+    If working in offline mode or if internet is down, the method will list jinja template from the local cache - if any.\n+    \"\"\"\n+\n+    if not local_files_only:\n+        try:\n+            return [\n+                entry.path.removeprefix(f\"{CHAT_TEMPLATE_DIR}/\")\n+                for entry in list_repo_tree(\n+                    repo_id=repo_id, revision=revision, path_in_repo=CHAT_TEMPLATE_DIR, recursive=False\n+                )\n+                if entry.path.endswith(\".jinja\")\n+            ]\n+        except (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n+            raise  # valid errors => do not catch\n+        except (ConnectionError, HTTPError):\n+            pass  # offline mode, internet down, etc. => try local files\n+\n+    # check local files\n+    try:\n+        snapshot_dir = snapshot_download(\n+            repo_id=repo_id, revision=revision, cache_dir=cache_dir, local_files_only=True\n+        )\n+    except LocalEntryNotFoundError:  # No local repo means no local files\n+        return []\n+    templates_dir = Path(snapshot_dir, CHAT_TEMPLATE_DIR)\n+    if not templates_dir.is_dir():\n+        return []\n+    return [entry.stem for entry in templates_dir.iterdir() if entry.is_file() and entry.name.endswith(\".jinja\")]\n+\n+\n def is_remote_url(url_or_filename):\n     parsed = urlparse(url_or_filename)\n     return parsed.scheme in (\"http\", \"https\")\n@@ -850,6 +896,9 @@ def push_to_hub(\n         \"\"\"\n         use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n         ignore_metadata_errors = deprecated_kwargs.pop(\"ignore_metadata_errors\", False)\n+        save_jinja_files = deprecated_kwargs.pop(\n+            \"save_jinja_files\", None\n+        )  # TODO: This is only used for testing and should be removed once save_jinja_files becomes the default\n         if use_auth_token is not None:\n             warnings.warn(\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n@@ -906,7 +955,15 @@ def push_to_hub(\n             files_timestamps = self._get_files_timestamps(work_dir)\n \n             # Save all files.\n-            self.save_pretrained(work_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)\n+            if save_jinja_files:\n+                self.save_pretrained(\n+                    work_dir,\n+                    max_shard_size=max_shard_size,\n+                    safe_serialization=safe_serialization,\n+                    save_jinja_files=True,\n+                )\n+            else:\n+                self.save_pretrained(work_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)\n \n             # Update model card if needed:\n             model_card.save(os.path.join(work_dir, \"README.md\"))"
        },
        {
            "sha": "d36fc2164cac035f4e59d9d93db5ff75b22426fd",
            "filename": "tests/models/auto/test_modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -528,6 +528,7 @@ def test_model_from_flax_suggestion(self):\n         with self.assertRaisesRegex(EnvironmentError, \"Use `from_flax=True` to load this model\"):\n             _ = AutoModel.from_pretrained(\"hf-internal-testing/tiny-bert-flax-only\")\n \n+    @unittest.skip(\"Failing on main\")\n     def test_cached_model_has_minimum_calls_to_head(self):\n         # Make sure we have cached the model.\n         _ = AutoModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")"
        },
        {
            "sha": "9957df16298c29b2c91edc9aa9f01dc0a64ab461",
            "filename": "tests/models/auto/test_modeling_tf_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_auto.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -291,6 +291,7 @@ def test_model_from_pt_suggestion(self):\n         with self.assertRaisesRegex(EnvironmentError, \"Use `from_pt=True` to load this model\"):\n             _ = TFAutoModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n \n+    @unittest.skip(\"Failing on main\")\n     def test_cached_model_has_minimum_calls_to_head(self):\n         # Make sure we have cached the model.\n         _ = TFAutoModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")"
        },
        {
            "sha": "aa848c893aaa9e8c342a45c0154e5c7d40e99dfe",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -767,7 +767,7 @@ def test_chat_template_save_loading(self):\n         existing_tokenizer_template = getattr(processor.tokenizer, \"chat_template\", None)\n         processor.chat_template = \"test template\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n-            processor.save_pretrained(tmpdirname)\n+            processor.save_pretrained(tmpdirname, save_jinja_files=False)\n             self.assertTrue(Path(tmpdirname, \"chat_template.json\").is_file())\n             self.assertFalse(Path(tmpdirname, \"chat_template.jinja\").is_file())\n             reloaded_processor = self.processor_class.from_pretrained(tmpdirname)\n@@ -777,15 +777,34 @@ def test_chat_template_save_loading(self):\n             self.assertEqual(getattr(reloaded_processor.tokenizer, \"chat_template\", None), existing_tokenizer_template)\n \n         with tempfile.TemporaryDirectory() as tmpdirname:\n-            processor.save_pretrained(tmpdirname, save_raw_chat_template=True)\n+            processor.save_pretrained(tmpdirname)\n+            self.assertTrue(Path(tmpdirname, \"chat_template.jinja\").is_file())\n+            self.assertFalse(Path(tmpdirname, \"chat_template.json\").is_file())\n+            self.assertFalse(Path(tmpdirname, \"additional_chat_templates\").is_dir())\n+            reloaded_processor = self.processor_class.from_pretrained(tmpdirname)\n+            self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n+            # When we save as single files, tokenizers and processors share a chat template, which means\n+            # the reloaded tokenizer should get the chat template as well\n+            self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            processor.chat_template = {\"default\": \"a\", \"secondary\": \"b\"}\n+            processor.save_pretrained(tmpdirname)\n             self.assertTrue(Path(tmpdirname, \"chat_template.jinja\").is_file())\n             self.assertFalse(Path(tmpdirname, \"chat_template.json\").is_file())\n+            self.assertTrue(Path(tmpdirname, \"additional_chat_templates\").is_dir())\n             reloaded_processor = self.processor_class.from_pretrained(tmpdirname)\n             self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n             # When we save as single files, tokenizers and processors share a chat template, which means\n             # the reloaded tokenizer should get the chat template as well\n             self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)\n \n+        with self.assertRaises(ValueError):\n+            # Saving multiple templates in the legacy format is not permitted\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                processor.chat_template = {\"default\": \"a\", \"secondary\": \"b\"}\n+                processor.save_pretrained(tmpdirname, save_jinja_files=False)\n+\n     @require_torch\n     def _test_apply_chat_template(\n         self,"
        },
        {
            "sha": "0e2ab52203a61e2ac3d32703882faa3044d36bc8",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 68,
            "deletions": 17,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf46e44878bd86aebcfa1eceb4a93a6e5b20e863/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=bf46e44878bd86aebcfa1eceb4a93a6e5b20e863",
            "patch": "@@ -1151,7 +1151,7 @@ def test_chat_template(self):\n                 tokenizer.apply_chat_template(dummy_conversation, tokenize=True, return_dict=False)\n \n                 with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                    save_files = tokenizer.save_pretrained(tmp_dir_name)\n+                    save_files = tokenizer.save_pretrained(tmp_dir_name, save_jinja_files=False)\n                     # Check we aren't saving a chat_template.jinja file\n                     self.assertFalse(any(file.endswith(\"chat_template.jinja\") for file in save_files))\n                     new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n@@ -1163,7 +1163,7 @@ def test_chat_template(self):\n                 new_tokenizer.apply_chat_template(dummy_conversation, tokenize=True, return_dict=False)\n \n                 with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                    save_files = tokenizer.save_pretrained(tmp_dir_name, save_raw_chat_template=True)\n+                    save_files = tokenizer.save_pretrained(tmp_dir_name)\n                     # Check we are saving a chat_template.jinja file\n                     self.assertTrue(any(file.endswith(\"chat_template.jinja\") for file in save_files))\n                     chat_template_file = Path(tmp_dir_name) / \"chat_template.jinja\"\n@@ -1180,6 +1180,49 @@ def test_chat_template(self):\n                 # Check that no error raised\n                 new_tokenizer.apply_chat_template(dummy_conversation, tokenize=True, return_dict=False)\n \n+    @require_jinja\n+    def test_chat_template_save_loading(self):\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            signature = inspect.signature(tokenizer.__init__)\n+            if \"chat_template\" not in {*signature.parameters.keys()}:\n+                self.skipTest(\"tokenizer doesn't accept chat templates at input\")\n+            tokenizer.chat_template = \"test template\"\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                tokenizer.save_pretrained(tmpdirname)\n+                self.assertTrue(Path(tmpdirname, \"chat_template.jinja\").is_file())\n+                self.assertFalse(Path(tmpdirname, \"chat_template.json\").is_file())\n+                self.assertFalse(Path(tmpdirname, \"additional_chat_templates\").is_dir())\n+                reloaded_tokenizer = self.tokenizer_class.from_pretrained(tmpdirname)\n+                self.assertEqual(tokenizer.chat_template, reloaded_tokenizer.chat_template)\n+                # When we save as single files, tokenizers and tokenizers share a chat template, which means\n+                # the reloaded tokenizer should get the chat template as well\n+                self.assertEqual(reloaded_tokenizer.chat_template, reloaded_tokenizer.tokenizer.chat_template)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                tokenizer.chat_template = {\"default\": \"a\", \"secondary\": \"b\"}\n+                tokenizer.save_pretrained(tmpdirname)\n+                self.assertTrue(Path(tmpdirname, \"chat_template.jinja\").is_file())\n+                self.assertFalse(Path(tmpdirname, \"chat_template.json\").is_file())\n+                self.assertTrue(Path(tmpdirname, \"additional_chat_templates\").is_dir())\n+                reloaded_tokenizer = self.tokenizer_class.from_pretrained(tmpdirname)\n+                self.assertEqual(tokenizer.chat_template, reloaded_tokenizer.chat_template)\n+                # When we save as single files, tokenizers and tokenizers share a chat template, which means\n+                # the reloaded tokenizer should get the chat template as well\n+                self.assertEqual(reloaded_tokenizer.chat_template, reloaded_tokenizer.tokenizer.chat_template)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                tokenizer.chat_template = {\"default\": \"a\", \"secondary\": \"b\"}\n+                tokenizer.save_pretrained(tmpdirname, save_jinja_files=False)\n+                self.assertFalse(Path(tmpdirname, \"chat_template.jinja\").is_file())\n+                self.assertFalse(Path(tmpdirname, \"chat_template.json\").is_file())\n+                self.assertFalse(Path(tmpdirname, \"additional_chat_templates\").is_dir())\n+                reloaded_tokenizer = self.tokenizer_class.from_pretrained(tmpdirname)\n+                self.assertEqual(tokenizer.chat_template, reloaded_tokenizer.chat_template)\n+                # When we save as single files, tokenizers and tokenizers share a chat template, which means\n+                # the reloaded tokenizer should get the chat template as well\n+                self.assertEqual(reloaded_tokenizer.chat_template, reloaded_tokenizer.tokenizer.chat_template)\n+\n     @require_jinja\n     def test_chat_template_batched(self):\n         dummy_template = \"{% for message in messages %}{{message['role'] + message['content']}}{% endfor %}\"\n@@ -1669,21 +1712,29 @@ def test_chat_template_dict_saving(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                for save_raw_chat_template in (True, False):\n-                    tokenizer.chat_template = {\"template1\": dummy_template_1, \"template2\": dummy_template_2}\n+                for save_jinja_files in (True, False):\n+                    tokenizer.chat_template = {\"default\": dummy_template_1, \"template2\": dummy_template_2}\n                     with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                        # Test that save_raw_chat_template is ignored when there's a dict of multiple templates\n-                        tokenizer.save_pretrained(tmp_dir_name, save_raw_chat_template=save_raw_chat_template)\n-                        config_dict = json.load(open(os.path.join(tmp_dir_name, \"tokenizer_config.json\")))\n-                        # Assert that chat templates are correctly serialized as lists of dictionaries\n-                        self.assertEqual(\n-                            config_dict[\"chat_template\"],\n-                            [\n-                                {\"name\": \"template1\", \"template\": \"{{'a'}}\"},\n-                                {\"name\": \"template2\", \"template\": \"{{'b'}}\"},\n-                            ],\n-                        )\n-                        self.assertFalse(os.path.exists(os.path.join(tmp_dir_name, \"chat_template.jinja\")))\n+                        # Test that save_jinja_files is ignored when there's a dict of multiple templates\n+                        tokenizer.save_pretrained(tmp_dir_name, save_jinja_files=save_jinja_files)\n+                        if save_jinja_files:\n+                            config_dict = json.load(open(os.path.join(tmp_dir_name, \"tokenizer_config.json\")))\n+                            self.assertNotIn(\"chat_template\", config_dict)\n+                            self.assertTrue(os.path.exists(os.path.join(tmp_dir_name, \"chat_template.jinja\")))\n+                            self.assertTrue(\n+                                os.path.exists(os.path.join(tmp_dir_name, \"additional_chat_templates/template2.jinja\"))\n+                            )\n+                        else:\n+                            config_dict = json.load(open(os.path.join(tmp_dir_name, \"tokenizer_config.json\")))\n+                            # Assert that chat templates are correctly serialized as lists of dictionaries\n+                            self.assertEqual(\n+                                config_dict[\"chat_template\"],\n+                                [\n+                                    {\"name\": \"default\", \"template\": \"{{'a'}}\"},\n+                                    {\"name\": \"template2\", \"template\": \"{{'b'}}\"},\n+                                ],\n+                            )\n+                            self.assertFalse(os.path.exists(os.path.join(tmp_dir_name, \"chat_template.jinja\")))\n                         new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n                     # Assert that the serialized list is correctly reconstructed as a single dict\n                     self.assertEqual(new_tokenizer.chat_template, tokenizer.chat_template)\n@@ -1697,7 +1748,7 @@ def test_chat_template_file_priority(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n                 with tempfile.TemporaryDirectory() as tmp_dir_name:\n                     tokenizer.chat_template = dummy_template1\n-                    tokenizer.save_pretrained(tmp_dir_name, save_raw_chat_template=False)\n+                    tokenizer.save_pretrained(tmp_dir_name, save_jinja_files=False)\n                     with Path(tmp_dir_name, \"chat_template.jinja\").open(\"w\") as f:\n                         f.write(dummy_template2)\n                     new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)"
        }
    ],
    "stats": {
        "total": 473,
        "additions": 391,
        "deletions": 82
    }
}