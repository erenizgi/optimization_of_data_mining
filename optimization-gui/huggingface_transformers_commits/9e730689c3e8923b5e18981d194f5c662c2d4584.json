{
    "author": "SunMarc",
    "message": "change XLA deprecated api (#37741)\n\n* deprecated api\n\n* fix",
    "sha": "9e730689c3e8923b5e18981d194f5c662c2d4584",
    "files": [
        {
            "sha": "568f2bfd7a54b1e46eacf6091631ebc0ce3e3ce0",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e730689c3e8923b5e18981d194f5c662c2d4584/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e730689c3e8923b5e18981d194f5c662c2d4584/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=9e730689c3e8923b5e18981d194f5c662c2d4584",
            "patch": "@@ -199,12 +199,12 @@\n if is_torch_xla_available():\n     import torch_xla.core.xla_model as xm\n     import torch_xla.debug.metrics as met\n+    import torch_xla.runtime as xr\n     from torch_xla import __version__ as XLA_VERSION\n \n     IS_XLA_FSDPV2_POST_2_2 = version.parse(XLA_VERSION) >= version.parse(XLA_FSDPV2_MIN_VERSION)\n     if IS_XLA_FSDPV2_POST_2_2:\n         import torch_xla.distributed.spmd as xs\n-        import torch_xla.runtime as xr\n else:\n     IS_XLA_FSDPV2_POST_2_2 = False\n \n@@ -1042,7 +1042,7 @@ def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.\n         if self.args.use_legacy_prediction_loop:\n             if is_torch_xla_available():\n                 return SequentialDistributedSampler(\n-                    eval_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n+                    eval_dataset, num_replicas=xr.world_size(), rank=xr.global_ordinal()\n                 )\n             elif is_sagemaker_mp_enabled():\n                 return SequentialDistributedSampler("
        },
        {
            "sha": "0fed4fb9041f51c16e6072186b1a47b2513d3c48",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e730689c3e8923b5e18981d194f5c662c2d4584/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e730689c3e8923b5e18981d194f5c662c2d4584/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=9e730689c3e8923b5e18981d194f5c662c2d4584",
            "patch": "@@ -52,7 +52,7 @@\n     logging.add_handler(StreamHandler(sys.stdout))\n \n if is_torch_xla_available():\n-    import torch_xla.core.xla_model as xm\n+    import torch_xla.runtime as xr\n \n if is_torch_available():\n     from torch.optim.lr_scheduler import LRScheduler\n@@ -398,9 +398,9 @@ def __len__(self):\n \n \n def get_tpu_sampler(dataset: torch.utils.data.Dataset, batch_size: int):\n-    if xm.xrt_world_size() <= 1:\n+    if xr.world_size() <= 1:\n         return RandomSampler(dataset)\n-    return DistributedSampler(dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal())\n+    return DistributedSampler(dataset, num_replicas=xr.world_size(), rank=xr.global_ordinal())\n \n \n def nested_new_like(arrays, num_samples, padding_index=-100):"
        },
        {
            "sha": "7b2d5c3432279ca98554ba43e61a1beb280ed5d6",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e730689c3e8923b5e18981d194f5c662c2d4584/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e730689c3e8923b5e18981d194f5c662c2d4584/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=9e730689c3e8923b5e18981d194f5c662c2d4584",
            "patch": "@@ -362,13 +362,13 @@ class HPSearchBackend(ExplicitEnum):\n \n def is_main_process(local_rank):\n     \"\"\"\n-    Whether or not the current process is the local process, based on `xm.get_ordinal()` (for TPUs) first, then on\n+    Whether or not the current process is the local process, based on `xr.global_ordinal()` (for TPUs) first, then on\n     `local_rank`.\n     \"\"\"\n     if is_torch_xla_available():\n-        import torch_xla.core.xla_model as xm\n+        import torch_xla.runtime as xr\n \n-        return xm.get_ordinal() == 0\n+        return xr.global_ordinal() == 0\n     return local_rank in [-1, 0]\n \n \n@@ -377,9 +377,9 @@ def total_processes_number(local_rank):\n     Return the number of processes launched in parallel. Works with `torch.distributed` and TPUs.\n     \"\"\"\n     if is_torch_xla_available():\n-        import torch_xla.core.xla_model as xm\n+        import torch_xla.runtime as xr\n \n-        return xm.xrt_world_size()\n+        return xr.world_size()\n     elif local_rank != -1 and is_torch_available():\n         import torch\n "
        }
    ],
    "stats": {
        "total": 20,
        "additions": 10,
        "deletions": 10
    }
}