{
    "author": "cyyever",
    "message": "Fix format of compressed_tensors.md (#41155)\n\n* Fix table format\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix format\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "6dc9ed87a02db8b4ecc26a5e98596cd2bba380b5",
    "files": [
        {
            "sha": "4f55f008aa8d5a7bd5dfb4ef59b213bab4431e63",
            "filename": "docs/source/en/quantization/compressed_tensors.md",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/6dc9ed87a02db8b4ecc26a5e98596cd2bba380b5/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6dc9ed87a02db8b4ecc26a5e98596cd2bba380b5/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md?ref=6dc9ed87a02db8b4ecc26a5e98596cd2bba380b5",
            "patch": "@@ -65,11 +65,11 @@ print(f\"{mem_params/2**30:.4f} GB\")\n \n ## Model checkpoint\n \n-compressed-tensor models are defined through its configuration entry. The following example is taken from the [nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf/blob/main/config.json) `config.json` file.\n+Compressed-tensor models are defined through its configuration entry. The following example is taken from the [nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf/blob/main/config.json) `config.json` file.\n \n There are a lot of entries to allow for flexible expression both during and after compression, but the entries for loading and inference can be simplified to focus on just a few key entries.\n \n-```yaml\n+```json\n \"quantization_config\": {\n   \"config_groups\": {\n     \"group_0\": {\n@@ -97,31 +97,31 @@ The config file specifies the quantization of a config group (`group_0`), which\n \n For a more detailed look at the model weights, use the [safetensors viewer](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf?show_file_info=model.safetensors.index.json) on the model card to see the quantized weights, input scale, and weight scale for all [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) modules.\n \n-| Tensors | Shape |\tPrecision |\n+| Tensors | Shape | Precision |\n | ------- | ----- | --------- |\n-model.layers.0.input_layernorm.weight\t| [4 096]\t| BF16\n-model.layers.0.mlp.down_proj.input_scale\t| [1]\t| BF16\n-model.layers.0.mlp.down_proj.weight\t| [4 096, 14 336] |\tF8_E4M3\n-model.layers.0.mlp.down_proj.weight_scale |\t[1]\t| BF16\n-model.layers.0.mlp.gate_proj.input_scale |\t[1]\t| BF16\n-model.layers.0.mlp.gate_proj.weight\t| [14 336, 4 096]\t| F8_E4M3\n-model.layers.0.mlp.gate_proj.weight_scale\t| [1] |\tBF16\n-model.layers.0.mlp.up_proj.input_scale|\t[1]\t|BF16\n-model.layers.0.mlp.up_proj.weight |\t[14 336, 4 096]\t| F8_E4M3\n-model.layers.0.mlp.up_proj.weight_scale | [1]\t| BF16\n-model.layers.0.post_attention_layernorm.weight |\t[4 096]\t|BF16\n-model.layers.0.self_attn.k_proj.input_scale |\t[1]\t|  BF16\n-model.layers.0.self_attn.k_proj.weight |\t[1 024, 4 096]|\tF8_E4M3\n-model.layers.0.self_attn.k_proj.weight_scale |[1]\t| BF16\n-model.layers.0.self_attn.o_proj.input_scale\t| [1]\t| BF16\n-model.layers.0.self_attn.o_proj.weight | [4 096, 4 096]\t| F8_E4M3\n-model.layers.0.self_attn.o_proj.weight_scale | [1]\t| BF16\n-model.layers.0.self_attn.q_proj.input_scale\t| [1]\t| BF16\n-model.layers.0.self_attn.q_proj.weight | [4 096, 4 096]\t| F8_E4M3\n-model.layers.0.self_attn.q_proj.weight_scale |\t[1] | BF16\n-model.layers.0.self_attn.v_proj.input_scale\t| [1] | BF16\n-model.layers.0.self_attn.v_proj.weight |\t[1 024, 4 096]\t| F8_E4M3\n-model.layers.0.self_attn.v_proj.weight_scale |\t[1] |\tBF16\n+|model.layers.0.input_layernorm.weight | [4 096] | BF16|\n+|model.layers.0.mlp.down_proj.input_scale | [1] | BF16|\n+|model.layers.0.mlp.down_proj.weight | [4 096, 14 336] | F8_E4M3|\n+|model.layers.0.mlp.down_proj.weight_scale | [1] | BF16|\n+|model.layers.0.mlp.gate_proj.input_scale | [1] | BF16|\n+|model.layers.0.mlp.gate_proj.weight | [14 336, 4 096] | F8_E4M3|\n+|model.layers.0.mlp.gate_proj.weight_scale | [1] | BF16|\n+|model.layers.0.mlp.up_proj.input_scale| [1] |BF16|\n+|model.layers.0.mlp.up_proj.weight | [14 336, 4 096] | F8_E4M3|\n+|model.layers.0.mlp.up_proj.weight_scale | [1] | BF16|\n+|model.layers.0.post_attention_layernorm.weight | [4 096] |BF16|\n+|model.layers.0.self_attn.k_proj.input_scale | [1] |  BF16|\n+|model.layers.0.self_attn.k_proj.weight | [1 024, 4 096]| F8_E4M3|\n+|model.layers.0.self_attn.k_proj.weight_scale |[1] | BF16|\n+|model.layers.0.self_attn.o_proj.input_scale | [1] | BF16|\n+|model.layers.0.self_attn.o_proj.weight | [4 096, 4 096] | F8_E4M3|\n+|model.layers.0.self_attn.o_proj.weight_scale | [1] | BF16|\n+|model.layers.0.self_attn.q_proj.input_scale | [1] | BF16|\n+|model.layers.0.self_attn.q_proj.weight | [4 096, 4 096] | F8_E4M3|\n+|model.layers.0.self_attn.q_proj.weight_scale | [1] | BF16|\n+|model.layers.0.self_attn.v_proj.input_scale | [1] | BF16|\n+|model.layers.0.self_attn.v_proj.weight | [1 024, 4 096] | F8_E4M3|\n+|model.layers.0.self_attn.v_proj.weight_scale | [1] | BF16|\n \n When loading a compressed-tensors model with the [`~quantizers.HFQuantizer`] integration, all the [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) modules specified in the quantization config are replaced by [CompressedLinear](https://github.com/neuralmagic/compressed-tensors/blob/975cb223b19fcac2b98a4271d17668462d4d6e1d/src/compressed_tensors/linear/compressed_linear.py#L30) modules that manage the compressed weights and forward pass for inference. The `lm_head` module is still kept as an unquantized nn.Linear module.\n "
        }
    ],
    "stats": {
        "total": 52,
        "additions": 26,
        "deletions": 26
    }
}