{
    "author": "ahnjj",
    "message": "ğŸŒ [i18n-KO] Translated albert.md to Korean (#39524)\n\n* docs: ko: albert.md\n\n* feat: nmt draft\n\n* fix: manual edits",
    "sha": "2f59c15b336c10a5920b52929446145f758bc2f8",
    "files": [
        {
            "sha": "9fc6938145274707987c3e8d1738f3b460c81f6d",
            "filename": "docs/source/ko/model_doc/albert.md",
            "status": "added",
            "additions": 263,
            "deletions": 0,
            "changes": 263,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f59c15b336c10a5920b52929446145f758bc2f8/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f59c15b336c10a5920b52929446145f758bc2f8/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md?ref=2f59c15b336c10a5920b52929446145f758bc2f8",
            "patch": "@@ -0,0 +1,263 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\" >\n+        <img alt= \"TensorFlow\" src= \"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\" >\n+        <img alt= \"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?styleâ€¦Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\">\n+        <img alt=\"SDPA\" src= \"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\" > \n+    </div>\n+</div>\n+\n+# ALBERT[[albert]]\n+\n+[ALBERT](https://huggingface.co/papers/1909.11942)ëŠ” [BERT](./bert)ì˜ í™•ì¥ì„±ê³¼ í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‘ ê°€ì§€ íŒŒë¼ë¯¸í„° ê°ì†Œ ê¸°ë²•ì„ ë„ì…í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ì„ë² ë”© í–‰ë ¬ ë¶„í•´(factorized embedding parametrization)ë¡œ, í° ì–´íœ˜ ì„ë² ë”© í–‰ë ¬ì„ ë‘ ê°œì˜ ì‘ì€ í–‰ë ¬ë¡œ ë¶„í•´í•˜ì—¬ íˆë“  ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë ¤ë„ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ í¬ê²Œ ì¦ê°€í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” ê³„ì¸µ ê°„ íŒŒë¼ë¯¸í„° ê³µìœ (cross-layer parameter sharing)ë¡œ, ì—¬ëŸ¬ ê³„ì¸µì´ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ì—¬ í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.\n+\n+ALBERTëŠ” BERTì—ì„œ ë°œìƒí•˜ëŠ” GPU/TPU ë©”ëª¨ë¦¬ í•œê³„, ê¸´ í•™ìŠµ ì‹œê°„, ê°‘ì‘ìŠ¤ëŸ° ì„±ëŠ¥ ì €í•˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. ALBERTëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë‘ ê°€ì§€ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  BERTì˜ í•™ìŠµ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤:\n+\n+- **ì„ë² ë”© í–‰ë ¬ ë¶„í•´:** í° ì–´íœ˜ ì„ë² ë”© í–‰ë ¬ì„ ë‘ ê°œì˜ ë” ì‘ì€ í–‰ë ¬ë¡œ ë¶„í•´í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.\n+- **ê³„ì¸µ ê°„ íŒŒë¼ë¯¸í„° ê³µìœ :** ê° íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì¸µë§ˆë‹¤ ë³„ë„ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , ì—¬ëŸ¬ ê³„ì¸µì´ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ì—¬ í•™ìŠµí•´ì•¼ í•  ê°€ì¤‘ì¹˜ ìˆ˜ë¥¼ ë”ìš± ì¤„ì…ë‹ˆë‹¤.\n+\n+ALBERTëŠ” BERTì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©(absolute position embeddings)ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì…ë ¥ íŒ¨ë”©ì€ ì˜¤ë¥¸ìª½ì— ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì„ë² ë”© í¬ê¸°ëŠ” 128ì´ë©°, BERTì˜ 768ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ALBERTëŠ” í•œ ë²ˆì— ìµœëŒ€ 512ê°œì˜ í† í°ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ëª¨ë“  ê³µì‹ ALBERT ì²´í¬í¬ì¸íŠ¸ëŠ” [ALBERT ì»¤ë®¤ë‹ˆí‹°](https://huggingface.co/albert) ì¡°ì§ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì˜ ALBERT ëª¨ë¸ì„ í´ë¦­í•˜ì‹œë©´ ë‹¤ì–‘í•œ ì–¸ì–´ ì‘ì—…ì— ALBERTë¥¼ ì ìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ë” í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” [`Pipeline`], [`AutoModel`] ê·¸ë¦¬ê³  ì»¤ë§¨ë“œë¼ì¸ì—ì„œ `[MASK]` í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"fill-mask\",\n+    model=\"albert-base-v2\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"ì‹ë¬¼ì€ ê´‘í•©ì„±ì´ë¼ê³  ì•Œë ¤ì§„ ê³¼ì •ì„ í†µí•´ [MASK]ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\", top_k=5)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n+model = AutoModelForMaskedLM.from_pretrained(\n+    \"albert/albert-base-v2\",\n+    torch_dtype=torch.float16,\n+    attn_implementation=\"sdpa\",\n+    device_map=\"auto\"\n+)\n+\n+prompt = \"ì‹ë¬¼ì€ [MASK]ì´ë¼ê³  ì•Œë ¤ì§„ ê³¼ì •ì„ í†µí•´ ì—ë„ˆì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n+\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n+    predictions = outputs.logits[0, mask_token_index]\n+\n+top_k = torch.topk(predictions, k=5).indices.tolist()\n+for token_id in top_k[0]:\n+    print(f\"ì˜ˆì¸¡: {tokenizer.decode([token_id])}\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers run --task fill-mask --model albert-base-v2 --device 0\n+```\n+\n+</hfoption>\n+\n+</hfoptions>\n+\n+## ì°¸ê³  ì‚¬í•­[[notes]]\n+\n+- BERTëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì˜¤ë¥¸ìª½ì— ì…ë ¥ì´ íŒ¨ë”©ë¼ì•¼ í•©ë‹ˆë‹¤.\n+- ì„ë² ë”© í¬ê¸° `E`ëŠ” íˆë“  í¬ê¸° `H`ì™€ ë‹¤ë¦…ë‹ˆë‹¤. ì„ë² ë”©ì€ ë¬¸ë§¥ì— ë…ë¦½ì (ê° í† í°ë§ˆë‹¤ í•˜ë‚˜ì˜ ì„ë² ë”© ë²¡í„°)ì´ê³ , ì€ë‹‰ ìƒíƒœëŠ” ë¬¸ë§¥ì— ì˜ì¡´ì (í† í° ì‹œí€€ìŠ¤ë§ˆë‹¤ í•˜ë‚˜ì˜ ì€ë‹‰ ìƒíƒœ)ì…ë‹ˆë‹¤. ì„ë² ë”© í–‰ë ¬ì€ `V x E`(V: ì–´íœ˜ í¬ê¸°)ì´ë¯€ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ `H >> E`ê°€ ë” ë…¼ë¦¬ì ì…ë‹ˆë‹¤. `E < H`ì¼ ë•Œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ë” ì ì–´ì§‘ë‹ˆë‹¤.\n+\n+## ì°¸ê³  ìë£Œ[[resources]]\n+\n+ì•„ë˜ ì„¹ì…˜ì˜ ìë£Œë“¤ì€ ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°(ğŸŒ í‘œì‹œ) ìë£Œë¡œ, AlBERTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì— ì¶”ê°€í•  ìë£Œê°€ ìˆë‹¤ë©´ Pull Requestë¥¼ ë³´ë‚´ì£¼ì„¸ìš”! ê¸°ì¡´ ìë£Œì™€ ì¤‘ë³µë˜ì§€ ì•Šê³  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.\n+\n+<PipelineTag pipeline=\"text-classification\"/>\n+\n+- [`AlbertForSequenceClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+- [`TFAlbertForSequenceClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+- [`FlaxAlbertForSequenceClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\n+\n+<PipelineTag pipeline=\"token-classification\"/>\n+\n+- [`AlbertForTokenClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+- [`TFAlbertForTokenClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+- [`FlaxAlbertForTokenClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Faceì˜ [í† í° ë¶„ë¥˜](https://huggingface.co/course/chapter7/2?fw=pt) ê°•ì¢Œ\n+- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\n+\n+<PipelineTag pipeline=\"fill-mask\"/>\n+\n+- [`AlbertForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFAlbertForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`FlaxAlbertForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Faceì˜ [ë§ˆìŠ¤í‚¹ ì–¸ì–´ ëª¨ë¸ë§](https://huggingface.co/course/chapter7/3?fw=pt) ê°•ì¢Œ\n+- [ë§ˆìŠ¤í‚¹ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\n+\n+<PipelineTag pipeline=\"question-answering\"/>\n+\n+- [`AlbertForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFAlbertForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`FlaxAlbertForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [ì§ˆì˜ì‘ë‹µ](https://huggingface.co/course/chapter7/7?fw=pt) ğŸ¤— Hugging Face ê°•ì¢Œì˜ ì±•í„°.\n+- [ì§ˆì˜ì‘ë‹µ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\n+\n+**ë‹¤ì¤‘ ì„ íƒ(Multiple choice)**\n+\n+- [`AlbertForMultipleChoice`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFAlbertForMultipleChoice`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+- [ë‹¤ì¤‘ ì„ íƒ ì‘ì—… ê°€ì´ë“œ](../tasks/multiple_choice)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\n+\n+## AlbertConfig[[albertconfig]]\n+\n+[[autodoc]] AlbertConfig\n+\n+## AlbertTokenizer[[alberttokenizer]]\n+\n+[[autodoc]] AlbertTokenizer - build_inputs_with_special_tokens - get_special_tokens_mask - create_token_type_ids_from_sequences - save_vocabulary\n+\n+## AlbertTokenizerFast[[alberttokenizerfast]]\n+\n+[[autodoc]] AlbertTokenizerFast\n+\n+## Albert íŠ¹í™” ì¶œë ¥[[albert-specific-outputs]]\n+\n+[[autodoc]] models.albert.modeling_albert.AlbertForPreTrainingOutput\n+\n+[[autodoc]] models.albert.modeling_tf_albert.TFAlbertForPreTrainingOutput\n+\n+<frameworkcontent>\n+<pt>\n+\n+## AlbertModel[[albertmodel]]\n+\n+[[autodoc]] AlbertModel - forward\n+\n+## AlbertForPreTraining[[albertforpretraining]]\n+\n+[[autodoc]] AlbertForPreTraining - forward\n+\n+## AlbertForMaskedLM[[albertformaskedlm]]\n+\n+[[autodoc]] AlbertForMaskedLM - forward\n+\n+## AlbertForSequenceClassification[[albertforsequenceclassification]]\n+\n+[[autodoc]] AlbertForSequenceClassification - forward\n+\n+## AlbertForMultipleChoice[[albertformultiplechoice]]\n+\n+[[autodoc]] AlbertForMultipleChoice\n+\n+## AlbertForTokenClassification[[albertfortokenclassification]]\n+\n+[[autodoc]] AlbertForTokenClassification - forward\n+\n+## AlbertForQuestionAnswering[[albertforquestionanswering]]\n+\n+[[autodoc]] AlbertForQuestionAnswering - forward\n+\n+</pt>\n+\n+<tf>\n+\n+## TFAlbertModel[[tfalbertmodel]]\n+\n+[[autodoc]] TFAlbertModel - call\n+\n+## TFAlbertForPreTraining[[tfalbertforpretraining]]\n+\n+[[autodoc]] TFAlbertForPreTraining - call\n+\n+## TFAlbertForMaskedLM[[tfalbertformaskedlm]]\n+\n+[[autodoc]] TFAlbertForMaskedLM - call\n+\n+## TFAlbertForSequenceClassification[[tfalbertforsequenceclassification]]\n+\n+[[autodoc]] TFAlbertForSequenceClassification - call\n+\n+## TFAlbertForMultipleChoice[[tfalbertformultiplechoice]]\n+\n+[[autodoc]] TFAlbertForMultipleChoice - call\n+\n+## TFAlbertForTokenClassification[[tfalbertfortokenclassification]]\n+\n+[[autodoc]] TFAlbertForTokenClassification - call\n+\n+## TFAlbertForQuestionAnswering[[tfalbertforquestionanswering]]\n+\n+[[autodoc]] TFAlbertForQuestionAnswering - call\n+\n+</tf>\n+<jax>\n+\n+## FlaxAlbertModel[[flaxalbertmodel]]\n+\n+[[autodoc]] FlaxAlbertModel - **call**\n+\n+## FlaxAlbertForPreTraining[[flaxalbertforpretraining]]\n+\n+[[autodoc]] FlaxAlbertForPreTraining - **call**\n+\n+## FlaxAlbertForMaskedLM[[flaxalbertformaskedlm]]\n+\n+[[autodoc]] FlaxAlbertForMaskedLM - **call**\n+\n+## FlaxAlbertForSequenceClassification[[flaxalbertforsequenceclassification]]\n+\n+[[autodoc]] FlaxAlbertForSequenceClassification - **call**\n+\n+## FlaxAlbertForMultipleChoice[[flaxalbertformultiplechoice]]\n+\n+[[autodoc]] FlaxAlbertForMultipleChoice - **call**\n+\n+## FlaxAlbertForTokenClassification[[flaxalbertfortokenclassification]]\n+\n+[[autodoc]] FlaxAlbertForTokenClassification - **call**\n+\n+## FlaxAlbertForQuestionAnswering[[flaxalbertforquestionanswering]]\n+\n+[[autodoc]] FlaxAlbertForQuestionAnswering - **call**\n+\n+</jax>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 263,
        "additions": 263,
        "deletions": 0
    }
}