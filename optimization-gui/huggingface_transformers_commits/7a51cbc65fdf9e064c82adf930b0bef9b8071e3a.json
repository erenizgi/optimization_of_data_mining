{
    "author": "jmamou",
    "message": "Dynamic number of speculative tokens in order to accelerate speculative decoding (#33258)\n\n* optimal Speculation Lookahead based on probability\r\n\r\n* update peer finished condition\r\n\r\n* add support to do_sample True\r\n\r\n* add stopping criteria\r\n\r\n* gitignore\r\n\r\n* add print\r\n\r\n* remove prints\r\n\r\n* minor\r\n\r\n* minor\r\n\r\n* git ignore\r\n\r\n* adding test to stopping ConfidenceCriteria\r\n\r\n* doc + format\r\n\r\n* add doc\r\n\r\n* Update .gitignore\r\n\r\n* update docstring and default value of assistant_confidence_threshold\r\n\r\n* add docstring\r\n\r\n* Update src/transformers/generation/configuration_utils.py\r\n\r\nimplicit default value (None)\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\r\n\r\n* style fix\r\n\r\n---------\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
    "files": [
        {
            "sha": "2bea00261951c726f8493f4b6d30ef9420c21b89",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
            "patch": "@@ -83,6 +83,7 @@\n         \"MaxNewTokensCriteria\",\n         \"MaxLengthCriteria\",\n         \"MaxTimeCriteria\",\n+        \"ConfidenceCriteria\",\n         \"EosTokenCriteria\",\n         \"StoppingCriteria\",\n         \"StoppingCriteriaList\",\n@@ -225,6 +226,7 @@\n             WhisperTimeStampLogitsProcessor,\n         )\n         from .stopping_criteria import (\n+            ConfidenceCriteria,\n             EosTokenCriteria,\n             MaxLengthCriteria,\n             MaxNewTokensCriteria,"
        },
        {
            "sha": "62d5fb6eed0c49d81f824d101f5636757c831843",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
            "patch": "@@ -108,6 +108,7 @@ def __init__(\n         # Prepare the assistant and the starting number of candidate tokens\n         self.assistant_model = assistant_model\n         self.num_assistant_tokens = assistant_model.generation_config.num_assistant_tokens\n+        self.assistant_confidence_threshold = assistant_model.generation_config.assistant_confidence_threshold\n \n         # Set eos in assistant same as in target model\n         self.assistant_model.generation_config.eos_token_id = generation_config.eos_token_id\n@@ -157,6 +158,7 @@ def __init__(\n         self.generation_config = copy.deepcopy(generation_config)\n         self.generation_config.return_dict_in_generate = True\n         self.generation_config.output_scores = True\n+        self.generation_config.assistant_confidence_threshold = self.assistant_confidence_threshold\n \n         # Disable sampling -- this implementation of assisted generation/speculative decoding uses the assistant\n         # greedily to maximize matches. Disables sampling-related flags to prevent warnings"
        },
        {
            "sha": "e2585b1b9ed49c0ad64d3d40c21cf22c96cdd4ab",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
            "patch": "@@ -350,6 +350,11 @@ class GenerationConfig(PushToHubMixin):\n               reduce by 1. `num_assistant_tokens` value is persistent over multiple generation calls with the same assistant model.\n             - `\"heuristic_transient\"`: Same as `\"heuristic\"` but `num_assistant_tokens` is reset to its initial value after each generation call.\n             - `\"constant\"`: `num_assistant_tokens` stays unchanged during generation\n+        assistant_confidence_threshold (`float`, *optional*):\n+            The confidence threshold for the assistant model. If the assistant model's confidence in its prediction for the current token is lower\n+            than this threshold, the assistant model stops the current token generation iteration, even if the number of _speculative tokens_\n+            (defined by `num_assistant_tokens`) is not yet reached. It is an unsupervised version of the dynamic speculation lookahead\n+            from Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models <https://arxiv.org/abs/2405.04304>.\n         prompt_lookup_num_tokens (`int`, *optional*, default to `None`):\n             The number of tokens to be output as candidate tokens.\n         max_matching_ngram_size (`int`, *optional*, default to `None`):\n@@ -449,6 +454,7 @@ def __init__(self, **kwargs):\n         # Assistant generation\n         self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 5)\n         self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"heuristic\")\n+        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", None)\n \n         # Prompt lookup decoding\n         self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)"
        },
        {
            "sha": "b950a69f8b64929a2c9fee87f28d44585ebb93ee",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
            "patch": "@@ -467,6 +467,27 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n         return is_done\n \n \n+class ConfidenceCriteria(StoppingCriteria):\n+    \"\"\"\n+    This class can be used to stop generation whenever assistant model's confidence in its prediction for the current token is lower than the threshold\n+        `model.generation_config.assistant_confidence_threshold` even if the number of speculative tokens (defined by `num_assistant_tokens`) is not yet reached.\n+\n+    Args:\n+        assistant_confidence_threshold (`float`):\n+            The value of the threshold.\n+    \"\"\"\n+\n+    def __init__(self, assistant_confidence_threshold):\n+        self.assistant_confidence_threshold = assistant_confidence_threshold\n+\n+    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n+        probs = scores[-1].softmax(-1)\n+        p = probs[0, input_ids[0, -1]].item()\n+        if p < self.assistant_confidence_threshold:\n+            return True\n+        return False\n+\n+\n class StoppingCriteriaList(list):\n     @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:"
        },
        {
            "sha": "17a234c62b285e1b066d37a4166ff7390b9645b4",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
            "patch": "@@ -97,6 +97,7 @@\n     WatermarkLogitsProcessor,\n )\n from .stopping_criteria import (\n+    ConfidenceCriteria,\n     EosTokenCriteria,\n     MaxLengthCriteria,\n     MaxTimeCriteria,\n@@ -958,6 +959,13 @@ def _get_stopping_criteria(\n             criteria.append(StopStringCriteria(stop_strings=generation_config.stop_strings, tokenizer=tokenizer))\n         if generation_config._eos_token_tensor is not None:\n             criteria.append(EosTokenCriteria(eos_token_id=generation_config._eos_token_tensor))\n+        if (\n+            generation_config.assistant_confidence_threshold is not None\n+            and generation_config.assistant_confidence_threshold > 0\n+        ):\n+            criteria.append(\n+                ConfidenceCriteria(assistant_confidence_threshold=generation_config.assistant_confidence_threshold)\n+            )\n         criteria = self._merge_criteria_processor_list(criteria, stopping_criteria)\n         return criteria\n "
        },
        {
            "sha": "e8594dcdb07e9005c50e478803884f8044de3619",
            "filename": "tests/generation/test_stopping_criteria.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a51cbc65fdf9e064c82adf930b0bef9b8071e3a/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_stopping_criteria.py?ref=7a51cbc65fdf9e064c82adf930b0bef9b8071e3a",
            "patch": "@@ -26,6 +26,7 @@\n     import torch\n \n     from transformers.generation import (\n+        ConfidenceCriteria,\n         EosTokenCriteria,\n         MaxLengthCriteria,\n         MaxTimeCriteria,\n@@ -100,6 +101,23 @@ def test_eos_token_criteria(self):\n         input_ids[:, -1] = 1\n         self.assertListEqual(criteria(input_ids, scores).tolist(), [False, False, False])\n \n+    def test_confidence_criteria(self):\n+        criteria = ConfidenceCriteria(assistant_confidence_threshold=0.5)\n+\n+        vocab_size = 250\n+        length = 5\n+\n+        input_ids = ids_tensor((1, length), vocab_size)\n+        scores = (torch.randn((1, vocab_size)),)\n+\n+        # Simulate high confidence by setting the probability of the last token to be high\n+        scores[0][0, input_ids[0, -1]] = 10.0  # Logits before softmax\n+        self.assertFalse(criteria(input_ids, scores))\n+\n+        # Simulate low confidence by setting the probability of the last token to be low\n+        scores[0][0, input_ids[0, -1]] = -10.0  # Logits before softmax\n+        self.assertTrue(criteria(input_ids, scores))\n+\n     def test_validate_stopping_criteria(self):\n         validate_stopping_criteria(StoppingCriteriaList([MaxLengthCriteria(10)]), 10)\n "
        }
    ],
    "stats": {
        "total": 57,
        "additions": 57,
        "deletions": 0
    }
}