{
    "author": "Cyrilvallez",
    "message": "[saving] Simplify general logic (#42766)\n\n* parallelize and cleanup\n\n* simplify offloading\n\n* fix\n\n* oupsi\n\n* add env variable to deactivate\n\n* revert threading -> safetensors does not release the GIL\n\n* comment\n\n* create helper\n\n* move it to accelerate integration",
    "sha": "e6b9d061472afc4ee376573789cd44b0180827f3",
    "files": [
        {
            "sha": "069472df5c3c06b92c95d539216aecce35dd466b",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6b9d061472afc4ee376573789cd44b0180827f3/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6b9d061472afc4ee376573789cd44b0180827f3/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=e6b9d061472afc4ee376573789cd44b0180827f3",
            "patch": "@@ -554,6 +554,32 @@ def offload_weight(weight: torch.Tensor, weight_name: str, offload_folder: str |\n     return offload_index\n \n \n+def load_offloaded_parameter(model: \"PreTrainedModel\", param_name: str) -> torch.Tensor:\n+    \"\"\"Load `param_name` from disk, if it was offloaded due to the device_map, and thus lives as a meta parameter\n+    inside `model`.\n+    This is needed when resaving a model, when some parameters were offloaded (we need to load them from disk, to\n+    then resave them to disk in the correct shard...).\"\"\"\n+    # Start from the most inner module, and try to find the hook that was used for offloading the param\n+    module_parts = param_name.split(\".\")\n+    modules_to_check = [\".\".join(module_parts[:-idx]) for idx in range(1, len(module_parts))] + [\"\"]\n+    for parent_name in modules_to_check:\n+        parent = model.get_submodule(parent_name)\n+        if hasattr(parent, \"_hf_hook\"):\n+            weights_map = parent._hf_hook.weights_map\n+            truncated_param_name = param_name.replace(f\"{parent_name}.\" if parent_name != \"\" else parent_name, \"\")\n+            break\n+    # If we did not break the loop, something is wrong\n+    else:\n+        raise ValueError(\n+            f\"{param_name} is on the meta device because it was offloaded, but we could not find \"\n+            \"the corresponding hook for it\"\n+        )\n+\n+    # This call loads it from disk\n+    tensor = weights_map[truncated_param_name]\n+    return tensor\n+\n+\n def _init_infer_auto_device_map(\n     model: nn.Module,\n     max_memory: dict[int | str, int | str] | None = None,"
        },
        {
            "sha": "2e1240703e470967e7cc699fc7b0db00a1a1f100",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 129,
            "deletions": 128,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6b9d061472afc4ee376573789cd44b0180827f3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6b9d061472afc4ee376573789cd44b0180827f3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e6b9d061472afc4ee376573789cd44b0180827f3",
            "patch": "@@ -16,7 +16,6 @@\n import collections\n import copy\n import functools\n-import gc\n import importlib.metadata\n import inspect\n import json\n@@ -64,6 +63,7 @@\n     check_and_set_device_map,\n     expand_device_map,\n     init_empty_weights,\n+    load_offloaded_parameter,\n )\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n from .integrations.eager_paged import eager_paged_attention_forward\n@@ -130,7 +130,6 @@\n if is_accelerate_available():\n     from accelerate.hooks import add_hook_to_module\n     from accelerate.utils import extract_model_from_parallel\n-    from accelerate.utils.modeling import get_state_dict_from_offload\n \n \n _torch_distributed_available = torch.distributed.is_available()\n@@ -410,6 +409,86 @@ def _find_identical(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]\n     return shared_tensors, identical\n \n \n+def remove_tied_weights_from_state_dict(\n+    state_dict: dict[str, torch.Tensor], model: \"PreTrainedModel\"\n+) -> dict[str, torch.Tensor]:\n+    \"\"\"\n+    Remove all tied weights from the given `state_dict`, making sure to keep only the main weight that `model`\n+    will expect when reloading (even if we know tie weights symmetrically, it's better to keep the intended one).\n+    This is because `safetensors` does not allow tensor aliasing - so we're going to remove aliases before saving.\n+    \"\"\"\n+    # To avoid any potential mistakes and mismatches between config and actual tied weights, here we check the pointers\n+    # of the Tensors themselves -> we are guaranteed to find all the actual tied weights\n+    ptrs = collections.defaultdict(list)\n+    for name, tensor in state_dict.items():\n+        if not isinstance(tensor, torch.Tensor):\n+            # Sometimes in the state_dict we have non-tensor objects.\n+            # e.g. in bitsandbytes we have some `str` objects in the state_dict\n+            # In the non-tensor case, fall back to the pointer of the object itself\n+            ptrs[id(tensor)].append(name)\n+\n+        elif tensor.device.type == \"meta\":\n+            # In offloaded cases, there may be meta tensors in the state_dict.\n+            # For these cases, key by the pointer of the original tensor object\n+            # (state_dict tensors are detached and therefore no longer shared)\n+            tensor = model.get_parameter(name)\n+            ptrs[id(tensor)].append(name)\n+\n+        else:\n+            ptrs[id_tensor_storage(tensor)].append(name)\n+\n+    shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n+\n+    # Recursively descend to find tied weight keys\n+    all_potential_tied_weights_keys = set(_get_tied_weight_keys(model))\n+    error_names = []\n+    to_delete_names = set()\n+    # Removing the keys which are declared as known duplicates on load. This allows to make sure the name which is\n+    # kept is consistent\n+    if all_potential_tied_weights_keys is not None:\n+        for names in shared_ptrs.values():\n+            found = 0\n+            for name in sorted(names):\n+                matches_pattern = any(re.search(pat, name) for pat in all_potential_tied_weights_keys)\n+                if matches_pattern and name in state_dict:\n+                    found += 1\n+                    if found < len(names):\n+                        to_delete_names.add(name)\n+    # We are entering a place where the weights and the transformers configuration do NOT match.\n+    shared_names, disjoint_names = _find_disjoint(shared_ptrs.values(), state_dict)\n+    # Those are actually tensor sharing but disjoint from each other, we can safely clone them\n+    # Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\n+    for name in disjoint_names:\n+        state_dict[name] = state_dict[name].clone()\n+\n+    # When not all duplicates have been cleaned, still remove those keys, but put a clear warning.\n+    # If the link between tensors was done at runtime then `from_pretrained` will not get\n+    # the key back leading to random tensor. A proper warning will be shown\n+    # during reload (if applicable), but since the file is not necessarily compatible with\n+    # the config, better show a proper warning.\n+    shared_names, identical_names = _find_identical(shared_names, state_dict)\n+    # delete tensors that have identical storage\n+    for inames in identical_names:\n+        known = inames.intersection(to_delete_names)\n+        for name in known:\n+            del state_dict[name]\n+        unknown = inames.difference(to_delete_names)\n+        if len(unknown) > 1:\n+            error_names.append(unknown)\n+\n+    if shared_names:\n+        error_names.extend(shared_names)\n+\n+    if len(error_names) > 0:\n+        raise RuntimeError(\n+            f\"The weights trying to be saved contained shared tensors {error_names} which are not properly defined. \"\n+            f\"We found all the potential target tied weights keys to be: {all_potential_tied_weights_keys}.\\n\"\n+            \"This can also just mean that the module's tied weight keys are wrong vs the actual tied weights in the model.\",\n+        )\n+\n+    return state_dict\n+\n+\n def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n     \"\"\"Cast a single parameter `param_name` into the `model`, with value `tensor`.\"\"\"\n     module, param_type = get_module_from_name(model, param_name)\n@@ -3131,29 +3210,23 @@ def save_pretrained(\n                 current_peft_config = self.peft_config[active_adapter]\n                 current_peft_config.save_pretrained(save_directory)\n \n-        # for offloaded modules\n-        module_map = {}\n-\n-        # Save the model\n+        # Get the model state_dict\n         if state_dict is None:\n-            # if any model parameters are offloaded, make module map\n-            if (\n-                hasattr(self, \"hf_device_map\")\n-                and len(set(self.hf_device_map.values())) > 1\n-                and (\"cpu\" in self.hf_device_map.values() or \"disk\" in self.hf_device_map.values())\n-            ):\n-                warnings.warn(\n-                    \"Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\"\n-                )\n-                for name, module in model_to_save.named_modules():\n-                    if name == \"\":\n-                        continue\n-                    module_state_dict = module.state_dict()\n-\n-                    for key in module_state_dict:\n-                        module_map[name + f\".{key}\"] = module\n             state_dict = model_to_save.state_dict()\n \n+        # if any model parameters are offloaded, we need to know it for later\n+        is_offloaded = False\n+        if (\n+            hasattr(self, \"hf_device_map\")\n+            and len(set(self.hf_device_map.values())) > 1\n+            and (\"cpu\" in self.hf_device_map.values() or \"disk\" in self.hf_device_map.values())\n+        ):\n+            is_offloaded = True\n+            warnings.warn(\n+                \"Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory \"\n+                \"exceeds the `shard_size` (50GB default)\"\n+            )\n+\n         # Translate state_dict from smp to hf if saving with smp >= 1.10\n         if IS_SAGEMAKER_MP_POST_1_10:\n             for smp_to_hf, _ in smp.state.module_manager.translate_functions:\n@@ -3170,76 +3243,12 @@ def save_pretrained(\n         if self._tp_size is not None:\n             state_dict = replace_state_dict_local_with_dtensor(state_dict, self._tp_plan, self._device_mesh)\n \n-        # Safetensors does not allow tensor aliasing - we're going to remove aliases before saving\n-        ptrs = collections.defaultdict(list)\n-        for name, tensor in state_dict.items():\n-            if not isinstance(tensor, torch.Tensor):\n-                # Sometimes in the state_dict we have non-tensor objects.\n-                # e.g. in bitsandbytes we have some `str` objects in the state_dict\n-                # In the non-tensor case, fall back to the pointer of the object itself\n-                ptrs[id(tensor)].append(name)\n-\n-            elif tensor.device.type == \"meta\":\n-                # In offloaded cases, there may be meta tensors in the state_dict.\n-                # For these cases, key by the pointer of the original tensor object\n-                # (state_dict tensors are detached and therefore no longer shared)\n-                tensor = self.get_parameter(name)\n-                ptrs[id(tensor)].append(name)\n-\n-            else:\n-                ptrs[id_tensor_storage(tensor)].append(name)\n-\n-        shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n-\n-        # Recursively descend to find tied weight keys\n-        _tied_weights_keys = set(_get_tied_weight_keys(self))\n-        error_names = []\n-        to_delete_names = set()\n-        for names in shared_ptrs.values():\n-            # Removing the keys which are declared as known duplicates on\n-            # load. This allows to make sure the name which is kept is consistent.\n-            if _tied_weights_keys is not None:\n-                found = 0\n-                for name in sorted(names):\n-                    matches_pattern = any(re.search(pat, name) for pat in _tied_weights_keys)\n-                    if matches_pattern and name in state_dict:\n-                        found += 1\n-                        if found < len(names):\n-                            to_delete_names.add(name)\n-        # We are entering a place where the weights and the transformers configuration do NOT match.\n-        shared_names, disjoint_names = _find_disjoint(shared_ptrs.values(), state_dict)\n-        # Those are actually tensor sharing but disjoint from each other, we can safely clone them\n-        # Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\n-        for name in disjoint_names:\n-            state_dict[name] = state_dict[name].clone()\n-\n-        # When not all duplicates have been cleaned, still remove those keys, but put a clear warning.\n-        # If the link between tensors was done at runtime then `from_pretrained` will not get\n-        # the key back leading to random tensor. A proper warning will be shown\n-        # during reload (if applicable), but since the file is not necessarily compatible with\n-        # the config, better show a proper warning.\n-        shared_names, identical_names = _find_identical(shared_names, state_dict)\n-        # delete tensors that have identical storage\n-        for inames in identical_names:\n-            known = inames.intersection(to_delete_names)\n-            for name in known:\n-                del state_dict[name]\n-            unknown = inames.difference(to_delete_names)\n-            if len(unknown) > 1:\n-                error_names.append(unknown)\n-\n-        if shared_names:\n-            error_names.extend(shared_names)\n-\n-        if len(error_names) > 0:\n-            raise RuntimeError(\n-                f\"The weights trying to be saved contained shared tensors {error_names} which are not properly defined. We found `_tied_weights_keys` to be: {_tied_weights_keys}.\\n\"\n-                \"This can also just mean that the module's tied weight keys are wrong vs the actual tied weights in the model.\",\n-            )\n+        # Remove tied weights as safetensors do not handle them\n+        state_dict = remove_tied_weights_from_state_dict(state_dict, model_to_save)\n \n         # Revert all renaming and/or weight operations\n         if save_original_format:\n-            state_dict = revert_weight_conversion(self, state_dict)\n+            state_dict = revert_weight_conversion(model_to_save, state_dict)\n \n         # Shard the model if it is too big.\n         if not _hf_peft_config_loaded:\n@@ -3279,47 +3288,39 @@ def save_pretrained(\n                 and reg.fullmatch(filename_no_suffix) is not None\n             ):\n                 os.remove(full_filename)\n+\n         # Save the model\n-        filename_to_tensors = state_dict_split.filename_to_tensors.items()\n-        if module_map:\n-            filename_to_tensors = logging.tqdm(filename_to_tensors, desc=\"Saving checkpoint shards\")\n-        for shard_file, tensors in filename_to_tensors:\n-            shard = {}\n-            for tensor in tensors:\n-                if _is_dtensor_available and isinstance(state_dict[tensor], DTensor):\n-                    full_tensor = state_dict[tensor].full_tensor()\n+        for shard_file, tensor_names in logging.tqdm(\n+            state_dict_split.filename_to_tensors.items(), desc=\"Writing model shards\"\n+        ):\n+            filename = os.path.join(save_directory, shard_file)\n+            shard_state_dict = {}\n+            for tensor_name in tensor_names:\n+                # Get the tensor, and remove it from state_dict to avoid keeping the ref\n+                tensor = state_dict.pop(tensor_name)\n+\n+                # In case of TP, get the full parameter back\n+                if _is_dtensor_available and isinstance(tensor, DTensor):\n+                    tensor = tensor.full_tensor()\n                     # to get the correctly ordered tensor we need to repack if packed\n-                    if _get_parameter_tp_plan(tensor, self._tp_plan) == \"local_packed_rowwise\":\n-                        full_tensor = repack_weights(full_tensor, -1, self._tp_size, 2)\n-                    shard[tensor] = full_tensor.contiguous()  # only do contiguous after it's permuted correctly\n-                else:\n-                    shard[tensor] = state_dict[tensor].contiguous()\n-                # delete reference, see https://github.com/huggingface/transformers/pull/34890\n-                del state_dict[tensor]\n-\n-            # remake shard with onloaded parameters if necessary\n-            if module_map:\n-                # init state_dict for this shard\n-                shard_state_dict = dict.fromkeys(shard, \"\")\n-                for module_name in shard:\n-                    # note that get_state_dict_from_offload can update with meta tensors\n-                    # if both a parent module and its descendant are offloaded\n-                    tensor = shard_state_dict[module_name]\n-                    if tensor == \"\" or (isinstance(tensor, torch.Tensor) and tensor.device.type == \"meta\"):\n-                        # update state dict with onloaded parameters\n-                        module = module_map[module_name]\n-                        shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)\n-\n-                # assign shard to be the completed state dict\n-                shard = shard_state_dict\n-                del shard_state_dict\n-                gc.collect()\n-\n-            # TODO: we should def parallelize this we are otherwise just waiting\n-            # too much before scheduling the next write when its in a different file\n-            safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n-\n-        del state_dict\n+                    if _get_parameter_tp_plan(tensor_name, self._tp_plan) == \"local_packed_rowwise\":\n+                        tensor = repack_weights(tensor, -1, self._tp_size, 2)\n+\n+                # If the param was offloaded, we need to load it back from disk to resave it. It's a strange pattern,\n+                # but it would otherwise not be contained in the saved shard if we were to simply move the file\n+                # or something\n+                if is_offloaded and tensor.device.type == \"meta\":\n+                    tensor = load_offloaded_parameter(model_to_save, tensor_name)\n+\n+                # only do contiguous after it's permuted correctly in case of TP\n+                shard_state_dict[tensor_name] = tensor.contiguous()\n+\n+            # TODO: it would be very nice to do the writing concurrently, but safetensors never releases the GIL,\n+            # so it's not possible for now....\n+            # Write the shard to disk\n+            safe_save_file(shard_state_dict, filename, metadata=metadata)\n+            # Cleanup the data before next loop (important with offloading, so we don't blowup cpu RAM)\n+            del shard_state_dict\n \n         if index is None:\n             path_to_weights = os.path.join(save_directory, weights_name)"
        }
    ],
    "stats": {
        "total": 283,
        "additions": 155,
        "deletions": 128
    }
}