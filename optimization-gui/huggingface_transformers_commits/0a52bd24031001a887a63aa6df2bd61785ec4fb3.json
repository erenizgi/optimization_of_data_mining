{
    "author": "zucchini-nlp",
    "message": "[fix] sliding window attention mask (#38045)\n\n* fix sliding attn\n\n* make style\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* no a second throught, should default to `True` fo BC\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "0a52bd24031001a887a63aa6df2bd61785ec4fb3",
    "files": [
        {
            "sha": "515fdc4f045f6741048c10d1d958ea89b20c1416",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1192,12 +1192,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "2748a71f527a5930d7c867c8fd70978a9c33e9ec",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -580,12 +580,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "d56f2d2f3708f86a6eefd5688017c3794cc76975",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -254,12 +254,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "cfb5b1be8b6d56df6abb2d44e218df75d70ddd7f",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -721,12 +721,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "fe0c102401660368c7ef8e9df353616c4220b417",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1216,12 +1216,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n@@ -1522,12 +1523,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "a914461ad73df1755643272b9fc1d9d15252c871",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -635,12 +635,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "2d6a2a7f155664922d1789a259d956806f240292",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1943,12 +1943,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "5002014006be2ed1b943d6648040297818835086",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1200,12 +1200,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "46fc9b720c8f5d73da467506ed868f5b121c49a6",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -593,12 +593,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "e973c5c64d9dd25d532f418d98848cc6e59b5bc7",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -2128,12 +2128,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n@@ -2833,12 +2834,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "18da004501b198797f0010495ab463bdaaaba7fe",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1354,12 +1354,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "ccd4a6b41915822c1b7b4c815172fe481653b602",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1047,12 +1047,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "17cd7d5dcac70432a699b0f7d4d3a39600e3ffa6",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -1314,12 +1314,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "082de2f193b4dce5056baf14eeebf028cec5a46b",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -620,12 +620,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "72591887a41166563302a897e5192041baaee3de",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -732,12 +732,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "1e1cfdde775705ff66cf1f4ef49f62ae29e3bf96",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -584,12 +584,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n                 -1, 1\n             )\n-            if config.get_text_config().sliding_window is not None:\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask"
        },
        {
            "sha": "92f3daf8b73e3230cabda1f7c94eaadd417c4291",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a52bd24031001a887a63aa6df2bd61785ec4fb3/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a52bd24031001a887a63aa6df2bd61785ec4fb3/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=0a52bd24031001a887a63aa6df2bd61785ec4fb3",
            "patch": "@@ -4323,6 +4323,45 @@ def _get_custom_4d_mask_test_data(self):\n \n         return input_ids, position_ids, input_ids_shared_prefix, mask_shared_prefix, position_ids_shared_prefix\n \n+    def test_sliding_window_mask(self):\n+        \"\"\"Tests that we can control the sliding window attention behavior of a model.\"\"\"\n+        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model does not support output_attentions\")\n+\n+        if not (hasattr(config, \"sliding_window\") and hasattr(config, \"use_sliding_window\")):\n+            self.skipTest(reason=\"Model does not support sliding window mask\")\n+\n+        seq_len = self.model_tester.seq_length\n+        batch_size = self.model_tester.batch_size\n+        sliding_window = 3  # set to arbitrary small number\n+\n+        sliding_mask = torch.zeros((seq_len, seq_len), dtype=torch.bool)\n+        for i in range(seq_len):\n+            start = max(0, i - sliding_window + 1)\n+            sliding_mask[i, start : i + 1] = True\n+        sliding_mask = sliding_mask.to(torch_device)\n+\n+        config.sliding_window = sliding_window\n+        inputs[\"attention_mask\"] = torch.ones(batch_size, seq_len).to(torch.int64).to(torch_device)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            # Set sliding window to `True` and check that all tokens beyond window size are masked\n+            model.config.use_sliding_window = True\n+            attentions = model(**inputs, output_attentions=True).attentions\n+            for layer_attention in attentions:\n+                self.assertTrue((layer_attention[:, :, ~sliding_mask] == 0).all().item())\n+\n+            # Set sliding window to `False` while keeping `sliding_window=3`\n+            # Check that all tokens beyond window size are not masked\n+            model.config.use_sliding_window = False\n+            attentions_not_sliding = model(**inputs, output_attentions=True).attentions\n+            for layer_attention in attentions_not_sliding:\n+                self.assertFalse((layer_attention[:, :, ~sliding_mask] == 0).all().item())\n+\n     def test_custom_4d_attention_mask(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 93,
        "deletions": 36
    }
}