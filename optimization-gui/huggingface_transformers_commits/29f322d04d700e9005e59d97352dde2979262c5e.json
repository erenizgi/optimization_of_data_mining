{
    "author": "gante",
    "message": "[generate, cache] handle more complex device maps (#37014)",
    "sha": "29f322d04d700e9005e59d97352dde2979262c5e",
    "files": [
        {
            "sha": "e39a2625277c8f846ea56c84d280af52df32ad56",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 54,
            "deletions": 10,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/29f322d04d700e9005e59d97352dde2979262c5e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29f322d04d700e9005e59d97352dde2979262c5e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=29f322d04d700e9005e59d97352dde2979262c5e",
            "patch": "@@ -1656,11 +1656,10 @@ def _get_initial_cache_position(self, input_ids, model_kwargs):\n         model_kwargs[\"cache_position\"] = cache_position\n         return model_kwargs\n \n-    def _get_layer_device_map_for_cache_init(self):\n+    def _get_layer_device_map_for_cache_init(self) -> Optional[Dict[int, Union[str, int]]]:\n         \"\"\"\n-        Taken from `dispatch_model` from accelerate.\n-        This is needed here if we don't want to make changes in accelerate in order to save execution_device\n-        For offloaded case, we need to get the execution device, not just the device where it is offloaded\n+        Returns the device map for each decoder layer, to allocate the cache on the right device.\n+        Inspired from `dispatch_model` in accelerate.\n         \"\"\"\n         execution_device_map = None\n \n@@ -1674,17 +1673,62 @@ def _get_layer_device_map_for_cache_init(self):\n                 for name, device in self.hf_device_map.items()\n             }\n \n-        num_hidden_layers = self.config.get_text_config().num_hidden_layers\n+        # No `execution_device_map` -> rely on `self.device` to allocate the cache\n         if execution_device_map is None:\n             return None\n-        elif len(execution_device_map) == 1 and \"\" in execution_device_map:\n+\n+        # Single device for all layers\n+        num_hidden_layers = self.config.get_text_config().num_hidden_layers\n+        if len(execution_device_map) == 1 and \"\" in execution_device_map:\n             return dict.fromkeys(range(num_hidden_layers), execution_device_map[\"\"])\n+\n+        # Multiple devices in `execution_device_map` -> we need to map decoder layers to the correct device.\n         layer_device_map = {}\n-        for layer in execution_device_map:\n-            for idx in range(num_hidden_layers):\n-                if f\".{idx}.\" in f\"{layer}.\":\n-                    layer_device_map[idx] = execution_device_map[layer]\n+        # Case 1: The model has a `get_decoder` method, we can use it to find the decoder name.\n+        if hasattr(self, \"get_decoder\"):\n+            decoder_name = None\n+            for name, module in self.named_modules():\n+                if module is self.get_decoder():\n+                    decoder_name = name\n                     break\n+            if decoder_name is None:\n+                raise RuntimeError(\n+                    \"`model.get_decoder()` is not returning a named module of the model. This is unexpected, please \"\n+                    \"open an issue on GitHub.\"\n+                )\n+\n+            decoder_mapped_modules = [\n+                module_name for module_name in execution_device_map.keys() if decoder_name in module_name\n+            ]\n+            # The decoder name may be present in `execution_device_map` in two forms:\n+            # a) each layer has a device mapping\n+            if len(decoder_mapped_modules) >= num_hidden_layers:\n+                for idx in range(num_hidden_layers):\n+                    for module_name in decoder_mapped_modules:\n+                        if f\".{idx}.\" in f\"{module_name}.\":\n+                            layer_device_map[idx] = execution_device_map[module_name]\n+                            break\n+\n+            # b) the whole module is mapped to a single device. If the decoder name is NOT present in the device map,\n+            # then the mapping is done in a parent module\n+            else:\n+                while True:\n+                    if decoder_name in execution_device_map:\n+                        layer_device_map = dict.fromkeys(range(num_hidden_layers), execution_device_map[decoder_name])\n+                        break\n+                    elif \".\" in decoder_name:\n+                        decoder_name = decoder_name.rsplit(\".\", 1)[0]  # gets the name of the parent module\n+                    else:\n+                        raise RuntimeError(f\"Decoder name {decoder_name} not found in execution device map\")\n+\n+        # Case 2: Legacy code path: assume the decoder layers are named as `(...).X` (X being the layer index)\n+        else:\n+            for layer in execution_device_map:\n+                for idx in range(num_hidden_layers):\n+                    if f\".{idx}.\" in f\"{layer}.\":\n+                        layer_device_map[idx] = execution_device_map[layer]\n+                        break\n+\n         for idx in range(num_hidden_layers):\n             if idx not in layer_device_map:\n                 raise RuntimeError(f\"layer {idx} has not been mapped to a device.\")"
        },
        {
            "sha": "bccc344f17aa0c35a74747d0fab14cbe4db4f58c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/29f322d04d700e9005e59d97352dde2979262c5e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29f322d04d700e9005e59d97352dde2979262c5e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=29f322d04d700e9005e59d97352dde2979262c5e",
            "patch": "@@ -56,6 +56,7 @@\n \n     from transformers import (\n         AutoModelForCausalLM,\n+        AutoModelForImageTextToText,\n         AutoModelForSeq2SeqLM,\n         AutoModelForSpeechSeq2Seq,\n         AutoModelForVision2Seq,\n@@ -4720,6 +4721,60 @@ def test_generate_vision2text_conditioning(self):\n         self.assertTrue(np.array_equal(output_sequences_decoder_input_ids, output_sequences_input_ids))\n         self.assertTrue(np.array_equal(output_sequences_decoder_input_ids[:, 1:2], conditioning_input))\n \n+    @slow\n+    @require_torch_gpu\n+    def test_cache_device_map_with_vision_layer_device_map(self):\n+        \"\"\"\n+        Test that the cache device map is correctly set when the vision layer has a device map. Regression test for\n+        #36942\n+        \"\"\"\n+        # gemma 3 uses hybrid cache, which can be compiled -> needs a device map at allocation time\n+        model_id = \"google/gemma-3-4b-it\"\n+\n+        # important part of this device map: the `.layers.` pattern is NOT present in the decoder\n+        device_map = {\n+            \"vision_tower.vision_model.embeddings\": 0,\n+            \"vision_tower.vision_model.encoder.layers.0\": 0,\n+            \"vision_tower.vision_model.encoder.layers.1\": 0,\n+            \"vision_tower.vision_model.encoder.layers.2\": 0,\n+            \"vision_tower.vision_model.encoder.layers.3\": 0,\n+            \"vision_tower.vision_model.encoder.layers.4\": 0,\n+            \"vision_tower.vision_model.encoder.layers.5\": 0,\n+            \"vision_tower.vision_model.encoder.layers.6\": 0,\n+            \"vision_tower.vision_model.encoder.layers.7\": 0,\n+            \"vision_tower.vision_model.encoder.layers.8\": 0,\n+            \"vision_tower.vision_model.encoder.layers.9\": 0,\n+            \"vision_tower.vision_model.encoder.layers.10\": 0,\n+            \"vision_tower.vision_model.encoder.layers.11\": 0,\n+            \"vision_tower.vision_model.encoder.layers.12\": 0,\n+            \"vision_tower.vision_model.encoder.layers.13\": 0,\n+            \"vision_tower.vision_model.encoder.layers.14\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.15\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.16\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.17\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.18\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.19\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.20\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.21\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.22\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.23\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.24\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.25\": \"cpu\",\n+            \"vision_tower.vision_model.encoder.layers.26\": \"cpu\",\n+            \"vision_tower.vision_model.post_layernorm\": \"cpu\",\n+            \"multi_modal_projector\": \"cpu\",\n+            \"language_model\": \"cpu\",\n+        }\n+\n+        model = AutoModelForImageTextToText.from_pretrained(\n+            model_id, device_map=device_map, torch_dtype=torch.bfloat16\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        inputs = tokenizer([\"This is a text input\"], return_tensors=\"pt\").to(model.device)\n+\n+        # If the generate doesn't infer the DECODER device map correctly, this will fail\n+        _ = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 109,
        "deletions": 10
    }
}