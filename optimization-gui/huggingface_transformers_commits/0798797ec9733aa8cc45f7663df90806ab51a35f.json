{
    "author": "remi-or",
    "message": "Fix an import error with PreTrainModel (#41571)",
    "sha": "0798797ec9733aa8cc45f7663df90806ab51a35f",
    "files": [
        {
            "sha": "9874c150e0946ee7874d98be8e83840b430725c0",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0798797ec9733aa8cc45f7663df90806ab51a35f/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0798797ec9733aa8cc45f7663df90806ab51a35f/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=0798797ec9733aa8cc45f7663df90806ab51a35f",
            "patch": "@@ -32,6 +32,9 @@\n \n \n def _assign_original_dtype(module, original_dtype):\n+    # not very nice in a recursive function but it avoids a circular import\n+    from ..modeling_utils import PreTrainedModel\n+\n     for child in module.children():\n         if isinstance(child, PreTrainedModel):\n             child.config._pre_quantization_dtype = original_dtype"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}