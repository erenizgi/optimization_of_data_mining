{
    "author": "tengomucho",
    "message": "Mitigate a conflict when using sentencepiece (#33327)\n\n* test(tokenizers): add a test showing conflict with sentencepiece\r\n\r\nThis is due to the fact that protobuf C implementation uses a global\r\npool for all added descriptors, so if two different files add\r\ndescriptors, they will end up conflicting.\r\n\r\n* fix(tokenizers): mitigate sentencepiece/protobuf conflict\r\n\r\nWhen sentencepiece is available, use that protobuf instead of the\r\ninternal one.\r\n\r\n* chore(style): fix with ruff",
    "sha": "7a5659872a68ce9939c975b5727e5ac61136f256",
    "files": [
        {
            "sha": "eb75a46a6d9bf279e9dabef1be70987842a8ac0c",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a5659872a68ce9939c975b5727e5ac61136f256/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a5659872a68ce9939c975b5727e5ac61136f256/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=7a5659872a68ce9939c975b5727e5ac61136f256",
            "patch": "@@ -26,14 +26,18 @@\n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE, Unigram, WordPiece\n \n-from .utils import is_protobuf_available, logging, requires_backends\n+from .utils import is_protobuf_available, is_sentencepiece_available, logging, requires_backends\n from .utils.import_utils import PROTOBUF_IMPORT_ERROR\n \n \n logger = logging.get_logger(__name__)\n \n \n def import_protobuf(error_message=\"\"):\n+    if is_sentencepiece_available():\n+        from sentencepiece import sentencepiece_model_pb2\n+\n+        return sentencepiece_model_pb2\n     if is_protobuf_available():\n         import google.protobuf\n "
        },
        {
            "sha": "b43923df84d712ec6848351cd6c732bce406890d",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a5659872a68ce9939c975b5727e5ac61136f256/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a5659872a68ce9939c975b5727e5ac61136f256/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=7a5659872a68ce9939c975b5727e5ac61136f256",
            "patch": "@@ -35,7 +35,15 @@\n     is_tokenizers_available,\n )\n from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n-from transformers.testing_utils import CaptureStderr, require_flax, require_tf, require_tokenizers, require_torch, slow\n+from transformers.testing_utils import (\n+    CaptureStderr,\n+    require_flax,\n+    require_sentencepiece,\n+    require_tf,\n+    require_tokenizers,\n+    require_torch,\n+    slow,\n+)\n \n \n if is_tokenizers_available():\n@@ -296,3 +304,13 @@ def test_len_tokenizer(self):\n                 self.assertEqual(len(tokenizer), tokenizer.vocab_size + 1)\n                 self.assertEqual(len(tokenizer.added_tokens_decoder), added_tokens_size + 1)\n                 self.assertEqual(len(tokenizer.added_tokens_encoder), added_tokens_size + 1)\n+\n+    @require_sentencepiece\n+    def test_sentencepiece_cohabitation(self):\n+        from sentencepiece import sentencepiece_model_pb2 as _original_protobuf  # noqa: F401\n+\n+        from transformers.convert_slow_tokenizer import import_protobuf  # noqa: F401\n+\n+        # Now this will try to import sentencepiece_model_pb2_new.py. This should not fail even if the protobuf\n+        # was already imported.\n+        import_protobuf()"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 24,
        "deletions": 2
    }
}