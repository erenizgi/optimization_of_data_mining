{
    "author": "jiqing-feng",
    "message": "Fix Aria tests (#37444)\n\n* update aria tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* add cuda tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* check outputs for cpu and cuda and xpu\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* check outputs for cpu and cuda and xpu\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* check outputs for cpu and cuda and xpu\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* check output for each device\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix style\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix style\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix xpu output\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* add comments and use assert list equal\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* rm pad token assign\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "b7f7aa78a0d299715b040463796484d5b0818875",
    "files": [
        {
            "sha": "fb3404d263d2a6c459be3cfe74388c12151e6d3b",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 44,
            "deletions": 16,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7f7aa78a0d299715b040463796484d5b0818875/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7f7aa78a0d299715b040463796484d5b0818875/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=b7f7aa78a0d299715b040463796484d5b0818875",
            "patch": "@@ -420,8 +420,11 @@ def test_small_model_integration_test_llama_batched_regression(self):\n     @require_vision\n     @require_bitsandbytes\n     def test_batched_generation(self):\n-        model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n-\n+        # Skip multihead_attn for 4bit because MHA will read the original weight without dequantize.\n+        # See https://github.com/huggingface/transformers/pull/37444#discussion_r2045852538.\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            \"rhymes-ai/Aria\", load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]\n+        )\n         processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n \n         prompt1 = \"<image>\\n<image>\\nUSER: What's the difference of two images?\\nASSISTANT:\"\n@@ -432,24 +435,49 @@ def test_batched_generation(self):\n         image1 = Image.open(requests.get(url1, stream=True).raw)\n         image2 = Image.open(requests.get(url2, stream=True).raw)\n \n-        inputs = processor(\n-            images=[image1, image2, image1, image2],\n-            text=[prompt1, prompt2, prompt3],\n-            return_tensors=\"pt\",\n-            padding=True,\n-        ).to(torch_device)\n-\n-        model = model.eval()\n-\n-        EXPECTED_OUTPUT = [\n-            \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n-            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small\",\n-            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the\",\n+        # Create inputs\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": prompt1},\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": prompt2},\n+                ],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": prompt3},\n+                ],\n+            },\n         ]\n \n+        prompts = [processor.apply_chat_template([message], add_generation_prompt=True) for message in messages]\n+        images = [[image1, image2], [image2]]\n+        inputs = processor(text=prompts, images=images, padding=True, return_tensors=\"pt\").to(\n+            device=model.device, dtype=model.dtype\n+        )\n+\n+        EXPECTED_OUTPUT = {\n+            \"cpu\": [\n+                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n+                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a grassy hill. The alpaca has\",\n+            ],  # cpu output\n+            \"cuda\": [\n+                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n+                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a patch of ground with some dry grass. The\",\n+            ],  # cuda output\n+            \"xpu\": [\n+                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n+                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a grassy hill. The alpaca has\",\n+            ],  # xpu output\n+        }\n         generate_ids = model.generate(**inputs, max_new_tokens=20)\n         outputs = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-        self.assertEqual(outputs, EXPECTED_OUTPUT)\n+        self.assertListEqual(outputs, EXPECTED_OUTPUT[model.device.type])\n \n     def test_tokenizer_integration(self):\n         model_id = \"rhymes-ai/Aria\""
        }
    ],
    "stats": {
        "total": 60,
        "additions": 44,
        "deletions": 16
    }
}