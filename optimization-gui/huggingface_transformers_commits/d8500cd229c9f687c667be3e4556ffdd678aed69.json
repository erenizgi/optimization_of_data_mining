{
    "author": "yonigozlan",
    "message": "Uniformize kwargs for Pixtral processor (#33521)\n\n* add uniformized pixtral and kwargs\r\n\r\n* update doc\r\n\r\n* fix _validate_images_text_input_order\r\n\r\n* nit",
    "sha": "d8500cd229c9f687c667be3e4556ffdd678aed69",
    "files": [
        {
            "sha": "1c610d19b681c4e1a96b3c71eada8e272b3618e1",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -24,7 +24,7 @@ The Pixtral model was released by the Mistral AI team on [Vllm](https://github.c\n Tips:\n \n - Pixtral is a multimodal model, the main contribution is the 2d ROPE on the images, and support for arbitrary image size (the images are not padded together nor are they resized)\n-- This model follows the `Llava` familiy, meaning image embeddings are placed instead of the `[IMG]` token placeholders. \n+- This model follows the `Llava` familiy, meaning image embeddings are placed instead of the `[IMG]` token placeholders.\n - The format for one or mulitple prompts is the following:\n ```\n \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n@@ -35,7 +35,7 @@ This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts)\n \n Here is an example of how to run it:\n \n-```python \n+```python\n from transformers import LlavaForConditionalGeneration, AutoProcessor\n from PIL import Image\n \n@@ -51,7 +51,7 @@ IMG_URLS = [\n ]\n PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n \n-inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(\"cuda\")\n+inputs = processor(images=IMG_URLS, text=PROMPT, return_tensors=\"pt\").to(\"cuda\")\n generate_ids = model.generate(**inputs, max_new_tokens=500)\n ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n "
        },
        {
            "sha": "8c32b8750b0358a7b569fe7ae95078445a687327",
            "filename": "src/transformers/models/pixtral/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -43,7 +43,8 @@\n \n \n if TYPE_CHECKING:\n-    from .configuration_pixtral import PixtralProcessor, PixtralVisionConfig\n+    from .configuration_pixtral import PixtralVisionConfig\n+    from .processing_pixtral import PixtralProcessor\n \n     try:\n         if not is_torch_available():"
        },
        {
            "sha": "1b07aa02771dc91d3db274c485651d0856148439",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 40,
            "deletions": 34,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -16,18 +16,36 @@\n Processor class for Pixtral.\n \"\"\"\n \n-from typing import List, Optional, Union\n+import sys\n+from typing import List, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType, is_torch_device, is_torch_dtype, is_torch_tensor, logging, requires_backends\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import is_torch_device, is_torch_dtype, is_torch_tensor, logging, requires_backends\n \n \n+if sys.version_info >= (3, 11):\n+    from typing import Unpack\n+else:\n+    from typing_extensions import Unpack\n+\n logger = logging.get_logger(__name__)\n \n \n+class PixtralProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"images_kwargs\": {},\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+        },\n+    }\n+\n+\n # Copied from transformers.models.idefics2.processing_idefics2.is_url\n def is_url(val) -> bool:\n     return isinstance(val, str) and val.startswith(\"http\")\n@@ -143,12 +161,11 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         images: ImageInput = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length=None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[PixtralProcessorKwargs],\n     ) -> BatchMixFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -158,26 +175,13 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n             text (`str`, `List[str]`, `List[List[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n \n@@ -195,6 +199,15 @@ def __call__(\n             `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            PixtralProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n         if images is not None:\n             if is_image_or_image_url(images):\n                 images = [[images]]\n@@ -209,7 +222,7 @@ def __call__(\n                     \"Invalid input images. Please provide a single image or a list of images or a list of list of images.\"\n                 )\n             images = [[load_image(im) for im in sample] for sample in images]\n-            image_inputs = self.image_processor(images, patch_size=self.patch_size, return_tensors=return_tensors)\n+            image_inputs = self.image_processor(images, patch_size=self.patch_size, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}\n \n@@ -246,16 +259,9 @@ def __call__(\n                 while \"<placeholder>\" in sample:\n                     replace_str = replace_strings.pop(0)\n                     sample = sample.replace(\"<placeholder>\", replace_str, 1)\n-\n                 prompt_strings.append(sample)\n \n-        text_inputs = self.tokenizer(\n-            prompt_strings,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-        )\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n         return BatchMixFeature(data={**text_inputs, **image_inputs})\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama"
        },
        {
            "sha": "ddf40c4fe6d44270231b77fdf16841e61297c8df",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -27,7 +27,7 @@\n import numpy as np\n \n from .dynamic_module_utils import custom_object_save\n-from .image_utils import ChannelDimension, is_vision_available, valid_images\n+from .image_utils import ChannelDimension, is_valid_image, is_vision_available\n \n \n if is_vision_available():\n@@ -1003,6 +1003,20 @@ def _validate_images_text_input_order(images, text):\n     in the processor's `__call__` method before calling this method.\n     \"\"\"\n \n+    def is_url(val) -> bool:\n+        return isinstance(val, str) and val.startswith(\"http\")\n+\n+    def _is_valid_images_input_for_processor(imgs):\n+        # If we have an list of images, make sure every image is valid\n+        if isinstance(imgs, (list, tuple)):\n+            for img in imgs:\n+                if not _is_valid_images_input_for_processor(img):\n+                    return False\n+        # If not a list or tuple, we have been given a single image or batched tensor of images\n+        elif not (is_valid_image(imgs) or is_url(imgs)):\n+            return False\n+        return True\n+\n     def _is_valid_text_input_for_processor(t):\n         if isinstance(t, str):\n             # Strings are fine\n@@ -1019,11 +1033,11 @@ def _is_valid_text_input_for_processor(t):\n     def _is_valid(input, validator):\n         return validator(input) or input is None\n \n-    images_is_valid = _is_valid(images, valid_images)\n-    images_is_text = _is_valid_text_input_for_processor(images) if not images_is_valid else False\n+    images_is_valid = _is_valid(images, _is_valid_images_input_for_processor)\n+    images_is_text = _is_valid_text_input_for_processor(images)\n \n     text_is_valid = _is_valid(text, _is_valid_text_input_for_processor)\n-    text_is_images = valid_images(text) if not text_is_valid else False\n+    text_is_images = _is_valid_images_input_for_processor(text)\n     # Handle cases where both inputs are valid\n     if images_is_valid and text_is_valid:\n         return images, text"
        },
        {
            "sha": "04aa3ee8a38b4ee3891e74286d8e99d3887f7714",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 178,
            "deletions": 20,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -11,14 +11,21 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import shutil\n+import tempfile\n import unittest\n \n import requests\n import torch\n \n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_vision,\n+)\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n     from PIL import Image\n@@ -27,7 +34,7 @@\n \n \n @require_vision\n-class PixtralProcessorTest(unittest.TestCase):\n+class PixtralProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = PixtralProcessor\n \n     @classmethod\n@@ -40,15 +47,20 @@ def setUpClass(cls):\n         cls.image_2 = Image.open(requests.get(cls.url_2, stream=True).raw)\n \n     def setUp(self):\n-        super().setUp()\n+        self.tmpdirname = tempfile.mkdtemp()\n \n         # FIXME - just load the processor directly from the checkpoint\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/pixtral-12b\")\n         image_processor = PixtralImageProcessor()\n-        self.processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n \n     @unittest.skip(\"No chat template was set for this model (yet)\")\n     def test_chat_template(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         expected_prompt = \"USER: [IMG]\\nWhat is shown in this image? ASSISTANT:\"\n \n         messages = [\n@@ -60,11 +72,12 @@ def test_chat_template(self):\n                 ],\n             },\n         ]\n-        formatted_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n \n     @unittest.skip(\"No chat template was set for this model (yet)\")\n     def test_image_token_filling(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         # Important to check with non square image\n         image = torch.randint(0, 2, (3, 500, 316))\n         expected_image_tokens = 1526\n@@ -79,23 +92,24 @@ def test_image_token_filling(self):\n                 ],\n             },\n         ]\n-        inputs = self.processor(\n-            text=[self.processor.apply_chat_template(messages)],\n+        inputs = processor(\n+            text=[processor.apply_chat_template(messages)],\n             images=[image],\n             return_tensors=\"pt\",\n         )\n         image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n         self.assertEqual(expected_image_tokens, image_tokens)\n \n     def test_processor_with_single_image(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         prompt_string = \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\"\n \n         # Make small for checking image token expansion\n-        self.processor.image_processor.size = {\"longest_edge\": 30}\n-        self.processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n \n         # Test passing in an image\n-        inputs_image = self.processor(text=prompt_string, images=self.image_0, return_tensors=\"pt\")\n+        inputs_image = processor(text=prompt_string, images=self.image_0, return_tensors=\"pt\")\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n@@ -115,7 +129,7 @@ def test_processor_with_single_image(self):\n         # fmt: on\n \n         # Test passing in a url\n-        inputs_url = self.processor(text=prompt_string, images=self.url_0, return_tensors=\"pt\")\n+        inputs_url = processor(text=prompt_string, images=self.url_0, return_tensors=\"pt\")\n         self.assertIn(\"input_ids\", inputs_url)\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n@@ -135,14 +149,15 @@ def test_processor_with_single_image(self):\n         # fmt: on\n \n     def test_processor_with_multiple_images_single_list(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         prompt_string = \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\"\n \n         # Make small for checking image token expansion\n-        self.processor.image_processor.size = {\"longest_edge\": 30}\n-        self.processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n \n         # Test passing in an image\n-        inputs_image = self.processor(text=prompt_string, images=[self.image_0, self.image_1], return_tensors=\"pt\")\n+        inputs_image = processor(text=prompt_string, images=[self.image_0, self.image_1], return_tensors=\"pt\")\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n@@ -162,7 +177,7 @@ def test_processor_with_multiple_images_single_list(self):\n         # fmt: on\n \n         # Test passing in a url\n-        inputs_url = self.processor(text=prompt_string, images=[self.url_0, self.url_1], return_tensors=\"pt\")\n+        inputs_url = processor(text=prompt_string, images=[self.url_0, self.url_1], return_tensors=\"pt\")\n         self.assertIn(\"input_ids\", inputs_url)\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n@@ -181,19 +196,20 @@ def test_processor_with_multiple_images_single_list(self):\n         # fmt: on\n \n     def test_processor_with_multiple_images_multiple_lists(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         prompt_string = [\n             \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\",\n             \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\",\n         ]\n-        self.processor.tokenizer.pad_token = \"</s>\"\n+        processor.tokenizer.pad_token = \"</s>\"\n         image_inputs = [[self.image_0, self.image_1], [self.image_2]]\n \n         # Make small for checking image token expansion\n-        self.processor.image_processor.size = {\"longest_edge\": 30}\n-        self.processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n \n         # Test passing in an image\n-        inputs_image = self.processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        inputs_image = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 2)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n@@ -213,7 +229,7 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         # fmt: on\n \n         # Test passing in a url\n-        inputs_url = self.processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        inputs_url = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n         self.assertIn(\"input_ids\", inputs_url)\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 2)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n@@ -231,3 +247,145 @@ def test_processor_with_multiple_images_multiple_lists(self):\n             [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n+\n+    # Override all tests requiring shape as returning tensor batches is not supported by PixtralProcessor\n+\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size={\"height\": 240, \"width\": 240})\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        # Added dimension by pixtral image processor\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0][0]), 240)\n+\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size={\"height\": 400, \"width\": 400})\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, size={\"height\": 240, \"width\": 240})\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0][0]), 240)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 240, \"width\": 240}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 240, \"width\": 240}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 240, \"width\": 240},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        # images needs to be nested to detect multiple prompts\n+        image_input = [self.prepare_image_inputs()] * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 240, \"width\": 240},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 4)"
        },
        {
            "sha": "f3111f5d57851f2530b14b7f21aba47740feda6d",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -64,6 +64,8 @@ def get_component(self, attribute, **kwargs):\n         component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n         if attribute == \"tokenizer\" and not component.pad_token:\n             component.pad_token = \"[TEST_PAD]\"\n+            if component.pad_token_id is None:\n+                component.pad_token_id = 0\n \n         return component\n "
        },
        {
            "sha": "f669da25385fab0fcccf1cfec53bd2ac397921c1",
            "filename": "tests/utils/test_processing_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8500cd229c9f687c667be3e4556ffdd678aed69/tests%2Futils%2Ftest_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8500cd229c9f687c667be3e4556ffdd678aed69/tests%2Futils%2Ftest_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_processing_utils.py?ref=d8500cd229c9f687c667be3e4556ffdd678aed69",
            "patch": "@@ -80,6 +80,18 @@ def test_validate_images_text_input_order(self):\n         self.assertTrue(np.array_equal(valid_images[0], images[0]))\n         self.assertEqual(valid_text, text)\n \n+        # list of strings and list of url images inputs\n+        images = [\"https://url1\", \"https://url2\"]\n+        text = [\"text1\", \"text2\"]\n+        # test correct text and images order\n+        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n+        self.assertEqual(valid_images, images)\n+        self.assertEqual(valid_text, text)\n+        # test incorrect text and images order\n+        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n+        self.assertEqual(valid_images, images)\n+        self.assertEqual(valid_text, text)\n+\n         # list of strings and nested list of numpy images inputs\n         images = [[np.random.rand(224, 224, 3), np.random.rand(224, 224, 3)], [np.random.rand(224, 224, 3)]]\n         text = [\"text1\", \"text2\"]"
        }
    ],
    "stats": {
        "total": 317,
        "additions": 255,
        "deletions": 62
    }
}