{
    "author": "devxaitist",
    "message": "Add Fast Image Processor for vilt (#37304)\n\n* init vilt image processor fast\n\n* Refactor image processor tests to use loop for all processors\n\n* Add ViltImageProcessorFast with PyTorch-based optimized image processing\n\n* Change made automatically by make fixup command\n\n* Change made automatically by make fix-copies command\n\n* Fix type hints in ViltImageProcessorFast for Python compatibility\n\n* Define constants for image resizing based on COCO dataset aspect ratio\n\n* Add missing property initializations to ViltImageProcessorFast\n\n* Extract resize logic into dedicated method in ViltImageProcessorFast\n\n* Extract padding logic into dedicated method\n\n* Implement shape-based image grouping for optimized processing in Vilt\n\n* Update test suite to verify ViltImageProcessorFast attributes\n\n* Move variable declarations to _preprocess method parameters\n\n* Remove unused parameters\n\n* Rename _resize method to resize to override existing function\n\n* Remove whitespace\n\n* Remove unnecessary type check and conversion for stacked_images\n\n* Remove redundant loop and apply padding directly to stacked images\n\n* Refactor pad function to return images and mask as tuple instead of dict\n\n* Add tests comparing padding masks in slow and fast implementations\n\n* Update ViltImageProcessor tests to ensure compatibility between slow and fast implementations\n\n* Replace add_start_docstrings with auto_docstring in ViltImageProcessorFast\n\n* Move docstrings of custom args to ViltFastImageProcessorKwargs\n\n* Use reorder_images function for both masks and images\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "342961f6696115970593a8ec8e727145cb5a499f",
    "files": [
        {
            "sha": "ea598cbbe25524bcc8bf6c877d3baa234ce54954",
            "filename": "docs/source/en/model_doc/vilt.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/342961f6696115970593a8ec8e727145cb5a499f/docs%2Fsource%2Fen%2Fmodel_doc%2Fvilt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/342961f6696115970593a8ec8e727145cb5a499f/docs%2Fsource%2Fen%2Fmodel_doc%2Fvilt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvilt.md?ref=342961f6696115970593a8ec8e727145cb5a499f",
            "patch": "@@ -72,6 +72,11 @@ This model was contributed by [nielsr](https://huggingface.co/nielsr). The origi\n [[autodoc]] ViltImageProcessor\n     - preprocess\n \n+## ViltImageProcessorFast\n+\n+[[autodoc]] ViltImageProcessorFast\n+    - preprocess\n+\n ## ViltProcessor\n \n [[autodoc]] ViltProcessor"
        },
        {
            "sha": "2ec63a14b12293886d3c018fe8ad1aa858e619a4",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/342961f6696115970593a8ec8e727145cb5a499f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/342961f6696115970593a8ec8e727145cb5a499f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=342961f6696115970593a8ec8e727145cb5a499f",
            "patch": "@@ -161,7 +161,7 @@\n             (\"upernet\", (\"SegformerImageProcessor\",)),\n             (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"videomae\", (\"VideoMAEImageProcessor\",)),\n-            (\"vilt\", (\"ViltImageProcessor\",)),\n+            (\"vilt\", (\"ViltImageProcessor\", \"ViltImageProcessorFast\")),\n             (\"vipllava\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"vit\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vit_hybrid\", (\"ViTHybridImageProcessor\",)),"
        },
        {
            "sha": "4f154e79e7b9985cefa83c0df78d6f3db3f70e6e",
            "filename": "src/transformers/models/vilt/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/342961f6696115970593a8ec8e727145cb5a499f/src%2Ftransformers%2Fmodels%2Fvilt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/342961f6696115970593a8ec8e727145cb5a499f/src%2Ftransformers%2Fmodels%2Fvilt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2F__init__.py?ref=342961f6696115970593a8ec8e727145cb5a499f",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_vilt import *\n     from .feature_extraction_vilt import *\n     from .image_processing_vilt import *\n+    from .image_processing_vilt_fast import *\n     from .modeling_vilt import *\n     from .processing_vilt import *\n else:"
        },
        {
            "sha": "764e1203bf6a1c5b485e5887262f67463eac8fca",
            "filename": "src/transformers/models/vilt/image_processing_vilt_fast.py",
            "status": "added",
            "additions": 259,
            "deletions": 0,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/342961f6696115970593a8ec8e727145cb5a499f/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/342961f6696115970593a8ec8e727145cb5a499f/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py?ref=342961f6696115970593a8ec8e727145cb5a499f",
            "patch": "@@ -0,0 +1,259 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Vilt.\"\"\"\n+\n+from typing import List, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    get_max_height_width,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling, SizeDict\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+# Set maximum size based on the typical aspect ratio of the COCO dataset\n+MAX_LONGER_EDGE = 1333\n+MAX_SHORTER_EDGE = 800\n+\n+\n+class ViltFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    Args:\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image. If `True`, will pad the images in the batch to the largest height and width\n+            in the batch. Padding will be applied to the bottom and right with zeros.\n+        size_divisor (`int`, *optional*, defaults to 32):\n+            The size to make the height and width divisible by.\n+        rescale_factor (`float`, *optional*, defaults to 1/255):\n+            The factor to rescale the image by.\n+    \"\"\"\n+\n+    do_pad: Optional[bool]\n+    size_divisor: Optional[int]\n+    rescale_factor: Optional[float]\n+\n+\n+@auto_docstring\n+class ViltImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"shortest_edge\": 384}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    size_divisor = 32\n+    do_pad = True\n+    default_to_square = False\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = ViltFastImageProcessorKwargs\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        size_divisor: Optional[int],\n+        do_pad: bool,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        This method overrides the base class method to include padding and pixel mask generation.\n+        \"\"\"\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size, interpolation, size_divisor)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Handle padding if required\n+        data = {}\n+        if do_pad:\n+            pixel_values, pixel_mask = self._pad_batch(processed_images, return_tensors)\n+            data = {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask}\n+        else:\n+            # If no padding, just return the processed images\n+            if return_tensors == \"pt\":\n+                processed_images = torch.stack(processed_images)\n+            data = {\"pixel_values\": processed_images}\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def resize(\n+        self,\n+        images: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        size_divisor: Optional[int] = None,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image or batch of images to specified size.\n+\n+        Args:\n+            images (`torch.Tensor`): Image or batch of images to resize.\n+            size (`Dict[str, int]`): Size dictionary with shortest_edge key.\n+            interpolation (`F.InterpolationMode`, *optional*): Interpolation method to use.\n+            size_divisor (`int`, *optional*): Value to ensure height/width are divisible by.\n+\n+        Returns:\n+            `torch.Tensor`: Resized image or batch of images.\n+        \"\"\"\n+        if interpolation is None:\n+            interpolation = self.resample\n+\n+        # Resize with aspect ratio preservation\n+        shorter = size.shortest_edge\n+        longer = int(MAX_LONGER_EDGE / MAX_SHORTER_EDGE * shorter)\n+\n+        heights = images.shape[-2]\n+        widths = images.shape[-1]\n+\n+        # Determine the new dimensions\n+        if heights < widths:\n+            new_heights = shorter\n+            new_widths = widths * (shorter / heights)\n+        else:\n+            new_heights = heights * (shorter / widths)\n+            new_widths = shorter\n+\n+        # Check if the longer side exceeds max size\n+        if max(new_heights, new_widths) > longer:\n+            scale = longer / max(new_heights, new_widths)\n+            new_heights = new_heights * scale\n+            new_widths = new_widths * scale\n+\n+        new_heights = int(new_heights + 0.5)\n+        new_widths = int(new_widths + 0.5)\n+\n+        # Make dimensions divisible by size_divisor\n+        if size_divisor is not None:\n+            new_heights = new_heights // size_divisor * size_divisor\n+            new_widths = new_widths // size_divisor * size_divisor\n+\n+        # Resize the image\n+        return F.resize(images, [new_heights, new_widths], interpolation=interpolation)\n+\n+    def _pad_batch(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> tuple:\n+        \"\"\"\n+        Pad a batch of images to the same size based on the maximum dimensions.\n+\n+        Args:\n+            images (`list[torch.Tensor]`): List of images to pad.\n+            return_tensors (`str` or `TensorType`, *optional*): The type of tensors to return.\n+\n+        Returns:\n+            `tuple`: Tuple containing padded images and pixel masks.\n+        \"\"\"\n+        # Calculate global maximum dimensions across all images\n+        max_size = get_max_height_width(images)\n+\n+        # Group images by shape before padding\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        processed_images = {}\n+        processed_masks = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            # Create mask template for efficient masking\n+            if return_tensors == \"pt\" and len(stacked_images) > 0:\n+                device = stacked_images.device\n+                mask_template = torch.zeros(max_size, dtype=torch.int64, device=device)\n+\n+            original_size = stacked_images.shape[-2:]\n+            needs_padding = original_size[0] != max_size[0] or original_size[1] != max_size[1]\n+\n+            if needs_padding:\n+                padding_bottom = max_size[0] - original_size[0]\n+                padding_right = max_size[1] - original_size[1]\n+                padding = [0, 0, padding_right, padding_bottom]\n+\n+                padded_images = F.pad(stacked_images, padding, fill=0)\n+                pixel_mask = mask_template.clone()\n+                pixel_mask[: original_size[0], : original_size[1]].fill_(1)\n+                pixel_masks = pixel_mask.unsqueeze(0).repeat(stacked_images.shape[0], 1, 1)\n+            else:\n+                padded_images = stacked_images\n+                pixel_masks = torch.ones(\n+                    (stacked_images.shape[0], max_size[0], max_size[1]),\n+                    dtype=torch.int64,\n+                    device=stacked_images.device,\n+                )\n+\n+            # Store processed group\n+            processed_images[shape] = padded_images\n+            processed_masks[shape] = pixel_masks\n+\n+        # Reorder images back to original order\n+        padded_images = reorder_images(processed_images, grouped_images_index)\n+        pixel_masks = reorder_images(processed_masks, grouped_images_index)\n+\n+        # Stack if tensors are requested for final result\n+        if return_tensors == \"pt\" and padded_images:\n+            padded_images = torch.stack(padded_images)\n+            pixel_masks = torch.stack(pixel_masks)\n+\n+        return padded_images, pixel_masks\n+\n+\n+__all__ = [\"ViltImageProcessorFast\"]"
        },
        {
            "sha": "6ad1108631012987a183f4d2ee386bc910c24330",
            "filename": "tests/models/vilt/test_image_processing_vilt.py",
            "status": "modified",
            "additions": 43,
            "deletions": 12,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/342961f6696115970593a8ec8e727145cb5a499f/tests%2Fmodels%2Fvilt%2Ftest_image_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/342961f6696115970593a8ec8e727145cb5a499f/tests%2Fmodels%2Fvilt%2Ftest_image_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_image_processing_vilt.py?ref=342961f6696115970593a8ec8e727145cb5a499f",
            "patch": "@@ -16,9 +16,10 @@\n import unittest\n \n import numpy as np\n+import torch\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -28,6 +29,9 @@\n \n     from transformers import ViltImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import ViltImageProcessorFast\n+\n \n class ViltImageProcessingTester:\n     def __init__(\n@@ -131,6 +135,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class ViltImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ViltImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ViltImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -141,17 +146,43 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"model_input_names\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 30})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 30})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+\n+    def test_slow_fast_equivalence(self):\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict, do_pad=True)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict, do_pad=True)\n+\n+        slow_outputs = image_processor_slow(image_inputs, return_tensors=\"pt\")\n+        slow_pixel_values = slow_outputs.pixel_values\n+        slow_pixel_mask = slow_outputs.pixel_mask\n+\n+        fast_outputs = image_processor_fast(image_inputs, return_tensors=\"pt\")\n+        fast_pixel_values = fast_outputs.pixel_values\n+        fast_pixel_mask = fast_outputs.pixel_mask\n+\n+        self.assertEqual(slow_pixel_values.shape, fast_pixel_values.shape)\n+        self.assertTrue(torch.allclose(slow_pixel_values, fast_pixel_values, atol=1e-2))\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+        self.assertEqual(slow_pixel_mask.shape, fast_pixel_mask.shape)\n+        self.assertTrue(torch.equal(slow_pixel_mask, fast_pixel_mask))"
        }
    ],
    "stats": {
        "total": 322,
        "additions": 309,
        "deletions": 13
    }
}