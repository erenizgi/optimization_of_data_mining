{
    "author": "hawon223",
    "message": "Move max_new_tokens recommendation into GenerationConfig docstring (#42384)\n\n* docs: clarify recommended usage of max_new_tokens in generate()\n\n* Move max_new_tokens recommendation into GenerationConfig docstring\n\n* Remove Note about max_new_tokens and max_length\n\n* Update src/transformers/generation/configuration_utils.py\r\n\r\nApply reviewer suggestion for clearer token explanation\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update src/transformers/generation/configuration_utils.py\r\n\r\nApply reviewer suggestion for clearer token explanation\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Remove Note about max_new_tokens and max_length\n\n* Docs: clean up empty line in text_generation.md\n\n* Docs: simplify max_length explanation in generation config\n\n* style: fix trailing whitespace in configuration_utils.py\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "f8e69286fb4b7f4cc778514e1c83c8a1f579f328",
    "files": [
        {
            "sha": "62d06b390c45ccfe9145c19667aa75beace68f07",
            "filename": "docs/source/en/main_classes/text_generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e69286fb4b7f4cc778514e1c83c8a1f579f328/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e69286fb4b7f4cc778514e1c83c8a1f579f328/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md?ref=f8e69286fb4b7f4cc778514e1c83c8a1f579f328",
            "patch": "@@ -41,4 +41,4 @@ like token streaming.\n \n [[autodoc]] GenerationMixin\n     - generate\n-    - compute_transition_scores\n+    - compute_transition_scores\n\\ No newline at end of file"
        },
        {
            "sha": "483ff2bce45272c52ddd05e86005fae56403caea",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e69286fb4b7f4cc778514e1c83c8a1f579f328/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e69286fb4b7f4cc778514e1c83c8a1f579f328/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=f8e69286fb4b7f4cc778514e1c83c8a1f579f328",
            "patch": "@@ -105,8 +105,9 @@ class GenerationConfig(PushToHubMixin):\n         > Parameters that control the length of the output\n \n         max_length (`int`, *optional*, defaults to 20):\n-            The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n-            `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set.\n+            `max_new_tokens` is recommended for controlling how many tokens the model generates.\n+            `max_length` remains for backward compatibility.\n+\n         max_new_tokens (`int`, *optional*):\n             The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n         min_length (`int`, *optional*, defaults to 0):"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 4,
        "deletions": 3
    }
}