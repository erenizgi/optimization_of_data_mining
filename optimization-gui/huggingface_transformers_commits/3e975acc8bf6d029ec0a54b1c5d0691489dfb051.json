{
    "author": "SunMarc",
    "message": "Fix docker quantization (#41201)\n\n* launch docker\n\n* remove gptq for now\n\n* run tests\n\n* Revert \"run tests\"\n\nThis reverts commit f85718ce3a21d5937bf7405b8925c125c67d1a3e.\n\n* revert",
    "sha": "3e975acc8bf6d029ec0a54b1c5d0691489dfb051",
    "files": [
        {
            "sha": "d025b32fd390d38de1b571ffe57099ad3a03fe0b",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e975acc8bf6d029ec0a54b1c5d0691489dfb051/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e975acc8bf6d029ec0a54b1c5d0691489dfb051/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=3e975acc8bf6d029ec0a54b1c5d0691489dfb051",
            "patch": "@@ -30,22 +30,21 @@ RUN python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio tor\n \n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate\n \n-# needed in bnb and awq\n-RUN python3 -m pip install --no-cache-dir einops\n-\n-# Add bitsandbytes for mixed int8 testing\n-RUN python3 -m pip install --no-cache-dir bitsandbytes\n-\n-# Add gptqmodel for gtpq quantization testing, installed from source for pytorch==2.6.0 compatibility\n-RUN python3 -m pip install lm_eval\n-RUN git clone https://github.com/ModelCloud/GPTQModel.git && cd GPTQModel && pip install -v . --no-build-isolation\n-\n # Add optimum for gptq quantization testing\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/optimum@main#egg=optimum\n \n # Add PEFT\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/peft@main#egg=peft\n \n+# needed in bnb and awq\n+RUN python3 -m pip install --no-cache-dir einops\n+\n+# Add bitsandbytes\n+RUN python3 -m pip install --no-cache-dir bitsandbytes\n+\n+# # Add gptqmodel\n+# RUN python3 -m pip install --no-cache-dir gptqmodel\n+\n # Add hqq for quantization testing\n RUN python3 -m pip install --no-cache-dir hqq\n "
        }
    ],
    "stats": {
        "total": 19,
        "additions": 9,
        "deletions": 10
    }
}