{
    "author": "ydshieh",
    "message": "Fix `phi4_multimodal` tests (#38816)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "309e8c96f2065a02167be7142077ffc49e817c34",
    "files": [
        {
            "sha": "6ef9aef96269e5402467cb7955f6032846ddf34f",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 26,
            "deletions": 10,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/309e8c96f2065a02167be7142077ffc49e817c34/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/309e8c96f2065a02167be7142077ffc49e817c34/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=309e8c96f2065a02167be7142077ffc49e817c34",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import gc\n import tempfile\n import unittest\n \n@@ -31,7 +30,15 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import backend_empty_cache, require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    require_soundfile,\n+    require_torch,\n+    require_torch_large_accelerator,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_soundfile_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -276,13 +283,14 @@ def test_flex_attention_with_grads(self):\n @slow\n class Phi4MultimodalIntegrationTest(unittest.TestCase):\n     checkpoint_path = \"microsoft/Phi-4-multimodal-instruct\"\n+    revision = \"refs/pr/70\"\n     image_url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n     audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n \n     def setUp(self):\n         # Currently, the Phi-4 checkpoint on the hub is not working with the latest Phi-4 code, so the slow integration tests\n         # won't pass without using the correct revision (refs/pr/70)\n-        self.processor = AutoProcessor.from_pretrained(self.checkpoint_path)\n+        self.processor = AutoProcessor.from_pretrained(self.checkpoint_path, revision=self.revision)\n         self.generation_config = GenerationConfig(max_new_tokens=20, do_sample=False)\n         self.user_token = \"<|user|>\"\n         self.assistant_token = \"<|assistant|>\"\n@@ -294,13 +302,14 @@ def setUp(self):\n             tmp.seek(0)\n             self.audio, self.sampling_rate = soundfile.read(tmp.name)\n \n+        cleanup(torch_device, gc_collect=True)\n+\n     def tearDown(self):\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     def test_text_only_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+            self.checkpoint_path, revision=self.revision, torch_dtype=torch.float16, device_map=torch_device\n         )\n \n         prompt = f\"{self.user_token}What is the answer for 1+1? Explain it.{self.end_token}{self.assistant_token}\"\n@@ -319,7 +328,7 @@ def test_text_only_generation(self):\n \n     def test_vision_text_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+            self.checkpoint_path, revision=self.revision, torch_dtype=torch.float16, device_map=torch_device\n         )\n \n         prompt = f\"{self.user_token}<|image|>What is shown in this image?{self.end_token}{self.assistant_token}\"\n@@ -332,13 +341,20 @@ def test_vision_text_generation(self):\n         output = output[:, inputs[\"input_ids\"].shape[1] :]\n         response = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n \n-        EXPECTED_RESPONSE = \"The image shows a vibrant scene at a street intersection in a city with a Chinese-influenced architectural\"\n+        EXPECTED_RESPONSES = Expectations(\n+            {\n+                (\"cuda\", 7): 'The image shows a vibrant scene at a traditional Chinese-style street entrance, known as a \"gate\"',\n+                (\"cuda\", 8): 'The image shows a vibrant scene at a street intersection in a city with a Chinese-influenced architectural',\n+            }\n+        )  # fmt: skip\n+        EXPECTED_RESPONSE = EXPECTED_RESPONSES.get_expectation()\n \n         self.assertEqual(response, EXPECTED_RESPONSE)\n \n+    @require_torch_large_accelerator\n     def test_multi_image_vision_text_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+            self.checkpoint_path, revision=self.revision, torch_dtype=torch.float16, device_map=torch_device\n         )\n \n         images = []\n@@ -365,7 +381,7 @@ def test_multi_image_vision_text_generation(self):\n     @require_soundfile\n     def test_audio_text_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+            self.checkpoint_path, revision=self.revision, torch_dtype=torch.float16, device_map=torch_device\n         )\n \n         prompt = f\"{self.user_token}<|audio|>What is happening in this audio?{self.end_token}{self.assistant_token}\""
        }
    ],
    "stats": {
        "total": 36,
        "additions": 26,
        "deletions": 10
    }
}