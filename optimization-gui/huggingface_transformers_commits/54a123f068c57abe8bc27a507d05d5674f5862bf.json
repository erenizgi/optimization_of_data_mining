{
    "author": "LysandreJik",
    "message": "Simplify soft dependencies and update the dummy-creation process (#36827)\n\n* Reverse dependency map shouldn't be created when test_all is set\n\n* [test_all] Remove dummies\n\n* Modular fixes\n\n* Update utils/check_repo.py\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\n\n* [test_all] Better docs\n\n* [test_all] Update src/transformers/commands/chat.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* [test_all] Remove deprecated AdaptiveEmbeddings from the tests\n\n* [test_all] Doc builder\n\n* [test_all] is_dummy\n\n* [test_all] Import utils\n\n* [test_all] Doc building should not require all deps\n\n---------\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "54a123f068c57abe8bc27a507d05d5674f5862bf",
    "files": [
        {
            "sha": "1a41368392fd338cc497321f256b173cc1ccbf63",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1078,6 +1078,8 @@\n       title: Utilities for Audio processing\n     - local: internal/file_utils\n       title: General Utilities\n+    - local: internal/import_utils\n+      title: Importing Utilities\n     - local: internal/time_series_utils\n       title: Utilities for Time Series\n     title: Internal helpers"
        },
        {
            "sha": "93daa2ced3a6fbfc348552a294f7515be3855df3",
            "filename": "docs/source/en/internal/import_utils.md",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -0,0 +1,91 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Import Utilities\n+\n+This page goes through the transformers utilities to enable lazy and fast object import.\n+While we strive for minimal dependencies, some models have specific dependencies requirements that cannot be\n+worked around. We don't want for all users of `transformers` to have to install those dependencies to use other models,\n+we therefore mark those as soft dependencies rather than hard dependencies.\n+\n+The transformers toolkit is not made to error-out on import of a model that has a specific dependency; instead, an\n+object for which you are lacking a dependency will error-out when calling any method on it. As an example, if \n+`torchvision` isn't installed, the fast image processors will not be available. \n+\n+This object is still importable:\n+\n+```python\n+>>> from transformers import DetrImageProcessorFast\n+>>> print(DetrImageProcessorFast)\n+<class 'DetrImageProcessorFast'>\n+```\n+\n+However, no method can be called on that object:\n+\n+```python\n+>>> DetrImageProcessorFast.from_pretrained()\n+ImportError: \n+DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Checkout the instructions on the\n+installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n+Please note that you may need to restart your runtime after installation.\n+```\n+\n+Let's see how to specify specific object dependencies.\n+\n+## Specifying Object Dependencies\n+\n+### Filename-based\n+\n+All objects under a given filename have an automatic dependency to the tool linked to the filename\n+\n+**TensorFlow**: All files starting with `modeling_tf_` have an automatic TensorFlow dependency.\n+\n+**Flax**: All files starting with `modeling_flax_` have an automatic Flax dependency\n+\n+**PyTorch**: All files starting with `modeling_` and not valid with the above (TensorFlow and Flax) have an automatic \n+PyTorch dependency\n+\n+**Tokenizers**: All files starting with `tokenization_` and ending with `_fast` have an automatic `tokenizers` dependency\n+\n+**Vision**: All files starting with `image_processing_` have an automatic dependency to the `vision` dependency group; \n+at the time of writing, this only contains the `pillow` dependency.\n+\n+**Vision + Torch + Torchvision**: All files starting with `image_processing_` and ending with `_fast` have an automatic\n+dependency to `vision`, `torch`, and `torchvision`.\n+\n+All of these automatic dependencies are added on top of the explicit dependencies that are detailed below.\n+\n+### Explicit Object Dependencies\n+\n+We add a method called `requires` that is used to explicitly specify the dependencies of a given object. As an\n+example, the `Trainer` class has two hard dependencies: `torch` and `accelerate`. Here is how we specify these \n+required dependencies:\n+\n+```python\n+from .utils.import_utils import requires\n+\n+@requires(backends=(\"torch\", \"accelerate\"))\n+class Trainer:\n+    ...\n+```\n+\n+Backends that can be added here are all the backends that are available in the `import_utils.py` module.\n+\n+## Methods\n+\n+[[autodoc]] utils.import_utils.define_import_structure\n+\n+[[autodoc]] utils.import_utils.requires"
        },
        {
            "sha": "efb6b27082afd0e3e020018442bb417ee3ce2901",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -88,6 +88,11 @@ The original code can be found [here](https://github.com/salesforce/BLIP).\n [[autodoc]] BlipTextModel\n     - forward\n \n+## BlipTextLMHeadModel\n+\n+[[autodoc]] BlipTextLMHeadModel\n+- forward\n+\n ## BlipVisionModel\n \n [[autodoc]] BlipVisionModel\n@@ -123,6 +128,11 @@ The original code can be found [here](https://github.com/salesforce/BLIP).\n [[autodoc]] TFBlipTextModel\n     - call\n \n+## TFBlipTextLMHeadModel\n+\n+[[autodoc]] TFBlipTextLMHeadModel\n+- forward\n+\n ## TFBlipVisionModel\n \n [[autodoc]] TFBlipVisionModel"
        },
        {
            "sha": "2d054920196c08db889c2a2a9bcd759ef24c5ca7",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 473,
            "deletions": 9275,
            "changes": 9748,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf"
        },
        {
            "sha": "dd08be294107856567a9bac2f03b5bf5bc072226",
            "filename": "src/transformers/image_processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fimage_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fimage_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n from .image_transforms import center_crop, normalize, rescale\n from .image_utils import ChannelDimension, get_image_size\n from .utils import logging\n+from .utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -33,6 +34,7 @@\n ]\n \n \n+@requires(backends=(\"vision\",))\n class BaseImageProcessor(ImageProcessingMixin):\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "78565ccbd22fde8fb6721c18c78ea4fed2f85a2b",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -68,6 +68,8 @@\n         from torchvision.transforms.v2 import functional as F\n     else:\n         from torchvision.transforms import functional as F\n+else:\n+    pil_torch_interpolation_mapping = None\n \n logger = logging.get_logger(__name__)\n "
        },
        {
            "sha": "21dbbe374cc2c834868aab882650cd0320ac2686",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -72,6 +72,8 @@\n             PILImageResampling.BICUBIC: InterpolationMode.BICUBIC,\n             PILImageResampling.LANCZOS: InterpolationMode.LANCZOS,\n         }\n+    else:\n+        pil_torch_interpolation_mapping = {}\n \n \n if TYPE_CHECKING:"
        },
        {
            "sha": "c419c2c273d62476446004d2353375553f1bff8a",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -20,7 +20,7 @@\n from contextlib import contextmanager\n from typing import Optional\n \n-from transformers.utils.import_utils import export\n+from transformers.utils.import_utils import requires\n \n from .utils import is_torch_available\n \n@@ -225,7 +225,7 @@ def final_hook(_, inputs, outputs):\n             break  # exit the loop after finding one (unsure, but should be just one call.)\n \n \n-@export(backends=(\"torch\",))\n+@requires(backends=(\"torch\",))\n def model_addition_debugger(cls):\n     \"\"\"\n     # Model addition debugger - a model adder tracer\n@@ -282,7 +282,7 @@ def wrapped_init(self, *args, **kwargs):\n     return cls\n \n \n-@export(backends=(\"torch\",))\n+@requires(backends=(\"torch\",))\n @contextmanager\n def model_addition_debugger_context(model, debug_path: Optional[str] = None):\n     \"\"\""
        },
        {
            "sha": "7b219104548ace6aff1fb4b976c3889c3516a9a3",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 316,
            "deletions": 312,
            "changes": 628,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,316 +11,320 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import TYPE_CHECKING\n \n-from . import (\n-    albert,\n-    align,\n-    altclip,\n-    aria,\n-    audio_spectrogram_transformer,\n-    auto,\n-    autoformer,\n-    aya_vision,\n-    bamba,\n-    bark,\n-    bart,\n-    barthez,\n-    bartpho,\n-    beit,\n-    bert,\n-    bert_generation,\n-    bert_japanese,\n-    bertweet,\n-    big_bird,\n-    bigbird_pegasus,\n-    biogpt,\n-    bit,\n-    blenderbot,\n-    blenderbot_small,\n-    blip,\n-    blip_2,\n-    bloom,\n-    bridgetower,\n-    bros,\n-    byt5,\n-    camembert,\n-    canine,\n-    chameleon,\n-    chinese_clip,\n-    clap,\n-    clip,\n-    clipseg,\n-    clvp,\n-    code_llama,\n-    codegen,\n-    cohere,\n-    cohere2,\n-    colpali,\n-    conditional_detr,\n-    convbert,\n-    convnext,\n-    convnextv2,\n-    cpm,\n-    cpmant,\n-    ctrl,\n-    cvt,\n-    dab_detr,\n-    dac,\n-    data2vec,\n-    dbrx,\n-    deberta,\n-    deberta_v2,\n-    decision_transformer,\n-    deepseek_v3,\n-    deformable_detr,\n-    deit,\n-    deprecated,\n-    depth_anything,\n-    depth_pro,\n-    detr,\n-    dialogpt,\n-    diffllama,\n-    dinat,\n-    dinov2,\n-    dinov2_with_registers,\n-    distilbert,\n-    dit,\n-    donut,\n-    dpr,\n-    dpt,\n-    efficientnet,\n-    electra,\n-    emu3,\n-    encodec,\n-    encoder_decoder,\n-    ernie,\n-    esm,\n-    falcon,\n-    falcon_mamba,\n-    fastspeech2_conformer,\n-    flaubert,\n-    flava,\n-    fnet,\n-    focalnet,\n-    fsmt,\n-    funnel,\n-    fuyu,\n-    gemma,\n-    gemma2,\n-    gemma3,\n-    git,\n-    glm,\n-    glm4,\n-    glpn,\n-    got_ocr2,\n-    gpt2,\n-    gpt_bigcode,\n-    gpt_neo,\n-    gpt_neox,\n-    gpt_neox_japanese,\n-    gpt_sw3,\n-    gptj,\n-    granite,\n-    granitemoe,\n-    granitemoeshared,\n-    grounding_dino,\n-    groupvit,\n-    helium,\n-    herbert,\n-    hiera,\n-    hubert,\n-    ibert,\n-    idefics,\n-    idefics2,\n-    idefics3,\n-    ijepa,\n-    imagegpt,\n-    informer,\n-    instructblip,\n-    instructblipvideo,\n-    jamba,\n-    jetmoe,\n-    kosmos2,\n-    layoutlm,\n-    layoutlmv2,\n-    layoutlmv3,\n-    layoutxlm,\n-    led,\n-    levit,\n-    lilt,\n-    llama,\n-    llama4,\n-    llava,\n-    llava_next,\n-    llava_next_video,\n-    llava_onevision,\n-    longformer,\n-    longt5,\n-    luke,\n-    lxmert,\n-    m2m_100,\n-    mamba,\n-    mamba2,\n-    marian,\n-    markuplm,\n-    mask2former,\n-    maskformer,\n-    mbart,\n-    mbart50,\n-    megatron_bert,\n-    megatron_gpt2,\n-    mgp_str,\n-    mimi,\n-    mistral,\n-    mistral3,\n-    mixtral,\n-    mllama,\n-    mluke,\n-    mobilebert,\n-    mobilenet_v1,\n-    mobilenet_v2,\n-    mobilevit,\n-    mobilevitv2,\n-    modernbert,\n-    moonshine,\n-    moshi,\n-    mpnet,\n-    mpt,\n-    mra,\n-    mt5,\n-    musicgen,\n-    musicgen_melody,\n-    mvp,\n-    myt5,\n-    nemotron,\n-    nllb,\n-    nllb_moe,\n-    nougat,\n-    nystromformer,\n-    olmo,\n-    olmo2,\n-    olmoe,\n-    omdet_turbo,\n-    oneformer,\n-    openai,\n-    opt,\n-    owlv2,\n-    owlvit,\n-    paligemma,\n-    patchtsmixer,\n-    patchtst,\n-    pegasus,\n-    pegasus_x,\n-    perceiver,\n-    persimmon,\n-    phi,\n-    phi3,\n-    phi4_multimodal,\n-    phimoe,\n-    phobert,\n-    pix2struct,\n-    pixtral,\n-    plbart,\n-    poolformer,\n-    pop2piano,\n-    prompt_depth_anything,\n-    prophetnet,\n-    pvt,\n-    pvt_v2,\n-    qwen2,\n-    qwen2_5_vl,\n-    qwen2_audio,\n-    qwen2_moe,\n-    qwen2_vl,\n-    qwen3,\n-    qwen3_moe,\n-    rag,\n-    recurrent_gemma,\n-    reformer,\n-    regnet,\n-    rembert,\n-    resnet,\n-    roberta,\n-    roberta_prelayernorm,\n-    roc_bert,\n-    roformer,\n-    rt_detr,\n-    rt_detr_v2,\n-    rwkv,\n-    sam,\n-    seamless_m4t,\n-    seamless_m4t_v2,\n-    segformer,\n-    seggpt,\n-    sew,\n-    sew_d,\n-    shieldgemma2,\n-    siglip,\n-    siglip2,\n-    smolvlm,\n-    speech_encoder_decoder,\n-    speech_to_text,\n-    speecht5,\n-    splinter,\n-    squeezebert,\n-    stablelm,\n-    starcoder2,\n-    superglue,\n-    superpoint,\n-    swiftformer,\n-    swin,\n-    swin2sr,\n-    swinv2,\n-    switch_transformers,\n-    t5,\n-    table_transformer,\n-    tapas,\n-    textnet,\n-    time_series_transformer,\n-    timesformer,\n-    timm_backbone,\n-    timm_wrapper,\n-    trocr,\n-    tvp,\n-    udop,\n-    umt5,\n-    unispeech,\n-    unispeech_sat,\n-    univnet,\n-    upernet,\n-    video_llava,\n-    videomae,\n-    vilt,\n-    vipllava,\n-    vision_encoder_decoder,\n-    vision_text_dual_encoder,\n-    visual_bert,\n-    vit,\n-    vit_mae,\n-    vit_msn,\n-    vitdet,\n-    vitmatte,\n-    vitpose,\n-    vitpose_backbone,\n-    vits,\n-    vivit,\n-    wav2vec2,\n-    wav2vec2_bert,\n-    wav2vec2_conformer,\n-    wav2vec2_phoneme,\n-    wav2vec2_with_lm,\n-    wavlm,\n-    whisper,\n-    x_clip,\n-    xglm,\n-    xlm,\n-    xlm_roberta,\n-    xlm_roberta_xl,\n-    xlnet,\n-    xmod,\n-    yolos,\n-    yoso,\n-    zamba,\n-    zamba2,\n-    zoedepth,\n-)\n+from ..utils import _LazyModule\n+from ..utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .albert import *\n+    from .align import *\n+    from .altclip import *\n+    from .aria import *\n+    from .audio_spectrogram_transformer import *\n+    from .auto import *\n+    from .autoformer import *\n+    from .aya_vision import *\n+    from .bamba import *\n+    from .bark import *\n+    from .bart import *\n+    from .barthez import *\n+    from .bartpho import *\n+    from .beit import *\n+    from .bert import *\n+    from .bert_generation import *\n+    from .bert_japanese import *\n+    from .bertweet import *\n+    from .big_bird import *\n+    from .bigbird_pegasus import *\n+    from .biogpt import *\n+    from .bit import *\n+    from .blenderbot import *\n+    from .blenderbot_small import *\n+    from .blip import *\n+    from .blip_2 import *\n+    from .bloom import *\n+    from .bridgetower import *\n+    from .bros import *\n+    from .byt5 import *\n+    from .camembert import *\n+    from .canine import *\n+    from .chameleon import *\n+    from .chinese_clip import *\n+    from .clap import *\n+    from .clip import *\n+    from .clipseg import *\n+    from .clvp import *\n+    from .code_llama import *\n+    from .codegen import *\n+    from .cohere import *\n+    from .cohere2 import *\n+    from .colpali import *\n+    from .conditional_detr import *\n+    from .convbert import *\n+    from .convnext import *\n+    from .convnextv2 import *\n+    from .cpm import *\n+    from .cpmant import *\n+    from .ctrl import *\n+    from .cvt import *\n+    from .dab_detr import *\n+    from .dac import *\n+    from .data2vec import *\n+    from .dbrx import *\n+    from .deberta import *\n+    from .deberta_v2 import *\n+    from .decision_transformer import *\n+    from .deformable_detr import *\n+    from .deit import *\n+    from .deprecated import *\n+    from .depth_anything import *\n+    from .depth_pro import *\n+    from .detr import *\n+    from .dialogpt import *\n+    from .diffllama import *\n+    from .dinat import *\n+    from .dinov2 import *\n+    from .dinov2_with_registers import *\n+    from .distilbert import *\n+    from .dit import *\n+    from .donut import *\n+    from .dpr import *\n+    from .dpt import *\n+    from .efficientnet import *\n+    from .electra import *\n+    from .emu3 import *\n+    from .encodec import *\n+    from .encoder_decoder import *\n+    from .ernie import *\n+    from .esm import *\n+    from .falcon import *\n+    from .falcon_mamba import *\n+    from .fastspeech2_conformer import *\n+    from .flaubert import *\n+    from .flava import *\n+    from .fnet import *\n+    from .focalnet import *\n+    from .fsmt import *\n+    from .funnel import *\n+    from .fuyu import *\n+    from .gemma import *\n+    from .gemma2 import *\n+    from .gemma3 import *\n+    from .git import *\n+    from .glm import *\n+    from .glm4 import *\n+    from .glpn import *\n+    from .got_ocr2 import *\n+    from .gpt2 import *\n+    from .gpt_bigcode import *\n+    from .gpt_neo import *\n+    from .gpt_neox import *\n+    from .gpt_neox_japanese import *\n+    from .gpt_sw3 import *\n+    from .gptj import *\n+    from .granite import *\n+    from .granitemoe import *\n+    from .granitemoeshared import *\n+    from .grounding_dino import *\n+    from .groupvit import *\n+    from .helium import *\n+    from .herbert import *\n+    from .hiera import *\n+    from .hubert import *\n+    from .ibert import *\n+    from .idefics import *\n+    from .idefics2 import *\n+    from .idefics3 import *\n+    from .ijepa import *\n+    from .imagegpt import *\n+    from .informer import *\n+    from .instructblip import *\n+    from .instructblipvideo import *\n+    from .jamba import *\n+    from .jetmoe import *\n+    from .kosmos2 import *\n+    from .layoutlm import *\n+    from .layoutlmv2 import *\n+    from .layoutlmv3 import *\n+    from .layoutxlm import *\n+    from .led import *\n+    from .levit import *\n+    from .lilt import *\n+    from .llama import *\n+    from .llama4 import *\n+    from .llava import *\n+    from .llava_next import *\n+    from .llava_next_video import *\n+    from .llava_onevision import *\n+    from .longformer import *\n+    from .longt5 import *\n+    from .luke import *\n+    from .lxmert import *\n+    from .m2m_100 import *\n+    from .mamba import *\n+    from .mamba2 import *\n+    from .marian import *\n+    from .markuplm import *\n+    from .mask2former import *\n+    from .maskformer import *\n+    from .mbart import *\n+    from .mbart50 import *\n+    from .megatron_bert import *\n+    from .megatron_gpt2 import *\n+    from .mgp_str import *\n+    from .mimi import *\n+    from .mistral import *\n+    from .mistral3 import *\n+    from .mixtral import *\n+    from .mllama import *\n+    from .mluke import *\n+    from .mobilebert import *\n+    from .mobilenet_v1 import *\n+    from .mobilenet_v2 import *\n+    from .mobilevit import *\n+    from .mobilevitv2 import *\n+    from .modernbert import *\n+    from .moonshine import *\n+    from .moshi import *\n+    from .mpnet import *\n+    from .mpt import *\n+    from .mra import *\n+    from .mt5 import *\n+    from .musicgen import *\n+    from .musicgen_melody import *\n+    from .mvp import *\n+    from .myt5 import *\n+    from .nemotron import *\n+    from .nllb import *\n+    from .nllb_moe import *\n+    from .nougat import *\n+    from .nystromformer import *\n+    from .olmo import *\n+    from .olmo2 import *\n+    from .olmoe import *\n+    from .omdet_turbo import *\n+    from .oneformer import *\n+    from .openai import *\n+    from .opt import *\n+    from .owlv2 import *\n+    from .owlvit import *\n+    from .paligemma import *\n+    from .patchtsmixer import *\n+    from .patchtst import *\n+    from .pegasus import *\n+    from .pegasus_x import *\n+    from .perceiver import *\n+    from .persimmon import *\n+    from .phi import *\n+    from .phi3 import *\n+    from .phi4_multimodal import *\n+    from .phimoe import *\n+    from .phobert import *\n+    from .pix2struct import *\n+    from .pixtral import *\n+    from .plbart import *\n+    from .poolformer import *\n+    from .pop2piano import *\n+    from .prophetnet import *\n+    from .pvt import *\n+    from .pvt_v2 import *\n+    from .qwen2 import *\n+    from .qwen2_5_vl import *\n+    from .qwen2_audio import *\n+    from .qwen2_moe import *\n+    from .qwen2_vl import *\n+    from .rag import *\n+    from .recurrent_gemma import *\n+    from .reformer import *\n+    from .regnet import *\n+    from .rembert import *\n+    from .resnet import *\n+    from .roberta import *\n+    from .roberta_prelayernorm import *\n+    from .roc_bert import *\n+    from .roformer import *\n+    from .rt_detr import *\n+    from .rt_detr_v2 import *\n+    from .rwkv import *\n+    from .sam import *\n+    from .seamless_m4t import *\n+    from .seamless_m4t_v2 import *\n+    from .segformer import *\n+    from .seggpt import *\n+    from .sew import *\n+    from .sew_d import *\n+    from .siglip import *\n+    from .siglip2 import *\n+    from .smolvlm import *\n+    from .speech_encoder_decoder import *\n+    from .speech_to_text import *\n+    from .speecht5 import *\n+    from .splinter import *\n+    from .squeezebert import *\n+    from .stablelm import *\n+    from .starcoder2 import *\n+    from .superglue import *\n+    from .superpoint import *\n+    from .swiftformer import *\n+    from .swin import *\n+    from .swin2sr import *\n+    from .swinv2 import *\n+    from .switch_transformers import *\n+    from .t5 import *\n+    from .table_transformer import *\n+    from .tapas import *\n+    from .textnet import *\n+    from .time_series_transformer import *\n+    from .timesformer import *\n+    from .timm_backbone import *\n+    from .timm_wrapper import *\n+    from .trocr import *\n+    from .tvp import *\n+    from .udop import *\n+    from .umt5 import *\n+    from .unispeech import *\n+    from .unispeech_sat import *\n+    from .univnet import *\n+    from .upernet import *\n+    from .video_llava import *\n+    from .videomae import *\n+    from .vilt import *\n+    from .vipllava import *\n+    from .vision_encoder_decoder import *\n+    from .vision_text_dual_encoder import *\n+    from .visual_bert import *\n+    from .vit import *\n+    from .vit_mae import *\n+    from .vit_msn import *\n+    from .vitdet import *\n+    from .vitmatte import *\n+    from .vitpose import *\n+    from .vitpose_backbone import *\n+    from .vits import *\n+    from .vivit import *\n+    from .wav2vec2 import *\n+    from .wav2vec2_bert import *\n+    from .wav2vec2_conformer import *\n+    from .wav2vec2_phoneme import *\n+    from .wav2vec2_with_lm import *\n+    from .wavlm import *\n+    from .whisper import *\n+    from .x_clip import *\n+    from .xglm import *\n+    from .xlm import *\n+    from .xlm_roberta import *\n+    from .xlm_roberta_xl import *\n+    from .xlnet import *\n+    from .xmod import *\n+    from .yolos import *\n+    from .yoso import *\n+    from .zamba import *\n+    from .zamba2 import *\n+    from .zoedepth import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7ecd9f907eb7b694e748094faf6ad145c388133d",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n-from ...utils.import_utils import export\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -33,7 +33,7 @@\n SPIECE_UNDERLINE = \"▁\"\n \n \n-@export(backends=(\"sentencepiece\",))\n+@requires(backends=(\"sentencepiece\",))\n class AlbertTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an ALBERT tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "6828030287e9472748f3f7ecf2e5b7fb06625e87",
            "filename": "src/transformers/models/auto/__init__.py",
            "status": "modified",
            "additions": 13,
            "deletions": 388,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,399 +11,24 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"auto_factory\": [\"get_values\"],\n-    \"configuration_auto\": [\"CONFIG_MAPPING\", \"MODEL_NAMES_MAPPING\", \"AutoConfig\"],\n-    \"feature_extraction_auto\": [\"FEATURE_EXTRACTOR_MAPPING\", \"AutoFeatureExtractor\"],\n-    \"image_processing_auto\": [\"IMAGE_PROCESSOR_MAPPING\", \"AutoImageProcessor\"],\n-    \"processing_auto\": [\"PROCESSOR_MAPPING\", \"AutoProcessor\"],\n-    \"tokenization_auto\": [\"TOKENIZER_MAPPING\", \"AutoTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_auto\"] = [\n-        \"MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_AUDIO_XVECTOR_MAPPING\",\n-        \"MODEL_FOR_BACKBONE_MAPPING\",\n-        \"MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING\",\n-        \"MODEL_FOR_CAUSAL_LM_MAPPING\",\n-        \"MODEL_FOR_CTC_MAPPING\",\n-        \"MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\",\n-        \"MODEL_FOR_DEPTH_ESTIMATION_MAPPING\",\n-        \"MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_IMAGE_MAPPING\",\n-        \"MODEL_FOR_IMAGE_SEGMENTATION_MAPPING\",\n-        \"MODEL_FOR_IMAGE_TO_IMAGE_MAPPING\",\n-        \"MODEL_FOR_KEYPOINT_DETECTION_MAPPING\",\n-        \"MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING\",\n-        \"MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\",\n-        \"MODEL_FOR_MASKED_LM_MAPPING\",\n-        \"MODEL_FOR_MASK_GENERATION_MAPPING\",\n-        \"MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n-        \"MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n-        \"MODEL_FOR_OBJECT_DETECTION_MAPPING\",\n-        \"MODEL_FOR_PRETRAINING_MAPPING\",\n-        \"MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n-        \"MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\",\n-        \"MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n-        \"MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n-        \"MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\",\n-        \"MODEL_FOR_TEXT_ENCODING_MAPPING\",\n-        \"MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING\",\n-        \"MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING\",\n-        \"MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING\",\n-        \"MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_VISION_2_SEQ_MAPPING\",\n-        \"MODEL_FOR_RETRIEVAL_MAPPING\",\n-        \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\",\n-        \"MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING\",\n-        \"MODEL_MAPPING\",\n-        \"MODEL_WITH_LM_HEAD_MAPPING\",\n-        \"MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING\",\n-        \"MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING\",\n-        \"MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING\",\n-        \"AutoModel\",\n-        \"AutoBackbone\",\n-        \"AutoModelForAudioClassification\",\n-        \"AutoModelForAudioFrameClassification\",\n-        \"AutoModelForAudioXVector\",\n-        \"AutoModelForCausalLM\",\n-        \"AutoModelForCTC\",\n-        \"AutoModelForDepthEstimation\",\n-        \"AutoModelForImageClassification\",\n-        \"AutoModelForImageSegmentation\",\n-        \"AutoModelForImageToImage\",\n-        \"AutoModelForInstanceSegmentation\",\n-        \"AutoModelForKeypointDetection\",\n-        \"AutoModelForMaskGeneration\",\n-        \"AutoModelForTextEncoding\",\n-        \"AutoModelForMaskedImageModeling\",\n-        \"AutoModelForMaskedLM\",\n-        \"AutoModelForMultipleChoice\",\n-        \"AutoModelForNextSentencePrediction\",\n-        \"AutoModelForObjectDetection\",\n-        \"AutoModelForPreTraining\",\n-        \"AutoModelForQuestionAnswering\",\n-        \"AutoModelForSemanticSegmentation\",\n-        \"AutoModelForSeq2SeqLM\",\n-        \"AutoModelForSequenceClassification\",\n-        \"AutoModelForSpeechSeq2Seq\",\n-        \"AutoModelForTableQuestionAnswering\",\n-        \"AutoModelForTextToSpectrogram\",\n-        \"AutoModelForTextToWaveform\",\n-        \"AutoModelForTokenClassification\",\n-        \"AutoModelForUniversalSegmentation\",\n-        \"AutoModelForVideoClassification\",\n-        \"AutoModelForVision2Seq\",\n-        \"AutoModelForVisualQuestionAnswering\",\n-        \"AutoModelForDocumentQuestionAnswering\",\n-        \"AutoModelWithLMHead\",\n-        \"AutoModelForZeroShotImageClassification\",\n-        \"AutoModelForZeroShotObjectDetection\",\n-        \"AutoModelForImageTextToText\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_auto\"] = [\n-        \"TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n-        \"TF_MODEL_FOR_CAUSAL_LM_MAPPING\",\n-        \"TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n-        \"TF_MODEL_FOR_MASK_GENERATION_MAPPING\",\n-        \"TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\",\n-        \"TF_MODEL_FOR_MASKED_LM_MAPPING\",\n-        \"TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n-        \"TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n-        \"TF_MODEL_FOR_PRETRAINING_MAPPING\",\n-        \"TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n-        \"TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\",\n-        \"TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\",\n-        \"TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n-        \"TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n-        \"TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n-        \"TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\",\n-        \"TF_MODEL_FOR_TEXT_ENCODING_MAPPING\",\n-        \"TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n-        \"TF_MODEL_FOR_VISION_2_SEQ_MAPPING\",\n-        \"TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\",\n-        \"TF_MODEL_MAPPING\",\n-        \"TF_MODEL_WITH_LM_HEAD_MAPPING\",\n-        \"TFAutoModel\",\n-        \"TFAutoModelForAudioClassification\",\n-        \"TFAutoModelForCausalLM\",\n-        \"TFAutoModelForImageClassification\",\n-        \"TFAutoModelForMaskedImageModeling\",\n-        \"TFAutoModelForMaskedLM\",\n-        \"TFAutoModelForMaskGeneration\",\n-        \"TFAutoModelForMultipleChoice\",\n-        \"TFAutoModelForNextSentencePrediction\",\n-        \"TFAutoModelForPreTraining\",\n-        \"TFAutoModelForDocumentQuestionAnswering\",\n-        \"TFAutoModelForQuestionAnswering\",\n-        \"TFAutoModelForSemanticSegmentation\",\n-        \"TFAutoModelForSeq2SeqLM\",\n-        \"TFAutoModelForSequenceClassification\",\n-        \"TFAutoModelForSpeechSeq2Seq\",\n-        \"TFAutoModelForTableQuestionAnswering\",\n-        \"TFAutoModelForTextEncoding\",\n-        \"TFAutoModelForTokenClassification\",\n-        \"TFAutoModelForVision2Seq\",\n-        \"TFAutoModelForZeroShotImageClassification\",\n-        \"TFAutoModelWithLMHead\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_auto\"] = [\n-        \"FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n-        \"FLAX_MODEL_FOR_CAUSAL_LM_MAPPING\",\n-        \"FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n-        \"FLAX_MODEL_FOR_MASKED_LM_MAPPING\",\n-        \"FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n-        \"FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n-        \"FLAX_MODEL_FOR_PRETRAINING_MAPPING\",\n-        \"FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n-        \"FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n-        \"FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n-        \"FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n-        \"FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n-        \"FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING\",\n-        \"FLAX_MODEL_MAPPING\",\n-        \"FlaxAutoModel\",\n-        \"FlaxAutoModelForCausalLM\",\n-        \"FlaxAutoModelForImageClassification\",\n-        \"FlaxAutoModelForMaskedLM\",\n-        \"FlaxAutoModelForMultipleChoice\",\n-        \"FlaxAutoModelForNextSentencePrediction\",\n-        \"FlaxAutoModelForPreTraining\",\n-        \"FlaxAutoModelForQuestionAnswering\",\n-        \"FlaxAutoModelForSeq2SeqLM\",\n-        \"FlaxAutoModelForSequenceClassification\",\n-        \"FlaxAutoModelForSpeechSeq2Seq\",\n-        \"FlaxAutoModelForTokenClassification\",\n-        \"FlaxAutoModelForVision2Seq\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .auto_factory import get_values\n-    from .configuration_auto import CONFIG_MAPPING, MODEL_NAMES_MAPPING, AutoConfig\n-    from .feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n-    from .image_processing_auto import IMAGE_PROCESSOR_MAPPING, AutoImageProcessor\n-    from .processing_auto import PROCESSOR_MAPPING, AutoProcessor\n-    from .tokenization_auto import TOKENIZER_MAPPING, AutoTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_auto import (\n-            MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_AUDIO_XVECTOR_MAPPING,\n-            MODEL_FOR_BACKBONE_MAPPING,\n-            MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n-            MODEL_FOR_CAUSAL_LM_MAPPING,\n-            MODEL_FOR_CTC_MAPPING,\n-            MODEL_FOR_DEPTH_ESTIMATION_MAPPING,\n-            MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING,\n-            MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_IMAGE_MAPPING,\n-            MODEL_FOR_IMAGE_SEGMENTATION_MAPPING,\n-            MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING,\n-            MODEL_FOR_IMAGE_TO_IMAGE_MAPPING,\n-            MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING,\n-            MODEL_FOR_KEYPOINT_DETECTION_MAPPING,\n-            MODEL_FOR_MASK_GENERATION_MAPPING,\n-            MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING,\n-            MODEL_FOR_MASKED_LM_MAPPING,\n-            MODEL_FOR_MULTIPLE_CHOICE_MAPPING,\n-            MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING,\n-            MODEL_FOR_OBJECT_DETECTION_MAPPING,\n-            MODEL_FOR_PRETRAINING_MAPPING,\n-            MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n-            MODEL_FOR_RETRIEVAL_MAPPING,\n-            MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING,\n-            MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-            MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n-            MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING,\n-            MODEL_FOR_TEXT_ENCODING_MAPPING,\n-            MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING,\n-            MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING,\n-            MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING,\n-            MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING,\n-            MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_VISION_2_SEQ_MAPPING,\n-            MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING,\n-            MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING,\n-            MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING,\n-            MODEL_MAPPING,\n-            MODEL_WITH_LM_HEAD_MAPPING,\n-            AutoBackbone,\n-            AutoModel,\n-            AutoModelForAudioClassification,\n-            AutoModelForAudioFrameClassification,\n-            AutoModelForAudioXVector,\n-            AutoModelForCausalLM,\n-            AutoModelForCTC,\n-            AutoModelForDepthEstimation,\n-            AutoModelForDocumentQuestionAnswering,\n-            AutoModelForImageClassification,\n-            AutoModelForImageSegmentation,\n-            AutoModelForImageTextToText,\n-            AutoModelForImageToImage,\n-            AutoModelForInstanceSegmentation,\n-            AutoModelForKeypointDetection,\n-            AutoModelForMaskedImageModeling,\n-            AutoModelForMaskedLM,\n-            AutoModelForMaskGeneration,\n-            AutoModelForMultipleChoice,\n-            AutoModelForNextSentencePrediction,\n-            AutoModelForObjectDetection,\n-            AutoModelForPreTraining,\n-            AutoModelForQuestionAnswering,\n-            AutoModelForSemanticSegmentation,\n-            AutoModelForSeq2SeqLM,\n-            AutoModelForSequenceClassification,\n-            AutoModelForSpeechSeq2Seq,\n-            AutoModelForTableQuestionAnswering,\n-            AutoModelForTextEncoding,\n-            AutoModelForTextToSpectrogram,\n-            AutoModelForTextToWaveform,\n-            AutoModelForTokenClassification,\n-            AutoModelForUniversalSegmentation,\n-            AutoModelForVideoClassification,\n-            AutoModelForVision2Seq,\n-            AutoModelForVisualQuestionAnswering,\n-            AutoModelForZeroShotImageClassification,\n-            AutoModelForZeroShotObjectDetection,\n-            AutoModelWithLMHead,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_auto import (\n-            TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING,\n-            TF_MODEL_FOR_CAUSAL_LM_MAPPING,\n-            TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING,\n-            TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n-            TF_MODEL_FOR_MASK_GENERATION_MAPPING,\n-            TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING,\n-            TF_MODEL_FOR_MASKED_LM_MAPPING,\n-            TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING,\n-            TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING,\n-            TF_MODEL_FOR_PRETRAINING_MAPPING,\n-            TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n-            TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING,\n-            TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-            TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n-            TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n-            TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING,\n-            TF_MODEL_FOR_TEXT_ENCODING_MAPPING,\n-            TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n-            TF_MODEL_FOR_VISION_2_SEQ_MAPPING,\n-            TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING,\n-            TF_MODEL_MAPPING,\n-            TF_MODEL_WITH_LM_HEAD_MAPPING,\n-            TFAutoModel,\n-            TFAutoModelForAudioClassification,\n-            TFAutoModelForCausalLM,\n-            TFAutoModelForDocumentQuestionAnswering,\n-            TFAutoModelForImageClassification,\n-            TFAutoModelForMaskedImageModeling,\n-            TFAutoModelForMaskedLM,\n-            TFAutoModelForMaskGeneration,\n-            TFAutoModelForMultipleChoice,\n-            TFAutoModelForNextSentencePrediction,\n-            TFAutoModelForPreTraining,\n-            TFAutoModelForQuestionAnswering,\n-            TFAutoModelForSemanticSegmentation,\n-            TFAutoModelForSeq2SeqLM,\n-            TFAutoModelForSequenceClassification,\n-            TFAutoModelForSpeechSeq2Seq,\n-            TFAutoModelForTableQuestionAnswering,\n-            TFAutoModelForTextEncoding,\n-            TFAutoModelForTokenClassification,\n-            TFAutoModelForVision2Seq,\n-            TFAutoModelForZeroShotImageClassification,\n-            TFAutoModelWithLMHead,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_auto import (\n-            FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING,\n-            FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n-            FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n-            FLAX_MODEL_FOR_MASKED_LM_MAPPING,\n-            FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING,\n-            FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING,\n-            FLAX_MODEL_FOR_PRETRAINING_MAPPING,\n-            FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n-            FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-            FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n-            FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n-            FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n-            FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING,\n-            FLAX_MODEL_MAPPING,\n-            FlaxAutoModel,\n-            FlaxAutoModelForCausalLM,\n-            FlaxAutoModelForImageClassification,\n-            FlaxAutoModelForMaskedLM,\n-            FlaxAutoModelForMultipleChoice,\n-            FlaxAutoModelForNextSentencePrediction,\n-            FlaxAutoModelForPreTraining,\n-            FlaxAutoModelForQuestionAnswering,\n-            FlaxAutoModelForSeq2SeqLM,\n-            FlaxAutoModelForSequenceClassification,\n-            FlaxAutoModelForSpeechSeq2Seq,\n-            FlaxAutoModelForTokenClassification,\n-            FlaxAutoModelForVision2Seq,\n-        )\n-\n+    from .auto_factory import *\n+    from .configuration_auto import *\n+    from .feature_extraction_auto import *\n+    from .image_processing_auto import *\n+    from .modeling_auto import *\n+    from .modeling_flax_auto import *\n+    from .modeling_tf_auto import *\n+    from .processing_auto import *\n+    from .tokenization_auto import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "d458c1b6f266f35510a39030efc5237260a28940",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -844,3 +844,6 @@ def register(self, key, value, exist_ok=False):\n                 raise ValueError(f\"'{key}' is already used by a Transformers model.\")\n \n         self._extra_content[key] = value\n+\n+\n+__all__ = [\"get_values\"]"
        },
        {
            "sha": "43c0b3498d68829b94bd2e1cd102c05d89c1fab3",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1173,3 +1173,6 @@ def register(model_type, config, exist_ok=False):\n                 \"match!\"\n             )\n         CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n+\n+\n+__all__ = [\"CONFIG_MAPPING\", \"MODEL_NAMES_MAPPING\", \"AutoConfig\"]"
        },
        {
            "sha": "2067d1797f2cb765c922a5b895d0376eda5ab128",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -406,3 +406,6 @@ def register(config_class, feature_extractor_class, exist_ok=False):\n             feature_extractor_class ([`FeatureExtractorMixin`]): The feature extractor to register.\n         \"\"\"\n         FEATURE_EXTRACTOR_MAPPING.register(config_class, feature_extractor_class, exist_ok=exist_ok)\n+\n+\n+__all__ = [\"FEATURE_EXTRACTOR_MAPPING\", \"AutoFeatureExtractor\"]"
        },
        {
            "sha": "ac4848dafde44d472e4c199dfca0f233600f1af4",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -36,6 +36,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n     CONFIG_MAPPING_NAMES,\n@@ -324,6 +325,7 @@ def _warning_fast_image_processor_available(fast_class):\n     )\n \n \n+@requires(backends=(\"vision\", \"torchvision\"))\n class AutoImageProcessor:\n     r\"\"\"\n     This is a generic image processor class that will be instantiated as one of the image processor classes of the\n@@ -640,3 +642,6 @@ def register(\n         IMAGE_PROCESSOR_MAPPING.register(\n             config_class, (slow_image_processor_class, fast_image_processor_class), exist_ok=exist_ok\n         )\n+\n+\n+__all__ = [\"IMAGE_PROCESSOR_MAPPING\", \"AutoImageProcessor\"]"
        },
        {
            "sha": "8c51d6576ef647dc0f9e6ad2d8571a6235621015",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1955,3 +1955,90 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n             FutureWarning,\n         )\n         return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n+\n+\n+__all__ = [\n+    \"MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_AUDIO_XVECTOR_MAPPING\",\n+    \"MODEL_FOR_BACKBONE_MAPPING\",\n+    \"MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING\",\n+    \"MODEL_FOR_CAUSAL_LM_MAPPING\",\n+    \"MODEL_FOR_CTC_MAPPING\",\n+    \"MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\",\n+    \"MODEL_FOR_DEPTH_ESTIMATION_MAPPING\",\n+    \"MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_IMAGE_MAPPING\",\n+    \"MODEL_FOR_IMAGE_SEGMENTATION_MAPPING\",\n+    \"MODEL_FOR_IMAGE_TO_IMAGE_MAPPING\",\n+    \"MODEL_FOR_KEYPOINT_DETECTION_MAPPING\",\n+    \"MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING\",\n+    \"MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\",\n+    \"MODEL_FOR_MASKED_LM_MAPPING\",\n+    \"MODEL_FOR_MASK_GENERATION_MAPPING\",\n+    \"MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n+    \"MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n+    \"MODEL_FOR_OBJECT_DETECTION_MAPPING\",\n+    \"MODEL_FOR_PRETRAINING_MAPPING\",\n+    \"MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n+    \"MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\",\n+    \"MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n+    \"MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n+    \"MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\",\n+    \"MODEL_FOR_TEXT_ENCODING_MAPPING\",\n+    \"MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING\",\n+    \"MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING\",\n+    \"MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING\",\n+    \"MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_VISION_2_SEQ_MAPPING\",\n+    \"MODEL_FOR_RETRIEVAL_MAPPING\",\n+    \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\",\n+    \"MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING\",\n+    \"MODEL_MAPPING\",\n+    \"MODEL_WITH_LM_HEAD_MAPPING\",\n+    \"MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING\",\n+    \"MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING\",\n+    \"AutoModel\",\n+    \"AutoBackbone\",\n+    \"AutoModelForAudioClassification\",\n+    \"AutoModelForAudioFrameClassification\",\n+    \"AutoModelForAudioXVector\",\n+    \"AutoModelForCausalLM\",\n+    \"AutoModelForCTC\",\n+    \"AutoModelForDepthEstimation\",\n+    \"AutoModelForImageClassification\",\n+    \"AutoModelForImageSegmentation\",\n+    \"AutoModelForImageToImage\",\n+    \"AutoModelForInstanceSegmentation\",\n+    \"AutoModelForKeypointDetection\",\n+    \"AutoModelForMaskGeneration\",\n+    \"AutoModelForTextEncoding\",\n+    \"AutoModelForMaskedImageModeling\",\n+    \"AutoModelForMaskedLM\",\n+    \"AutoModelForMultipleChoice\",\n+    \"AutoModelForNextSentencePrediction\",\n+    \"AutoModelForObjectDetection\",\n+    \"AutoModelForPreTraining\",\n+    \"AutoModelForQuestionAnswering\",\n+    \"AutoModelForSemanticSegmentation\",\n+    \"AutoModelForSeq2SeqLM\",\n+    \"AutoModelForSequenceClassification\",\n+    \"AutoModelForSpeechSeq2Seq\",\n+    \"AutoModelForTableQuestionAnswering\",\n+    \"AutoModelForTextToSpectrogram\",\n+    \"AutoModelForTextToWaveform\",\n+    \"AutoModelForTokenClassification\",\n+    \"AutoModelForUniversalSegmentation\",\n+    \"AutoModelForVideoClassification\",\n+    \"AutoModelForVision2Seq\",\n+    \"AutoModelForVisualQuestionAnswering\",\n+    \"AutoModelForDocumentQuestionAnswering\",\n+    \"AutoModelWithLMHead\",\n+    \"AutoModelForZeroShotImageClassification\",\n+    \"AutoModelForZeroShotObjectDetection\",\n+    \"AutoModelForImageTextToText\",\n+]"
        },
        {
            "sha": "0588d03cb6cdb43b94cc3fcd73b1791d1a5ee809",
            "filename": "src/transformers/models/auto/modeling_flax_auto.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -381,3 +381,33 @@ class FlaxAutoModelForSpeechSeq2Seq(_BaseAutoModelClass):\n FlaxAutoModelForSpeechSeq2Seq = auto_class_update(\n     FlaxAutoModelForSpeechSeq2Seq, head_doc=\"sequence-to-sequence speech-to-text modeling\"\n )\n+\n+__all__ = [\n+    \"FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n+    \"FLAX_MODEL_FOR_CAUSAL_LM_MAPPING\",\n+    \"FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n+    \"FLAX_MODEL_FOR_MASKED_LM_MAPPING\",\n+    \"FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n+    \"FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n+    \"FLAX_MODEL_FOR_PRETRAINING_MAPPING\",\n+    \"FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n+    \"FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n+    \"FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n+    \"FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n+    \"FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n+    \"FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING\",\n+    \"FLAX_MODEL_MAPPING\",\n+    \"FlaxAutoModel\",\n+    \"FlaxAutoModelForCausalLM\",\n+    \"FlaxAutoModelForImageClassification\",\n+    \"FlaxAutoModelForMaskedLM\",\n+    \"FlaxAutoModelForMultipleChoice\",\n+    \"FlaxAutoModelForNextSentencePrediction\",\n+    \"FlaxAutoModelForPreTraining\",\n+    \"FlaxAutoModelForQuestionAnswering\",\n+    \"FlaxAutoModelForSeq2SeqLM\",\n+    \"FlaxAutoModelForSequenceClassification\",\n+    \"FlaxAutoModelForSpeechSeq2Seq\",\n+    \"FlaxAutoModelForTokenClassification\",\n+    \"FlaxAutoModelForVision2Seq\",\n+]"
        },
        {
            "sha": "cf39f4d7c9c40bd87a8e4c5e3037e2cbe3574a29",
            "filename": "src/transformers/models/auto/modeling_tf_auto.py",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -726,3 +726,51 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n             FutureWarning,\n         )\n         return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n+\n+\n+__all__ = [\n+    \"TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n+    \"TF_MODEL_FOR_CAUSAL_LM_MAPPING\",\n+    \"TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n+    \"TF_MODEL_FOR_MASK_GENERATION_MAPPING\",\n+    \"TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\",\n+    \"TF_MODEL_FOR_MASKED_LM_MAPPING\",\n+    \"TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n+    \"TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n+    \"TF_MODEL_FOR_PRETRAINING_MAPPING\",\n+    \"TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n+    \"TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\",\n+    \"TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\",\n+    \"TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n+    \"TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n+    \"TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n+    \"TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\",\n+    \"TF_MODEL_FOR_TEXT_ENCODING_MAPPING\",\n+    \"TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n+    \"TF_MODEL_FOR_VISION_2_SEQ_MAPPING\",\n+    \"TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\",\n+    \"TF_MODEL_MAPPING\",\n+    \"TF_MODEL_WITH_LM_HEAD_MAPPING\",\n+    \"TFAutoModel\",\n+    \"TFAutoModelForAudioClassification\",\n+    \"TFAutoModelForCausalLM\",\n+    \"TFAutoModelForImageClassification\",\n+    \"TFAutoModelForMaskedImageModeling\",\n+    \"TFAutoModelForMaskedLM\",\n+    \"TFAutoModelForMaskGeneration\",\n+    \"TFAutoModelForMultipleChoice\",\n+    \"TFAutoModelForNextSentencePrediction\",\n+    \"TFAutoModelForPreTraining\",\n+    \"TFAutoModelForDocumentQuestionAnswering\",\n+    \"TFAutoModelForQuestionAnswering\",\n+    \"TFAutoModelForSemanticSegmentation\",\n+    \"TFAutoModelForSeq2SeqLM\",\n+    \"TFAutoModelForSequenceClassification\",\n+    \"TFAutoModelForSpeechSeq2Seq\",\n+    \"TFAutoModelForTableQuestionAnswering\",\n+    \"TFAutoModelForTextEncoding\",\n+    \"TFAutoModelForTokenClassification\",\n+    \"TFAutoModelForVision2Seq\",\n+    \"TFAutoModelForZeroShotImageClassification\",\n+    \"TFAutoModelWithLMHead\",\n+]"
        },
        {
            "sha": "4cda1ebd19d68cb7a090b6b6e86c5025cdef78b0",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -389,3 +389,6 @@ def register(config_class, processor_class, exist_ok=False):\n             processor_class ([`ProcessorMixin`]): The processor to register.\n         \"\"\"\n         PROCESSOR_MAPPING.register(config_class, processor_class, exist_ok=exist_ok)\n+\n+\n+__all__ = [\"PROCESSOR_MAPPING\", \"AutoProcessor\"]"
        },
        {
            "sha": "d6f4fcf564a63ca14f1b4ff0e63ad9d9c3d1086e",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1083,3 +1083,6 @@ def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None,\n                 fast_tokenizer_class = existing_fast\n \n         TOKENIZER_MAPPING.register(config_class, (slow_tokenizer_class, fast_tokenizer_class), exist_ok=exist_ok)\n+\n+\n+__all__ = [\"TOKENIZER_MAPPING\", \"AutoTokenizer\"]"
        },
        {
            "sha": "48a329608039b1a4a96ac1f99c20ee427f845cd9",
            "filename": "src/transformers/models/autoformer/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 36,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fautoformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fautoformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,45 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-# rely on isort to merge the imports\n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_autoformer\": [\"AutoformerConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_autoformer\"] = [\n-        \"AutoformerForPrediction\",\n-        \"AutoformerModel\",\n-        \"AutoformerPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_autoformer import (\n-        AutoformerConfig,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_autoformer import (\n-            AutoformerForPrediction,\n-            AutoformerModel,\n-            AutoformerPreTrainedModel,\n-        )\n-\n+    from .configuration_autoformer import *\n+    from .modeling_autoformer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "aba83f19a5d942d528b1e443199e264feb51f83c",
            "filename": "src/transformers/models/autoformer/configuration_autoformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -240,3 +240,6 @@ def _number_of_features(self) -> int:\n             + self.num_static_real_features\n             + self.input_size * 2  # the log1p(abs(loc)) and log(scale) features\n         )\n+\n+\n+__all__ = [\"AutoformerConfig\"]"
        },
        {
            "sha": "eb32013d5e7324273b9a978b477b5593982e6fe5",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2147,3 +2147,6 @@ def generate(\n                 (-1, num_parallel_samples, self.config.prediction_length) + self.target_shape,\n             )\n         )\n+\n+\n+__all__ = [\"AutoformerForPrediction\", \"AutoformerModel\", \"AutoformerPreTrainedModel\"]"
        },
        {
            "sha": "0db6634d986881e5dbb03b726748f6c31167f5da",
            "filename": "src/transformers/models/barthez/tokenization_barthez.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -34,6 +35,7 @@\n # TODO this class is useless. This is the most standard sentencpiece model. Let's find which one is closest and nuke this.\n \n \n+@requires(backends=(\"sentencepiece\",))\n class BarthezTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`CamembertTokenizer`] and [`BartTokenizer`]. Construct a BARThez tokenizer. Based on"
        },
        {
            "sha": "c3e121089e62977591a5c6f85a2af1c3a0d208a4",
            "filename": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -31,6 +32,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\", \"monolingual_vocab_file\": \"dict.txt\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class BartphoTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`XLMRobertaTokenizer`]. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "7886897c6d1018e8b27e19170ef562cebc4462da",
            "filename": "src/transformers/models/beit/feature_extraction_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbeit%2Ffeature_extraction_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbeit%2Ffeature_extraction_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Ffeature_extraction_beit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_beit import BeitImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class BeitFeatureExtractor(BeitImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "eb2950f0e20e44cdbd1d65b7130c36044f0970ad",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -42,6 +42,7 @@\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -54,6 +55,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class BeitImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a BEiT image processor."
        },
        {
            "sha": "e98fe340ee8b2bc90a0c7effa66c90867a42d884",
            "filename": "src/transformers/models/bert/tokenization_bert_tf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -6,9 +6,11 @@\n from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n \n from ...modeling_tf_utils import keras\n+from ...utils.import_utils import requires\n from .tokenization_bert import BertTokenizer\n \n \n+@requires(backends=(\"tf\", \"tensorflow_text\"))\n class TFBertTokenizer(keras.layers.Layer):\n     \"\"\"\n     This is an in-graph tokenizer for BERT. It should be initialized similarly to other tokenizers, using the"
        },
        {
            "sha": "727727d4a11d6127872224f64db93772944b57a3",
            "filename": "src/transformers/models/bert_generation/tokenization_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,13 +22,15 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class BertGenerationTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a BertGeneration tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "43172e20f35fd9df4b9a19fce0f0097bdec28403",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,13 +23,15 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class BigBirdTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a BigBird tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "952de2f855a71591e950b31751af1fe6cfbae20a",
            "filename": "src/transformers/models/blip/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,7 +22,9 @@\n     from .image_processing_blip import *\n     from .image_processing_blip_fast import *\n     from .modeling_blip import *\n+    from .modeling_blip_text import *\n     from .modeling_tf_blip import *\n+    from .modeling_tf_blip_text import *\n     from .processing_blip import *\n else:\n     import sys"
        },
        {
            "sha": "f26f269c7b90fc8d5c11001531d95aa81db02729",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -955,3 +955,6 @@ def _reorder_cache(self, past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\"BlipTextModel\", \"BlipTextLMHeadModel\", \"BlipTextPreTrainedModel\"]"
        },
        {
            "sha": "6414bfa3b7e1de5f244b0e67caef0e4af343b804",
            "filename": "src/transformers/models/blip/modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1120,3 +1120,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"cls\", None) is not None:\n             with tf.name_scope(self.cls.name):\n                 self.cls.build(None)\n+\n+\n+__all__ = [\"TFBlipTextLMHeadModel\", \"TFBlipTextModel\", \"TFBlipTextPreTrainedModel\"]"
        },
        {
            "sha": "23cc569d49fc1cd372ec8b56d5c933a0c2e1ad88",
            "filename": "src/transformers/models/camembert/tokenization_camembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -32,6 +33,7 @@\n SPIECE_UNDERLINE = \"▁\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class CamembertTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`RobertaTokenizer`] and [`XLNetTokenizer`]. Construct a CamemBERT tokenizer. Based on"
        },
        {
            "sha": "c4895bb06b510cfeb64294759c31bcc8d0e3d098",
            "filename": "src/transformers/models/chinese_clip/feature_extraction_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Ffeature_extraction_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Ffeature_extraction_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Ffeature_extraction_chinese_clip.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_chinese_clip import ChineseCLIPImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ChineseCLIPFeatureExtractor(ChineseCLIPImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "d14d286b57d143a3b32b7967df9c97f83da81738",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -41,13 +41,17 @@\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n if is_vision_available():\n     import PIL\n \n \n+from ...utils.import_utils import requires\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@requires(backends=(\"vision\",))\n class ChineseCLIPImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Chinese-CLIP image processor."
        },
        {
            "sha": "cbe51cab7293db1482d4bda727299fb197579435",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -24,11 +24,13 @@\n from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n from ...feature_extraction_utils import BatchFeature\n from ...utils import TensorType, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"torch\",))\n class ClapFeatureExtractor(SequenceFeatureExtractor):\n     r\"\"\"\n     Constructs a CLAP feature extractor."
        },
        {
            "sha": "bed6b59eaa4595a2b7600660e34842ac92bf1784",
            "filename": "src/transformers/models/clip/feature_extraction_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fclip%2Ffeature_extraction_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fclip%2Ffeature_extraction_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ffeature_extraction_clip.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_clip import CLIPImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class CLIPFeatureExtractor(CLIPImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "77215ad636d4659774219933bc6883890ff6db07",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -40,6 +40,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -49,6 +50,7 @@\n     import PIL\n \n \n+@requires(backends=(\"vision\",))\n class CLIPImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a CLIP image processor."
        },
        {
            "sha": "99bdcb5e64da1b8cc8ff6cb3f8c9a66abaf5602e",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -25,6 +25,7 @@\n from ...convert_slow_tokenizer import import_protobuf\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging, requires_backends\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -46,6 +47,7 @@\n # fmt: on\n \n \n+@requires(backends=(\"sentencepiece\",))\n class CodeLlamaTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a CodeLlama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as"
        },
        {
            "sha": "f1d6b3a04760387cc7e51f39ffeda07fbf248380",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -288,6 +288,5 @@ def resize_token_embeddings(\n \n __all__ = [\n     \"ColPaliForRetrieval\",\n-    \"ColPaliForRetrievalOutput\",\n     \"ColPaliPreTrainedModel\",\n ]"
        },
        {
            "sha": "eeed4db007e5a6b19fbc0f83bb3983d344b93e7f",
            "filename": "src/transformers/models/conditional_detr/feature_extraction_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Ffeature_extraction_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Ffeature_extraction_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Ffeature_extraction_conditional_detr.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -18,6 +18,7 @@\n \n from ...image_transforms import rgb_to_id as _rgb_to_id\n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_conditional_detr import ConditionalDetrImageProcessor\n \n \n@@ -33,6 +34,7 @@ def rgb_to_id(x):\n     return _rgb_to_id(x)\n \n \n+@requires(backends=(\"vision\",))\n class ConditionalDetrFeatureExtractor(ConditionalDetrImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "3c256e4f70bd9a391ed53ab7dc4b57eff2898709",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -64,6 +64,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -801,6 +802,7 @@ def compute_segments(\n     return segmentation, segments\n \n \n+@requires(backends=(\"vision\",))\n class ConditionalDetrImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Conditional Detr image processor."
        },
        {
            "sha": "1fbb5184cf37cb0aedcafeab3a6b363ab047d9a0",
            "filename": "src/transformers/models/convnext/feature_extraction_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconvnext%2Ffeature_extraction_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconvnext%2Ffeature_extraction_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Ffeature_extraction_convnext.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_convnext import ConvNextImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ConvNextFeatureExtractor(ConvNextImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "0087e87ae12c46c9216593e66a01a6329b97c2db",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -39,6 +39,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -48,6 +49,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ConvNextImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a ConvNeXT image processor."
        },
        {
            "sha": "496c0b1ccb37de71e7ff7cb5366254be248e81e2",
            "filename": "src/transformers/models/cpm/tokenization_cpm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,13 +23,15 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import SPIECE_UNDERLINE, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class CpmTokenizer(PreTrainedTokenizer):\n     \"\"\"Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models.\"\"\"\n "
        },
        {
            "sha": "c8b23a962d420cc241ee970e830accca4716d0c2",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -30,6 +31,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"spm.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class DebertaV2Tokenizer(PreTrainedTokenizer):\n     r\"\"\"\n     Constructs a DeBERTa-v2 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "e349ca3db0ca92b349752484877ad8dd64d153b1",
            "filename": "src/transformers/models/deformable_detr/feature_extraction_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Ffeature_extraction_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Ffeature_extraction_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Ffeature_extraction_deformable_detr.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -18,6 +18,7 @@\n \n from ...image_transforms import rgb_to_id as _rgb_to_id\n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_deformable_detr import DeformableDetrImageProcessor\n \n \n@@ -33,6 +34,7 @@ def rgb_to_id(x):\n     return _rgb_to_id(x)\n \n \n+@requires(backends=(\"vision\",))\n class DeformableDetrFeatureExtractor(DeformableDetrImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "f7ad8a14997807f653e564b79bbec1f48a17837c",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -64,6 +64,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -799,6 +800,7 @@ def compute_segments(\n     return segmentation, segments\n \n \n+@requires(backends=(\"torch\", \"vision\"))\n class DeformableDetrImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Deformable DETR image processor."
        },
        {
            "sha": "2a820e07f72f650e7a490f2566a65bc0ea6447a0",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -39,6 +39,7 @@\n     is_torchvision_v2_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n from .image_processing_deformable_detr import get_size_with_aspect_ratio\n \n \n@@ -288,6 +289,7 @@ def prepare_coco_panoptic_annotation(\n             Whether to return segmentation masks.\n     \"\"\",\n )\n+@requires(backends=(\"torchvision\", \"torch\"))\n class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN"
        },
        {
            "sha": "d040fd08395f8e921ec688228d7d5faa8963ab81",
            "filename": "src/transformers/models/deit/feature_extraction_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeit%2Ffeature_extraction_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeit%2Ffeature_extraction_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Ffeature_extraction_deit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_deit import DeiTImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class DeiTFeatureExtractor(DeiTImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "b05622be06586107e682dafba85e55dbacc237ba",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -34,6 +34,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -43,6 +44,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class DeiTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a DeiT image processor."
        },
        {
            "sha": "e293c354e1e92a431a601da77d7555f2ecfe29ef",
            "filename": "src/transformers/models/deprecated/__init__.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -0,0 +1,49 @@\n+# Copyright 2020 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .bort import *\n+    from .deta import *\n+    from .efficientformer import *\n+    from .ernie_m import *\n+    from .gptsan_japanese import *\n+    from .graphormer import *\n+    from .jukebox import *\n+    from .mctct import *\n+    from .mega import *\n+    from .mmbt import *\n+    from .nat import *\n+    from .nezha import *\n+    from .open_llama import *\n+    from .qdqbert import *\n+    from .realm import *\n+    from .retribert import *\n+    from .speech_to_text_2 import *\n+    from .tapex import *\n+    from .trajectory_transformer import *\n+    from .transfo_xl import *\n+    from .tvlt import *\n+    from .van import *\n+    from .vit_hybrid import *\n+    from .xlm_prophetnet import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6e06e186741991b2f9183f30b6591477bc60dfb5",
            "filename": "src/transformers/models/deprecated/deta/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 50,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,61 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\n-    \"configuration_deta\": [\"DetaConfig\"],\n-}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_deta\"] = [\"DetaImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_deta\"] = [\n-        \"DetaForObjectDetection\",\n-        \"DetaModel\",\n-        \"DetaPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_deta import DetaConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_deta import DetaImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_deta import (\n-            DetaForObjectDetection,\n-            DetaModel,\n-            DetaPreTrainedModel,\n-        )\n-\n+    from .configuration_deta import *\n+    from .image_processing_deta import *\n+    from .modeling_deta import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "558bf5967907b7b70ff98cab0199dfc16a280928",
            "filename": "src/transformers/models/deprecated/deta/configuration_deta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -265,3 +265,6 @@ def num_attention_heads(self) -> int:\n     @property\n     def hidden_size(self) -> int:\n         return self.d_model\n+\n+\n+__all__ = [\"DetaConfig\"]"
        },
        {
            "sha": "c63be138276c6e92766c5931a0e1502934d665e2",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1222,3 +1222,6 @@ def post_process_object_detection(\n             )\n \n         return results\n+\n+\n+__all__ = [\"DetaImageProcessor\"]"
        },
        {
            "sha": "69154f9e0dc36125017b1b33bd42b659adf2ab14",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2822,3 +2822,6 @@ def forward(self, outputs, targets):\n \n     def postprocess_indices(self, pr_inds, gt_inds, iou):\n         return sample_topk_per_gt(pr_inds, gt_inds, iou, self.k)\n+\n+\n+__all__ = [\"DetaForObjectDetection\", \"DetaModel\", \"DetaPreTrainedModel\"]"
        },
        {
            "sha": "db3d0a634051c713aee33b48f2962647ac48d0eb",
            "filename": "src/transformers/models/deprecated/efficientformer/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 79,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,88 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_torch_available,\n-    is_vision_available,\n-)\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_efficientformer\": [\"EfficientFormerConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_efficientformer\"] = [\"EfficientFormerImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_efficientformer\"] = [\n-        \"EfficientFormerForImageClassification\",\n-        \"EfficientFormerForImageClassificationWithTeacher\",\n-        \"EfficientFormerModel\",\n-        \"EfficientFormerPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_efficientformer\"] = [\n-        \"TFEfficientFormerForImageClassification\",\n-        \"TFEfficientFormerForImageClassificationWithTeacher\",\n-        \"TFEfficientFormerModel\",\n-        \"TFEfficientFormerPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_efficientformer import EfficientFormerConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_efficientformer import EfficientFormerImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_efficientformer import (\n-            EfficientFormerForImageClassification,\n-            EfficientFormerForImageClassificationWithTeacher,\n-            EfficientFormerModel,\n-            EfficientFormerPreTrainedModel,\n-        )\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_efficientformer import (\n-            TFEfficientFormerForImageClassification,\n-            TFEfficientFormerForImageClassificationWithTeacher,\n-            TFEfficientFormerModel,\n-            TFEfficientFormerPreTrainedModel,\n-        )\n-\n+    from .configuration_efficientformer import *\n+    from .image_processing_efficientformer import *\n+    from .modeling_efficientformer import *\n+    from .modeling_tf_efficientformer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "5e2dca8bcd01c853025ec4364c10b4250421cf2d",
            "filename": "src/transformers/models/deprecated/efficientformer/configuration_efficientformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -165,3 +165,8 @@ def __init__(\n         self.layer_scale_init_value = layer_scale_init_value\n         self.image_size = image_size\n         self.batch_norm_eps = batch_norm_eps\n+\n+\n+__all__ = [\n+    \"EfficientFormerConfig\",\n+]"
        },
        {
            "sha": "74d16a048de1e59a6d6e0f7fe001bb7194273abd",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -319,3 +319,6 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"EfficientFormerImageProcessor\"]"
        },
        {
            "sha": "a45fe7da5da3f3077b48b3aa23ca557cc60dd5e9",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -797,3 +797,11 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"EfficientFormerForImageClassification\",\n+    \"EfficientFormerForImageClassificationWithTeacher\",\n+    \"EfficientFormerModel\",\n+    \"EfficientFormerPreTrainedModel\",\n+]"
        },
        {
            "sha": "e11fa1edf9d08f6ae773a025a9f3fbf7039cb1c9",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_tf_efficientformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1188,3 +1188,11 @@ def build(self, input_shape=None):\n             if hasattr(self.distillation_classifier, \"name\"):\n                 with tf.name_scope(self.distillation_classifier.name):\n                     self.distillation_classifier.build([None, None, self.config.hidden_sizes[-1]])\n+\n+\n+__all__ = [\n+    \"TFEfficientFormerForImageClassification\",\n+    \"TFEfficientFormerForImageClassificationWithTeacher\",\n+    \"TFEfficientFormerModel\",\n+    \"TFEfficientFormerPreTrainedModel\",\n+]"
        },
        {
            "sha": "2beb8f463ff10af33ea980599ea4d5fc05888acb",
            "filename": "src/transformers/models/deprecated/ernie_m/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 59,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,68 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-# rely on isort to merge the imports\n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_ernie_m\": [\"ErnieMConfig\"],\n-}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_ernie_m\"] = [\"ErnieMTokenizer\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_ernie_m\"] = [\n-        \"ErnieMForMultipleChoice\",\n-        \"ErnieMForQuestionAnswering\",\n-        \"ErnieMForSequenceClassification\",\n-        \"ErnieMForTokenClassification\",\n-        \"ErnieMModel\",\n-        \"ErnieMPreTrainedModel\",\n-        \"ErnieMForInformationExtraction\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_ernie_m import ErnieMConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_ernie_m import ErnieMTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_ernie_m import (\n-            ErnieMForInformationExtraction,\n-            ErnieMForMultipleChoice,\n-            ErnieMForQuestionAnswering,\n-            ErnieMForSequenceClassification,\n-            ErnieMForTokenClassification,\n-            ErnieMModel,\n-            ErnieMPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_ernie_m import *\n+    from .modeling_ernie_m import *\n+    from .tokenization_ernie_m import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7a45106131850a82cb169fb01cecf96ff5f3e1a2",
            "filename": "src/transformers/models/deprecated/ernie_m/configuration_ernie_m.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -109,3 +109,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.classifier_dropout = classifier_dropout\n         self.act_dropout = act_dropout\n+\n+\n+__all__ = [\"ErnieMConfig\"]"
        },
        {
            "sha": "28c17afa3f7a2ca10d340794ac0b0a6b28e27915",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1045,3 +1045,14 @@ def forward(\n             hidden_states=result.hidden_states,\n             attentions=result.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"ErnieMForMultipleChoice\",\n+    \"ErnieMForQuestionAnswering\",\n+    \"ErnieMForSequenceClassification\",\n+    \"ErnieMForTokenClassification\",\n+    \"ErnieMModel\",\n+    \"ErnieMPreTrainedModel\",\n+    \"ErnieMForInformationExtraction\",\n+]"
        },
        {
            "sha": "44bc197a4f7c463c71eb64dd941dacce51148675",
            "filename": "src/transformers/models/deprecated/ernie_m/tokenization_ernie_m.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,6 +23,7 @@\n \n from ....tokenization_utils import PreTrainedTokenizer\n from ....utils import logging\n+from ....utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -38,6 +39,7 @@\n \n \n # Adapted from paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer\n+@requires(backends=(\"sentencepiece\",))\n class ErnieMTokenizer(PreTrainedTokenizer):\n     r\"\"\"\n     Constructs a Ernie-M tokenizer. It uses the `sentencepiece` tools to cut the words to sub-words.\n@@ -403,3 +405,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n             fi.write(content_spiece_model)\n \n         return (vocab_file,)\n+\n+\n+__all__ = [\"ErnieMTokenizer\"]"
        },
        {
            "sha": "3c23b58f35012f1ddc00e2275c484810287de6f8",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 47,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,58 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_gptsan_japanese\": [\"GPTSanJapaneseConfig\"],\n-    \"tokenization_gptsan_japanese\": [\"GPTSanJapaneseTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gptsan_japanese\"] = [\n-        \"GPTSanJapaneseForConditionalGeneration\",\n-        \"GPTSanJapaneseModel\",\n-        \"GPTSanJapanesePreTrainedModel\",\n-    ]\n-    _import_structure[\"tokenization_gptsan_japanese\"] = [\n-        \"GPTSanJapaneseTokenizer\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n-    from .tokenization_gptsan_japanese import GPTSanJapaneseTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gptsan_japanese import (\n-            GPTSanJapaneseForConditionalGeneration,\n-            GPTSanJapaneseModel,\n-            GPTSanJapanesePreTrainedModel,\n-        )\n-        from .tokenization_gptsan_japanese import GPTSanJapaneseTokenizer\n-\n-\n+    from .configuration_gptsan_japanese import *\n+    from .modeling_gptsan_japanese import *\n+    from .tokenization_gptsan_japanese import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "cd565810095955c1d7a6e199db93d2ef1a499173",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/configuration_gptsan_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -152,3 +152,6 @@ def __init__(\n             eos_token_id=eos_token_id,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"GPTSanJapaneseConfig\"]"
        },
        {
            "sha": "a35ea4a31199887a32e6912bfb2d3d6036635b38",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1332,3 +1332,6 @@ def _unpack_router_logits(self, router_outputs):\n                 total_router_logits.append(router_logits)\n                 total_expert_indexes.append(expert_indexes)\n         return torch.cat(total_router_logits, dim=1), torch.cat(total_expert_indexes, dim=1)\n+\n+\n+__all__ = [\"GPTSanJapaneseForConditionalGeneration\", \"GPTSanJapaneseModel\", \"GPTSanJapanesePreTrainedModel\"]"
        },
        {
            "sha": "c93ea87278d767fb12b48554ffd6b7f611f7e034",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -513,3 +513,6 @@ def checku2e(x):\n \n     def convert_id_to_token(self, index):\n         return self.ids_to_tokens[index][0]\n+\n+\n+__all__ = [\"GPTSanJapaneseTokenizer\"]"
        },
        {
            "sha": "3a4b3eb1be2b4e69dcad1540abdd91f412de0ca2",
            "filename": "src/transformers/models/deprecated/graphormer/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 34,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,43 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_graphormer\": [\"GraphormerConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_graphormer\"] = [\n-        \"GraphormerForGraphClassification\",\n-        \"GraphormerModel\",\n-        \"GraphormerPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_graphormer import GraphormerConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_graphormer import (\n-            GraphormerForGraphClassification,\n-            GraphormerModel,\n-            GraphormerPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_graphormer import *\n+    from .modeling_graphormer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "1ecde152e4e522ab7ca5c7d1c5d2bc5b4fdda029",
            "filename": "src/transformers/models/deprecated/graphormer/configuration_graphormer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -215,3 +215,6 @@ def __init__(\n             eos_token_id=eos_token_id,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"GraphormerConfig\"]"
        },
        {
            "sha": "1253d1365e8528da0ec4543d39d9a8f66f2255f7",
            "filename": "src/transformers/models/deprecated/graphormer/modeling_graphormer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -906,3 +906,6 @@ def forward(\n         if not return_dict:\n             return tuple(x for x in [loss, logits, hidden_states] if x is not None)\n         return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=hidden_states, attentions=None)\n+\n+\n+__all__ = [\"GraphormerForGraphClassification\", \"GraphormerModel\", \"GraphormerPreTrainedModel\"]"
        },
        {
            "sha": "826bdbddc1f182de43a795ab0d78ad4009507c14",
            "filename": "src/transformers/models/deprecated/jukebox/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 45,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,56 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_jukebox\": [\n-        \"JukeboxConfig\",\n-        \"JukeboxPriorConfig\",\n-        \"JukeboxVQVAEConfig\",\n-    ],\n-    \"tokenization_jukebox\": [\"JukeboxTokenizer\"],\n-}\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_jukebox\"] = [\n-        \"JukeboxModel\",\n-        \"JukeboxPreTrainedModel\",\n-        \"JukeboxVQVAE\",\n-        \"JukeboxPrior\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_jukebox import (\n-        JukeboxConfig,\n-        JukeboxPriorConfig,\n-        JukeboxVQVAEConfig,\n-    )\n-    from .tokenization_jukebox import JukeboxTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_jukebox import (\n-            JukeboxModel,\n-            JukeboxPreTrainedModel,\n-            JukeboxPrior,\n-            JukeboxVQVAE,\n-        )\n-\n+    from .configuration_jukebox import *\n+    from .modeling_jukebox import *\n+    from .tokenization_jukebox import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "d10cbc2d82cfbd310cff2b84b54de4d693cea1c7",
            "filename": "src/transformers/models/deprecated/jukebox/configuration_jukebox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -608,3 +608,6 @@ def to_dict(self):\n         result = super().to_dict()\n         result[\"prior_config_list\"] = [config.to_dict() for config in result.pop(\"prior_configs\")]\n         return result\n+\n+\n+__all__ = [\"JukeboxConfig\", \"JukeboxPriorConfig\", \"JukeboxVQVAEConfig\"]"
        },
        {
            "sha": "566148ceda36b3581c653c2c29ec7265d2582135",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2665,3 +2665,6 @@ def primed_sample(self, raw_audio, labels, **sampling_kwargs) -> List[torch.Long\n             )\n         music_tokens = self._sample(music_tokens, labels, sample_levels, **sampling_kwargs)\n         return music_tokens\n+\n+\n+__all__ = [\"JukeboxModel\", \"JukeboxPreTrainedModel\", \"JukeboxVQVAE\", \"JukeboxPrior\"]"
        },
        {
            "sha": "e08ab179a807d9af7050e92838ced80c7d0d1742",
            "filename": "src/transformers/models/deprecated/jukebox/tokenization_jukebox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -402,3 +402,6 @@ def _convert_id_to_token(self, artists_index, genres_index, lyric_index):\n         genres = [self.genres_decoder.get(genre) for genre in genres_index]\n         lyrics = [self.lyrics_decoder.get(character) for character in lyric_index]\n         return artist, genres, lyrics\n+\n+\n+__all__ = [\"JukeboxTokenizer\"]"
        },
        {
            "sha": "53ec5ed37c13614266d04cde838ff7360946a451",
            "filename": "src/transformers/models/deprecated/mctct/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 34,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,43 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_mctct\": [\"MCTCTConfig\"],\n-    \"feature_extraction_mctct\": [\"MCTCTFeatureExtractor\"],\n-    \"processing_mctct\": [\"MCTCTProcessor\"],\n-}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_mctct\"] = [\n-        \"MCTCTForCTC\",\n-        \"MCTCTModel\",\n-        \"MCTCTPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_mctct import MCTCTConfig\n-    from .feature_extraction_mctct import MCTCTFeatureExtractor\n-    from .processing_mctct import MCTCTProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_mctct import MCTCTForCTC, MCTCTModel, MCTCTPreTrainedModel\n-\n+    from .configuration_mctct import *\n+    from .feature_extraction_mctct import *\n+    from .modeling_mctct import *\n+    from .processing_mctct import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "9cba190a0f460e3fed5a3ebbc773e9ab31283c1a",
            "filename": "src/transformers/models/deprecated/mctct/configuration_mctct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -179,3 +179,6 @@ def __init__(\n                 f\"but is `len(config.conv_kernel) = {len(self.conv_kernel)}`, \"\n                 f\"`config.num_conv_layers = {self.num_conv_layers}`.\"\n             )\n+\n+\n+__all__ = [\"MCTCTConfig\"]"
        },
        {
            "sha": "f210031f3097e084c288fa58773d4271301e7c29",
            "filename": "src/transformers/models/deprecated/mctct/feature_extraction_mctct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -286,3 +286,6 @@ def __call__(\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n         return padded_inputs\n+\n+\n+__all__ = [\"MCTCTFeatureExtractor\"]"
        },
        {
            "sha": "2dd074b28c53d6d501ec680013afc517118bc227",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -786,3 +786,6 @@ def forward(\n         return CausalLMOutput(\n             loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n         )\n+\n+\n+__all__ = [\"MCTCTForCTC\", \"MCTCTModel\", \"MCTCTPreTrainedModel\"]"
        },
        {
            "sha": "f953c5895a0d2e1cf3a343fbdefc99861da2dadd",
            "filename": "src/transformers/models/deprecated/mctct/processing_mctct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -141,3 +141,6 @@ def as_target_processor(self):\n         yield\n         self.current_processor = self.feature_extractor\n         self._in_target_context_manager = False\n+\n+\n+__all__ = [\"MCTCTProcessor\"]"
        },
        {
            "sha": "cff2c19505f9306c28edb5bdabb66f668363f5fa",
            "filename": "src/transformers/models/deprecated/mega/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 47,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,58 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_mega\": [\"MegaConfig\", \"MegaOnnxConfig\"],\n-}\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_mega\"] = [\n-        \"MegaForCausalLM\",\n-        \"MegaForMaskedLM\",\n-        \"MegaForMultipleChoice\",\n-        \"MegaForQuestionAnswering\",\n-        \"MegaForSequenceClassification\",\n-        \"MegaForTokenClassification\",\n-        \"MegaModel\",\n-        \"MegaPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_mega import MegaConfig, MegaOnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_mega import (\n-            MegaForCausalLM,\n-            MegaForMaskedLM,\n-            MegaForMultipleChoice,\n-            MegaForQuestionAnswering,\n-            MegaForSequenceClassification,\n-            MegaForTokenClassification,\n-            MegaModel,\n-            MegaPreTrainedModel,\n-        )\n-\n+    from .configuration_mega import *\n+    from .modeling_mega import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "3b9d53d52079f0dde5237b50ce36730685a3767e",
            "filename": "src/transformers/models/deprecated/mega/configuration_mega.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -238,3 +238,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"attention_mask\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"MegaConfig\", \"MegaOnnxConfig\"]"
        },
        {
            "sha": "d5a490b01d3cc893758490c669f3928b93edab65",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2271,3 +2271,15 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"MegaForCausalLM\",\n+    \"MegaForMaskedLM\",\n+    \"MegaForMultipleChoice\",\n+    \"MegaForQuestionAnswering\",\n+    \"MegaForSequenceClassification\",\n+    \"MegaForTokenClassification\",\n+    \"MegaModel\",\n+    \"MegaPreTrainedModel\",\n+]"
        },
        {
            "sha": "03b556e2eddf6d5f81f6f4a15346596dd5a32f85",
            "filename": "src/transformers/models/deprecated/mmbt/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,35 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\"configuration_mmbt\": [\"MMBTConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_mmbt\"] = [\"MMBTForClassification\", \"MMBTModel\", \"ModalEmbeddings\"]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_mmbt import MMBTConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_mmbt import MMBTForClassification, MMBTModel, ModalEmbeddings\n-\n+    from .configuration_mmbt import *\n+    from .modeling_mmbt import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "1c58e4e6cdd0d0a8c87b9b94ca896c87970b5654",
            "filename": "src/transformers/models/deprecated/mmbt/configuration_mmbt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fconfiguration_mmbt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fconfiguration_mmbt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fconfiguration_mmbt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -40,3 +40,6 @@ def __init__(self, config, num_labels=None, modal_hidden_size=2048):\n         self.modal_hidden_size = modal_hidden_size\n         if num_labels:\n             self.num_labels = num_labels\n+\n+\n+__all__ = [\"MMBTConfig\"]"
        },
        {
            "sha": "45ae577f7fced2d276f4f54c5bf859e27e08ebae",
            "filename": "src/transformers/models/deprecated/mmbt/modeling_mmbt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -405,3 +405,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"MMBTForClassification\", \"MMBTModel\", \"ModalEmbeddings\"]"
        },
        {
            "sha": "c5373969ce7831491b0d5fa5495078fb1d3f6e4e",
            "filename": "src/transformers/models/deprecated/nat/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 33,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,42 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_nat\": [\"NatConfig\"]}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_nat\"] = [\n-        \"NatForImageClassification\",\n-        \"NatModel\",\n-        \"NatPreTrainedModel\",\n-        \"NatBackbone\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_nat import NatConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_nat import (\n-            NatBackbone,\n-            NatForImageClassification,\n-            NatModel,\n-            NatPreTrainedModel,\n-        )\n-\n+    from .configuration_nat import *\n+    from .modeling_nat import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "85961aa2fe8d0bc547b4919855ff8a28815a0fcd",
            "filename": "src/transformers/models/deprecated/nat/configuration_nat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -143,3 +143,6 @@ def __init__(\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n         )\n+\n+\n+__all__ = [\"NatConfig\"]"
        },
        {
            "sha": "5871b03299af14f4246457d63257c60f341925e3",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -948,3 +948,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"NatForImageClassification\", \"NatModel\", \"NatPreTrainedModel\", \"NatBackbone\"]"
        },
        {
            "sha": "f0690129ae9edf84829278790b5e065bbd6608ee",
            "filename": "src/transformers/models/deprecated/nezha/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 46,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,55 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_nezha\": [\"NezhaConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_nezha\"] = [\n-        \"NezhaForNextSentencePrediction\",\n-        \"NezhaForMaskedLM\",\n-        \"NezhaForPreTraining\",\n-        \"NezhaForMultipleChoice\",\n-        \"NezhaForQuestionAnswering\",\n-        \"NezhaForSequenceClassification\",\n-        \"NezhaForTokenClassification\",\n-        \"NezhaModel\",\n-        \"NezhaPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_nezha import NezhaConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_nezha import (\n-            NezhaForMaskedLM,\n-            NezhaForMultipleChoice,\n-            NezhaForNextSentencePrediction,\n-            NezhaForPreTraining,\n-            NezhaForQuestionAnswering,\n-            NezhaForSequenceClassification,\n-            NezhaForTokenClassification,\n-            NezhaModel,\n-            NezhaPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_nezha import *\n+    from .modeling_nezha import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "00d193cd1ae68655e1631b7caea2220987a29f0b",
            "filename": "src/transformers/models/deprecated/nezha/configuration_nezha.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -100,3 +100,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.classifier_dropout = classifier_dropout\n         self.use_cache = use_cache\n+\n+\n+__all__ = [\"NezhaConfig\"]"
        },
        {
            "sha": "7be52bee5847cb9c82b2efb32e8729f198f9ae75",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1682,3 +1682,16 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"NezhaForNextSentencePrediction\",\n+    \"NezhaForMaskedLM\",\n+    \"NezhaForPreTraining\",\n+    \"NezhaForMultipleChoice\",\n+    \"NezhaForQuestionAnswering\",\n+    \"NezhaForSequenceClassification\",\n+    \"NezhaForTokenClassification\",\n+    \"NezhaModel\",\n+    \"NezhaPreTrainedModel\",\n+]"
        },
        {
            "sha": "2b3964d194bed041987c6236b5c60bfcf3b7caf4",
            "filename": "src/transformers/models/deprecated/open_llama/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 74,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,83 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_sentencepiece_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_open_llama\": [\"OpenLlamaConfig\"],\n-}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_open_llama\"] = [\"LlamaTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_open_llama_fast\"] = [\"LlamaTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_open_llama\"] = [\n-        \"OpenLlamaForCausalLM\",\n-        \"OpenLlamaModel\",\n-        \"OpenLlamaPreTrainedModel\",\n-        \"OpenLlamaForSequenceClassification\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_open_llama import OpenLlamaConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from transformers import LlamaTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from transformers import LlamaTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_open_llama import (\n-            OpenLlamaForCausalLM,\n-            OpenLlamaForSequenceClassification,\n-            OpenLlamaModel,\n-            OpenLlamaPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_open_llama import *\n+    from .modeling_open_llama import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "b4bc9cc72a7661fe571cce6649b40f2307d9b3ce",
            "filename": "src/transformers/models/deprecated/open_llama/configuration_open_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -164,3 +164,6 @@ def _rope_scaling_validation(self):\n             )\n         if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n             raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n+\n+\n+__all__ = [\"OpenLlamaConfig\"]"
        },
        {
            "sha": "79d79ea546a950bd3e6deb81b93ff6ee7b5d60a4",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -970,3 +970,6 @@ def forward(\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )\n+\n+\n+__all__ = [\"OpenLlamaPreTrainedModel\", \"OpenLlamaModel\", \"OpenLlamaForCausalLM\", \"OpenLlamaForSequenceClassification\"]"
        },
        {
            "sha": "864b321bc2ee3a521e3d6da5403cab7363dea56b",
            "filename": "src/transformers/models/deprecated/qdqbert/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 48,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,57 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\"configuration_qdqbert\": [\"QDQBertConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_qdqbert\"] = [\n-        \"QDQBertForMaskedLM\",\n-        \"QDQBertForMultipleChoice\",\n-        \"QDQBertForNextSentencePrediction\",\n-        \"QDQBertForQuestionAnswering\",\n-        \"QDQBertForSequenceClassification\",\n-        \"QDQBertForTokenClassification\",\n-        \"QDQBertLayer\",\n-        \"QDQBertLMHeadModel\",\n-        \"QDQBertModel\",\n-        \"QDQBertPreTrainedModel\",\n-        \"load_tf_weights_in_qdqbert\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_qdqbert import QDQBertConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_qdqbert import (\n-            QDQBertForMaskedLM,\n-            QDQBertForMultipleChoice,\n-            QDQBertForNextSentencePrediction,\n-            QDQBertForQuestionAnswering,\n-            QDQBertForSequenceClassification,\n-            QDQBertForTokenClassification,\n-            QDQBertLayer,\n-            QDQBertLMHeadModel,\n-            QDQBertModel,\n-            QDQBertPreTrainedModel,\n-            load_tf_weights_in_qdqbert,\n-        )\n-\n-\n+    from .configuration_qdqbert import *\n+    from .modeling_qdqbert import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "91ac82bc5a0292355ac659548c7c3d4336f2536c",
            "filename": "src/transformers/models/deprecated/qdqbert/configuration_qdqbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -118,3 +118,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n+\n+\n+__all__ = [\"QDQBertConfig\"]"
        },
        {
            "sha": "8b68a4e426d445f5c6e3c472e3dc5ea54e661d57",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1732,3 +1732,18 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"QDQBertForMaskedLM\",\n+    \"QDQBertForMultipleChoice\",\n+    \"QDQBertForNextSentencePrediction\",\n+    \"QDQBertForQuestionAnswering\",\n+    \"QDQBertForSequenceClassification\",\n+    \"QDQBertForTokenClassification\",\n+    \"QDQBertLayer\",\n+    \"QDQBertLMHeadModel\",\n+    \"QDQBertModel\",\n+    \"QDQBertPreTrainedModel\",\n+    \"load_tf_weights_in_qdqbert\",\n+]"
        },
        {
            "sha": "cdfdeb5d179c8e954c9c6822d3f154d632394964",
            "filename": "src/transformers/models/deprecated/realm/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 62,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,71 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_realm\": [\"RealmConfig\"],\n-    \"tokenization_realm\": [\"RealmTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_realm_fast\"] = [\"RealmTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_realm\"] = [\n-        \"RealmEmbedder\",\n-        \"RealmForOpenQA\",\n-        \"RealmKnowledgeAugEncoder\",\n-        \"RealmPreTrainedModel\",\n-        \"RealmReader\",\n-        \"RealmScorer\",\n-        \"load_tf_weights_in_realm\",\n-    ]\n-    _import_structure[\"retrieval_realm\"] = [\"RealmRetriever\"]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_realm import RealmConfig\n-    from .tokenization_realm import RealmTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_realm import RealmTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_realm import (\n-            RealmEmbedder,\n-            RealmForOpenQA,\n-            RealmKnowledgeAugEncoder,\n-            RealmPreTrainedModel,\n-            RealmReader,\n-            RealmScorer,\n-            load_tf_weights_in_realm,\n-        )\n-        from .retrieval_realm import RealmRetriever\n-\n-\n+    from .configuration_realm import *\n+    from .modeling_realm import *\n+    from .retrieval_realm import *\n+    from .tokenization_realm import *\n+    from .tokenization_realm_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "fbf32378a604beca939ff6e0241f69f4c4382120",
            "filename": "src/transformers/models/deprecated/realm/configuration_realm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -164,3 +164,6 @@ def __init__(\n         # Retrieval config\n         self.num_block_records = num_block_records\n         self.searcher_beam_size = searcher_beam_size\n+\n+\n+__all__ = [\"RealmConfig\"]"
        },
        {
            "sha": "ac25a177333ef4eb5eb18f628adf1235d2084fee",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1849,3 +1849,14 @@ def forward(\n             reader_output=reader_output,\n             predicted_answer_ids=predicted_answer_ids,\n         )\n+\n+\n+__all__ = [\n+    \"RealmEmbedder\",\n+    \"RealmForOpenQA\",\n+    \"RealmKnowledgeAugEncoder\",\n+    \"RealmPreTrainedModel\",\n+    \"RealmReader\",\n+    \"RealmScorer\",\n+    \"load_tf_weights_in_realm\",\n+]"
        },
        {
            "sha": "b3c084f1d2090975f8d6ed91665caf21e3e7829c",
            "filename": "src/transformers/models/deprecated/realm/retrieval_realm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -20,7 +20,8 @@\n import numpy as np\n from huggingface_hub import hf_hub_download\n \n-from .... import AutoTokenizer\n+from transformers import AutoTokenizer\n+\n from ....utils import logging, strtobool\n \n \n@@ -170,3 +171,6 @@ def block_has_answer(self, concat_inputs, answer_ids):\n                 start_pos_ += padded\n                 end_pos_ += padded\n         return has_answers, start_pos, end_pos\n+\n+\n+__all__ = [\"RealmRetriever\"]"
        },
        {
            "sha": "70e69bc4bc2bce4024322950add44e5f58b04f1c",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -558,3 +558,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"RealmTokenizer\"]"
        },
        {
            "sha": "7c173227befd31e7f6e1fed26877dceb13ad4041",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -247,3 +247,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"RealmTokenizerFast\"]"
        },
        {
            "sha": "a875576607430c041abab01eccaf468a6cc9272e",
            "filename": "src/transformers/models/deprecated/retribert/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 50,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,61 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_retribert\": [\"RetriBertConfig\"],\n-    \"tokenization_retribert\": [\"RetriBertTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_retribert_fast\"] = [\"RetriBertTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_retribert\"] = [\n-        \"RetriBertModel\",\n-        \"RetriBertPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_retribert import RetriBertConfig\n-    from .tokenization_retribert import RetriBertTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_retribert_fast import RetriBertTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_retribert import (\n-            RetriBertModel,\n-            RetriBertPreTrainedModel,\n-        )\n-\n+    from .configuration_retribert import *\n+    from .modeling_retribert import *\n+    from .tokenization_retribert import *\n+    from .tokenization_retribert_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "80d755a16961450cae783d834385e5e6873dc24e",
            "filename": "src/transformers/models/deprecated/retribert/configuration_retribert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -103,3 +103,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.share_encoders = share_encoders\n         self.projection_dim = projection_dim\n+\n+\n+__all__ = [\"RetriBertConfig\"]"
        },
        {
            "sha": "bcae1c0239e6f9b214e6227bf279ad9fc7b8381a",
            "filename": "src/transformers/models/deprecated/retribert/modeling_retribert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -212,3 +212,6 @@ def forward(\n         loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n         loss = (loss_qa + loss_aq) / 2\n         return loss\n+\n+\n+__all__ = [\"RetriBertModel\", \"RetriBertPreTrainedModel\"]"
        },
        {
            "sha": "35a1874aa0e3c8c39e3d96c20a31d3b0595235f1",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -499,3 +499,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"RetriBertTokenizer\"]"
        },
        {
            "sha": "cc51d0e2a1de9280e6b8635804eaf46f7859e5b9",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -174,3 +174,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"RetriBertTokenizerFast\"]"
        },
        {
            "sha": "78c549b6e294e1ca97a718ef601472d4d400e12a",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 42,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,51 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_sentencepiece_available,\n-    is_speech_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_speech_to_text_2\": [\"Speech2Text2Config\"],\n-    \"processing_speech_to_text_2\": [\"Speech2Text2Processor\"],\n-    \"tokenization_speech_to_text_2\": [\"Speech2Text2Tokenizer\"],\n-}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_speech_to_text_2\"] = [\n-        \"Speech2Text2ForCausalLM\",\n-        \"Speech2Text2PreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_speech_to_text_2 import Speech2Text2Config\n-    from .processing_speech_to_text_2 import Speech2Text2Processor\n-    from .tokenization_speech_to_text_2 import Speech2Text2Tokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_speech_to_text_2 import (\n-            Speech2Text2ForCausalLM,\n-            Speech2Text2PreTrainedModel,\n-        )\n-\n+    from .configuration_speech_to_text_2 import *\n+    from .modeling_speech_to_text_2 import *\n+    from .processing_speech_to_text_2 import *\n+    from .tokenization_speech_to_text_2 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "2afd79feb28dd99c6ca94d2483b8f88b8cdc0a0e",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/configuration_speech_to_text_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -129,3 +129,6 @@ def __init__(\n             decoder_start_token_id=decoder_start_token_id,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"Speech2Text2Config\"]"
        },
        {
            "sha": "6f1dd18d97ff61015759b99587b846f4825c427c",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -925,3 +925,6 @@ def _reorder_cache(past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\"Speech2Text2ForCausalLM\", \"Speech2Text2PreTrainedModel\"]"
        },
        {
            "sha": "3b8edaa46d10b8ca04f68d530733fcb7cb390ee8",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/processing_speech_to_text_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -114,3 +114,6 @@ def as_target_processor(self):\n         yield\n         self.current_processor = self.feature_extractor\n         self._in_target_context_manager = False\n+\n+\n+__all__ = [\"Speech2Text2Processor\"]"
        },
        {
            "sha": "f5aa7ef8067c57d2fdae3b10c9ad3b01c3e23299",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/tokenization_speech_to_text_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Ftokenization_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Ftokenization_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Ftokenization_speech_to_text_2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -247,3 +247,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 index += 1\n \n         return (vocab_file, merges_file)\n+\n+\n+__all__ = [\"Speech2Text2Tokenizer\"]"
        },
        {
            "sha": "b535eb1df2c40a7680bbf8d57fc70b78e23437f4",
            "filename": "src/transformers/models/deprecated/tapex/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -14,16 +14,13 @@\n from typing import TYPE_CHECKING\n \n from ....utils import _LazyModule\n-\n-\n-_import_structure = {\"tokenization_tapex\": [\"TapexTokenizer\"]}\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .tokenization_tapex import TapexTokenizer\n-\n-\n+    from .tokenization_tapex import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "3d554872c48df0c4f8ae9f4f63e98affca511688",
            "filename": "src/transformers/models/deprecated/tapex/tokenization_tapex.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1465,3 +1465,6 @@ def delete_unrelated_rows(self, table_content: Dict, question: str, answer: List\n         # only when the drop ratio is too large, logging for warning.\n         if \"id\" in table_content and len(drop_row_indices) > 0:\n             logger.warning(\"Delete {:.2f} rows in table {}\".format(len(drop_row_indices), table_content[\"id\"]))\n+\n+\n+__all__ = [\"TapexTokenizer\"]"
        },
        {
            "sha": "b4bccdd12c9703313951e223d0ac1955f9ca1581",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 36,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,45 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_trajectory_transformer\": [\"TrajectoryTransformerConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_trajectory_transformer\"] = [\n-        \"TrajectoryTransformerModel\",\n-        \"TrajectoryTransformerPreTrainedModel\",\n-        \"load_tf_weights_in_trajectory_transformer\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_trajectory_transformer import (\n-        TrajectoryTransformerConfig,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_trajectory_transformer import (\n-            TrajectoryTransformerModel,\n-            TrajectoryTransformerPreTrainedModel,\n-            load_tf_weights_in_trajectory_transformer,\n-        )\n-\n-\n+    from .configuration_trajectory_transformer import *\n+    from .modeling_trajectory_transformer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "5a267cc4c4b82c6703eaa3a732d3fca1606a60d7",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/configuration_trajectory_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -150,3 +150,6 @@ def __init__(\n         self.kaiming_initializer_range = kaiming_initializer_range\n         self.use_cache = use_cache\n         super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+\n+__all__ = [\"TrajectoryTransformerConfig\"]"
        },
        {
            "sha": "0a9a0111d22218af6af00d7561107d0c70def2db",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -601,3 +601,10 @@ def forward(\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )\n+\n+\n+__all__ = [\n+    \"TrajectoryTransformerModel\",\n+    \"TrajectoryTransformerPreTrainedModel\",\n+    \"load_tf_weights_in_trajectory_transformer\",\n+]"
        },
        {
            "sha": "0ac9a2cbf4766bd187d90fdaf46505db3e92b68f",
            "filename": "src/transformers/models/deprecated/transfo_xl/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 72,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,83 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_transfo_xl\": [\"TransfoXLConfig\"],\n-    \"tokenization_transfo_xl\": [\"TransfoXLCorpus\", \"TransfoXLTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_transfo_xl\"] = [\n-        \"AdaptiveEmbedding\",\n-        \"TransfoXLForSequenceClassification\",\n-        \"TransfoXLLMHeadModel\",\n-        \"TransfoXLModel\",\n-        \"TransfoXLPreTrainedModel\",\n-        \"load_tf_weights_in_transfo_xl\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_transfo_xl\"] = [\n-        \"TFAdaptiveEmbedding\",\n-        \"TFTransfoXLForSequenceClassification\",\n-        \"TFTransfoXLLMHeadModel\",\n-        \"TFTransfoXLMainLayer\",\n-        \"TFTransfoXLModel\",\n-        \"TFTransfoXLPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_transfo_xl import TransfoXLConfig\n-    from .tokenization_transfo_xl import TransfoXLCorpus, TransfoXLTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_transfo_xl import (\n-            AdaptiveEmbedding,\n-            TransfoXLForSequenceClassification,\n-            TransfoXLLMHeadModel,\n-            TransfoXLModel,\n-            TransfoXLPreTrainedModel,\n-            load_tf_weights_in_transfo_xl,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_transfo_xl import (\n-            TFAdaptiveEmbedding,\n-            TFTransfoXLForSequenceClassification,\n-            TFTransfoXLLMHeadModel,\n-            TFTransfoXLMainLayer,\n-            TFTransfoXLModel,\n-            TFTransfoXLPreTrainedModel,\n-        )\n-\n+    from .configuration_transfo_xl import *\n+    from .modeling_tf_transfo_xl import *\n+    from .modeling_transfo_xl import *\n+    from .tokenization_transfo_xl import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "23972deae2ca9b31ce5adcf2aaf4f6e6cd4b7587",
            "filename": "src/transformers/models/deprecated/transfo_xl/configuration_transfo_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -184,3 +184,6 @@ def max_position_embeddings(self, value):\n         raise NotImplementedError(\n             f\"The model {self.model_type} is one of the few models that has no sequence length limit.\"\n         )\n+\n+\n+__all__ = [\"TransfoXLConfig\"]"
        },
        {
            "sha": "90e2ebc3436db35a002120601ff60b66169318bb",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1117,3 +1117,13 @@ def call(\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"TFAdaptiveEmbedding\",\n+    \"TFTransfoXLForSequenceClassification\",\n+    \"TFTransfoXLLMHeadModel\",\n+    \"TFTransfoXLMainLayer\",\n+    \"TFTransfoXLModel\",\n+    \"TFTransfoXLPreTrainedModel\",\n+]"
        },
        {
            "sha": "cf843850c05c411cec507971aeb8557a3038591b",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1291,3 +1291,13 @@ def forward(\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"AdaptiveEmbedding\",\n+    \"TransfoXLForSequenceClassification\",\n+    \"TransfoXLLMHeadModel\",\n+    \"TransfoXLModel\",\n+    \"TransfoXLPreTrainedModel\",\n+    \"load_tf_weights_in_transfo_xl\",\n+]"
        },
        {
            "sha": "f9a5fb7b3466b0091717dfe2a264c16b0a1aeeeb",
            "filename": "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -816,3 +816,6 @@ def get_lm_corpus(datadir, dataset):\n         torch.save(corpus, fn)\n \n     return corpus\n+\n+\n+__all__ = [\"TransfoXLCorpus\", \"TransfoXLTokenizer\"]"
        },
        {
            "sha": "941db2f6ac5fefe66e06d598e1adbe0db1cf1769",
            "filename": "src/transformers/models/deprecated/tvlt/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 75,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1,86 +1,20 @@\n # flake8: noqa\n # There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n # module, but to preserve other warnings. So, don't check this module at all.\n-\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-    is_vision_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_tvlt\": [\"TvltConfig\"],\n-    \"feature_extraction_tvlt\": [\"TvltFeatureExtractor\"],\n-    \"processing_tvlt\": [\"TvltProcessor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tvlt\"] = [\n-        \"TvltModel\",\n-        \"TvltForPreTraining\",\n-        \"TvltForAudioVisualClassification\",\n-        \"TvltPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_tvlt\"] = [\"TvltImageProcessor\"]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_tvlt import TvltConfig\n-    from .processing_tvlt import TvltProcessor\n-    from .feature_extraction_tvlt import TvltFeatureExtractor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tvlt import (\n-            TvltForAudioVisualClassification,\n-            TvltForPreTraining,\n-            TvltModel,\n-            TvltPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_tvlt import TvltImageProcessor\n-\n-\n+    from .configuration_tvlt import *\n+    from .feature_extraction_tvlt import *\n+    from .processing_tvlt import *\n+    from .modeling_tvlt import *\n+    from .image_processing_tvlt import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bf159fa7e0b7754778ebda57b46dea4479fdc3c9",
            "filename": "src/transformers/models/deprecated/tvlt/configuration_tvlt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -182,3 +182,6 @@ def __init__(\n         self.task_matching = task_matching\n         self.task_mae = task_mae\n         self.loss_type = loss_type\n+\n+\n+__all__ = [\"TvltConfig\"]"
        },
        {
            "sha": "bbbfac9031b9be499bd168c74681f32e32357c5b",
            "filename": "src/transformers/models/deprecated/tvlt/feature_extraction_tvlt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -228,3 +228,6 @@ def __call__(\n \n         encoded_inputs = BatchFeature(data=data, tensor_type=return_tensors)\n         return encoded_inputs\n+\n+\n+__all__ = [\"TvltFeatureExtractor\"]"
        },
        {
            "sha": "db87d6e8d5686e9d5f5b04ab9020d0e4bed7aa19",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -433,3 +433,6 @@ def preprocess(\n             data = {\"pixel_values\": videos, \"pixel_mask\": video_masks}\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"TvltImageProcessor\"]"
        },
        {
            "sha": "279224ac4d24a2c4b3a7694dbd20a77390a0a793",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1286,3 +1286,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"TvltModel\", \"TvltForPreTraining\", \"TvltForAudioVisualClassification\", \"TvltPreTrainedModel\"]"
        },
        {
            "sha": "d9f8e0978d8a2c4158094ebdf3bc2279700e7ba5",
            "filename": "src/transformers/models/deprecated/tvlt/processing_tvlt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -87,3 +87,6 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         return list(dict.fromkeys(image_processor_input_names + feature_extractor_input_names))\n+\n+\n+__all__ = [\"TvltProcessor\"]"
        },
        {
            "sha": "9552c827365d3913539be3eafaf822146ada5829",
            "filename": "src/transformers/models/deprecated/van/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,40 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_van\": [\"VanConfig\"]}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_van\"] = [\n-        \"VanForImageClassification\",\n-        \"VanModel\",\n-        \"VanPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_van import VanConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_van import (\n-            VanForImageClassification,\n-            VanModel,\n-            VanPreTrainedModel,\n-        )\n-\n+    from .configuration_van import *\n+    from .modeling_van import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "08f1db7a4b48e7fb32b986cdc557b7aa3d328ed0",
            "filename": "src/transformers/models/deprecated/van/configuration_van.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -105,3 +105,6 @@ def __init__(\n         self.layer_scale_init_value = layer_scale_init_value\n         self.drop_path_rate = drop_path_rate\n         self.dropout_rate = dropout_rate\n+\n+\n+__all__ = [\"VanConfig\"]"
        },
        {
            "sha": "fd11a04ec21c7e7de1902c086ba9ed96b6373d09",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -534,3 +534,6 @@ def forward(\n             return ((loss,) + output) if loss is not None else output\n \n         return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\n+\n+\n+__all__ = [\"VanForImageClassification\", \"VanModel\", \"VanPreTrainedModel\"]"
        },
        {
            "sha": "f5bd93aa4dabea9b2adbc54eeeb3664de589f43a",
            "filename": "src/transformers/models/deprecated/vit_hybrid/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 48,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,57 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\"configuration_vit_hybrid\": [\"ViTHybridConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_vit_hybrid\"] = [\n-        \"ViTHybridForImageClassification\",\n-        \"ViTHybridModel\",\n-        \"ViTHybridPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_vit_hybrid\"] = [\"ViTHybridImageProcessor\"]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_vit_hybrid import ViTHybridConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_vit_hybrid import (\n-            ViTHybridForImageClassification,\n-            ViTHybridModel,\n-            ViTHybridPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_vit_hybrid import ViTHybridImageProcessor\n-\n-\n+    from .configuration_vit_hybrid import *\n+    from .image_processing_vit_hybrid import *\n+    from .modeling_vit_hybrid import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "65b6a3e5ef515218afb72e8b871c4ffb2465666f",
            "filename": "src/transformers/models/deprecated/vit_hybrid/configuration_vit_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -167,3 +167,6 @@ def __init__(\n         self.patch_size = patch_size\n         self.num_channels = num_channels\n         self.qkv_bias = qkv_bias\n+\n+\n+__all__ = [\"ViTHybridConfig\"]"
        },
        {
            "sha": "72410878933d7bd9726794588dc241c08b405a25",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -336,3 +336,6 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"ViTHybridImageProcessor\"]"
        },
        {
            "sha": "c4d2511019395f97ad790f9ab548667a4cac3b5b",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -765,3 +765,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"ViTHybridForImageClassification\", \"ViTHybridModel\", \"ViTHybridPreTrainedModel\"]"
        },
        {
            "sha": "c13c67012fa157a94bae21734a1f59b26c78588d",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 55,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,64 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ....utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_xlm_prophetnet\": [\"XLMProphetNetConfig\"],\n-}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_xlm_prophetnet\"] = [\"XLMProphetNetTokenizer\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_xlm_prophetnet\"] = [\n-        \"XLMProphetNetDecoder\",\n-        \"XLMProphetNetEncoder\",\n-        \"XLMProphetNetForCausalLM\",\n-        \"XLMProphetNetForConditionalGeneration\",\n-        \"XLMProphetNetModel\",\n-        \"XLMProphetNetPreTrainedModel\",\n-    ]\n+from ....utils import _LazyModule\n+from ....utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_xlm_prophetnet import XLMProphetNetConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_xlm_prophetnet import XLMProphetNetTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_xlm_prophetnet import (\n-            XLMProphetNetDecoder,\n-            XLMProphetNetEncoder,\n-            XLMProphetNetForCausalLM,\n-            XLMProphetNetForConditionalGeneration,\n-            XLMProphetNetModel,\n-            XLMProphetNetPreTrainedModel,\n-        )\n-\n+    from .configuration_xlm_prophetnet import *\n+    from .modeling_xlm_prophetnet import *\n+    from .tokenization_xlm_prophetnet import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "2d7751d9541ec1a3b39799915d742955d863cd24",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/configuration_xlm_prophetnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -176,3 +176,6 @@ def num_hidden_layers(self, value):\n             \"This model does not support the setting of `num_hidden_layers`. Please set `num_encoder_layers` and\"\n             \" `num_decoder_layers`.\"\n         )\n+\n+\n+__all__ = [\"XLMProphetNetConfig\"]"
        },
        {
            "sha": "17bc9ffada60028e6531c90b3939cf22f846cdf3",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2334,3 +2334,13 @@ def _tie_weights(self):\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n+\n+\n+__all__ = [\n+    \"XLMProphetNetDecoder\",\n+    \"XLMProphetNetEncoder\",\n+    \"XLMProphetNetForCausalLM\",\n+    \"XLMProphetNetForConditionalGeneration\",\n+    \"XLMProphetNetModel\",\n+    \"XLMProphetNetPreTrainedModel\",\n+]"
        },
        {
            "sha": "1a5da12859f29e9e3cabe354c63d25c584edbd49",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -321,3 +321,6 @@ def build_inputs_with_special_tokens(\n             return token_ids_0 + [self.sep_token_id]\n         sep = [self.sep_token_id]\n         return token_ids_0 + sep + token_ids_1 + sep\n+\n+\n+__all__ = [\"XLMProphetNetTokenizer\"]"
        },
        {
            "sha": "978a32da7e911aaf3756a918e1b1fc5bf30a3183",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -18,6 +18,8 @@\n \n import numpy as np\n \n+from ...utils.import_utils import requires\n+\n \n if TYPE_CHECKING:\n     from .modeling_depth_pro import DepthProDepthEstimatorOutput\n@@ -53,6 +55,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"torchvision\", \"torch\"))\n class DepthProImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a DepthPro image processor."
        },
        {
            "sha": "a0a4cd96b861ae286c5af7555c891e87a12ca86b",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,6 +38,7 @@\n     logging,\n     requires_backends,\n )\n+from ...utils.import_utils import requires\n \n \n if TYPE_CHECKING:\n@@ -63,6 +64,7 @@\n     \"Constructs a fast DepthPro image processor.\",\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n )\n+@requires(backends=(\"torchvision\", \"torch\"))\n class DepthProImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_STANDARD_MEAN"
        },
        {
            "sha": "a81f83c8c313bdb8a904f0b359360c0e100a83d9",
            "filename": "src/transformers/models/detr/feature_extraction_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdetr%2Ffeature_extraction_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdetr%2Ffeature_extraction_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Ffeature_extraction_detr.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -18,6 +18,7 @@\n \n from ...image_transforms import rgb_to_id as _rgb_to_id\n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_detr import DetrImageProcessor\n \n \n@@ -33,6 +34,7 @@ def rgb_to_id(x):\n     return _rgb_to_id(x)\n \n \n+@requires(backends=(\"vision\",))\n class DetrFeatureExtractor(DetrImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "75d7e74adde07d32393fe51673c5577d221bf148",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -63,6 +63,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -784,6 +785,7 @@ def compute_segments(\n     return segmentation, segments\n \n \n+@requires(backends=(\"vision\",))\n class DetrImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Detr image processor."
        },
        {
            "sha": "dc14ec61f0659a316c366484655cac9eee367ec5",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -56,6 +56,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n from .image_processing_detr import (\n     compute_segments,\n     convert_segmentation_to_rle,\n@@ -313,6 +314,7 @@ class DetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n             Whether to return segmentation masks.\n     \"\"\",\n )\n+@requires(backends=(\"torchvision\", \"torch\"))\n class DetrImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN"
        },
        {
            "sha": "e37a58ddd3055e040c6c29cbd5f5cc3c34270cbe",
            "filename": "src/transformers/models/donut/feature_extraction_donut.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdonut%2Ffeature_extraction_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdonut%2Ffeature_extraction_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Ffeature_extraction_donut.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_donut import DonutImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class DonutFeatureExtractor(DonutImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "e4afd4a4f78f57fc3cfe4d3f78bb9bd48265577a",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -40,7 +40,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...utils.import_utils import is_vision_available\n+from ...utils.import_utils import is_vision_available, requires\n \n \n logger = logging.get_logger(__name__)\n@@ -50,6 +50,7 @@\n     import PIL\n \n \n+@requires(backends=(\"vision\",))\n class DonutImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Donut image processor."
        },
        {
            "sha": "b6ab8ccbed8d33b1e5b15d429b6cb057ff781f78",
            "filename": "src/transformers/models/dpt/feature_extraction_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdpt%2Ffeature_extraction_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdpt%2Ffeature_extraction_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Ffeature_extraction_dpt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_dpt import DPTImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class DPTFeatureExtractor(DPTImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "095cd1a48b4f08843c80b48dbff9768f98d17bb7",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,6 +17,8 @@\n import math\n from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple, Union\n \n+from ...utils.import_utils import requires\n+\n \n if TYPE_CHECKING:\n     from ...modeling_outputs import DepthEstimatorOutput\n@@ -102,6 +104,7 @@ def constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n     return (new_height, new_width)\n \n \n+@requires(backends=(\"vision\",))\n class DPTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a DPT image processor."
        },
        {
            "sha": "19bcccc889f546442af71229c998880fbbb2db31",
            "filename": "src/transformers/models/flava/feature_extraction_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fflava%2Ffeature_extraction_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fflava%2Ffeature_extraction_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Ffeature_extraction_flava.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_flava import FlavaImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class FlavaFeatureExtractor(FlavaImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "2b85a64cb84ee992cd35ccea35014e619c746cdb",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -37,6 +37,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -133,6 +134,7 @@ def __call__(self):\n         return mask\n \n \n+@requires(backends=(\"vision\",))\n class FlavaImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Flava image processor."
        },
        {
            "sha": "3601b022610daec1284f0a4591e663d97417c0da",
            "filename": "src/transformers/models/fnet/tokenization_fnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -32,6 +33,7 @@\n SPIECE_UNDERLINE = \"▁\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class FNetTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an FNet tokenizer. Adapted from [`AlbertTokenizer`]. Based on"
        },
        {
            "sha": "4eab188f2ab7baa456b91626d03943f4ab5f7e9c",
            "filename": "src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconvert_funnel_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconvert_funnel_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconvert_funnel_original_tf_checkpoint_to_pytorch.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -62,6 +62,3 @@ def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, config_file, pytorch_du\n     convert_tf_checkpoint_to_pytorch(\n         args.tf_checkpoint_path, args.config_file, args.pytorch_dump_path, args.base_model\n     )\n-\n-\n-__all__ = []"
        },
        {
            "sha": "a87f769a255fb0d2df7f4fbf9713169fe14efa02",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -25,6 +25,7 @@\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torch_available, logging, requires_backends\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -326,6 +327,7 @@ def scale_bbox_to_transformed_image(\n     return [top_scaled, left_scaled, bottom_scaled, right_scaled]\n \n \n+@requires(backends=(\"vision\",))\n class FuyuProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Fuyu processor which wraps a Fuyu image processor and a Llama tokenizer into a single processor."
        },
        {
            "sha": "9850fb532a4685153542be49923b9f1025df2c53",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n if TYPE_CHECKING:\n@@ -39,6 +40,7 @@\n SPIECE_UNDERLINE = \"▁\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class GemmaTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a Gemma tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is"
        },
        {
            "sha": "327fee4a11fd308f980845a971a9fc8335decaf1",
            "filename": "src/transformers/models/glpn/feature_extraction_glpn.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fglpn%2Ffeature_extraction_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fglpn%2Ffeature_extraction_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Ffeature_extraction_glpn.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_glpn import GLPNImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class GLPNFeatureExtractor(GLPNImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "60f339e0f1363caac43925450eaa470c74b1267e",
            "filename": "src/transformers/models/glpn/image_processing_glpn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -16,6 +16,8 @@\n \n from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n \n+from ...utils.import_utils import requires\n+\n \n if TYPE_CHECKING:\n     from ...modeling_outputs import DepthEstimatorOutput\n@@ -47,6 +49,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class GLPNImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a GLPN image processor."
        },
        {
            "sha": "4a8454a8bc4250be2b29f342f8c2216f5f6a6ae5",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_tf.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2,13 +2,18 @@\n from typing import Dict, List, Optional, Union\n \n import tensorflow as tf\n-from keras_nlp.tokenizers import BytePairTokenizer\n from tensorflow_text import pad_model_inputs\n \n from ...modeling_tf_utils import keras\n+from ...utils.import_utils import is_keras_nlp_available, requires\n from .tokenization_gpt2 import GPT2Tokenizer\n \n \n+if is_keras_nlp_available():\n+    from keras_nlp.tokenizers import BytePairTokenizer\n+\n+\n+@requires(backends=(\"keras_nlp\",))\n class TFGPT2Tokenizer(keras.layers.Layer):\n     \"\"\"\n     This is an in-graph tokenizer for GPT2. It should be initialized similarly to other tokenizers, using the\n@@ -37,6 +42,7 @@ def __init__(\n         self.max_length = max_length\n         self.vocab = vocab\n         self.merges = merges\n+\n         self.tf_tokenizer = BytePairTokenizer(vocab, merges, sequence_length=max_length)\n \n     @classmethod"
        },
        {
            "sha": "37eaecd84479e34a15c50f0e625db9d026cd9e22",
            "filename": "src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -10,6 +10,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import is_torch_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -20,6 +21,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class GPTSw3Tokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an GPTSw3 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "08b74d14ca871a7752b61fb71f08b7a7886d80f7",
            "filename": "src/transformers/models/granite/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 36,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,45 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_granite\": [\"GraniteConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_granite\"] = [\n-        \"GraniteForCausalLM\",\n-        \"GraniteModel\",\n-        \"GranitePreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_granite import GraniteConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_granite import (\n-            GraniteForCausalLM,\n-            GraniteModel,\n-            GranitePreTrainedModel,\n-        )\n-\n+    from .configuration_granite import *\n+    from .modeling_granite import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bfe98c3a5296be0f47a371caa074b1c7d44fad9e",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -192,3 +192,6 @@ def __init__(\n         )\n \n         rope_config_validation(self)\n+\n+\n+__all__ = [\"GraniteConfig\"]"
        },
        {
            "sha": "6f15f9ca095a1a1442491d4fb4a76f900fd5db7f",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 37,
            "deletions": 34,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -322,40 +322,6 @@ def forward(\n         return outputs\n \n \n-class GraniteRotaryEmbedding(nn.Module):\n-    def __init__(self, config: GraniteConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n GRANITE_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -403,6 +369,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class GraniteRotaryEmbedding(nn.Module):\n+    def __init__(self, config: GraniteConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n GRANITE_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -862,3 +862,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"GraniteForCausalLM\", \"GraniteModel\", \"GranitePreTrainedModel\"]"
        },
        {
            "sha": "dd88957fdbae3f832955eb5a9a8f7ea62f1bdfd5",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -25,7 +25,13 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import LossKwargs, logging\n-from ..llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+)\n from .configuration_granite import GraniteConfig\n \n \n@@ -112,6 +118,10 @@ def forward(\n         return outputs\n \n \n+class GranitePreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n class GraniteModel(LlamaModel):\n     def __init__(self, config: GraniteConfig):\n         super().__init__(config)\n@@ -281,3 +291,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"GraniteForCausalLM\", \"GraniteModel\", \"GranitePreTrainedModel\"]"
        },
        {
            "sha": "46787f139f10a0c339e4cea9524d34c00a03ceb6",
            "filename": "src/transformers/models/imagegpt/feature_extraction_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fimagegpt%2Ffeature_extraction_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fimagegpt%2Ffeature_extraction_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Ffeature_extraction_imagegpt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_imagegpt import ImageGPTImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ImageGPTFeatureExtractor(ImageGPTImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "a0d50459c8e4d8a22625366dc4c826eb933d66f9",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -32,6 +32,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -56,6 +57,7 @@ def color_quantize(x, clusters):\n     return np.argmin(d, axis=1)\n \n \n+@requires(backends=(\"vision\",))\n class ImageGPTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a ImageGPT image processor. This image processor can be used to resize images to a smaller resolution"
        },
        {
            "sha": "816c6b23052932d903e86086d5a5e2c22d182983",
            "filename": "src/transformers/models/instructblipvideo/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 62,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,71 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_instructblipvideo\": [\n-        \"InstructBlipVideoConfig\",\n-        \"InstructBlipVideoQFormerConfig\",\n-        \"InstructBlipVideoVisionConfig\",\n-    ],\n-    \"processing_instructblipvideo\": [\"InstructBlipVideoProcessor\"],\n-}\n-\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_instructblipvideo\"] = [\"InstructBlipVideoImageProcessor\"]\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_instructblipvideo\"] = [\n-        \"InstructBlipVideoQFormerModel\",\n-        \"InstructBlipVideoPreTrainedModel\",\n-        \"InstructBlipVideoForConditionalGeneration\",\n-        \"InstructBlipVideoVisionModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_instructblipvideo import (\n-        InstructBlipVideoConfig,\n-        InstructBlipVideoQFormerConfig,\n-        InstructBlipVideoVisionConfig,\n-    )\n-    from .processing_instructblipvideo import InstructBlipVideoProcessor\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_instructblipvideo import InstructBlipVideoImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_instructblipvideo import (\n-            InstructBlipVideoForConditionalGeneration,\n-            InstructBlipVideoPreTrainedModel,\n-            InstructBlipVideoQFormerModel,\n-            InstructBlipVideoVisionModel,\n-        )\n-\n+    from .configuration_instructblipvideo import *\n+    from .image_processing_instructblipvideo import *\n+    from .modeling_instructblipvideo import *\n+    from .processing_instructblipvideo import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "d4cdf659763e899b994bd7fee71b46dc776b63cf",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -337,3 +337,6 @@ def from_vision_qformer_text_configs(\n             text_config=text_config.to_dict(),\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"InstructBlipVideoConfig\", \"InstructBlipVideoQFormerConfig\", \"InstructBlipVideoVisionConfig\"]"
        },
        {
            "sha": "9c55ba60d397df4d9da9652974be68b53dc84a4c",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -323,3 +323,6 @@ def _preprocess_image(\n         image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n \n         return image\n+\n+\n+__all__ = [\"InstructBlipVideoImageProcessor\"]"
        },
        {
            "sha": "cdc64c680208126261398129a72aa4dd63a63832",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 79,
            "deletions": 71,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -56,39 +56,6 @@\n logger = logging.get_logger(__name__)\n \n \n-@dataclass\n-class InstructBlipVideoForConditionalGenerationModelOutput(ModelOutput):\n-    \"\"\"\n-    Class defining the outputs of [`InstructBlipVideoForConditionalGeneration`].\n-\n-    Args:\n-        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Language modeling loss from the language model.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head of the language model.\n-        vision_outputs (`BaseModelOutputWithPooling`):\n-            Outputs of the vision encoder.\n-        qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            Outputs of the Q-Former (Querying Transformer).\n-        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n-            Outputs of the language model.\n-    \"\"\"\n-\n-    loss: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n-    vision_outputs: Optional[torch.FloatTensor] = None\n-    qformer_outputs: Optional[Tuple[torch.FloatTensor]] = None\n-    language_model_outputs: Optional[Tuple[torch.FloatTensor]] = None\n-\n-    def to_tuple(self) -> Tuple[Any]:\n-        return tuple(\n-            self[k]\n-            if k not in [\"vision_outputs\", \"qformer_outputs\", \"language_model_outputs\"]\n-            else getattr(self, k).to_tuple()\n-            for k in self.keys()\n-        )\n-\n-\n class InstructBlipVideoVisionEmbeddings(nn.Module):\n     def __init__(self, config: InstructBlipVideoVisionConfig):\n         super().__init__()\n@@ -163,6 +130,44 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n+class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = InstructBlipVideoConfig\n+    base_model_prefix = \"blip\"\n+    supports_gradient_checkpointing = True\n+\n+    _no_split_modules = [\n+        \"InstructBlipVideoQFormerEmbeddings\",\n+        \"InstructBlipVideoAttention\",\n+        \"InstructBlipVideoQFormerMultiHeadAttention\",\n+        \"InstructBlipVideoQFormerSelfOutput\",\n+    ]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_range\n+        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n+\n+        if isinstance(module, InstructBlipVideoVisionEmbeddings):\n+            if hasattr(self.config, \"vision_config\") and not isinstance(self.config, InstructBlipVideoVisionConfig):\n+                factor = self.config.vision_config.initializer_range\n+            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n class InstructBlipVideoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -307,44 +312,6 @@ def forward(\n         return outputs\n \n \n-class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = InstructBlipVideoConfig\n-    base_model_prefix = \"blip\"\n-    supports_gradient_checkpointing = True\n-\n-    _no_split_modules = [\n-        \"InstructBlipVideoQFormerEmbeddings\",\n-        \"InstructBlipVideoAttention\",\n-        \"InstructBlipVideoQFormerMultiHeadAttention\",\n-        \"InstructBlipVideoQFormerSelfOutput\",\n-    ]\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_range\n-        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n-\n-        if isinstance(module, InstructBlipVideoVisionEmbeddings):\n-            if hasattr(self.config, \"vision_config\") and not isinstance(self.config, InstructBlipVideoVisionConfig):\n-                factor = self.config.vision_config.initializer_range\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n-\n-\n class InstructBlipVideoEncoder(nn.Module):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -1186,6 +1153,39 @@ def forward(\n         )\n \n \n+@dataclass\n+class InstructBlipVideoForConditionalGenerationModelOutput(ModelOutput):\n+    \"\"\"\n+    Class defining the outputs of [`InstructBlipVideoForConditionalGeneration`].\n+\n+    Args:\n+        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+            Language modeling loss from the language model.\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head of the language model.\n+        vision_outputs (`BaseModelOutputWithPooling`):\n+            Outputs of the vision encoder.\n+        qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+            Outputs of the Q-Former (Querying Transformer).\n+        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n+            Outputs of the language model.\n+    \"\"\"\n+\n+    loss: Optional[Tuple[torch.FloatTensor]] = None\n+    logits: Optional[Tuple[torch.FloatTensor]] = None\n+    vision_outputs: Optional[torch.FloatTensor] = None\n+    qformer_outputs: Optional[Tuple[torch.FloatTensor]] = None\n+    language_model_outputs: Optional[Tuple[torch.FloatTensor]] = None\n+\n+    def to_tuple(self) -> Tuple[Any]:\n+        return tuple(\n+            self[k]\n+            if k not in [\"vision_outputs\", \"qformer_outputs\", \"language_model_outputs\"]\n+            else getattr(self, k).to_tuple()\n+            for k in self.keys()\n+        )\n+\n+\n INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -1677,3 +1677,11 @@ def generate(\n         outputs = self.language_model.generate(**inputs, **generate_kwargs)\n \n         return outputs\n+\n+\n+__all__ = [\n+    \"InstructBlipVideoVisionModel\",\n+    \"InstructBlipVideoPreTrainedModel\",\n+    \"InstructBlipVideoQFormerModel\",\n+    \"InstructBlipVideoForConditionalGeneration\",\n+]"
        },
        {
            "sha": "ed2364edce2a54bec8e9ce340d9b8e0aecefd075",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -27,6 +27,9 @@\n from transformers.models.instructblip.modeling_instructblip import (\n     InstructBlipForConditionalGeneration,\n     InstructBlipForConditionalGenerationModelOutput,\n+    InstructBlipPreTrainedModel,\n+    InstructBlipQFormerModel,\n+    InstructBlipVisionModel,\n )\n \n from ...configuration_utils import PretrainedConfig\n@@ -168,6 +171,18 @@ def from_vision_qformer_text_configs(\n         )\n \n \n+class InstructBlipVideoPreTrainedModel(InstructBlipPreTrainedModel):\n+    pass\n+\n+\n+class InstructBlipVideoVisionModel(InstructBlipVisionModel):\n+    pass\n+\n+\n+class InstructBlipVideoQFormerModel(InstructBlipQFormerModel):\n+    pass\n+\n+\n @dataclass\n class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForConditionalGenerationModelOutput):\n     pass\n@@ -481,3 +496,14 @@ def generate(\n         outputs = self.language_model.generate(**inputs, **generate_kwargs)\n \n         return outputs\n+\n+\n+__all__ = [\n+    \"InstructBlipVideoConfig\",\n+    \"InstructBlipVideoQFormerConfig\",\n+    \"InstructBlipVideoVisionConfig\",\n+    \"InstructBlipVideoVisionModel\",\n+    \"InstructBlipVideoPreTrainedModel\",\n+    \"InstructBlipVideoQFormerModel\",\n+    \"InstructBlipVideoForConditionalGeneration\",\n+]"
        },
        {
            "sha": "427a12d68ad4a59abc4cb3c7768c57cd1c25ff3d",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -234,3 +234,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         qformer_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"qformer_tokenizer\")\n         processor.qformer_tokenizer = qformer_tokenizer\n         return processor\n+\n+\n+__all__ = [\"InstructBlipVideoProcessor\"]"
        },
        {
            "sha": "8c70e1ed643101401373ae29637d4d597873485e",
            "filename": "src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ffeature_extraction_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ffeature_extraction_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ffeature_extraction_layoutlmv2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -19,12 +19,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_layoutlmv2 import LayoutLMv2ImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class LayoutLMv2FeatureExtractor(LayoutLMv2ImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "5d946982fa7bd6ea5373e41ddf1efd339e7d57b7",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,6 +38,7 @@\n     logging,\n     requires_backends,\n )\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -98,6 +99,7 @@ def apply_tesseract(\n     return words, normalized_boxes\n \n \n+@requires(backends=(\"vision\",))\n class LayoutLMv2ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LayoutLMv2 image processor."
        },
        {
            "sha": "5ea779a48f12bf1f0036a3a02cd018adabebc2fc",
            "filename": "src/transformers/models/layoutlmv3/feature_extraction_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ffeature_extraction_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ffeature_extraction_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ffeature_extraction_layoutlmv3.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -19,12 +19,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_layoutlmv3 import LayoutLMv3ImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class LayoutLMv3FeatureExtractor(LayoutLMv3ImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "d322c78d7e68bc32197e5e782706571b89d39d86",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -41,6 +41,7 @@\n     logging,\n     requires_backends,\n )\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -100,6 +101,7 @@ def apply_tesseract(\n     return words, normalized_boxes\n \n \n+@requires(backends=(\"vision\",))\n class LayoutLMv3ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LayoutLMv3 image processor."
        },
        {
            "sha": "8dc459ba9408ebf1808707ff8affdc76224e1bbf",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -30,6 +30,7 @@\n     TruncationStrategy,\n )\n from ...utils import PaddingStrategy, TensorType, add_end_docstrings, logging\n+from ...utils.import_utils import requires\n from ..xlm_roberta.tokenization_xlm_roberta import (\n     SPIECE_UNDERLINE,\n     VOCAB_FILES_NAMES,\n@@ -143,6 +144,7 @@\n \"\"\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class LayoutXLMTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on"
        },
        {
            "sha": "d634239b24500fd279d41101188937548df4bfa2",
            "filename": "src/transformers/models/levit/feature_extraction_levit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flevit%2Ffeature_extraction_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flevit%2Ffeature_extraction_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Ffeature_extraction_levit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_levit import LevitImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class LevitFeatureExtractor(LevitImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "d980bea55529d5ae3324dd765418d8bbb0b15ac4",
            "filename": "src/transformers/models/levit/image_processing_levit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,11 +38,13 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class LevitImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LeViT image processor."
        },
        {
            "sha": "4869ba04ea1ce11844aa6146b4c7a87fa4a09663",
            "filename": "src/transformers/models/llama/tokenization_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -29,6 +29,7 @@\n from ...convert_slow_tokenizer import import_protobuf\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n if TYPE_CHECKING:\n@@ -51,6 +52,7 @@\n correct. If you don't know the answer to a question, please don't share false information.\"\"\"  # fmt: skip\n \n \n+@requires(backends=(\"sentencepiece\",))\n class LlamaTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is"
        },
        {
            "sha": "61ef776db89728c66139bfcd612e2aa808c94007",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -39,11 +39,13 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class LlavaOnevisionVideoProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LLaVa-Onevisino-Video video processor. Based on [`SiglipImageProcessor`] with incorporation of processing each video frame."
        },
        {
            "sha": "f47f968731787b479435ea19869e243010a6badb",
            "filename": "src/transformers/models/m2m_100/tokenization_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fm2m_100%2Ftokenization_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fm2m_100%2Ftokenization_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Ftokenization_m2m_100.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...tokenization_utils import BatchEncoding, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -44,6 +45,7 @@\n # fmt: on\n \n \n+@requires(backends=(\"sentencepiece\",))\n class M2M100Tokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an M2M100 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "bf9e0a8a2a1023f63977745b46058f42a1b0f6ce",
            "filename": "src/transformers/models/marian/tokenization_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -41,6 +42,7 @@\n # Example URL https://huggingface.co/Helsinki-NLP/opus-mt-en-de/resolve/main/vocab.json\n \n \n+@requires(backends=(\"sentencepiece\",))\n class MarianTokenizer(PreTrainedTokenizer):\n     r\"\"\"\n     Construct a Marian tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "98f7075fab83882bb84ab955bd763c2f1c1f067b",
            "filename": "src/transformers/models/maskformer/feature_extraction_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmaskformer%2Ffeature_extraction_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmaskformer%2Ffeature_extraction_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Ffeature_extraction_maskformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_maskformer import MaskFormerImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MaskFormerFeatureExtractor(MaskFormerImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "f433678019dcc3a003b359f0d1e754138556e522",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -51,6 +51,7 @@\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -352,6 +353,7 @@ def get_maskformer_resize_output_image_size(\n     return output_size\n \n \n+@requires(backends=(\"vision\",))\n class MaskFormerImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a MaskFormer image processor. The image processor can be used to prepare image(s) and optional targets"
        },
        {
            "sha": "b6c55b383315e412ee99a31b1e3cc395ccf6247b",
            "filename": "src/transformers/models/mbart/tokenization_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -21,6 +21,7 @@\n \n from ...tokenization_utils import AddedToken, BatchEncoding, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -33,6 +34,7 @@\n FAIRSEQ_LANGUAGE_CODES = [\"ar_AR\", \"cs_CZ\", \"de_DE\", \"en_XX\", \"es_XX\", \"et_EE\", \"fi_FI\", \"fr_XX\", \"gu_IN\", \"hi_IN\", \"it_IT\", \"ja_XX\", \"kk_KZ\", \"ko_KR\", \"lt_LT\", \"lv_LV\", \"my_MM\", \"ne_NP\", \"nl_XX\", \"ro_RO\", \"ru_RU\", \"si_LK\", \"tr_TR\", \"vi_VN\", \"zh_CN\"]  # fmt: skip\n \n \n+@requires(backends=(\"sentencepiece\",))\n class MBartTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an MBART tokenizer."
        },
        {
            "sha": "a212012463becf3b4d90de6a7d0d958da87b8f27",
            "filename": "src/transformers/models/mbart50/tokenization_mbart50.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -21,6 +21,7 @@\n \n from ...tokenization_utils import AddedToken, BatchEncoding, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -33,6 +34,7 @@\n FAIRSEQ_LANGUAGE_CODES = [\"ar_AR\", \"cs_CZ\", \"de_DE\", \"en_XX\", \"es_XX\", \"et_EE\", \"fi_FI\", \"fr_XX\", \"gu_IN\", \"hi_IN\", \"it_IT\", \"ja_XX\", \"kk_KZ\", \"ko_KR\", \"lt_LT\", \"lv_LV\", \"my_MM\", \"ne_NP\", \"nl_XX\", \"ro_RO\", \"ru_RU\", \"si_LK\", \"tr_TR\", \"vi_VN\", \"zh_CN\", \"af_ZA\", \"az_AZ\", \"bn_IN\", \"fa_IR\", \"he_IL\", \"hr_HR\", \"id_ID\", \"ka_GE\", \"km_KH\", \"mk_MK\", \"ml_IN\", \"mn_MN\", \"mr_IN\", \"pl_PL\", \"ps_AF\", \"pt_XX\", \"sv_SE\", \"sw_KE\", \"ta_IN\", \"te_IN\", \"th_TH\", \"tl_XX\", \"uk_UA\", \"ur_PK\", \"xh_ZA\", \"gl_ES\", \"sl_SI\"]  # fmt: skip\n \n \n+@requires(backends=(\"sentencepiece\",))\n class MBart50Tokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a MBart50 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "src/transformers/models/megatron_gpt2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1,13 +0,0 @@\n-# Copyright 2021  NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License."
        },
        {
            "sha": "9e68db0c816fdd08eeb61f3ee3657392ececdb56",
            "filename": "src/transformers/models/mgp_str/processing_mgp_str.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -21,6 +21,7 @@\n from transformers.utils.generic import ExplicitEnum\n \n from ...processing_utils import ProcessorMixin\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -36,6 +37,7 @@ class DecodeType(ExplicitEnum):\n SUPPORTED_ANNOTATION_FORMATS = (DecodeType.CHARACTER, DecodeType.BPE, DecodeType.WORDPIECE)\n \n \n+@requires(backends=(\"sentencepiece\",))\n class MgpstrProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a MGP-STR processor which wraps an image processor and MGP-STR tokenizers into a single"
        },
        {
            "sha": "18a5657cd2ec6dbf39c6e794fae09bf65753da8f",
            "filename": "src/transformers/models/mistral/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 97,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,106 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_mistral\": [\"MistralConfig\"],\n-}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_mistral\"] = [\n-        \"MistralForCausalLM\",\n-        \"MistralForQuestionAnswering\",\n-        \"MistralModel\",\n-        \"MistralPreTrainedModel\",\n-        \"MistralForSequenceClassification\",\n-        \"MistralForTokenClassification\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_mistral\"] = [\n-        \"FlaxMistralForCausalLM\",\n-        \"FlaxMistralModel\",\n-        \"FlaxMistralPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_mistral\"] = [\n-        \"TFMistralModel\",\n-        \"TFMistralForCausalLM\",\n-        \"TFMistralForSequenceClassification\",\n-        \"TFMistralPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_mistral import MistralConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_mistral import (\n-            MistralForCausalLM,\n-            MistralForQuestionAnswering,\n-            MistralForSequenceClassification,\n-            MistralForTokenClassification,\n-            MistralModel,\n-            MistralPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_mistral import (\n-            FlaxMistralForCausalLM,\n-            FlaxMistralModel,\n-            FlaxMistralPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_mistral import (\n-            TFMistralForCausalLM,\n-            TFMistralForSequenceClassification,\n-            TFMistralModel,\n-            TFMistralPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_mistral import *\n+    from .modeling_flax_mistral import *\n+    from .modeling_mistral import *\n+    from .modeling_tf_mistral import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c362dbef2860f190cd371cae5886ef3a31736bc7",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -164,3 +164,6 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"MistralConfig\"]"
        },
        {
            "sha": "fe909856788cfd4eadc954aed5abcb4f80c761b0",
            "filename": "src/transformers/models/mistral/modeling_flax_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -740,3 +740,5 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     _CONFIG_FOR_DOC,\n     real_checkpoint=_REAL_CHECKPOINT_FOR_DOC,\n )\n+\n+__all__ = [\"FlaxMistralForCausalLM\", \"FlaxMistralModel\", \"FlaxMistralPreTrainedModel\"]"
        },
        {
            "sha": "8f1b416d5b16ac7b8cc8e91abf486b89f8702bd4",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 44,
            "deletions": 34,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -274,40 +274,6 @@ def forward(\n         return outputs\n \n \n-class MistralRotaryEmbedding(nn.Module):\n-    def __init__(self, config: MistralConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n MISTRAL_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -355,6 +321,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class MistralRotaryEmbedding(nn.Module):\n+    def __init__(self, config: MistralConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n MISTRAL_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1101,3 +1101,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"MistralForCausalLM\",\n+    \"MistralForQuestionAnswering\",\n+    \"MistralModel\",\n+    \"MistralPreTrainedModel\",\n+    \"MistralForSequenceClassification\",\n+    \"MistralForTokenClassification\",\n+]"
        },
        {
            "sha": "34b4da4c33624c8faacdd63e922c5aaae86432b8",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1041,3 +1041,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"score\", None) is not None:\n             with tf.name_scope(self.score.name):\n                 self.score.build((self.config.hidden_size,))\n+\n+\n+__all__ = [\"TFMistralModel\", \"TFMistralForCausalLM\", \"TFMistralForSequenceClassification\", \"TFMistralPreTrainedModel\"]"
        },
        {
            "sha": "3f977381439c3bf4a707e9f34b5504037ad7da4a",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -20,6 +20,7 @@\n     LlamaForTokenClassification,\n     LlamaMLP,\n     LlamaModel,\n+    LlamaPreTrainedModel,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -106,6 +107,10 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.mlp = MistralMLP(config)\n \n \n+class MistralPreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n class MistralModel(LlamaModel):\n     def __init__(self, config: MistralConfig):\n         super().__init__(config)\n@@ -344,3 +349,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"MistralForCausalLM\",\n+    \"MistralForQuestionAnswering\",\n+    \"MistralModel\",\n+    \"MistralPreTrainedModel\",\n+    \"MistralForSequenceClassification\",\n+    \"MistralForTokenClassification\",\n+]"
        },
        {
            "sha": "e4ca36bacbee79261b8fe19e1df5cd2fb91b7344",
            "filename": "src/transformers/models/mixtral/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 45,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,54 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_mixtral\": [\"MixtralConfig\"],\n-}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_mixtral\"] = [\n-        \"MixtralForCausalLM\",\n-        \"MixtralForQuestionAnswering\",\n-        \"MixtralModel\",\n-        \"MixtralPreTrainedModel\",\n-        \"MixtralForSequenceClassification\",\n-        \"MixtralForTokenClassification\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_mixtral import MixtralConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_mixtral import (\n-            MixtralForCausalLM,\n-            MixtralForQuestionAnswering,\n-            MixtralForSequenceClassification,\n-            MixtralForTokenClassification,\n-            MixtralModel,\n-            MixtralPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_mixtral import *\n+    from .modeling_mixtral import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c8f7eaccdbba81db75ecfca22c7f64940aa2e89f",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -186,3 +186,6 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"MixtralConfig\"]"
        },
        {
            "sha": "da6beed1214ffa579709df79cc2f0bbcfb55bbe6",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1334,3 +1334,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"MixtralForCausalLM\",\n+    \"MixtralForQuestionAnswering\",\n+    \"MixtralModel\",\n+    \"MixtralPreTrainedModel\",\n+    \"MixtralForSequenceClassification\",\n+    \"MixtralForTokenClassification\",\n+]"
        },
        {
            "sha": "f85836b5677076dd8015acd8bbc276f9b7913c6f",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -571,3 +571,13 @@ class MixtralForTokenClassification(MistralForTokenClassification):\n \n class MixtralForQuestionAnswering(MistralForQuestionAnswering):\n     pass\n+\n+\n+__all__ = [\n+    \"MixtralForCausalLM\",\n+    \"MixtralForQuestionAnswering\",\n+    \"MixtralModel\",\n+    \"MixtralPreTrainedModel\",\n+    \"MixtralForSequenceClassification\",\n+    \"MixtralForTokenClassification\",\n+]"
        },
        {
            "sha": "90619befc22edaded2f70b78be17a31b0891638c",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,6 +38,7 @@\n     to_py_obj,\n )\n from ...utils import add_end_docstrings, is_tf_tensor, is_torch_tensor, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -128,6 +129,7 @@\n \"\"\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class MLukeTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`XLMRobertaTokenizer`] and [`LukeTokenizer`]. Based on"
        },
        {
            "sha": "02a5401bc145996d1126641ee656180a48c92e20",
            "filename": "src/transformers/models/mobilenet_v1/feature_extraction_mobilenet_v1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Ffeature_extraction_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Ffeature_extraction_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Ffeature_extraction_mobilenet_v1.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_mobilenet_v1 import MobileNetV1ImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MobileNetV1FeatureExtractor(MobileNetV1ImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "c9f96a955e0a9c6f5fcf2264e69afb03d97e4485",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,11 +38,13 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MobileNetV1ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a MobileNetV1 image processor."
        },
        {
            "sha": "e36b50cffaada3293d1dada34f06022f3ecf8163",
            "filename": "src/transformers/models/mobilenet_v2/feature_extraction_mobilenet_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Ffeature_extraction_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Ffeature_extraction_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Ffeature_extraction_mobilenet_v2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_mobilenet_v2 import MobileNetV2ImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MobileNetV2FeatureExtractor(MobileNetV2ImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "ca6aa04c148817cab9d27922f99e6a27bdbd757e",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -44,9 +44,13 @@\n     import torch\n \n \n+from ...utils.import_utils import requires\n+\n+\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MobileNetV2ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a MobileNetV2 image processor."
        },
        {
            "sha": "6c220df918647341a289e39ef0f885b80dcd9df3",
            "filename": "src/transformers/models/mobilevit/feature_extraction_mobilevit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilevit%2Ffeature_extraction_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilevit%2Ffeature_extraction_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Ffeature_extraction_mobilevit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_mobilevit import MobileViTImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MobileViTFeatureExtractor(MobileViTImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "23ceae679f8254f59f33604146868dc8caa382b8",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -39,6 +39,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -51,6 +52,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class MobileViTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a MobileViT image processor."
        },
        {
            "sha": "130d58f59b8b64de9c038c7e1647487535c9ccb6",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2566,5 +2566,4 @@ def forward(\n     \"MT5ForTokenClassification\",\n     \"MT5Model\",\n     \"MT5PreTrainedModel\",\n-    \"MT5Stack\",\n ]"
        },
        {
            "sha": "51456aac76b0e0c5666acadf0d63008a7c3b50d9",
            "filename": "src/transformers/models/musicgen_melody/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 65,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,74 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-    is_torchaudio_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_musicgen_melody\": [\n-        \"MusicgenMelodyConfig\",\n-        \"MusicgenMelodyDecoderConfig\",\n-    ],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_musicgen_melody\"] = [\n-        \"MusicgenMelodyForConditionalGeneration\",\n-        \"MusicgenMelodyForCausalLM\",\n-        \"MusicgenMelodyModel\",\n-        \"MusicgenMelodyPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_torchaudio_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_musicgen_melody\"] = [\"MusicgenMelodyFeatureExtractor\"]\n-    _import_structure[\"processing_musicgen_melody\"] = [\"MusicgenMelodyProcessor\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_musicgen_melody import (\n-        MusicgenMelodyConfig,\n-        MusicgenMelodyDecoderConfig,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_musicgen_melody import (\n-            MusicgenMelodyForCausalLM,\n-            MusicgenMelodyForConditionalGeneration,\n-            MusicgenMelodyModel,\n-            MusicgenMelodyPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_torchaudio_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_musicgen_melody import MusicgenMelodyFeatureExtractor\n-        from .processing_musicgen_melody import MusicgenMelodyProcessor\n-\n-\n+    from .configuration_musicgen_melody import *\n+    from .modeling_musicgen_melody import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "24c8a17faa06b19a3322f672353d6b3a91dc1f5e",
            "filename": "src/transformers/models/musicgen_melody/configuration_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -256,3 +256,6 @@ def from_sub_models_config(\n     # This is a property because you might want to change the codec model on the fly\n     def sampling_rate(self):\n         return self.audio_encoder.sampling_rate\n+\n+\n+__all__ = [\"MusicgenMelodyConfig\", \"MusicgenMelodyDecoderConfig\"]"
        },
        {
            "sha": "d823adf649d2da79eedb604c0a5fe8ab5516c7e8",
            "filename": "src/transformers/models/musicgen_melody/feature_extraction_musicgen_melody.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Ffeature_extraction_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Ffeature_extraction_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Ffeature_extraction_musicgen_melody.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -25,6 +25,7 @@\n from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n from ...feature_extraction_utils import BatchFeature\n from ...utils import TensorType, is_torch_available, is_torchaudio_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -36,6 +37,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"torchaudio\",))\n class MusicgenMelodyFeatureExtractor(SequenceFeatureExtractor):\n     r\"\"\"\n     Constructs a MusicgenMelody feature extractor.\n@@ -329,3 +331,6 @@ def to_dict(self) -> Dict[str, Any]:\n         if \"spectrogram\" in output:\n             del output[\"spectrogram\"]\n         return output\n+\n+\n+__all__ = [\"MusicgenMelodyFeatureExtractor\"]"
        },
        {
            "sha": "07a593ca2ac0e8611c1a8f04bdf2ae8a26102673",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2563,3 +2563,11 @@ def generate(\n             return outputs\n         else:\n             return output_values\n+\n+\n+__all__ = [\n+    \"MusicgenMelodyForConditionalGeneration\",\n+    \"MusicgenMelodyForCausalLM\",\n+    \"MusicgenMelodyModel\",\n+    \"MusicgenMelodyPreTrainedModel\",\n+]"
        },
        {
            "sha": "f42d6af60164c91d2804109e434fb6916f025e00",
            "filename": "src/transformers/models/musicgen_melody/processing_musicgen_melody.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,8 +22,10 @@\n \n from ...processing_utils import ProcessorMixin\n from ...utils import to_numpy\n+from ...utils.import_utils import requires\n \n \n+@requires(backends=(\"torchaudio\",))\n class MusicgenMelodyProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a MusicGen Melody processor which wraps a Wav2Vec2 feature extractor - for raw audio waveform processing - and a T5 tokenizer into a single processor\n@@ -173,3 +175,6 @@ def get_unconditional_inputs(self, num_samples=1, return_tensors=\"pt\"):\n         inputs[\"attention_mask\"][:] = 0\n \n         return inputs\n+\n+\n+__all__ = [\"MusicgenMelodyProcessor\"]"
        },
        {
            "sha": "1ac0059ddd21760811a71b12fd4d3fa0a0a25388",
            "filename": "src/transformers/models/nllb/tokenization_nllb.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -21,6 +21,7 @@\n \n from ...tokenization_utils import AddedToken, BatchEncoding, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -33,6 +34,7 @@\n FAIRSEQ_LANGUAGE_CODES = ['ace_Arab', 'ace_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'afr_Latn', 'ajp_Arab', 'aka_Latn', 'amh_Ethi', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'asm_Beng', 'ast_Latn', 'awa_Deva', 'ayr_Latn', 'azb_Arab', 'azj_Latn', 'bak_Cyrl', 'bam_Latn', 'ban_Latn', 'bel_Cyrl', 'bem_Latn', 'ben_Beng', 'bho_Deva', 'bjn_Arab', 'bjn_Latn', 'bod_Tibt', 'bos_Latn', 'bug_Latn', 'bul_Cyrl', 'cat_Latn', 'ceb_Latn', 'ces_Latn', 'cjk_Latn', 'ckb_Arab', 'crh_Latn', 'cym_Latn', 'dan_Latn', 'deu_Latn', 'dik_Latn', 'dyu_Latn', 'dzo_Tibt', 'ell_Grek', 'eng_Latn', 'epo_Latn', 'est_Latn', 'eus_Latn', 'ewe_Latn', 'fao_Latn', 'pes_Arab', 'fij_Latn', 'fin_Latn', 'fon_Latn', 'fra_Latn', 'fur_Latn', 'fuv_Latn', 'gla_Latn', 'gle_Latn', 'glg_Latn', 'grn_Latn', 'guj_Gujr', 'hat_Latn', 'hau_Latn', 'heb_Hebr', 'hin_Deva', 'hne_Deva', 'hrv_Latn', 'hun_Latn', 'hye_Armn', 'ibo_Latn', 'ilo_Latn', 'ind_Latn', 'isl_Latn', 'ita_Latn', 'jav_Latn', 'jpn_Jpan', 'kab_Latn', 'kac_Latn', 'kam_Latn', 'kan_Knda', 'kas_Arab', 'kas_Deva', 'kat_Geor', 'knc_Arab', 'knc_Latn', 'kaz_Cyrl', 'kbp_Latn', 'kea_Latn', 'khm_Khmr', 'kik_Latn', 'kin_Latn', 'kir_Cyrl', 'kmb_Latn', 'kon_Latn', 'kor_Hang', 'kmr_Latn', 'lao_Laoo', 'lvs_Latn', 'lij_Latn', 'lim_Latn', 'lin_Latn', 'lit_Latn', 'lmo_Latn', 'ltg_Latn', 'ltz_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'lus_Latn', 'mag_Deva', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'min_Latn', 'mkd_Cyrl', 'plt_Latn', 'mlt_Latn', 'mni_Beng', 'khk_Cyrl', 'mos_Latn', 'mri_Latn', 'zsm_Latn', 'mya_Mymr', 'nld_Latn', 'nno_Latn', 'nob_Latn', 'npi_Deva', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'oci_Latn', 'gaz_Latn', 'ory_Orya', 'pag_Latn', 'pan_Guru', 'pap_Latn', 'pol_Latn', 'por_Latn', 'prs_Arab', 'pbt_Arab', 'quy_Latn', 'ron_Latn', 'run_Latn', 'rus_Cyrl', 'sag_Latn', 'san_Deva', 'sat_Beng', 'scn_Latn', 'shn_Mymr', 'sin_Sinh', 'slk_Latn', 'slv_Latn', 'smo_Latn', 'sna_Latn', 'snd_Arab', 'som_Latn', 'sot_Latn', 'spa_Latn', 'als_Latn', 'srd_Latn', 'srp_Cyrl', 'ssw_Latn', 'sun_Latn', 'swe_Latn', 'swh_Latn', 'szl_Latn', 'tam_Taml', 'tat_Cyrl', 'tel_Telu', 'tgk_Cyrl', 'tgl_Latn', 'tha_Thai', 'tir_Ethi', 'taq_Latn', 'taq_Tfng', 'tpi_Latn', 'tsn_Latn', 'tso_Latn', 'tuk_Latn', 'tum_Latn', 'tur_Latn', 'twi_Latn', 'tzm_Tfng', 'uig_Arab', 'ukr_Cyrl', 'umb_Latn', 'urd_Arab', 'uzn_Latn', 'vec_Latn', 'vie_Latn', 'war_Latn', 'wol_Latn', 'xho_Latn', 'ydd_Hebr', 'yor_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'zul_Latn']  # fmt: skip\n \n \n+@requires(backends=(\"sentencepiece\",))\n class NllbTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an NLLB tokenizer."
        },
        {
            "sha": "139af5473e41fabbe978ee4b9c956dfd9b6e4453",
            "filename": "src/transformers/models/olmo/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 38,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,47 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_sentencepiece_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_olmo\": [\"OlmoConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_olmo\"] = [\n-        \"OlmoForCausalLM\",\n-        \"OlmoModel\",\n-        \"OlmoPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_olmo import OlmoConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_olmo import (\n-            OlmoForCausalLM,\n-            OlmoModel,\n-            OlmoPreTrainedModel,\n-        )\n-\n+    from .configuration_olmo import *\n+    from .modeling_olmo import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4ad5de6152055a42cf0178a6ca5ea4f32a7802d0",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -193,3 +193,6 @@ def _rope_scaling_validation(self):\n             )\n         if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n             raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n+\n+\n+__all__ = [\"OlmoConfig\"]"
        },
        {
            "sha": "8b8783d1ad8d46a7ae8e6ea68ab25676a1d9db94",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 37,
            "deletions": 34,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -284,40 +284,6 @@ def forward(\n         return outputs\n \n \n-class OlmoRotaryEmbedding(nn.Module):\n-    def __init__(self, config: OlmoConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n OLMO_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -365,6 +331,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class OlmoRotaryEmbedding(nn.Module):\n+    def __init__(self, config: OlmoConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n OLMO_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -824,3 +824,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"OlmoForCausalLM\", \"OlmoModel\", \"OlmoPreTrainedModel\"]"
        },
        {
            "sha": "ca290ba9ae77f4ed2fed3f2281b800740701f207",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -14,6 +14,7 @@\n     LlamaForCausalLM,\n     LlamaMLP,\n     LlamaModel,\n+    LlamaPreTrainedModel,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -113,6 +114,10 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.self_attn = OlmoAttention(config=config, layer_idx=layer_idx)\n \n \n+class OlmoPreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n class OlmoModel(LlamaModel):\n     def __init__(self, config: OlmoConfig):\n         super().__init__(config)\n@@ -124,3 +129,6 @@ def __init__(self, config: OlmoConfig):\n \n class OlmoForCausalLM(LlamaForCausalLM):\n     pass\n+\n+\n+__all__ = [\"OlmoForCausalLM\", \"OlmoModel\", \"OlmoPreTrainedModel\"]"
        },
        {
            "sha": "bcf990ccda60792f1a12bc60b68bf3a0c9b41bc0",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -287,40 +287,6 @@ def forward(\n         return outputs\n \n \n-class Olmo2RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Olmo2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n OLMO2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -368,6 +334,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class Olmo2RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Olmo2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n OLMO2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "3f5c51779b9d6b26be3aabd6fd1725dc992518a8",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -30,6 +30,7 @@\n     is_torchvision_available,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.import_utils import requires\n \n \n if TYPE_CHECKING:\n@@ -199,6 +200,7 @@ def _post_process_boxes_for_image(\n     return boxes_per_image, scores_per_image, labels_per_image\n \n \n+@requires(backends=(\"vision\", \"torchvision\"))\n class OmDetTurboProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a OmDet-Turbo processor which wraps a Deformable DETR image processor and an AutoTokenizer into a"
        },
        {
            "sha": "ee3a8d0b145cc54e8ccebf7acc84538236fac644",
            "filename": "src/transformers/models/owlvit/feature_extraction_owlvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fowlvit%2Ffeature_extraction_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fowlvit%2Ffeature_extraction_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Ffeature_extraction_owlvit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_owlvit import OwlViTImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class OwlViTFeatureExtractor(OwlViTImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "dfd5007f990c504c3c7fb3a8fd00f2fb48065937",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -41,6 +41,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_torch_available, logging\n+from ...utils.import_utils import requires\n \n \n if TYPE_CHECKING:\n@@ -120,6 +121,7 @@ def box_iou(boxes1, boxes2):\n     return iou, union\n \n \n+@requires(backends=(\"vision\",))\n class OwlViTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs an OWL-ViT image processor."
        },
        {
            "sha": "c338e0fac1f3ddef4c92a81f4cd7f3d8aa4b7dfb",
            "filename": "src/transformers/models/pegasus/tokenization_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -20,6 +20,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n SPIECE_UNDERLINE = \"▁\"\n@@ -31,6 +32,9 @@\n \n \n # TODO ArthurZ refactor this to only use the added_tokens_encoder\n+\n+\n+@requires(backends=(\"sentencepiece\",))\n class PegasusTokenizer(PreTrainedTokenizer):\n     r\"\"\"\n     Construct a PEGASUS tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "566cb31fea78a52c0c7cfaa3e509437c55dab578",
            "filename": "src/transformers/models/perceiver/feature_extraction_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fperceiver%2Ffeature_extraction_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fperceiver%2Ffeature_extraction_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Ffeature_extraction_perceiver.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_perceiver import PerceiverImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class PerceiverFeatureExtractor(PerceiverImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "82d571347392ece5057f62f67061c722edd5046f",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -35,6 +35,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -44,6 +45,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class PerceiverImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Perceiver image processor."
        },
        {
            "sha": "cffe33da73ee42eb20a2e7f33ea9351bb7da75c2",
            "filename": "src/transformers/models/phi/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 46,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -11,57 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_sentencepiece_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_phi\": [\"PhiConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_phi\"] = [\n-        \"PhiPreTrainedModel\",\n-        \"PhiModel\",\n-        \"PhiForCausalLM\",\n-        \"PhiForSequenceClassification\",\n-        \"PhiForTokenClassification\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_phi import PhiConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_phi import (\n-            PhiForCausalLM,\n-            PhiForSequenceClassification,\n-            PhiForTokenClassification,\n-            PhiModel,\n-            PhiPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_phi import *\n+    from .modeling_phi import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4ffd89db3adc2caa5d918e2044b2385b783cea44",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -212,3 +212,6 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"PhiConfig\"]"
        },
        {
            "sha": "ffb36ed45fbc6f21ecfd90283ce03a2af73d6fac",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 43,
            "deletions": 34,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -279,40 +279,6 @@ def forward(\n         return outputs\n \n \n-class PhiRotaryEmbedding(nn.Module):\n-    def __init__(self, config: PhiConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n PHI_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -360,6 +326,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class PhiRotaryEmbedding(nn.Module):\n+    def __init__(self, config: PhiConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n PHI_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1000,3 +1000,12 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"PhiPreTrainedModel\",\n+    \"PhiModel\",\n+    \"PhiForCausalLM\",\n+    \"PhiForSequenceClassification\",\n+    \"PhiForTokenClassification\",\n+]"
        },
        {
            "sha": "5661432f8d117376a7576ff08af4dd0a752809f5",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -19,6 +19,7 @@\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n     LlamaModel,\n+    LlamaPreTrainedModel,\n     apply_rotary_pos_emb,\n     eager_attention_forward,  # copied from Llama\n )\n@@ -169,6 +170,10 @@ def forward(\n         return outputs\n \n \n+class PhiPreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n class PhiModel(LlamaModel):\n     def __init__(self, config: PhiConfig):\n         super().__init__(config)\n@@ -296,3 +301,12 @@ class PhiForSequenceClassification(LlamaForSequenceClassification):\n \n class PhiForTokenClassification(LlamaForTokenClassification):\n     pass\n+\n+\n+__all__ = [\n+    \"PhiPreTrainedModel\",\n+    \"PhiModel\",\n+    \"PhiForCausalLM\",\n+    \"PhiForSequenceClassification\",\n+    \"PhiForTokenClassification\",\n+]"
        },
        {
            "sha": "b9b73b6f47414361bee9b85152ec8c5e6258f28e",
            "filename": "src/transformers/models/plbart/tokenization_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -21,6 +21,7 @@\n \n from ...tokenization_utils import AddedToken, BatchEncoding, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -46,6 +47,7 @@\n }\n \n \n+@requires(backends=(\"sentencepiece\",))\n class PLBartTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an PLBART tokenizer."
        },
        {
            "sha": "bde18b3ec0960f4425a1788b24cf838ff6459922",
            "filename": "src/transformers/models/poolformer/feature_extraction_poolformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpoolformer%2Ffeature_extraction_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpoolformer%2Ffeature_extraction_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Ffeature_extraction_poolformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_poolformer import PoolFormerImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class PoolFormerFeatureExtractor(PoolFormerImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "0b191ab106b100d9f4f8d57a92b56f715abd8fad",
            "filename": "src/transformers/models/pop2piano/feature_extraction_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -31,6 +31,7 @@\n     logging,\n     requires_backends,\n )\n+from ...utils.import_utils import requires\n \n \n if is_essentia_available():\n@@ -47,6 +48,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"essentia\", \"librosa\", \"scipy\", \"torch\"))\n class Pop2PianoFeatureExtractor(SequenceFeatureExtractor):\n     r\"\"\"\n     Constructs a Pop2Piano feature extractor."
        },
        {
            "sha": "3b839f8b1fd548a821f9358475fa0b9f9ea7fd09",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,8 +23,10 @@\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils import BatchEncoding, PaddingStrategy, TruncationStrategy\n from ...utils import TensorType\n+from ...utils.import_utils import requires\n \n \n+@requires(backends=(\"essentia\", \"librosa\", \"pretty_midi\", \"scipy\", \"torch\"))\n class Pop2PianoProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs an Pop2Piano processor which wraps a Pop2Piano Feature Extractor and Pop2Piano Tokenizer into a single"
        },
        {
            "sha": "0a7a8a558669186541281d2168fe3314fbf356f2",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,6 +23,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...tokenization_utils import AddedToken, BatchEncoding, PaddingStrategy, PreTrainedTokenizer, TruncationStrategy\n from ...utils import TensorType, is_pretty_midi_available, logging, requires_backends, to_numpy\n+from ...utils.import_utils import requires\n \n \n if is_pretty_midi_available():\n@@ -59,6 +60,7 @@ def token_note_to_note(number, current_velocity, default_velocity, note_onsets_r\n     return notes\n \n \n+@requires(backends=(\"pretty_midi\", \"torch\"))\n class Pop2PianoTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Constructs a Pop2Piano tokenizer. This tokenizer does not require training."
        },
        {
            "sha": "e447a94c54c86d07eee6c534a6860a2a5a6d0c3e",
            "filename": "src/transformers/models/qwen2/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 63,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -13,72 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_qwen2\": [\"Qwen2Config\"],\n-    \"tokenization_qwen2\": [\"Qwen2Tokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_qwen2_fast\"] = [\"Qwen2TokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_qwen2\"] = [\n-        \"Qwen2ForCausalLM\",\n-        \"Qwen2ForQuestionAnswering\",\n-        \"Qwen2Model\",\n-        \"Qwen2PreTrainedModel\",\n-        \"Qwen2ForSequenceClassification\",\n-        \"Qwen2ForTokenClassification\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_qwen2 import Qwen2Config\n-    from .tokenization_qwen2 import Qwen2Tokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_qwen2_fast import Qwen2TokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_qwen2 import (\n-            Qwen2ForCausalLM,\n-            Qwen2ForQuestionAnswering,\n-            Qwen2ForSequenceClassification,\n-            Qwen2ForTokenClassification,\n-            Qwen2Model,\n-            Qwen2PreTrainedModel,\n-        )\n-\n-\n+    from .configuration_qwen2 import *\n+    from .modeling_qwen2 import *\n+    from .tokenization_qwen2 import *\n+    from .tokenization_qwen2_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "fde5bd502b549c0624d7b675d35909a7cf06dc94",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -199,3 +199,6 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"Qwen2Config\"]"
        },
        {
            "sha": "d3180b35b3a42b45dc980c7b52bb77f489160c5d",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 44,
            "deletions": 34,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -287,40 +287,6 @@ def forward(\n         return outputs\n \n \n-class Qwen2RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Qwen2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n QWEN2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -368,6 +334,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class Qwen2RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Qwen2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n QWEN2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1114,3 +1114,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"Qwen2PreTrainedModel\",\n+    \"Qwen2Model\",\n+    \"Qwen2ForCausalLM\",\n+    \"Qwen2ForSequenceClassification\",\n+    \"Qwen2ForTokenClassification\",\n+    \"Qwen2ForQuestionAnswering\",\n+]"
        },
        {
            "sha": "afb3d1cf35e5e4cb9377bd1c9633f905bf63274c",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,6 +17,7 @@\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n     LlamaMLP,\n+    LlamaPreTrainedModel,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -114,6 +115,10 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n             )\n \n \n+class Qwen2PreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n class Qwen2Model(MistralModel):\n     pass\n \n@@ -132,3 +137,13 @@ class Qwen2ForTokenClassification(LlamaForTokenClassification):\n \n class Qwen2ForQuestionAnswering(LlamaForQuestionAnswering):\n     pass\n+\n+\n+__all__ = [\n+    \"Qwen2PreTrainedModel\",\n+    \"Qwen2Model\",\n+    \"Qwen2ForCausalLM\",\n+    \"Qwen2ForSequenceClassification\",\n+    \"Qwen2ForTokenClassification\",\n+    \"Qwen2ForQuestionAnswering\",\n+]"
        },
        {
            "sha": "c388789b728ff06172b5297f8238f2519d2eabca",
            "filename": "src/transformers/models/qwen2/tokenization_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -337,3 +337,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n     def prepare_for_tokenization(self, text, **kwargs):\n         text = unicodedata.normalize(\"NFC\", text)\n         return (text, kwargs)\n+\n+\n+__all__ = [\"Qwen2Tokenizer\"]"
        },
        {
            "sha": "b7312755ef580f157c024f9cf2b79c0ce6603077",
            "filename": "src/transformers/models/qwen2/tokenization_qwen2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -132,3 +132,6 @@ def __init__(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"Qwen2TokenizerFast\"]"
        },
        {
            "sha": "5852470d1c231d194ce9334cae6239ce2c340667",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -314,40 +314,6 @@ def forward(\n         return outputs\n \n \n-class Qwen3RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Qwen3Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n QWEN3_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -395,6 +361,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class Qwen3RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Qwen3Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n QWEN3_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "db2faf5dbc75727514b3cf84fbec8ea5d564d546",
            "filename": "src/transformers/models/reformer/tokenization_reformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -32,6 +33,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class ReformerTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a Reformer tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece) ."
        },
        {
            "sha": "2f2053425d727d3ef72a2dc219c0cd81392f96cc",
            "filename": "src/transformers/models/rembert/tokenization_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,13 +22,15 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class RemBertTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a RemBERT tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "e9bd83f1b230131caca55c93da96a07b0c485570",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -39,6 +39,7 @@\n     is_torchvision_v2_available,\n     requires_backends,\n )\n+from ...utils.import_utils import requires\n from .image_processing_rt_detr import get_size_with_aspect_ratio\n \n \n@@ -145,6 +146,7 @@ def prepare_coco_detection_annotation(\n             Whether to return segmentation masks.\n     \"\"\",\n )\n+@requires(backends=(\"torchvision\", \"torch\"))\n class RTDetrImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN"
        },
        {
            "sha": "0d73e6a099a5c6ae5c8923715f124c0264fe2c11",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -29,6 +29,7 @@\n )\n from ...tokenization_utils_base import AddedToken\n from ...utils import PaddingStrategy, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -40,6 +41,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class SeamlessM4TTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a SeamlessM4T tokenizer."
        },
        {
            "sha": "b979acf71ae312976c62fbfac63fb5752e049d09",
            "filename": "src/transformers/models/segformer/feature_extraction_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsegformer%2Ffeature_extraction_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsegformer%2Ffeature_extraction_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Ffeature_extraction_segformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_segformer import SegformerImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class SegformerFeatureExtractor(SegformerImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "79cbe47482dca3b3d21a54d667a8a46ddca21722",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -42,6 +42,7 @@\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -54,6 +55,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class SegformerImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Segformer image processor."
        },
        {
            "sha": "e1e62dcd049df5887460d00c1b933115086f324b",
            "filename": "src/transformers/models/siglip/tokenization_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsiglip%2Ftokenization_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsiglip%2Ftokenization_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Ftokenization_siglip.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -31,6 +31,7 @@\n if TYPE_CHECKING:\n     from ...tokenization_utils_base import TextInput\n from ...utils import logging, requires_backends\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -41,6 +42,7 @@\n SPIECE_UNDERLINE = \"▁\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class SiglipTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a Siglip tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "abb6dd5da7a5ca3c1505b870e9030d1ac33900f1",
            "filename": "src/transformers/models/speech_to_text/tokenization_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Ftokenization_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Ftokenization_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Ftokenization_speech_to_text.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -45,6 +46,7 @@\n LANGUAGES = {\"mustc\": MUSTC_LANGS}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class Speech2TextTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an Speech2Text tokenizer."
        },
        {
            "sha": "5817fb20fe0c098a632ef08a266b3ff66a473e5d",
            "filename": "src/transformers/models/speecht5/tokenization_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fspeecht5%2Ftokenization_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fspeecht5%2Ftokenization_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Ftokenization_speecht5.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n from .number_normalizer import EnglishNumberNormalizer\n \n \n@@ -30,6 +31,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"spm_char.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class SpeechT5Tokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a SpeechT5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "d357da439795a730d4436405ca5f0d65d23b7073",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -275,40 +275,6 @@ def forward(\n         return outputs\n \n \n-class Starcoder2RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Starcoder2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n STARCODER2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -356,6 +322,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class Starcoder2RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Starcoder2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n STARCODER2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "a74e1649961f485f3f5491c073678c383b193a12",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -35,6 +35,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, logging, requires_backends\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -131,6 +132,7 @@ def _is_valid_image(image):\n     raise ValueError(error_message)\n \n \n+@requires(backends=(\"torch\",))\n class SuperGlueImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a SuperGlue image processor."
        },
        {
            "sha": "01447232b890e4dfa126564dbca7de3c44902bc1",
            "filename": "src/transformers/models/t5/tokenization_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -30,18 +30,18 @@\n if TYPE_CHECKING:\n     from ...tokenization_utils_base import TextInput\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n \n-# TODO(PVP) - this should be removed in Transformers v5\n-\n SPIECE_UNDERLINE = \"▁\"\n \n \n+@requires(backends=(\"sentencepiece\",))\n class T5Tokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct a T5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "de54f8a936cda645e622664864612fd56f26407b",
            "filename": "src/transformers/models/timm_wrapper/image_processing_timm_wrapper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_transforms import to_pil_image\n from ...image_utils import ImageInput, make_list_of_images\n from ...utils import TensorType, logging, requires_backends\n-from ...utils.import_utils import is_timm_available, is_torch_available\n+from ...utils.import_utils import is_timm_available, is_torch_available, requires\n \n \n if is_timm_available():\n@@ -35,6 +35,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"torch\", \"timm\", \"torchvision\"))\n class TimmWrapperImageProcessor(BaseImageProcessor):\n     \"\"\"\n     Wrapper class for timm models to be used within transformers."
        },
        {
            "sha": "08eccaec7ba8371c13a15189979cde660fb15e83",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -33,6 +33,7 @@\n     TruncationStrategy,\n )\n from ...utils import PaddingStrategy, TensorType, add_end_docstrings, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -147,6 +148,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class UdopTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`LayoutXLMTokenizer`] and [`T5Tokenizer`]. Based on"
        },
        {
            "sha": "44a1a42a1876017f4179ca0ff810d5966a3e4f1f",
            "filename": "src/transformers/models/videomae/feature_extraction_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvideomae%2Ffeature_extraction_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvideomae%2Ffeature_extraction_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Ffeature_extraction_videomae.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_videomae import VideoMAEImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class VideoMAEFeatureExtractor(VideoMAEImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "358a77ac9b6688c55c52f7de0bc68dc23c4345be",
            "filename": "src/transformers/models/videomae/image_processing_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,6 +38,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -60,6 +61,7 @@ def make_batched(videos) -> List[List[ImageInput]]:\n     raise ValueError(f\"Could not make batched video from {videos}\")\n \n \n+@requires(backends=(\"vision\",))\n class VideoMAEImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a VideoMAE image processor."
        },
        {
            "sha": "e76fc89b9e8b93d7a106010d8c187ed68f8a59c0",
            "filename": "src/transformers/models/vilt/feature_extraction_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvilt%2Ffeature_extraction_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvilt%2Ffeature_extraction_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Ffeature_extraction_vilt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_vilt import ViltImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ViltFeatureExtractor(ViltImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "d4ac8cca32c410789e21de28c86fabedda8e2e64",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -35,6 +35,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n@@ -118,6 +119,7 @@ def get_resize_output_image_size(\n     return new_height, new_width\n \n \n+@requires(backends=(\"vision\",))\n class ViltImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a ViLT image processor."
        },
        {
            "sha": "c6a93df0bd8f82c81a715f78713a27b17672ecf6",
            "filename": "src/transformers/models/vit/feature_extraction_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvit%2Ffeature_extraction_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvit%2Ffeature_extraction_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Ffeature_extraction_vit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -17,12 +17,14 @@\n import warnings\n \n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_vit import ViTImageProcessor\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ViTFeatureExtractor(ViTImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "ade7495b1d4a690a0f41319aed6bde3a5078de77",
            "filename": "src/transformers/models/vit/image_processing_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -34,11 +34,13 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n \n+@requires(backends=(\"vision\",))\n class ViTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a ViT image processor."
        },
        {
            "sha": "858e93797d26948970f209e6429e4a586a5044a6",
            "filename": "src/transformers/models/vitpose_backbone/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 43,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2F__init__.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1,54 +1,17 @@\n # flake8: noqa\n # There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n # module, but to preserve other warnings. So, don't check this module at all.\n-\n-# Copyright 2024 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {\"configuration_vitpose_backbone\": [\"VitPoseBackboneConfig\"]}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_vitpose_backbone\"] = [\n-        \"VitPoseBackbonePreTrainedModel\",\n-        \"VitPoseBackbone\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_vitpose_backbone import VitPoseBackboneConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_vitpose_backbone import (\n-            VitPoseBackbone,\n-            VitPoseBackbonePreTrainedModel,\n-        )\n-\n+    from .configuration_vitpose_backbone import *\n+    from .modeling_vitpose_backbone import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "439e596273dbc3739b0b8f1cf7621e427571bc4c",
            "filename": "src/transformers/models/vitpose_backbone/configuration_vitpose_backbone.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fconfiguration_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fconfiguration_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fconfiguration_vitpose_backbone.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -134,3 +134,6 @@ def __init__(\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n         )\n+\n+\n+__all__ = [\"VitPoseBackboneConfig\"]"
        },
        {
            "sha": "8abce8dfdc3d84bc35ea3ea0a9b4575aa6e57312",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -574,3 +574,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"VitPoseBackbonePreTrainedModel\", \"VitPoseBackbone\"]"
        },
        {
            "sha": "e54191a57f57f38167c081f783edf17fd5e29316",
            "filename": "src/transformers/models/xglm/tokenization_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -31,6 +32,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class XGLMTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on"
        },
        {
            "sha": "04d725bd6cacfb1cf28cd6c41a19b8d5e956e8ec",
            "filename": "src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -31,6 +32,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\"}\n \n \n+@requires(backends=(\"sentencepiece\",))\n class XLMRobertaTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Adapted from [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on"
        },
        {
            "sha": "6f956fe84cb400916c0e2f0ed95950b3b7d4e606",
            "filename": "src/transformers/models/xlnet/tokenization_xlnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import SPIECE_UNDERLINE, logging\n+from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n@@ -38,6 +39,7 @@\n SEG_ID_PAD = 4\n \n \n+@requires(backends=(\"sentencepiece\",))\n class XLNetTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an XLNet tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)."
        },
        {
            "sha": "a1c9165832f7b181503e48af42eae559400f170a",
            "filename": "src/transformers/models/yolos/feature_extraction_yolos.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fyolos%2Ffeature_extraction_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fyolos%2Ffeature_extraction_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Ffeature_extraction_yolos.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -18,6 +18,7 @@\n \n from ...image_transforms import rgb_to_id as _rgb_to_id\n from ...utils import logging\n+from ...utils.import_utils import requires\n from .image_processing_yolos import YolosImageProcessor\n \n \n@@ -33,6 +34,7 @@ def rgb_to_id(x):\n     return _rgb_to_id(x)\n \n \n+@requires(backends=(\"vision\",))\n class YolosFeatureExtractor(YolosImageProcessor):\n     def __init__(self, *args, **kwargs) -> None:\n         warnings.warn("
        },
        {
            "sha": "2c1f0d1d2b085f96d42804c2587d8a0142182148",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -62,6 +62,7 @@\n     is_vision_available,\n     logging,\n )\n+from ...utils.import_utils import requires\n \n \n if is_torch_available():\n@@ -720,6 +721,7 @@ def compute_segments(\n     return segmentation, segments\n \n \n+@requires(backends=(\"vision\",))\n class YolosImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Detr image processor."
        },
        {
            "sha": "f752de39de61c731e7caeeb46beef0defe659dcd",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -177,6 +177,7 @@\n     strtobool,\n )\n from .utils.deprecation import deprecate_kwarg\n+from .utils.import_utils import requires\n from .utils.quantization_config import QuantizationMethod\n \n \n@@ -312,6 +313,12 @@ def safe_globals():\n FSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n \n \n+@requires(\n+    backends=(\n+        \"torch\",\n+        \"accelerate\",\n+    )\n+)\n class Trainer:\n     \"\"\"\n     Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers."
        },
        {
            "sha": "6f886de2824636a6a5e58ee28423a006fd006a8e",
            "filename": "src/transformers/utils/dummy_flax_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1323,
            "changes": 1323,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_flax_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_flax_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_flax_objects.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -105,1326 +105,3 @@ class FlaxPreTrainedModel(metaclass=DummyObject):\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertForPreTraining(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAlbertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_CAUSAL_LM_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_MASKED_LM_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_PRETRAINING_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING = None\n-\n-\n-FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING = None\n-\n-\n-FLAX_MODEL_MAPPING = None\n-\n-\n-class FlaxAutoModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForImageClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForNextSentencePrediction(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForPreTraining(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForSeq2SeqLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForSpeechSeq2Seq(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxAutoModelForVision2Seq(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartDecoderPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBartPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBeitForImageClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBeitForMaskedImageModeling(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBeitModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBeitPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForNextSentencePrediction(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForPreTraining(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForPreTraining(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBigBirdPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBlenderbotForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBlenderbotModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBlenderbotPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBlenderbotSmallForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBlenderbotSmallModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBlenderbotSmallPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBloomForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBloomModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxBloomPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPTextModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPTextModelWithProjection(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPTextPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPVisionModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxCLIPVisionPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDinov2ForImageClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDinov2Model(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDinov2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxDistilBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForPreTraining(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxElectraPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxEncoderDecoderModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGemmaForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGemmaModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGemmaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPT2LMHeadModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPT2Model(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPT2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPTNeoForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPTNeoModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPTNeoPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPTJForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPTJModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxGPTJPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxLlamaForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxLlamaModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxLlamaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxLongT5ForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxLongT5Model(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxLongT5PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMarianModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMarianMTModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMarianPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMBartForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMBartForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMBartForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMBartModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMBartPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMistralForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMistralModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMistralPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMT5EncoderModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMT5ForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxMT5Model(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxOPTForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxOPTModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxOPTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxPegasusForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxPegasusModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxPegasusPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRegNetForImageClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRegNetModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRegNetPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxResNetForImageClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxResNetModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxResNetPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRobertaPreLayerNormPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxRoFormerPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxSpeechEncoderDecoderModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxT5EncoderModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxT5ForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxT5Model(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxT5PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxVisionEncoderDecoderModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxVisionTextDualEncoderModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxViTForImageClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxViTModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxViTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWav2Vec2ForCTC(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWav2Vec2ForPreTraining(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWav2Vec2Model(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWav2Vec2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWhisperForAudioClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWhisperForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWhisperModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxWhisperPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXGLMForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXGLMModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXGLMPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaForCausalLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])\n-\n-\n-class FlaxXLMRobertaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"flax\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"flax\"])"
        },
        {
            "sha": "c6bb86a6d9b49e78f8936f3c1eb3cfc8b8db7951",
            "filename": "src/transformers/utils/dummy_keras_nlp_objects.py",
            "status": "removed",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/931126b929ee573c2bb951db87bdbaea51567cea/src%2Ftransformers%2Futils%2Fdummy_keras_nlp_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/931126b929ee573c2bb951db87bdbaea51567cea/src%2Ftransformers%2Futils%2Fdummy_keras_nlp_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_keras_nlp_objects.py?ref=931126b929ee573c2bb951db87bdbaea51567cea",
            "patch": "@@ -1,9 +0,0 @@\n-# This file is autogenerated by the command `make fix-copies`, do not edit.\n-from ..utils import DummyObject, requires_backends\n-\n-\n-class TFGPT2Tokenizer(metaclass=DummyObject):\n-    _backends = [\"keras_nlp\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"keras_nlp\"])"
        },
        {
            "sha": "55c592082c5120f39d7c419efc22afdd82e33f1d",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10567,
            "changes": 10567,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf"
        },
        {
            "sha": "de7b6f505df505966d8a2522dcc846dca2b53ea9",
            "filename": "src/transformers/utils/dummy_tf_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2691,
            "changes": 2691,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -153,2697 +153,6 @@ def shape_list(*args, **kwargs):\n     requires_backends(shape_list, [\"tf\"])\n \n \n-class TFAlbertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAlbertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_CAUSAL_LM_MAPPING = None\n-\n-\n-TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING = None\n-\n-\n-TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_MASK_GENERATION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING = None\n-\n-\n-TF_MODEL_FOR_MASKED_LM_MAPPING = None\n-\n-\n-TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING = None\n-\n-\n-TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_PRETRAINING_MAPPING = None\n-\n-\n-TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING = None\n-\n-\n-TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING = None\n-\n-\n-TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING = None\n-\n-\n-TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING = None\n-\n-\n-TF_MODEL_FOR_TEXT_ENCODING_MAPPING = None\n-\n-\n-TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING = None\n-\n-\n-TF_MODEL_FOR_VISION_2_SEQ_MAPPING = None\n-\n-\n-TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING = None\n-\n-\n-TF_MODEL_MAPPING = None\n-\n-\n-TF_MODEL_WITH_LM_HEAD_MAPPING = None\n-\n-\n-class TFAutoModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForAudioClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForDocumentQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForMaskedImageModeling(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForMaskGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForNextSentencePrediction(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForSemanticSegmentation(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForSeq2SeqLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForSpeechSeq2Seq(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForTableQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForTextEncoding(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForVision2Seq(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelForZeroShotImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAutoModelWithLMHead(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBartForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBartForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBartModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBartPretrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForNextSentencePrediction(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlenderbotForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlenderbotModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlenderbotPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlenderbotSmallForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlenderbotSmallModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlenderbotSmallPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipForImageTextRetrieval(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipTextModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFBlipVisionModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCamembertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCLIPModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCLIPPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCLIPTextModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCLIPVisionModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvNextForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvNextModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvNextPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvNextV2ForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvNextV2Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFConvNextV2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCTRLForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCTRLLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCTRLModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCTRLPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCvtForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCvtModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFCvtPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFData2VecVisionForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFData2VecVisionForSemanticSegmentation(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFData2VecVisionModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFData2VecVisionPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2ForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2ForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2ForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2ForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2ForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDebertaV2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDeiTForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDeiTForImageClassificationWithTeacher(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDeiTForMaskedImageModeling(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDeiTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDeiTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEfficientFormerForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEfficientFormerForImageClassificationWithTeacher(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEfficientFormerModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEfficientFormerPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFAdaptiveEmbedding(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTransfoXLForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTransfoXLLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTransfoXLMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTransfoXLModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTransfoXLPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDistilBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDPRContextEncoder(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDPRPretrainedContextEncoder(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDPRPretrainedQuestionEncoder(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDPRPretrainedReader(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDPRQuestionEncoder(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFDPRReader(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFElectraPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEncoderDecoderModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEsmForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEsmForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEsmForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEsmModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFEsmPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertForQuestionAnsweringSimple(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFlaubertWithLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelBaseModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFFunnelPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPT2DoubleHeadsModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPT2ForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPT2LMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPT2MainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPT2Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPT2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPTJForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPTJForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPTJForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPTJModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGPTJPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGroupViTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGroupViTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGroupViTTextModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFGroupViTVisionModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFHubertForCTC(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFHubertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFHubertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFIdeficsForVisionText2Text(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFIdeficsModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFIdeficsPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMv3ForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMv3ForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMv3ForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMv3Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLayoutLMv3PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLEDForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLEDModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLEDPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLongformerPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLxmertForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLxmertMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLxmertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLxmertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFLxmertVisualFeatureEncoder(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMarianModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMarianMTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMarianPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMBartForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMBartModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMBartPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMistralForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMistralForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMistralModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMistralPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForNextSentencePrediction(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileViTForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileViTForSemanticSegmentation(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileViTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMobileViTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMPNetPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMT5EncoderModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMT5ForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFMT5Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOpenAIGPTDoubleHeadsModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOpenAIGPTForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOpenAIGPTLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOpenAIGPTMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOpenAIGPTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOpenAIGPTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOPTForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOPTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFOPTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFPegasusForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFPegasusModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFPegasusPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRagModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRagPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRagSequenceForGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRagTokenForGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRegNetForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRegNetModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRegNetPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRemBertPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFResNetForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFResNetModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFResNetPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRobertaPreLayerNormPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFRoFormerPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSamModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSamPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSamVisionModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSegformerDecodeHead(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSegformerForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSegformerForSemanticSegmentation(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSegformerModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSegformerPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSpeech2TextForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSpeech2TextModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSpeech2TextPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwiftFormerForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwiftFormerModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwiftFormerPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwinForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwinForMaskedImageModeling(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwinModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFSwinPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFT5EncoderModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFT5ForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFT5Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFT5PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTapasForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTapasForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTapasForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTapasModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFTapasPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFVisionEncoderDecoderModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFVisionTextDualEncoderModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFViTForImageClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFViTModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFViTPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFViTMAEForPreTraining(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFViTMAEModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFViTMAEPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWav2Vec2ForCTC(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWav2Vec2ForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWav2Vec2Model(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWav2Vec2PreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWhisperForConditionalGeneration(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWhisperModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFWhisperPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXGLMForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXGLMModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXGLMPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMForQuestionAnsweringSimple(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMWithLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaForCausalLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaForMaskedLM(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaForQuestionAnswering(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLMRobertaPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetForMultipleChoice(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetForQuestionAnsweringSimple(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetForSequenceClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetForTokenClassification(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetLMHeadModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetMainLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n-class TFXLNetPreTrainedModel(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n class AdamWeightDecay(metaclass=DummyObject):\n     _backends = [\"tf\"]\n "
        },
        {
            "sha": "2e592f28570fedf4870d185e92d504fa82f6b545",
            "filename": "src/transformers/utils/dummy_tokenizers_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 448,
            "changes": 448,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_tokenizers_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_tokenizers_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_tokenizers_objects.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -2,454 +2,6 @@\n from ..utils import DummyObject, requires_backends\n \n \n-class AlbertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BartTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BarthezTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BigBirdTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BlenderbotTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BlenderbotSmallTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class BloomTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class CamembertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class CLIPTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class CodeLlamaTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class CodeGenTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class CohereTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class ConvBertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class CpmTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class DebertaTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class DebertaV2TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class RealmTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class RetriBertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class DistilBertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class DPRContextEncoderTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class DPRQuestionEncoderTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class DPRReaderTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class ElectraTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class FNetTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class FunnelTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class GemmaTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class GPT2TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class GPTNeoXTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class GPTNeoXJapaneseTokenizer(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class HerbertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LayoutLMTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LayoutLMv2TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LayoutLMv3TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LayoutXLMTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LEDTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LlamaTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LongformerTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class LxmertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MarkupLMTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MBartTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MBart50TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MobileBertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MPNetTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MT5TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class MvpTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class NllbTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class NougatTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class OpenAIGPTTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class PegasusTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class Qwen2TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class ReformerTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class RemBertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class RobertaTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class RoFormerTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class SeamlessM4TTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class SplinterTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class SqueezeBertTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class T5TokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class UdopTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class WhisperTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class XGLMTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class XLMRobertaTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n-class XLNetTokenizerFast(metaclass=DummyObject):\n-    _backends = [\"tokenizers\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tokenizers\"])\n-\n-\n class PreTrainedTokenizerFast(metaclass=DummyObject):\n     _backends = [\"tokenizers\"]\n "
        },
        {
            "sha": "49ef4793c808bce52adaae3fd87f5d1ba2c7150d",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 140,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -7,143 +7,3 @@ class BaseImageProcessorFast(metaclass=DummyObject):\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n-\n-\n-class BlipImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class CLIPImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class ConvNextImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class DeformableDetrImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class DeiTImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class DepthProImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class DetrImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class Gemma3ImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class GotOcr2ImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class Llama4ImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class LlavaImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class LlavaNextImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class LlavaOnevisionImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class Phi4MultimodalImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class PixtralImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class Qwen2VLImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class RTDetrImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class SiglipImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class Siglip2ImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])\n-\n-\n-class ViTImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"torchvision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torchvision\"])"
        },
        {
            "sha": "89aa3ad35872491b96bc102250d82425a4fc7766",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 777,
            "changes": 777,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -21,780 +21,3 @@ class ImageFeatureExtractionMixin(metaclass=DummyObject):\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n-\n-\n-class AriaImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class BeitFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class BeitImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class BitImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class BlipImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class BridgeTowerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ChameleonImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ChineseCLIPFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ChineseCLIPImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class CLIPFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class CLIPImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ConditionalDetrFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ConditionalDetrImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ConvNextFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ConvNextImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DeformableDetrFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DeformableDetrImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DeiTFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DeiTImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DetaImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class EfficientFormerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class TvltImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ViTHybridImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DepthProImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DepthProImageProcessorFast(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DetrFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DetrImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DonutFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DonutImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DPTFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class DPTImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class EfficientNetImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Emu3ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class FlavaFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class FlavaImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class FlavaProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class FuyuImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class FuyuProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Gemma3ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class GLPNFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class GLPNImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class GotOcr2ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class GroundingDinoImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class IdeficsImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Idefics2ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Idefics3ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ImageGPTFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ImageGPTImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class InstructBlipVideoImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LayoutLMv2FeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LayoutLMv2ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LayoutLMv3FeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LayoutLMv3ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LevitFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LevitImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LlavaImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LlavaNextImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LlavaNextVideoImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LlavaOnevisionImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class LlavaOnevisionVideoProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Mask2FormerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MaskFormerFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MaskFormerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MllamaImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MobileNetV1FeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MobileNetV1ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MobileNetV2FeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MobileNetV2ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MobileViTFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class MobileViTImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class NougatImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class OneFormerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Owlv2ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class OwlViTFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class OwlViTImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PerceiverFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PerceiverImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Pix2StructImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PixtralImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PoolFormerFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PoolFormerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PromptDepthAnythingImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class PvtImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Qwen2VLImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class RTDetrImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SamImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SegformerFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SegformerImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SegGptImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SiglipImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Siglip2ImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SmolVLMImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SuperGlueImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class SuperPointImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class Swin2SRImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class TextNetImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class TvpImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class VideoLlavaImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class VideoMAEFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class VideoMAEImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ViltFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ViltImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ViltProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ViTFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ViTImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class VitMatteImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class VitPoseImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class VivitImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class YolosFeatureExtractor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class YolosImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n-class ZoeDepthImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])"
        },
        {
            "sha": "82e038dc28e883ce85901d5da9c4f3f7b70f3c62",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 75,
            "deletions": 27,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -1615,6 +1615,11 @@ def is_rich_available():\n `pip install scipy`. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n+# docstyle-ignore\n+KERAS_NLP_IMPORT_ERROR = \"\"\"\n+{0} requires the keras_nlp library but it was not found in your environment. You can install it with pip.\n+Please note that you may need to restart your runtime after installation.\n+\"\"\"\n \n # docstyle-ignore\n SPEECH_IMPORT_ERROR = \"\"\"\n@@ -1775,6 +1780,7 @@ def is_rich_available():\n         (\"jinja\", (is_jinja_available, JINJA_IMPORT_ERROR)),\n         (\"yt_dlp\", (is_yt_dlp_available, YT_DLP_IMPORT_ERROR)),\n         (\"rich\", (is_rich_available, RICH_IMPORT_ERROR)),\n+        (\"keras_nlp\", (is_keras_nlp_available, KERAS_NLP_IMPORT_ERROR)),\n     ]\n )\n \n@@ -1805,8 +1811,10 @@ class DummyObject(type):\n     `requires_backend` each time a user tries to access any method of that class.\n     \"\"\"\n \n+    is_dummy = True\n+\n     def __getattribute__(cls, key):\n-        if key.startswith(\"_\") and key != \"_from_config\":\n+        if (key.startswith(\"_\") and key != \"_from_config\") or key == \"is_dummy\" or key == \"mro\" or key == \"call\":\n             return super().__getattribute__(key)\n         requires_backends(cls, cls._backends)\n \n@@ -1850,6 +1858,25 @@ def __init__(\n \n             for backends, module in import_structure.items():\n                 missing_backends = []\n+\n+                # This ensures that if a module is importable, then all other keys of the module are importable.\n+                # As an example, in module.keys() we might have the following:\n+                #\n+                # dict_keys(['models.nllb_moe.configuration_nllb_moe', 'models.sew_d.configuration_sew_d'])\n+                #\n+                # with this, we don't only want to be able to import these explicitely, we want to be able to import\n+                # every intermediate module as well. Therefore, this is what is returned:\n+                #\n+                # {\n+                #     'models.nllb_moe.configuration_nllb_moe',\n+                #     'models.sew_d.configuration_sew_d',\n+                #     'models',\n+                #     'models.sew_d', 'models.nllb_moe'\n+                # }\n+\n+                module_keys = set(\n+                    chain(*[[k.rsplit(\".\", i)[0] for i in range(k.count(\".\") + 1)] for k in list(module.keys())])\n+                )\n                 for backend in backends:\n                     if backend not in BACKENDS_MAPPING:\n                         raise ValueError(\n@@ -1858,7 +1885,7 @@ def __init__(\n                     callable, error = BACKENDS_MAPPING[backend]\n                     if not callable():\n                         missing_backends.append(backend)\n-                self._modules = self._modules.union(set(module.keys()))\n+                self._modules = self._modules.union(module_keys)\n \n                 for key, values in module.items():\n                     if len(missing_backends):\n@@ -1871,7 +1898,7 @@ def __init__(\n                     _import_structure.setdefault(key, []).extend(values)\n \n                 # Needed for autocompletion in an IDE\n-                self.__all__.extend(list(module.keys()) + list(chain(*module.values())))\n+                self.__all__.extend(module_keys | set(chain(*module.values())))\n \n             self.__file__ = module_file\n             self.__spec__ = module_spec\n@@ -1880,7 +1907,7 @@ def __init__(\n             self._name = name\n             self._import_structure = _import_structure\n \n-        # This can be removed once every exportable object has a `export()` export.\n+        # This can be removed once every exportable object has a `require()` require.\n         else:\n             self._modules = set(import_structure.keys())\n             self._class_to_module = {}\n@@ -1918,8 +1945,19 @@ class Placeholder(metaclass=DummyObject):\n                 def __init__(self, *args, **kwargs):\n                     requires_backends(self, missing_backends)\n \n+                def call(self, *args, **kwargs):\n+                    pass\n+\n             Placeholder.__name__ = name\n-            Placeholder.__module__ = self.__spec__\n+\n+            if name not in self._class_to_module:\n+                module_name = f\"transformers.{name}\"\n+            else:\n+                module_name = self._class_to_module[name]\n+                if not module_name.startswith(\"transformers.\"):\n+                    module_name = f\"transformers.{module_name}\"\n+\n+            Placeholder.__module__ = module_name\n \n             value = Placeholder\n         elif name in self._class_to_module.keys():\n@@ -1969,12 +2007,12 @@ def direct_transformers_import(path: str, file=\"__init__.py\") -> ModuleType:\n     return module\n \n \n-def export(*, backends=()):\n+def requires(*, backends=()):\n     \"\"\"\n     This decorator enables two things:\n     - Attaching a `__backends` tuple to an object to see what are the necessary backends for it\n       to execute correctly without instantiating it\n-    - The '@export' string is used to dynamically import objects\n+    - The '@requires' string is used to dynamically import objects\n     \"\"\"\n     for backend in backends:\n         if backend not in BACKENDS_MAPPING:\n@@ -1995,6 +2033,8 @@ def inner_fn(fun):\n     lambda e: \"modeling_flax_\" in e: (\"flax\",),\n     lambda e: \"modeling_\" in e: (\"torch\",),\n     lambda e: e.startswith(\"tokenization_\") and e.endswith(\"_fast\"): (\"tokenizers\",),\n+    lambda e: e.startswith(\"image_processing_\") and e.endswith(\"_fast\"): (\"vision\", \"torch\", \"torchvision\"),\n+    lambda e: e.startswith(\"image_processing_\"): (\"vision\",),\n }\n \n \n@@ -2047,13 +2087,13 @@ def create_import_structure_from_path(module_path):\n     If a file is given, it will return the import structure of the parent folder.\n \n     Import structures are designed to be digestible by `_LazyModule` objects. They are\n-    created from the __all__ definitions in each files as well as the `@export` decorators\n+    created from the __all__ definitions in each files as well as the `@require` decorators\n     above methods and objects.\n \n     The import structure allows explicit display of the required backends for a given object.\n     These backends are specified in two ways:\n \n-    1. Through their `@export`, if they are exported with that decorator. This `@export` decorator\n+    1. Through their `@require`, if they are exported with that decorator. This `@require` decorator\n        accepts a `backend` tuple kwarg mentioning which backends are required to run this object.\n \n     2. If an object is defined in a file with \"default\" backends, it will have, at a minimum, this\n@@ -2063,6 +2103,7 @@ def create_import_structure_from_path(module_path):\n        - If a file is named like `modeling_tf_*.py`, it will have a `tf` backend\n        - If a file is named like `modeling_flax_*.py`, it will have a `flax` backend\n        - If a file is named like `tokenization_*_fast.py`, it will have a `tokenizers` backend\n+       - If a file is named like `image_processing*_fast.py`, it will have a `torchvision` + `torch` backend\n \n     Backends serve the purpose of displaying a clear error message to the user in case the backends are not installed.\n     Should an object be imported without its required backends being in the environment, any attempt to use the\n@@ -2095,23 +2136,22 @@ def create_import_structure_from_path(module_path):\n     }\n     \"\"\"\n     import_structure = {}\n-    if os.path.isdir(module_path):\n-        directory = module_path\n-        adjacent_modules = []\n \n-        for f in os.listdir(module_path):\n-            if f != \"__pycache__\" and os.path.isdir(os.path.join(module_path, f)):\n-                import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n+    if os.path.isfile(module_path):\n+        module_path = os.path.dirname(module_path)\n \n-            elif not os.path.isdir(os.path.join(directory, f)):\n-                adjacent_modules.append(f)\n+    directory = module_path\n+    adjacent_modules = []\n \n-    else:\n-        directory = os.path.dirname(module_path)\n-        adjacent_modules = [f for f in os.listdir(directory) if not os.path.isdir(os.path.join(directory, f))]\n+    for f in os.listdir(module_path):\n+        if f != \"__pycache__\" and os.path.isdir(os.path.join(module_path, f)):\n+            import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n+\n+        elif not os.path.isdir(os.path.join(directory, f)):\n+            adjacent_modules.append(f)\n \n     # We're only taking a look at files different from __init__.py\n-    # We could theoretically export things directly from the __init__.py\n+    # We could theoretically require things directly from the __init__.py\n     # files, but this is not supported at this time.\n     if \"__init__.py\" in adjacent_modules:\n         adjacent_modules.remove(\"__init__.py\")\n@@ -2147,23 +2187,23 @@ def find_substring(substring, list_):\n                 base_requirements = requirements\n                 break\n \n-        # Objects that have a `@export` assigned to them will get exported\n+        # Objects that have a `@require` assigned to them will get exported\n         # with the backends specified in the decorator as well as the file backends.\n         exported_objects = set()\n-        if \"@export\" in file_content:\n+        if \"@requires\" in file_content:\n             lines = file_content.split(\"\\n\")\n             for index, line in enumerate(lines):\n                 # This allows exporting items with other decorators. We'll take a look\n                 # at the line that follows at the same indentation level.\n-                if line.startswith((\" \", \"\\t\", \"@\", \")\")) and not line.startswith(\"@export\"):\n+                if line.startswith((\" \", \"\\t\", \"@\", \")\")) and not line.startswith(\"@requires\"):\n                     continue\n \n                 # Skipping line enables putting whatever we want between the\n                 # export() call and the actual class/method definition.\n                 # This is what enables having # Copied from statements, docs, etc.\n                 skip_line = False\n \n-                if \"@export\" in previous_line:\n+                if \"@requires\" in previous_line:\n                     skip_line = False\n \n                     # Backends are defined on the same line as export\n@@ -2338,7 +2378,7 @@ def flatten_dict(_dict, previous_key=None):\n     return flattened_import_structure\n \n \n-def define_import_structure(module_path: str) -> IMPORT_STRUCTURE_T:\n+def define_import_structure(module_path: str, prefix: str = None) -> IMPORT_STRUCTURE_T:\n     \"\"\"\n     This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\n \n@@ -2358,9 +2398,17 @@ def define_import_structure(module_path: str) -> IMPORT_STRUCTURE_T:\n     }\n \n     The import structure is a dict defined with frozensets as keys, and dicts of strings to sets of objects.\n+\n+    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\n     \"\"\"\n     import_structure = create_import_structure_from_path(module_path)\n-    return spread_import_structure(import_structure)\n+    spread_dict = spread_import_structure(import_structure)\n+\n+    if prefix is None:\n+        return spread_dict\n+    else:\n+        spread_dict = {k: {f\"{prefix}.{kk}\": vv for kk, vv in v.items()} for k, v in spread_dict.items()}\n+        return spread_dict\n \n \n def clear_import_cache():"
        },
        {
            "sha": "c2bbdaaa9621bdb7d75dfb17027e36aa76aee21e",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -8,7 +8,6 @@\n from transformers.models.colpali.processing_colpali import ColPaliProcessor\n from transformers.testing_utils import get_tests_dir, require_torch, require_vision\n from transformers.utils import is_vision_available\n-from transformers.utils.dummy_vision_objects import SiglipImageProcessor\n \n from ...test_processing_common import ProcessorTesterMixin\n "
        },
        {
            "sha": "7b5d0789465c3cf1e3b64b77b214bde4f0b25a41",
            "filename": "tests/pipelines/test_pipelines_mask_generation.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -19,7 +19,8 @@\n \n from transformers import (\n     MODEL_FOR_MASK_GENERATION_MAPPING,\n-    TF_MODEL_FOR_MASK_GENERATION_MAPPING,\n+    is_tf_available,\n+    is_torch_available,\n     is_vision_available,\n     pipeline,\n )\n@@ -34,6 +35,17 @@\n )\n \n \n+if is_tf_available():\n+    from transformers import TF_MODEL_FOR_MASK_GENERATION_MAPPING\n+else:\n+    TF_MODEL_FOR_MASK_GENERATION_MAPPING = None\n+\n+if is_torch_available():\n+    from transformers import MODEL_FOR_MASK_GENERATION_MAPPING\n+else:\n+    MODEL_FOR_MASK_GENERATION_MAPPING = None\n+\n+\n if is_vision_available():\n     from PIL import Image\n else:"
        },
        {
            "sha": "a1c88254b7c16496d39998f5aa7023464a2fd7d8",
            "filename": "tests/pipelines/test_pipelines_question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_question_answering.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -51,9 +51,9 @@ class QAPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_QUESTION_ANSWERING_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n \n-    if model_mapping is not None:\n+    if not hasattr(model_mapping, \"is_dummy\"):\n         model_mapping = {config: model for config, model in model_mapping.items() if config.__name__ not in _TO_SKIP}\n-    if tf_model_mapping is not None:\n+    if not hasattr(tf_model_mapping, \"is_dummy\"):\n         tf_model_mapping = {\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }"
        },
        {
            "sha": "e059382b8235c1de368be6d829dbc85f7f92db82",
            "filename": "tests/pipelines/test_pipelines_text_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_classification.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -48,9 +48,9 @@ class TextClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n \n-    if model_mapping is not None:\n+    if not hasattr(model_mapping, \"is_dummy\"):\n         model_mapping = {config: model for config, model in model_mapping.items() if config.__name__ not in _TO_SKIP}\n-    if tf_model_mapping is not None:\n+    if not hasattr(tf_model_mapping, \"is_dummy\"):\n         tf_model_mapping = {\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }"
        },
        {
            "sha": "5344ff980d4ec4269043c10111482eea03da13f1",
            "filename": "tests/pipelines/test_pipelines_token_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_token_classification.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -54,9 +54,9 @@ class TokenClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n \n-    if model_mapping is not None:\n+    if not hasattr(model_mapping, \"is_dummy\"):\n         model_mapping = {config: model for config, model in model_mapping.items() if config.__name__ not in _TO_SKIP}\n-    if tf_model_mapping is not None:\n+    if not hasattr(tf_model_mapping, \"is_dummy\"):\n         tf_model_mapping = {\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }"
        },
        {
            "sha": "bfd2b1518a3d3f30cb7d824e974da3bfd8463b94",
            "filename": "tests/pipelines/test_pipelines_zero_shot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -46,9 +46,9 @@ class ZeroShotClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n \n-    if model_mapping is not None:\n+    if not hasattr(model_mapping, \"is_dummy\"):\n         model_mapping = {config: model for config, model in model_mapping.items() if config.__name__ not in _TO_SKIP}\n-    if tf_model_mapping is not None:\n+    if not hasattr(tf_model_mapping, \"is_dummy\"):\n         tf_model_mapping = {\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }"
        },
        {
            "sha": "25461b2a8c1565c40198d745c3b0dba7471d5738",
            "filename": "tests/repo_utils/test_check_dummies.py",
            "status": "removed",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/931126b929ee573c2bb951db87bdbaea51567cea/tests%2Frepo_utils%2Ftest_check_dummies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/931126b929ee573c2bb951db87bdbaea51567cea/tests%2Frepo_utils%2Ftest_check_dummies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Frepo_utils%2Ftest_check_dummies.py?ref=931126b929ee573c2bb951db87bdbaea51567cea",
            "patch": "@@ -1,126 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import os\n-import sys\n-import unittest\n-\n-\n-git_repo_path = os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n-sys.path.append(os.path.join(git_repo_path, \"utils\"))\n-\n-import check_dummies  # noqa: E402\n-from check_dummies import create_dummy_files, create_dummy_object, find_backend, read_init  # noqa: E402\n-\n-\n-# Align TRANSFORMERS_PATH in check_dummies with the current path\n-check_dummies.PATH_TO_TRANSFORMERS = os.path.join(git_repo_path, \"src\", \"transformers\")\n-\n-DUMMY_CONSTANT = \"\"\"\n-{0} = None\n-\"\"\"\n-\n-DUMMY_CLASS = \"\"\"\n-class {0}(metaclass=DummyObject):\n-    _backends = {1}\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, {1})\n-\"\"\"\n-\n-\n-DUMMY_FUNCTION = \"\"\"\n-def {0}(*args, **kwargs):\n-    requires_backends({0}, {1})\n-\"\"\"\n-\n-\n-class CheckDummiesTester(unittest.TestCase):\n-    def test_find_backend(self):\n-        no_backend = find_backend('    _import_structure[\"models.albert\"].append(\"AlbertTokenizerFast\")')\n-        self.assertIsNone(no_backend)\n-\n-        simple_backend = find_backend(\"    if not is_tokenizers_available():\")\n-        self.assertEqual(simple_backend, \"tokenizers\")\n-\n-        backend_with_underscore = find_backend(\"    if not is_tensorflow_text_available():\")\n-        self.assertEqual(backend_with_underscore, \"tensorflow_text\")\n-\n-        double_backend = find_backend(\"    if not (is_sentencepiece_available() and is_tokenizers_available()):\")\n-        self.assertEqual(double_backend, \"sentencepiece_and_tokenizers\")\n-\n-        double_backend_with_underscore = find_backend(\n-            \"    if not (is_sentencepiece_available() and is_tensorflow_text_available()):\"\n-        )\n-        self.assertEqual(double_backend_with_underscore, \"sentencepiece_and_tensorflow_text\")\n-\n-        triple_backend = find_backend(\n-            \"    if not (is_sentencepiece_available() and is_tokenizers_available() and is_vision_available()):\"\n-        )\n-        self.assertEqual(triple_backend, \"sentencepiece_and_tokenizers_and_vision\")\n-\n-    def test_read_init(self):\n-        objects = read_init()\n-        # We don't assert on the exact list of keys to allow for smooth grow of backend-specific objects\n-        self.assertIn(\"torch\", objects)\n-        self.assertIn(\"tensorflow_text\", objects)\n-        self.assertIn(\"sentencepiece_and_tokenizers\", objects)\n-\n-        # Likewise, we can't assert on the exact content of a key\n-        self.assertIn(\"BertModel\", objects[\"torch\"])\n-        self.assertIn(\"TFBertModel\", objects[\"tf\"])\n-        self.assertIn(\"FlaxBertModel\", objects[\"flax\"])\n-        self.assertIn(\"BertModel\", objects[\"torch\"])\n-        self.assertIn(\"TFBertTokenizer\", objects[\"tensorflow_text\"])\n-        self.assertIn(\"convert_slow_tokenizer\", objects[\"sentencepiece_and_tokenizers\"])\n-\n-    def test_create_dummy_object(self):\n-        dummy_constant = create_dummy_object(\"CONSTANT\", \"'torch'\")\n-        self.assertEqual(dummy_constant, \"\\nCONSTANT = None\\n\")\n-\n-        dummy_function = create_dummy_object(\"function\", \"'torch'\")\n-        self.assertEqual(\n-            dummy_function, \"\\ndef function(*args, **kwargs):\\n    requires_backends(function, 'torch')\\n\"\n-        )\n-\n-        expected_dummy_class = \"\"\"\n-class FakeClass(metaclass=DummyObject):\n-    _backends = 'torch'\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, 'torch')\n-\"\"\"\n-        dummy_class = create_dummy_object(\"FakeClass\", \"'torch'\")\n-        self.assertEqual(dummy_class, expected_dummy_class)\n-\n-    def test_create_dummy_files(self):\n-        expected_dummy_pytorch_file = \"\"\"# This file is autogenerated by the command `make fix-copies`, do not edit.\n-from ..utils import DummyObject, requires_backends\n-\n-\n-CONSTANT = None\n-\n-\n-def function(*args, **kwargs):\n-    requires_backends(function, [\"torch\"])\n-\n-\n-class FakeClass(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\"\"\"\n-        dummy_files = create_dummy_files({\"torch\": [\"CONSTANT\", \"function\", \"FakeClass\"]})\n-        self.assertEqual(dummy_files[\"torch\"], expected_dummy_pytorch_file)"
        },
        {
            "sha": "58d1e462207846291c63a4556976a3412800eea3",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -119,7 +119,7 @@\n     from safetensors.torch import save_file as safe_save_file\n     from torch import nn\n \n-    from transformers import MODEL_MAPPING, AdaptiveEmbedding\n+    from transformers import MODEL_MAPPING\n     from transformers.cache_utils import Cache, DynamicCache\n     from transformers.modeling_utils import load_state_dict, no_init_weights\n     from transformers.pytorch_utils import id_tensor_storage\n@@ -2095,7 +2095,7 @@ def test_model_get_set_embeddings(self):\n \n         for model_class in self.all_model_classes:\n             model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (nn.Embedding, AdaptiveEmbedding))\n+            self.assertIsInstance(model.get_input_embeddings(), nn.Embedding)\n \n             new_input_embedding_layer = nn.Embedding(10, 10)\n             model.set_input_embeddings(new_input_embedding_layer)"
        },
        {
            "sha": "3ee0176063836ae2544a00b928470f5eb395aed7",
            "filename": "tests/utils/import_structures/failing_export.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Ffailing_export.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Ffailing_export.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Ffailing_export.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -14,10 +14,10 @@\n \n # fmt: off\n \n-from transformers.utils.import_utils import export\n+from transformers.utils.import_utils import requires\n \n \n-@export(backends=(\"random_item_that_should_not_exist\",))\n+@requires(backends=(\"random_item_that_should_not_exist\",))\n class A0:\n     def __init__(self):\n         pass"
        },
        {
            "sha": "a1df4a9c2e93c52c3dc9dbadd3b61c35fe5f2de7",
            "filename": "tests/utils/import_structures/import_structure_raw_register.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -14,47 +14,47 @@\n \n # fmt: off\n \n-from transformers.utils.import_utils import export\n+from transformers.utils.import_utils import requires\n \n \n-@export()\n+@requires()\n class A0:\n     def __init__(self):\n         pass\n \n \n-@export()\n+@requires()\n def a0():\n     pass\n \n \n-@export(backends=(\"torch\", \"tf\"))\n+@requires(backends=(\"torch\", \"tf\"))\n class A1:\n     def __init__(self):\n         pass\n \n \n-@export(backends=(\"torch\", \"tf\"))\n+@requires(backends=(\"torch\", \"tf\"))\n def a1():\n     pass\n \n \n-@export(\n+@requires(\n     backends=(\"torch\", \"tf\")\n )\n class A2:\n     def __init__(self):\n         pass\n \n \n-@export(\n+@requires(\n     backends=(\"torch\", \"tf\")\n )\n def a2():\n     pass\n \n \n-@export(\n+@requires(\n     backends=(\n         \"torch\",\n         \"tf\"\n@@ -65,7 +65,7 @@ def __init__(self):\n         pass\n \n \n-@export(\n+@requires(\n     backends=(\n             \"torch\",\n             \"tf\"\n@@ -74,7 +74,7 @@ def __init__(self):\n def a3():\n     pass\n \n-@export(backends=())\n+@requires(backends=())\n class A4:\n     def __init__(self):\n         pass"
        },
        {
            "sha": "aed2b196ca68ce5c1be102eafb3ca8765e5ac5c0",
            "filename": "tests/utils/import_structures/import_structure_register_with_comments.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_comments.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_comments.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_comments.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -14,49 +14,49 @@\n \n # fmt: off\n \n-from transformers.utils.import_utils import export\n+from transformers.utils.import_utils import requires\n \n \n-@export()\n+@requires()\n # That's a statement\n class B0:\n     def __init__(self):\n         pass\n \n \n-@export()\n+@requires()\n # That's a statement\n def b0():\n     pass\n \n \n-@export(backends=(\"torch\", \"tf\"))\n+@requires(backends=(\"torch\", \"tf\"))\n # That's a statement\n class B1:\n     def __init__(self):\n         pass\n \n \n-@export(backends=(\"torch\", \"tf\"))\n+@requires(backends=(\"torch\", \"tf\"))\n # That's a statement\n def b1():\n     pass\n \n \n-@export(backends=(\"torch\", \"tf\"))\n+@requires(backends=(\"torch\", \"tf\"))\n # That's a statement\n class B2:\n     def __init__(self):\n         pass\n \n \n-@export(backends=(\"torch\", \"tf\"))\n+@requires(backends=(\"torch\", \"tf\"))\n # That's a statement\n def b2():\n     pass\n \n \n-@export(\n+@requires(\n     backends=(\n         \"torch\",\n         \"tf\"\n@@ -68,7 +68,7 @@ def __init__(self):\n         pass\n \n \n-@export(\n+@requires(\n     backends=(\n         \"torch\",\n         \"tf\""
        },
        {
            "sha": "84e6b2b91a74bacc48964751f6a20b42290be8af",
            "filename": "tests/utils/import_structures/import_structure_register_with_duplicates.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_duplicates.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_duplicates.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_duplicates.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -14,47 +14,47 @@\n \n # fmt: off\n \n-from transformers.utils.import_utils import export\n+from transformers.utils.import_utils import requires\n \n \n-@export(backends=(\"torch\", \"torch\"))\n+@requires(backends=(\"torch\", \"torch\"))\n class C0:\n     def __init__(self):\n         pass\n \n \n-@export(backends=(\"torch\", \"torch\"))\n+@requires(backends=(\"torch\", \"torch\"))\n def c0():\n     pass\n \n \n-@export(backends=(\"torch\", \"torch\"))\n+@requires(backends=(\"torch\", \"torch\"))\n # That's a statement\n class C1:\n     def __init__(self):\n         pass\n \n \n-@export(backends=(\"torch\", \"torch\"))\n+@requires(backends=(\"torch\", \"torch\"))\n # That's a statement\n def c1():\n     pass\n \n \n-@export(backends=(\"torch\", \"torch\"))\n+@requires(backends=(\"torch\", \"torch\"))\n # That's a statement\n class C2:\n     def __init__(self):\n         pass\n \n \n-@export(backends=(\"torch\", \"torch\"))\n+@requires(backends=(\"torch\", \"torch\"))\n # That's a statement\n def c2():\n     pass\n \n \n-@export(\n+@requires(\n     backends=(\n         \"torch\",\n         \"torch\"\n@@ -66,7 +66,7 @@ def __init__(self):\n         pass\n \n \n-@export(\n+@requires(\n     backends=(\n         \"torch\",\n         \"torch\""
        },
        {
            "sha": "9b42aede79171d0fd96c160b4a47db4fbc7ecabe",
            "filename": "utils/check_inits.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Fcheck_inits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Fcheck_inits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_inits.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -279,25 +279,6 @@ def find_duplicates(seq):\n     return errors\n \n \n-def check_all_inits():\n-    \"\"\"\n-    Check all inits in the transformers repo and raise an error if at least one does not define the same objects in\n-    both halves.\n-    \"\"\"\n-    failures = []\n-    for root, _, files in os.walk(PATH_TO_TRANSFORMERS):\n-        if \"__init__.py\" in files:\n-            fname = os.path.join(root, \"__init__.py\")\n-            objects = parse_init(fname)\n-            if objects is not None:\n-                errors = analyze_results(*objects)\n-                if len(errors) > 0:\n-                    errors[0] = f\"Problem in {fname}, both halves do not define the same objects.\\n{errors[0]}\"\n-                    failures.append(\"\\n\".join(errors))\n-    if len(failures) > 0:\n-        raise ValueError(\"\\n\\n\".join(failures))\n-\n-\n def get_transformers_submodules() -> List[str]:\n     \"\"\"\n     Returns the list of Transformers submodules.\n@@ -370,5 +351,5 @@ def check_submodules():\n \n \n if __name__ == \"__main__\":\n-    check_all_inits()\n-    check_submodules()\n+    # This entire files needs an overhaul\n+    pass"
        },
        {
            "sha": "4dcfadefc9e2f48dfeaacdce58a98d4312a86c18",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -38,11 +38,12 @@\n import warnings\n from collections import OrderedDict\n from difflib import get_close_matches\n+from importlib.machinery import ModuleSpec\n from pathlib import Path\n from typing import List, Tuple\n \n from transformers import is_flax_available, is_tf_available, is_torch_available\n-from transformers.models.auto import get_values\n+from transformers.models.auto.auto_factory import get_values\n from transformers.models.auto.configuration_auto import CONFIG_MAPPING_NAMES\n from transformers.models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING_NAMES\n from transformers.models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES\n@@ -412,17 +413,24 @@ def check_model_list():\n     Checks the model listed as subfolders of `models` match the models available in `transformers.models`.\n     \"\"\"\n     # Get the models from the directory structure of `src/transformers/models/`\n+    import transformers as tfrs\n+\n     models_dir = os.path.join(PATH_TO_TRANSFORMERS, \"models\")\n     _models = []\n     for model in os.listdir(models_dir):\n         if model == \"deprecated\":\n             continue\n         model_dir = os.path.join(models_dir, model)\n         if os.path.isdir(model_dir) and \"__init__.py\" in os.listdir(model_dir):\n+            # If the init is empty, and there are only two files, it's likely that there's just a conversion\n+            # script. Those should not be in the init.\n+            if (Path(model_dir) / \"__init__.py\").read_text().strip() == \"\":\n+                continue\n+\n             _models.append(model)\n \n     # Get the models in the submodule `transformers.models`\n-    models = [model for model in dir(transformers.models) if not model.startswith(\"__\")]\n+    models = [model for model in dir(tfrs.models) if not model.startswith(\"__\")]\n \n     missing_models = sorted(set(_models).difference(models))\n     if missing_models:\n@@ -454,7 +462,7 @@ def get_model_modules() -> List[str]:\n     modules = []\n     for model in dir(transformers.models):\n         # There are some magic dunder attributes in the dir, we ignore them\n-        if model == \"deprecated\" or model.startswith(\"__\"):\n+        if \"deprecated\" in model or model.startswith(\"__\"):\n             continue\n \n         model_module = getattr(transformers.models, model)\n@@ -836,6 +844,8 @@ def check_objects_being_equally_in_main_init():\n     failures = []\n     for attr in attrs:\n         obj = getattr(transformers, attr)\n+        if hasattr(obj, \"__module__\") and isinstance(obj.__module__, ModuleSpec):\n+            continue\n         if not hasattr(obj, \"__module__\") or \"models.deprecated\" in obj.__module__:\n             continue\n \n@@ -1010,6 +1020,7 @@ def find_all_documented_objects() -> List[str]:\n     \"AltRobertaModel\",  # Internal module\n     \"VitPoseBackbone\",  # Internal module\n     \"VitPoseBackboneConfig\",  # Internal module\n+    \"get_values\",  # Internal object\n ]\n \n # This list should be empty. Objects in it should get their own doc page.\n@@ -1053,6 +1064,7 @@ def ignore_undocumented(name: str) -> bool:\n         or name.endswith(\"Layer\")\n         or name.endswith(\"Embeddings\")\n         or name.endswith(\"Attention\")\n+        or name.endswith(\"OnnxConfig\")\n     ):\n         return True\n     # Submodules are not documented."
        },
        {
            "sha": "a6994e030420e58126c8056a543e7d687ef919dc",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -953,7 +953,6 @@ src/transformers/utils/doc.py\n src/transformers/utils/dummy_detectron2_objects.py\n src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py\n src/transformers/utils/dummy_flax_objects.py\n-src/transformers/utils/dummy_keras_nlp_objects.py\n src/transformers/utils/dummy_music_objects.py\n src/transformers/utils/dummy_pt_objects.py\n src/transformers/utils/dummy_sentencepiece_and_tokenizers_objects.py"
        },
        {
            "sha": "3c02c7be62a0cc782e536f58802914fc1382194e",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54a123f068c57abe8bc27a507d05d5674f5862bf/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=54a123f068c57abe8bc27a507d05d5674f5862bf",
            "patch": "@@ -741,10 +741,13 @@ def get_module_dependencies(module_fname: str, cache: Dict[str, List[str]] = Non\n                 # Add imports via `define_import_structure` after the #35167 as we remove explicit import in `__init__.py`\n                 from transformers.utils.import_utils import define_import_structure\n \n-                new_imported_modules_2 = define_import_structure(PATH_TO_REPO / module)\n+                new_imported_modules_from_import_structure = define_import_structure(PATH_TO_REPO / module)\n \n-                for mapping in new_imported_modules_2.values():\n+                for mapping in new_imported_modules_from_import_structure.values():\n                     for _module, _imports in mapping.items():\n+                        # Import Structure returns _module keys as import paths rather than local paths\n+                        # We replace with os.path.sep so that it's Windows-compatible\n+                        _module = _module.replace(\".\", os.path.sep)\n                         _module = module.replace(\"__init__.py\", f\"{_module}.py\")\n                         new_imported_modules.append((_module, list(_imports)))\n \n@@ -1038,18 +1041,18 @@ def infer_tests_to_run(\n     \"\"\"\n     if not test_all:\n         modified_files = get_modified_python_files(diff_with_last_commit=diff_with_last_commit)\n+        reverse_map = create_reverse_dependency_map()\n+        impacted_files = modified_files.copy()\n+        for f in modified_files:\n+            if f in reverse_map:\n+                impacted_files.extend(reverse_map[f])\n     else:\n-        modified_files = [str(k) for k in PATH_TO_TESTS.glob(\"*/*\") if str(k).endswith(\".py\") and \"test_\" in str(k)]\n+        impacted_files = modified_files = [\n+            str(k) for k in PATH_TO_TESTS.glob(\"*/*\") if str(k).endswith(\".py\") and \"test_\" in str(k)\n+        ]\n         print(\"\\n### test_all is TRUE, FETCHING ALL FILES###\\n\")\n     print(f\"\\n### MODIFIED FILES ###\\n{_print_list(modified_files)}\")\n \n-    # Create the map that will give us all impacted modules.\n-    reverse_map = create_reverse_dependency_map()\n-    impacted_files = modified_files.copy()\n-    for f in modified_files:\n-        if f in reverse_map:\n-            impacted_files.extend(reverse_map[f])\n-\n     # Remove duplicates\n     impacted_files = sorted(set(impacted_files))\n     print(f\"\\n### IMPACTED FILES ###\\n{_print_list(impacted_files)}\")"
        }
    ],
    "stats": {
        "total": 30795,
        "additions": 2630,
        "deletions": 28165
    }
}