{
    "author": "jeongin601",
    "message": "Fix: Enable prefill phase key value caching of nemotron/minitron models (#34742)\n\n* modeling nemotron kv caching bugfix\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\n\r\n* test file deleted\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\n\r\n* code refinement\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\n\r\n* remove unused variables\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\n\r\n* import block sorted\r\n\r\n* removed deprecation warning\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\n\r\n* removed support for tuple shape past_key_values\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\n\r\n* Update conditional statement for cache initialization\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n---------\r\n\r\nSigned-off-by: jeongin601 <0200angela@gmail.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "318fe25f22a99ce1226f8d2aadc268b40f7e55af",
    "files": [
        {
            "sha": "1c56ecd56f54ae6121a5ed330b79f7160e27a785",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/318fe25f22a99ce1226f8d2aadc268b40f7e55af/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/318fe25f22a99ce1226f8d2aadc268b40f7e55af/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=318fe25f22a99ce1226f8d2aadc268b40f7e55af",
            "patch": "@@ -24,7 +24,7 @@\n from torch import Size, Tensor, nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n@@ -783,8 +783,14 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n         if cache_position is None:\n-            cache_position = torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device)\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 8,
        "deletions": 2
    }
}