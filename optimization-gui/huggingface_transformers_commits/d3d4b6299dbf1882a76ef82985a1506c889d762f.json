{
    "author": "MekkCyber",
    "message": "[Quantization] rm _pre_quantization_dtype from quantization tests (#42939)\n\nrm",
    "sha": "d3d4b6299dbf1882a76ef82985a1506c889d762f",
    "files": [
        {
            "sha": "fb345ddfc81301152741d7c46476ee84dc312be2",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=d3d4b6299dbf1882a76ef82985a1506c889d762f",
            "patch": "@@ -181,14 +181,6 @@ def test_memory_footprint(self):\n         linear = get_some_linear_layer(self.model_4bit)\n         self.assertTrue(linear.weight.__class__ == Params4bit)\n \n-    def test_original_dtype(self):\n-        r\"\"\"\n-        A simple test to check if the model successfully stores the original dtype\n-        \"\"\"\n-        self.assertTrue(hasattr(self.model_4bit.config, \"_pre_quantization_dtype\"))\n-        self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n-        self.assertTrue(self.model_4bit.config._pre_quantization_dtype == torch.float16)\n-\n     def test_linear_are_4bit(self):\n         r\"\"\"\n         A simple test to check if the model conversion has been done correctly by checking on the\n@@ -266,7 +258,6 @@ def test_clear_quantization_trace(self):\n \n         self.assertFalse(hasattr(model_4bit, \"hf_quantizer\"))\n         self.assertFalse(hasattr(model_4bit.config, \"quantization_config\"))\n-        self.assertFalse(hasattr(model_4bit.config, \"_pre_quantization_dtype\"))\n         self.assertFalse(hasattr(model_4bit, \"quantization_method\"))\n         self.assertFalse(model_4bit.is_quantized)\n "
        },
        {
            "sha": "0d3235e0cc8b1d8b387f3bb7d985153d6f17875d",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=d3d4b6299dbf1882a76ef82985a1506c889d762f",
            "patch": "@@ -197,14 +197,6 @@ def test_quantization_config_json_serialization(self):\n \n         _ = config.to_json_string()\n \n-    def test_original_dtype(self):\n-        r\"\"\"\n-        A simple test to check if the model successfully stores the original dtype\n-        \"\"\"\n-        self.assertTrue(hasattr(self.model_8bit.config, \"_pre_quantization_dtype\"))\n-        self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n-        self.assertTrue(self.model_8bit.config._pre_quantization_dtype == torch.float16)\n-\n     def test_memory_footprint(self):\n         r\"\"\"\n         A simple test to check if the model conversion has been done correctly by checking on the"
        },
        {
            "sha": "8069c8216db3ef4c565fe1dc01b8fe5e4f9a76ad",
            "filename": "tests/quantization/gptq/test_gptq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fgptq%2Ftest_gptq.py?ref=d3d4b6299dbf1882a76ef82985a1506c889d762f",
            "patch": "@@ -174,14 +174,6 @@ def test_device_and_dtype_assignment(self):\n             # Tries with a `dtype``\n             self.quantized_model.to(torch.float16)\n \n-    def test_original_dtype(self):\n-        r\"\"\"\n-        A simple test to check if the model successfully stores the original dtype\n-        \"\"\"\n-        self.assertTrue(hasattr(self.quantized_model.config, \"_pre_quantization_dtype\"))\n-        self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n-        self.assertEqual(self.quantized_model.config._pre_quantization_dtype, torch.float16)\n-\n     def test_quantized_layers_class(self):\n         \"\"\"\n         Simple test to check if the model conversion has been done correctly by checking on"
        },
        {
            "sha": "9cad56a7cfff9e2a52895c9e2d7dda42d3fabf85",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d4b6299dbf1882a76ef82985a1506c889d762f/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=d3d4b6299dbf1882a76ef82985a1506c889d762f",
            "patch": "@@ -105,14 +105,10 @@ def test_device_and_dtype_assignment(self):\n             # Tries with a `dtype``\n             self.quantized_model.to(torch.float16)\n \n-    def test_original_dtype(self):\n+    def test_quantized_layers_class(self):\n         r\"\"\"\n-        A simple test to check if the model successfully stores the original dtype\n+        A simple test to check if the model successfully changes the class type of the linear layers\n         \"\"\"\n-        self.assertTrue(hasattr(self.quantized_model.config, \"_pre_quantization_dtype\"))\n-        self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n-        self.assertTrue(self.quantized_model.config._pre_quantization_dtype == torch.float16)\n-\n         self.assertTrue(isinstance(self.quantized_model.model.layers[0].mlp.gate_proj, QParamsLinear))\n \n     def check_inference_correctness(self, model):"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 2,
        "deletions": 31
    }
}