{
    "author": "ArthurZucker",
    "message": "[`Mxfp4`] Add a way to save with a quantization method (#40176)\n\n* add a test\n\n* tempdir\n\n* fix import issue[\n\n* wow I am tired\n\n* properly init\n\n* i am not super familiar with quantizer api :|\n\n* set to TRUE fro now\n\n* full support\n\n* push current changes\n\n* will clean this later but the imports are a shitshow here\n\n* this correctly saves the block and scales but forward seems broken\n\n* quanitze was not correct\n\n* fix storage\n\n* why were bias even included\n\n* finally!\n\n* style\n\n* fix style\n\n* remove print\n\n* lazy import\n\n* up\n\n* not sure what happens this works now?\n\n* holy molly it was not so far\n\n* okay this seems to work!\n\n* workings!!!\n\n* allow save_pretrained to create PR\n\n* Apply suggestions from code review\n\n* fixup\n\n* add deqyabtze fakse as wek\n\n* working new\n\n* fix\n\n* rm swizzle and unswizzle during saving\n\n* rm print\n\n* Update src/transformers/modeling_utils.py\n\n* fix\n\n* style\n\n---------\n\nCo-authored-by: Marc Sun <marc@huggingface.co>",
    "sha": "6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
    "files": [
        {
            "sha": "58ca68bb332649a0a5557d54621005fe5eb9ea3f",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -126,6 +126,7 @@\n         \"load_and_swizzle_mxfp4\",\n         \"quantize_to_mxfp4\",\n         \"replace_with_mxfp4_linear\",\n+        \"swizzle_mxfp4\",\n     ],\n     \"peft\": [\"PeftAdapterMixin\"],\n     \"quanto\": [\"replace_with_quanto_layers\"],\n@@ -269,6 +270,7 @@\n         load_and_swizzle_mxfp4,\n         quantize_to_mxfp4,\n         replace_with_mxfp4_linear,\n+        swizzle_mxfp4,\n     )\n     from .peft import PeftAdapterMixin\n     from .quanto import replace_with_quanto_layers"
        },
        {
            "sha": "c40b202c54e83231c6ad1b96c8c0945e9706710d",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 67,
            "deletions": 86,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -48,15 +48,16 @@\n \n \n # Copied from GPT_OSS repo and vllm\n-def quantize_to_mxfp4(w):\n-    downcast_to_mxfp = triton_kernels_hub.numerics_details.mxfp.downcast_to_mxfp\n-\n-    w, w_scale = downcast_to_mxfp(w.to(torch.bfloat16), torch.uint8, axis=1)\n-    w, w_scale = swizzle_mxfp4(w, w_scale)\n+def quantize_to_mxfp4(w, triton_kernels_hub):\n+    downcast_to_mxfp_torch = triton_kernels_hub.numerics_details.mxfp.downcast_to_mxfp_torch\n+    w, w_scale = downcast_to_mxfp_torch(w.to(torch.bfloat16), torch.uint8, axis=1)\n     return w, w_scale\n \n \n-def swizzle_mxfp4(w, w_scale):\n+def swizzle_mxfp4(w, w_scale, triton_kernels_hub):\n+    \"\"\"\n+    Changes the layout of the tensors depending on the hardware\n+    \"\"\"\n     FP4, convert_layout, wrap_torch_tensor = (\n         triton_kernels_hub.tensor.FP4,\n         triton_kernels_hub.tensor.convert_layout,\n@@ -67,18 +68,6 @@ def swizzle_mxfp4(w, w_scale):\n \n     value_layout, value_layout_opts = layout.make_default_matmul_mxfp4_w_layout(mx_axis=1)\n     w = convert_layout(wrap_torch_tensor(w, dtype=FP4), value_layout, **value_layout_opts)\n-    # TODO : add that when we are actually sure that it works on B200\n-    # if torch.cuda.get_device_capability()[0] == 10:\n-    #     constraints = {\n-    #         \"is_persistent\": True,\n-    #         \"epilogue_subtile\": 1,\n-    #     }\n-    #     opt_flags.update_opt_flags_constraints(constraints)\n-    # # transpose the tensor so that the quantization axis is on dim1\n-\n-    # TODO: there is still an issue with the scales on hopper\n-    # scale_layout, scale_layout_opts = layout.make_default_matmul_mxfp4_w_scale_layout(mx_axis=1, num_warps=8)\n-    # w_scale = convert_layout(wrap_torch_tensor(w_scale), scale_layout, **scale_layout_opts)\n     w_scale = convert_layout(wrap_torch_tensor(w_scale), StridedLayout)\n     return w, w_scale\n \n@@ -90,18 +79,22 @@ def convert_moe_packed_tensors(\n     scales,\n     *,\n     dtype: torch.dtype = torch.bfloat16,\n-    rows_per_chunk: int = 32768 * 1024,\n+    rows_per_chunk: int = 32768 * 1024,  # TODO these values are not here by mistake ;)\n ) -> torch.Tensor:\n+    \"\"\"\n+    Convert the mxfp4 weights again, dequantizing and makes them compatible with the forward\n+    pass of GPT_OSS.\n+    \"\"\"\n     import math\n \n     # Check if blocks and scales are on CPU, and move to GPU if so\n     if not blocks.is_cuda and torch.cuda.is_available():\n         blocks = blocks.cuda()\n         scales = scales.cuda()\n \n-    scales = scales.to(torch.int32) - 127\n+    scales = scales.to(torch.int32) - 127  # TODO that's because 128=2**7\n \n-    assert blocks.shape[:-1] == scales.shape, f\"{blocks.shape=} does not match {scales.shape=}\"\n+    assert blocks.shape[:-1] == scales.shape, f\"{blocks.shape[:-1]=} does not match {scales.shape=}\"\n \n     lut = torch.tensor(FP4_VALUES, dtype=dtype, device=blocks.device)\n \n@@ -131,13 +124,8 @@ def convert_moe_packed_tensors(\n         del idx_lo, idx_hi, blk, exp, sub\n \n     out = out.reshape(*prefix_shape, G, B * 2).view(*prefix_shape, G * B * 2)\n-\n-    # TODO: Delete after making sure this is not necessary! since we go back to cpu in the end in create_quantized_param using .to(target_device)\n-    # Move back to CPU if needed\n-    # if need_to_move_back:\n-    #     out = out.cpu()\n     del blocks, scales, lut\n-    return out\n+    return out.transpose(1, 2).contiguous()\n \n \n class Mxfp4GptOssExperts(nn.Module):\n@@ -175,6 +163,7 @@ def __init__(self, config):\n         self.limit = getattr(config, \"swiglu_limit\", 7.0)\n         self.gate_up_proj_precision_config = None\n         self.down_proj_precision_config = None\n+        self.limit = getattr(config, \"swiglu_limit\", 7.0)\n \n     def forward(self, hidden_states: torch.Tensor, routing_data, gather_idx, scatter_idx) -> torch.Tensor:\n         FnSpecs, FusedActivation, matmul_ogs = (\n@@ -207,7 +196,6 @@ def forward(self, hidden_states: torch.Tensor, routing_data, gather_idx, scatter\n                 precision_config=self.down_proj_precision_config,\n                 gammas=routing_data.gate_scal,\n             )\n-\n         return intermediate_cache3\n \n \n@@ -289,7 +277,6 @@ def mlp_forward(self, hidden_states):\n     else:\n         routing = triton_kernels_hub.routing.routing\n \n-        routing = routing\n     batch_size = hidden_states.shape[0]\n     hidden_states = hidden_states.reshape(-1, self.router.hidden_dim)\n     router_logits = nn.functional.linear(hidden_states, self.router.weight, self.router.bias)\n@@ -340,16 +327,17 @@ def dequantize(module, param_name, param_value, target_device, dq_param_name, **\n             setattr(module, param_name.rsplit(\".\", 1)[1], param_value)\n             if hasattr(module, blocks_attr) and hasattr(module, scales_attr):\n                 dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n-                dequantized = dequantized.transpose(1, 2).contiguous().to(target_device)\n-                # TODO: this is perhaps necessary since if target_device is cpu, and the param was on gpu\n                 if target_device == \"cpu\" and torch.cuda.is_available():\n                     torch.cuda.empty_cache()\n-                setattr(module, proj, torch.nn.Parameter(dequantized))\n+                setattr(module, proj, torch.nn.Parameter(dequantized.to(target_device)))\n                 delattr(module, blocks_attr)\n                 delattr(module, scales_attr)\n \n \n-def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, **kwargs):\n+def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, triton_kernels_hub, **kwargs):\n+    \"\"\"\n+    This transforms the weights obtained using `convert_gpt_oss.py` to load them into `Mxfp4GptOssExperts`.\n+    \"\"\"\n     PrecisionConfig, FlexCtx, InFlexData = (\n         triton_kernels_hub.matmul_ogs.PrecisionConfig,\n         triton_kernels_hub.matmul_ogs.FlexCtx,\n@@ -363,61 +351,54 @@ def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, **kwa\n     to_contiguous = kwargs.get(\"to_contiguous\")\n     rank = kwargs.get(\"rank\")\n     device_mesh = kwargs.get(\"device_mesh\")\n+    if \"blocks\" in param_name:\n+        proj = param_name.split(\".\")[-1].split(\"_blocks\")[0]\n+    if \"scales\" in param_name:\n+        proj = param_name.split(\".\")[-1].split(\"_scales\")[0]\n+    if device_mesh is not None:\n+        shard_and_distribute_module(\n+            model, param_value, empty_param, param_name, casting_dtype, to_contiguous, rank, device_mesh\n+        )\n+    else:\n+        setattr(module, param_name.rsplit(\".\", 1)[1], torch.nn.Parameter(param_value, requires_grad=False))\n+    blocks_attr = f\"{proj}_blocks\"\n+    scales_attr = f\"{proj}_scales\"\n+    blocks = getattr(module, blocks_attr)  # at this point values were loaded from ckpt\n+    scales = getattr(module, scales_attr)\n+    # Check if both blocks and scales both not on meta device\n+    if blocks.device.type != \"meta\" and scales.device.type != \"meta\":\n+        local_experts = blocks.size(0)\n+        if proj == \"gate_up_proj\":\n+            blocks = blocks.reshape(local_experts, module.intermediate_size * 2, -1)\n+        else:\n+            blocks = blocks.reshape(local_experts, -1, module.intermediate_size // 2)\n+        if getattr(target_device, \"type\", target_device) == \"cpu\":\n+            target_device = \"cuda\"\n+        blocks = blocks.to(target_device).contiguous()\n+        scales = scales.to(target_device).contiguous()\n+        with torch.cuda.device(target_device):\n+            triton_weight_tensor, weight_scale = swizzle_mxfp4(\n+                blocks.transpose(-2, -1), scales.transpose(-2, -1), triton_kernels_hub\n+            )\n \n-    for proj in [\"gate_up_proj\", \"down_proj\"]:\n-        if proj in param_name:\n-            if device_mesh is not None:\n-                shard_and_distribute_module(\n-                    model, param_value, empty_param, param_name, casting_dtype, to_contiguous, rank, device_mesh\n-                )\n-            else:\n-                setattr(module, param_name.rsplit(\".\", 1)[1], torch.nn.Parameter(param_value, requires_grad=False))\n-            blocks_attr = f\"{proj}_blocks\"\n-            scales_attr = f\"{proj}_scales\"\n-            blocks = getattr(module, blocks_attr)\n-            scales = getattr(module, scales_attr)\n-            # Check if both blocks and scales both not on on meta device\n-            if blocks.device.type != \"meta\" and scales.device.type != \"meta\":\n-                # need it for ep\n-                local_experts = blocks.size(0)\n-                if proj == \"gate_up_proj\":\n-                    blocks = blocks.view(local_experts, module.intermediate_size * 2, -1)\n-                else:\n-                    blocks = blocks.view(local_experts, -1, module.intermediate_size // 2)\n-                # TODO: we need to have the weights on cuda, refactor later\n-                if getattr(target_device, \"type\", target_device) == \"cpu\":\n-                    target_device = \"cuda\"\n-                # TODO: check why we still do move the tensors despite the context manager\n-                blocks = blocks.to(target_device)\n-                scales = scales.to(target_device)\n-                with torch.cuda.device(target_device):\n-                    triton_weight_tensor, weight_scale = swizzle_mxfp4(\n-                        blocks.transpose(-2, -1), scales.transpose(-2, -1)\n-                    )\n-\n-                # need to overwrite the shapes for the kernels\n-                if proj == \"gate_up_proj\":\n-                    triton_weight_tensor.shape = torch.Size(\n-                        [local_experts, module.hidden_size, module.intermediate_size * 2]\n-                    )\n-                else:\n-                    triton_weight_tensor.shape = torch.Size(\n-                        [local_experts, module.intermediate_size, module.hidden_size]\n-                    )\n-\n-                # triton_weight_tensor is what needs to be passed in oai kernels. It stores the data, the shapes and any more objects. It is like a subtensor\n-                setattr(module, proj, triton_weight_tensor)\n-                setattr(\n-                    module,\n-                    f\"{proj}_precision_config\",\n-                    PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n-                )\n+        # need to overwrite the shapes for the kernels\n+        if proj == \"gate_up_proj\":\n+            triton_weight_tensor.shape = torch.Size([local_experts, module.hidden_size, module.intermediate_size * 2])\n+        else:\n+            triton_weight_tensor.shape = torch.Size([local_experts, module.intermediate_size, module.hidden_size])\n+\n+        # triton_weight_tensor is what needs to be passed in oai kernels. It stores the data, the shapes and any more objects. It is like a subtensor\n+        setattr(module, proj, triton_weight_tensor)\n+        setattr(\n+            module,\n+            f\"{proj}_precision_config\",\n+            PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n+        )\n \n-                # delete blocks and scales\n-                delattr(module, scales_attr)\n-                delattr(module, blocks_attr)\n-                # setattr(module, blocks_attr, torch.nn.Parameter(triton_weight_tensor.storage.data, requires_grad=False))\n-                del blocks\n+        # delete blocks and scales\n+        delattr(module, scales_attr)\n+        delattr(module, blocks_attr)\n+        del blocks\n \n \n def _replace_with_mxfp4_linear("
        },
        {
            "sha": "7dc4f9324928f9ab34f61c4a80d3c33efde5260d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 38,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -84,7 +84,8 @@\n     prune_layer,\n     prune_linear_layer,\n )\n-from .quantizers import AutoHfQuantizer, HfQuantizer\n+from .quantizers import HfQuantizer\n+from .quantizers.auto import get_hf_quantizer\n from .quantizers.quantizers_utils import get_module_from_name\n from .safetensors_conversion import auto_conversion\n from .utils import (\n@@ -868,6 +869,7 @@ def _load_state_dict_into_meta_model(\n                 _load_parameter_into_model(model, param_name, param.to(param_device))\n \n             else:\n+                # TODO naming is stupid it loads it as well\n                 hf_quantizer.create_quantized_param(\n                     model, param, param_name, param_device, state_dict, unexpected_keys\n                 )\n@@ -4037,12 +4039,14 @@ def save_pretrained(\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n+            create_pr = kwargs.pop(\"create_pr\", False)\n             repo_id = self._create_repo(repo_id, **kwargs)\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n+        if hf_quantizer is not None:\n+            state_dict = hf_quantizer.get_state_dict(self)\n         # Only save the model itself if we are using distributed training\n         model_to_save = unwrap_model(self)\n-\n         # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n         # we currently don't use this setting automatically, but may start to use with v5\n         dtype = get_parameter_dtype(model_to_save)\n@@ -4354,6 +4358,7 @@ def save_pretrained(\n                 files_timestamps,\n                 commit_message=commit_message,\n                 token=token,\n+                create_pr=create_pr,\n             )\n \n     @wraps(PushToHubMixin.push_to_hub)\n@@ -5035,41 +5040,9 @@ def from_pretrained(\n                     f\"{transformers_explicit_filename}\"\n                 )\n \n-        pre_quantized = hasattr(config, \"quantization_config\")\n-        if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n-            pre_quantized = False\n-\n-        if pre_quantized or quantization_config is not None:\n-            if pre_quantized:\n-                config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n-                    config.quantization_config, quantization_config\n-                )\n-            else:\n-                config.quantization_config = quantization_config\n-\n-            hf_quantizer = AutoHfQuantizer.from_config(\n-                config.quantization_config,\n-                pre_quantized=pre_quantized,\n-            )\n-        else:\n-            hf_quantizer = None\n-\n-        if hf_quantizer is not None:\n-            hf_quantizer.validate_environment(\n-                dtype=dtype,\n-                from_tf=from_tf,\n-                from_flax=from_flax,\n-                device_map=device_map,\n-                weights_only=weights_only,\n-            )\n-            dtype = hf_quantizer.update_dtype(dtype)\n-            device_map = hf_quantizer.update_device_map(device_map)\n-            config = hf_quantizer.update_tp_plan(config)\n-\n-            # In order to ensure popular quantization methods are supported. Can be disable with `disable_telemetry`\n-            if not getattr(hf_quantizer.quantization_config, \"dequantize\", False):\n-                quant_method = hf_quantizer.quantization_config.quant_method\n-                user_agent[\"quant\"] = getattr(quant_method, \"value\", quant_method)\n+        hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n+            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n+        )\n \n         if gguf_file is not None and hf_quantizer is not None:\n             raise ValueError(\n@@ -5483,7 +5456,7 @@ def _load_pretrained_model(\n         key_mapping: Optional[dict[str, str]] = None,\n         weights_only: bool = True,\n     ):\n-        # Useful flags\n+        # TODO: we should only be calling hf_quantizer.skip_placement or something like that\n         is_quantized = hf_quantizer is not None\n         is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n             QuantizationMethod.HQQ,"
        },
        {
            "sha": "610fd31486046c56bc3ec2a42c2dfff8f43f052c",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -102,6 +102,9 @@ def convert_moe_packed_tensors(\n     dtype: torch.dtype = torch.bfloat16,\n     rows_per_chunk: int = 32768 * 1024,\n ) -> torch.Tensor:\n+    \"\"\"\n+    TODO this needs to be documented\n+    \"\"\"\n     import math\n \n     scales = scales.to(torch.int32) - 127\n@@ -136,8 +139,8 @@ def convert_moe_packed_tensors(\n         del idx_lo, idx_hi, blk, exp\n \n     out = out.reshape(*prefix_shape, G, B * 2).view(*prefix_shape, G * B * 2)\n-    # to match for now existing implementation\n-    return out.to(torch.float8_e5m2)\n+    out = out.to(torch.float8_e5m2).permute(0, 2, 1).contiguous()\n+    return out\n \n \n def write_model(\n@@ -212,7 +215,6 @@ def write_model(\n                     scales = final_[key.replace(\"blocks\", \"scales\")]\n                     new_key = new_key.replace(\".blocks\", \"\")\n                     unpacked_tensors = convert_moe_packed_tensors(blocks, scales, dtype=torch.bfloat16)\n-                    unpacked_tensors = unpacked_tensors.permute(0, 2, 1).contiguous()  # einsum in orignal, I use bmm\n                     state_dict[new_key] = unpacked_tensors\n                 else:\n                     raise (f\"Unidentified {key}, please double check the state dict\")"
        },
        {
            "sha": "22c08ded0ed2241f8533066a2cb0f9149c1867e4",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 39,
            "deletions": 1,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -229,7 +229,6 @@ def merge_quantization_configs(\n             )\n             and quantization_config_from_args is not None\n         ):\n-            # special case for GPTQ / AWQ / FbgemmFp8 config collision\n             loading_attr_dict = quantization_config_from_args.get_loading_attributes()\n             for attr, val in loading_attr_dict.items():\n                 setattr(quantization_config, attr, val)\n@@ -294,3 +293,42 @@ def register_quantizer_fn(cls):\n         return cls\n \n     return register_quantizer_fn\n+\n+\n+def get_hf_quantizer(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent):\n+    pre_quantized = hasattr(config, \"quantization_config\")\n+    if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n+        pre_quantized = False\n+\n+    if pre_quantized or quantization_config is not None:\n+        if pre_quantized:\n+            config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n+                config.quantization_config, quantization_config\n+            )\n+        else:\n+            config.quantization_config = quantization_config\n+\n+        hf_quantizer = AutoHfQuantizer.from_config(\n+            config.quantization_config,\n+            pre_quantized=pre_quantized,\n+        )\n+    else:\n+        hf_quantizer = None\n+\n+    if hf_quantizer is not None:\n+        hf_quantizer.validate_environment(\n+            dtype=dtype,\n+            from_tf=from_tf,\n+            from_flax=from_flax,\n+            device_map=device_map,\n+            weights_only=weights_only,\n+        )\n+        dtype = hf_quantizer.update_dtype(dtype)\n+        device_map = hf_quantizer.update_device_map(device_map)\n+        config = hf_quantizer.update_tp_plan(config)\n+\n+        # In order to ensure popular quantization methods are supported. Can be disable with `disable_telemetry`\n+        if not getattr(hf_quantizer.quantization_config, \"dequantize\", False):\n+            quant_method = hf_quantizer.quantization_config.quant_method\n+            user_agent[\"quant\"] = getattr(quant_method, \"value\", quant_method)\n+    return hf_quantizer, config, dtype, device_map"
        },
        {
            "sha": "31c68320adb65c3e6acda1cc11bdbd08ee22bb04",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -334,6 +334,10 @@ def is_compileable(self) -> bool:\n         \"\"\"Flag indicating whether the quantized model can be compiled\"\"\"\n         return False\n \n+    def get_state_dict(self, model):\n+        \"\"\"Get state dict. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n+        return None\n+\n     @abstractmethod\n     def _process_model_before_weight_loading(self, model, **kwargs): ...\n "
        },
        {
            "sha": "964fc0bdfa49bb753ef9164378b6aee81527931a",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 100,
            "deletions": 46,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -33,6 +33,7 @@\n     import torch\n \n logger = logging.get_logger(__name__)\n+triton_kernels_hub = None\n \n \n class Mxfp4HfQuantizer(HfQuantizer):\n@@ -41,14 +42,25 @@ class Mxfp4HfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_parameters_quantization = True\n-    # to remove if we decide to allow quantizing weights with this method\n     requires_calibration = False\n \n     required_packages = [\"accelerate\"]\n \n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n         self.quantization_config = quantization_config\n+        self.triton_kernels_hub = None\n+\n+    def _lazy_import_kernels(self):\n+        \"\"\"Lazy import and initialize kernels only when needed\"\"\"\n+        if self.triton_kernels_hub is None:\n+            try:\n+                from kernels import get_kernel\n+\n+                self.triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n+            except ImportError:\n+                raise ImportError(\"kernels package is required for MXFP4 quantization\")\n+        return self.triton_kernels_hub\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_torch_available():\n@@ -103,10 +115,7 @@ def validate_environment(self, *args, **kwargs):\n             raise ValueError(\"MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed\")\n \n         if not self.pre_quantized:\n-            from kernels import get_kernel\n-\n-            global triton_kernels_hub\n-            triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n+            self._lazy_import_kernels()\n \n         device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n@@ -154,7 +163,6 @@ def check_quantized_param(\n             module, tensor_name = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n         else:\n             module, tensor_name = get_module_from_name(model, param_name)\n-\n         if isinstance(module, Mxfp4GptOssExperts) or (\n             isinstance(module, GptOssExperts) and self.quantization_config.dequantize\n         ):\n@@ -173,57 +181,51 @@ def create_quantized_param(\n         unexpected_keys: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        from ..integrations import Mxfp4GptOssExperts, dequantize, load_and_swizzle_mxfp4, quantize_to_mxfp4\n+        from ..integrations import (\n+            Mxfp4GptOssExperts,\n+            dequantize,\n+            load_and_swizzle_mxfp4,\n+            quantize_to_mxfp4,\n+            swizzle_mxfp4,\n+        )\n         from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n \n         if not self.pre_quantized:\n-            PrecisionConfig, FlexCtx, InFlexData = (\n-                triton_kernels_hub.matmul_ogs.PrecisionConfig,\n-                triton_kernels_hub.matmul_ogs.FlexCtx,\n-                triton_kernels_hub.matmul_ogs.InFlexData,\n-            )\n+            triton_kernels_hub = self._lazy_import_kernels()\n             module, _ = get_module_from_name(model, param_name)\n-            with torch.cuda.device(target_device):\n+            with torch.device(target_device):\n                 if isinstance(module, Mxfp4GptOssExperts):\n-                    if \"gate_up_proj\" in param_name:\n-                        right_pad = module.gate_up_proj_right_pad\n-                        bottom_pad = module.gate_up_proj_bottom_pad\n-                        loaded_weight = torch.nn.functional.pad(\n-                            param_value, (0, right_pad, 0, bottom_pad, 0, 0), mode=\"constant\", value=0\n-                        )\n-                        triton_weight_tensor, weight_scale = quantize_to_mxfp4(loaded_weight)\n-                        module.gate_up_proj_precision_config = PrecisionConfig(\n-                            weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())\n-                        )\n-                        module.gate_up_proj = triton_weight_tensor\n-                        module.gate_up_proj_blocks = torch.nn.Parameter(\n-                            triton_weight_tensor.storage.data, requires_grad=False\n-                        )\n-                    elif \"down_proj\" in param_name:\n-                        right_pad = module.down_proj_right_pad\n-                        bottom_pad = module.down_proj_bottom_pad\n-                        loaded_weight = torch.nn.functional.pad(\n-                            param_value, (0, right_pad, 0, bottom_pad, 0, 0), mode=\"constant\", value=0\n-                        ).to(target_device)\n-                        triton_weight_tensor, weight_scale = quantize_to_mxfp4(loaded_weight)\n-                        module.down_proj_precision_config = PrecisionConfig(\n-                            weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())\n-                        )\n-                        module.down_proj = triton_weight_tensor\n-                        module.down_proj_blocks = torch.nn.Parameter(\n-                            triton_weight_tensor.storage.data, requires_grad=False\n-                        )\n-\n-        # we take this path if already quantized but not in a compatible way\n+                    triton_weight_tensor, weight_scale = quantize_to_mxfp4(param_value, triton_kernels_hub)\n+                    PrecisionConfig, FlexCtx, InFlexData = (\n+                        triton_kernels_hub.matmul_ogs.PrecisionConfig,\n+                        triton_kernels_hub.matmul_ogs.FlexCtx,\n+                        triton_kernels_hub.matmul_ogs.InFlexData,\n+                    )\n+                    triton_weight_tensor, weight_scale = swizzle_mxfp4(\n+                        triton_weight_tensor, weight_scale, triton_kernels_hub\n+                    )\n+\n+                    proj = \"gate_up_proj\" if \"gate_up_proj\" in param_name else \"down_proj\"\n+                    setattr(module, proj, triton_weight_tensor)\n+                    setattr(\n+                        module,\n+                        f\"{proj}_precision_config\",\n+                        PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n+                    )\n+\n+                    delattr(module, f\"{proj}_blocks\")\n+                    delattr(module, f\"{proj}_scales\")\n+\n         # The params going here are either gate_up_proj_blocks, or down_proj_blocks, or gate_up_proj_scales, or down_proj_scales\n         else:\n+            #  This is when loading a quantized model (blocks and scales exist)\n             empty_param = kwargs.get(\"empty_param\")\n             casting_dtype = kwargs.get(\"casting_dtype\")\n             to_contiguous = kwargs.get(\"to_contiguous\")\n             rank = kwargs.get(\"rank\")\n             device_mesh = kwargs.get(\"device_mesh\")\n             if (\"blocks\" in param_name or \"scales\" in param_name) and self.quantization_config.dequantize:\n-                # blocks and scales have the same length that's this works for both\n+                # blocks and scales have the same length that's why this works for both\n                 module, _ = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n             else:\n                 module, _ = get_module_from_name(model, param_name)\n@@ -251,6 +253,7 @@ def create_quantized_param(\n                         param_name,\n                         param_value,\n                         target_device,\n+                        self._lazy_import_kernels(),\n                         **shard_kwargs,\n                     )\n \n@@ -274,6 +277,19 @@ def update_expected_keys(self, model: \"PreTrainedModel\", expected_keys: list[str\n                 base = key[: -len(\"down_proj\")]\n                 new_expected_keys.append(base + \"down_proj_blocks\")\n                 new_expected_keys.append(base + \"down_proj_scales\")\n+            elif not self.pre_quantized:\n+                # in this case, we are quantizing the model so we need to update the keys as we changed the layers\n+                if key.endswith(\".mlp.experts.down_proj_blocks\"):\n+                    base = key[: -len(\"down_proj_blocks\")]\n+                    new_expected_keys.append(base + \"down_proj\")\n+                elif key.endswith(\".mlp.experts.gate_up_proj_blocks\"):\n+                    base = key[: -len(\"gate_up_proj_blocks\")]\n+                    new_expected_keys.append(base + \"gate_up_proj\")\n+                elif key.endswith(\"scales\"):\n+                    # we remove it the scales as the checkpoint don't contain them\n+                    continue\n+                else:\n+                    new_expected_keys.append(key)\n             else:\n                 new_expected_keys.append(key)\n         return new_expected_keys\n@@ -343,11 +359,49 @@ def update_param_name(self, param_name: str) -> str:\n                 return param_name.replace(\"_blocks\", \"\")\n             elif \"_scales\" in param_name:\n                 return param_name.replace(\"_scales\", \"\")\n+        elif not self.pre_quantized:\n+            if param_name.endswith(\"gate_up_proj\"):\n+                return param_name.replace(\"gate_up_proj\", \"gate_up_proj_blocks\")\n+            if param_name.endswith(\"down_proj\"):\n+                return param_name.replace(\"down_proj\", \"down_proj_blocks\")\n         return param_name\n \n+    def get_state_dict(self, model):\n+        from ..integrations import Mxfp4GptOssExperts\n+\n+        state_dict = model.state_dict()\n+\n+        for name, module in model.named_modules():\n+            if (\n+                isinstance(module, Mxfp4GptOssExperts)\n+                and hasattr(module, \"gate_up_proj\")\n+                and hasattr(module, \"down_proj\")\n+            ):\n+                state_dict[f\"{name}.gate_up_proj_blocks\"] = (\n+                    module.gate_up_proj.storage.layout.unswizzle_data(module.gate_up_proj.storage.data)\n+                    .transpose(-1, -2)\n+                    .reshape(32, -1, 90, 16)\n+                )\n+                state_dict[f\"{name}.gate_up_proj_scales\"] = (\n+                    module.gate_up_proj_precision_config.weight_scale.storage.layout.unswizzle_data(\n+                        module.gate_up_proj_precision_config.weight_scale.storage.data\n+                    ).transpose(-1, -2)\n+                )\n+                state_dict[f\"{name}.down_proj_blocks\"] = (\n+                    module.down_proj.storage.layout.unswizzle_data(module.down_proj.storage.data)\n+                    .transpose(-1, -2)\n+                    .reshape(32, 2880, 90, -1)\n+                )\n+                state_dict[f\"{name}.down_proj_scales\"] = (\n+                    module.down_proj_precision_config.weight_scale.storage.layout.unswizzle_data(\n+                        module.down_proj_precision_config.weight_scale.storage.data\n+                    ).transpose(-1, -2)\n+                )\n+\n+        return state_dict\n+\n     def is_serializable(self, safe_serialization=None):\n-        logger.warning_once(\"MXFP4 quantization is not serializable using safetensors for now\")\n-        return False\n+        return True\n \n     @property\n     def is_trainable(self) -> bool:"
        },
        {
            "sha": "85d021e4ba93a76abec1e57162ef72df2315a042",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -2061,6 +2061,8 @@ class Mxfp4Config(QuantizationConfigMixin):\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision.\n+        dequantize (`bool`, *optional*, default to `False`):\n+            Whether we dequantize the model to bf16 precision or not\n     \"\"\"\n \n     def __init__(\n@@ -2074,6 +2076,11 @@ def __init__(\n         self.dequantize = dequantize\n \n     def get_loading_attributes(self):\n-        return {\n-            \"dequantize\": self.dequantize,\n-        }\n+        return {\"dequantize\": self.dequantize}\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. Returns:\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+        \"\"\"\n+        return {\"quant_method\": self.quant_method, \"modules_to_not_convert\": self.modules_to_not_convert}"
        },
        {
            "sha": "59763cd274764c8d0f221a7fb73e765c6414a5be",
            "filename": "tests/quantization/mxfp4/test_mxfp4.py",
            "status": "modified",
            "additions": 83,
            "deletions": 59,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py?ref=6bf6f8490cd5b0d77e711a1461c9c2fc5f5254c3",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import gc\n+import tempfile\n import unittest\n from unittest.mock import patch\n \n@@ -57,15 +58,16 @@ def test_get_loading_attributes(self):\n         \"\"\"Test get_loading_attributes method\"\"\"\n         config = Mxfp4Config(dequantize=True)\n         attrs = config.get_loading_attributes()\n-        self.assertEqual(attrs, {\"dequantize\": True})\n+        self.assertEqual(attrs[\"dequantize\"], True)\n \n     def test_to_dict(self):\n         \"\"\"Test configuration serialization to dict\"\"\"\n         config = Mxfp4Config(modules_to_not_convert=[\"lm_head\"], dequantize=True)\n         config_dict = config.to_dict()\n         self.assertEqual(config_dict[\"quant_method\"], \"mxfp4\")\n         self.assertEqual(config_dict[\"modules_to_not_convert\"], [\"lm_head\"])\n-        self.assertTrue(config_dict[\"dequantize\"])\n+        # we don't keep dequantize in config_dict\n+        self.assertTrue(\"dequantize\" not in config_dict)\n \n     def test_from_dict(self):\n         \"\"\"Test configuration creation from dict\"\"\"\n@@ -101,6 +103,7 @@ def test_quantizer_validation_no_cuda(self):\n \n             config = Mxfp4Config()\n             quantizer = Mxfp4HfQuantizer(config)\n+            quantizer.pre_quantized = False\n \n             with self.assertRaises(RuntimeError):\n                 quantizer.validate_environment()\n@@ -144,57 +147,32 @@ def test_quantizer_validation_low_compute_capability_with_dequantize(self):\n                 if \"compute capability\" in str(e):\n                     self.fail(\"Should not raise compute capability error when dequantize=True\")\n \n-    def test_quantizer_validation_dequantize_on_cpu(self):\n-        \"\"\"Test quantizer validation with dequantize enabled on CPU-only environment\"\"\"\n-        with patch(\"torch.cuda.is_available\", return_value=False):\n-            from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n-\n-            config = Mxfp4Config(dequantize=True)\n-            quantizer = Mxfp4HfQuantizer(config)\n-\n-            # Should not raise error when dequantize=True even without CUDA\n-            try:\n-                quantizer.validate_environment()\n-            except RuntimeError as e:\n-                if \"requires a GPU\" in str(e):\n-                    self.fail(\"Should not raise GPU requirement error when dequantize=True on CPU\")\n-\n     def test_quantizer_validation_order_dequantize_before_cuda_check(self):\n         \"\"\"Test that dequantize check happens before CUDA availability check\"\"\"\n-        # Mock both torch.cuda.is_available and is_accelerate_available to return False\n-        with (\n-            patch(\"torch.cuda.is_available\", return_value=False),\n-            patch(\n-                \"transformers.quantizers.quantizer_mxfp4.is_accelerate_available\",\n-                return_value=False,\n-            ),\n-        ):\n+        # Mock torch.cuda.is_available\n+        with patch(\"torch.cuda.is_available\", return_value=False):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n             # Test with dequantize=True - should pass even without CUDA and accelerate\n             config = Mxfp4Config(dequantize=True)\n             quantizer = Mxfp4HfQuantizer(config)\n \n             # This should not raise any error because dequantize check comes first\n-            try:\n-                quantizer.validate_environment()\n-            except (RuntimeError, ImportError) as e:\n-                if \"requires a GPU\" in str(e) or \"requires Accelerate\" in str(e):\n-                    self.fail(f\"Should not raise error when dequantize=True: {e}\")\n+            quantizer.validate_environment()\n \n             # Test with dequantize=False - should still fail due to missing CUDA\n             config = Mxfp4Config(dequantize=False)\n             quantizer = Mxfp4HfQuantizer(config)\n+            quantizer.pre_quantized = False\n \n-            with self.assertRaises(RuntimeError) as context:\n+            with self.assertRaises(RuntimeError):\n                 quantizer.validate_environment()\n-            self.assertIn(\"requires a GPU\", str(context.exception))\n \n     def test_quantizer_validation_missing_triton(self):\n         \"\"\"Test quantizer validation when triton is not available\"\"\"\n         with (\n             patch(\"transformers.quantizers.quantizer_mxfp4.is_triton_available\", return_value=False),\n-            patch(\"transformers.quantizers.quantizer_mxfp4.is_kernels_availalble\", return_value=False),\n+            patch(\"transformers.quantizers.quantizer_mxfp4.is_kernels_available\", return_value=False),\n         ):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -208,7 +186,7 @@ def test_quantizer_validation_missing_triton_pre_quantized_no_dequantize(self):\n         \"\"\"Test quantizer validation when triton is not available but model is pre-quantized and dequantize is False\"\"\"\n         with (\n             patch(\"transformers.quantizers.quantizer_mxfp4.is_triton_available\", return_value=False),\n-            patch(\"transformers.quantizers.quantizer_mxfp4.is_kernels_availalble\", return_value=False),\n+            patch(\"transformers.quantizers.quantizer_mxfp4.is_kernels_available\", return_value=False),\n         ):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -293,16 +271,6 @@ def test_update_param_name_no_dequantize(self):\n         updated_name = quantizer.update_param_name(param_name)\n         self.assertEqual(updated_name, param_name)\n \n-    def test_is_serializable(self):\n-        \"\"\"Test serialization capability\"\"\"\n-        from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n-\n-        config = Mxfp4Config()\n-        quantizer = Mxfp4HfQuantizer(config)\n-\n-        # MXFP4 is not serializable with safetensors\n-        self.assertFalse(quantizer.is_serializable())\n-\n     def test_is_trainable(self):\n         \"\"\"Test trainability\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n@@ -338,13 +306,11 @@ def test_convert_moe_packed_tensors(self):\n         from transformers.integrations.mxfp4 import convert_moe_packed_tensors\n \n         # Create dummy packed tensors\n-        blocks = torch.randint(0, 255, (2, 4, 8), dtype=torch.uint8)\n-        scales = torch.randint(100, 150, (2, 4), dtype=torch.uint8)\n+        blocks = torch.randint(0, 255, (2, 4, 8, 16), dtype=torch.uint8)\n+        scales = torch.randint(100, 150, (2, 4, 8), dtype=torch.uint8)\n \n         result = convert_moe_packed_tensors(blocks, scales, dtype=torch.bfloat16)\n-\n-        # Check output shape - should be [2, 4, 16] (8 * 2 for unpacking)\n-        self.assertEqual(result.shape, (2, 4 * 16))\n+        self.assertEqual(result.shape, (2, 8 * 16 * 2, 4))\n         self.assertEqual(result.dtype, torch.bfloat16)\n \n     @require_triton(min_version=\"3.4.0\")\n@@ -354,16 +320,18 @@ def test_convert_moe_packed_tensors(self):\n     def test_quantize_to_mxfp4(self):\n         \"\"\"Test quantization function\"\"\"\n         from transformers.integrations.mxfp4 import quantize_to_mxfp4\n+        from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n+\n+        config = Mxfp4Config()\n+        quantizer = Mxfp4HfQuantizer(config)\n \n         # Create dummy weight tensor\n         w = torch.randn(32, 64, 128, dtype=torch.bfloat16, device=torch.device(\"cuda\"))\n \n-        quantized_w, flex_data, mx_ctx = quantize_to_mxfp4(w, None, None)\n+        quantized_w, w_scale = quantize_to_mxfp4(w, quantizer._lazy_import_kernels())\n \n         # Check that shapes are reasonable\n         self.assertEqual(quantized_w.dtype, torch.uint8)\n-        self.assertIsNotNone(flex_data)\n-        self.assertIsNotNone(mx_ctx)\n \n \n @require_torch\n@@ -381,7 +349,7 @@ class Mxfp4ModelTest(unittest.TestCase):\n \n     # Expected outputs for generation tests\n     EXPECTED_OUTPUTS = set()\n-    EXPECTED_OUTPUTS.add(\"Once upon a time, in a small village, there lived a young\")\n+    EXPECTED_OUTPUTS.add(\"Once upon a time, in a small town, there lived a young\")\n \n     def setUp(self):\n         gc.collect()\n@@ -417,14 +385,8 @@ def check_inference_correctness_quantized(self, model, tokenizer):\n     def test_gpt_oss_model_loading_quantized_with_device_map(self):\n         \"\"\"Test loading OpenAI MoE model with mxfp4 quantization and device_map\"\"\"\n \n-        quantization_config = Mxfp4Config(dequantize=False)\n-\n-        # Test that config is properly set up\n-        self.assertFalse(quantization_config.dequantize)\n-\n         model = GptOssForCausalLM.from_pretrained(\n             self.model_name,\n-            quantization_config=quantization_config,\n             dtype=torch.bfloat16,\n             device_map=\"auto\",\n         )\n@@ -479,3 +441,65 @@ def test_memory_footprint_comparison(self):\n         quantized_mem = quantized_model.get_memory_footprint()\n         dequantized_mem = dequantized_model.get_memory_footprint()\n         self.assertLess(quantized_mem, dequantized_mem)\n+\n+    def test_save_mxfp4(self):\n+        \"\"\"Test saving quantized OpenAI MoE model with device_map\"\"\"\n+\n+        model = GptOssForCausalLM.from_pretrained(\n+            self.model_name,\n+            torch_dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+        with tempfile.TemporaryDirectory() as tmp:\n+            # Save the model in mxfp4 format\n+            model.save_pretrained(tmp)\n+            torch.cuda.empty_cache()\n+            gc.collect()\n+            # test quantized model\n+            loaded_model = GptOssForCausalLM.from_pretrained(\n+                tmp,\n+                torch_dtype=torch.bfloat16,\n+                device_map=\"auto\",\n+            )\n+            self.check_inference_correctness_quantized(loaded_model, tokenizer)\n+\n+            # test dequantized model\n+            loaded_model = GptOssForCausalLM.from_pretrained(\n+                tmp,\n+                quantization_config=Mxfp4Config(dequantize=True),\n+                torch_dtype=torch.bfloat16,\n+                device_map=\"auto\",\n+            )\n+            self.check_inference_correctness_quantized(loaded_model, tokenizer)\n+\n+    def test_save_mxfp4_non_quantized(self):\n+        \"\"\"Test saving dequantized OpenAI MoE model with mxfp4 quantization and device_map\"\"\"\n+        non_quantized_model_name = \"hf-internal-testing/gpt-oss-20b-bf16\"\n+        tokenizer = AutoTokenizer.from_pretrained(non_quantized_model_name)\n+        loaded_model = GptOssForCausalLM.from_pretrained(\n+            non_quantized_model_name,\n+            quantization_config=Mxfp4Config(),\n+            torch_dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+        )\n+        # save the quantized model\n+        with tempfile.TemporaryDirectory() as tmp:\n+            loaded_model.save_pretrained(tmp)\n+            torch.cuda.empty_cache()\n+            gc.collect()\n+            # load it back to check with everything works as expected\n+            loaded_model = GptOssForCausalLM.from_pretrained(\n+                tmp,\n+                torch_dtype=torch.bfloat16,\n+                device_map=\"auto\",\n+            )\n+            self.check_inference_correctness_quantized(loaded_model, tokenizer)\n+\n+            loaded_model = GptOssForCausalLM.from_pretrained(\n+                tmp,\n+                quantization_config=Mxfp4Config(dequantized=True),\n+                torch_dtype=torch.bfloat16,\n+                device_map=\"auto\",\n+            )\n+            self.check_inference_correctness_quantized(loaded_model, tokenizer)"
        }
    ],
    "stats": {
        "total": 557,
        "additions": 321,
        "deletions": 236
    }
}