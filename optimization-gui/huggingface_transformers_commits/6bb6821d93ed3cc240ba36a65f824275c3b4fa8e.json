{
    "author": "yaswanth19",
    "message": "Refactor `get_XXX_dataloader` from Trainer (#38090)\n\n* Remove test_dataloader\n\n* refactor",
    "sha": "6bb6821d93ed3cc240ba36a65f824275c3b4fa8e",
    "files": [
        {
            "sha": "84725ee4f91f9413a6673c83db5e278d2ba6b62b",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 72,
            "deletions": 80,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bb6821d93ed3cc240ba36a65f824275c3b4fa8e/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bb6821d93ed3cc240ba36a65f824275c3b4fa8e/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6bb6821d93ed3cc240ba36a65f824275c3b4fa8e",
            "patch": "@@ -972,16 +972,16 @@ def _get_collator_with_removed_columns(\n         )\n         return remove_columns_collator\n \n-    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n-        if self.train_dataset is None or not has_length(self.train_dataset):\n+    def _get_train_sampler(self, train_dataset) -> Optional[torch.utils.data.Sampler]:\n+        if train_dataset is None or not has_length(train_dataset):\n             return None\n \n         # Build the sampler.\n         if self.args.group_by_length:\n-            if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):\n+            if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n                 lengths = (\n-                    self.train_dataset[self.args.length_column_name]\n-                    if self.args.length_column_name in self.train_dataset.column_names\n+                    train_dataset[self.args.length_column_name]\n+                    if self.args.length_column_name in train_dataset.column_names\n                     else None\n                 )\n             else:\n@@ -991,50 +991,80 @@ def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n             )\n             return LengthGroupedSampler(\n                 self.args.train_batch_size * self.args.gradient_accumulation_steps,\n-                dataset=self.train_dataset,\n+                dataset=train_dataset,\n                 lengths=lengths,\n                 model_input_name=model_input_name,\n             )\n \n         else:\n-            return RandomSampler(self.train_dataset)\n+            return RandomSampler(train_dataset)\n \n-    def get_train_dataloader(self) -> DataLoader:\n-        \"\"\"\n-        Returns the training [`~torch.utils.data.DataLoader`].\n-\n-        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n-        training if necessary) otherwise.\n-\n-        Subclass and override this method if you want to inject some custom behavior.\n-        \"\"\"\n-        if self.train_dataset is None:\n-            raise ValueError(\"Trainer: training requires a train_dataset.\")\n+    def _get_dataloader(\n+        self,\n+        dataset: Dataset,\n+        description: str,\n+        batch_size: int,\n+        sampler_fn: Optional[Callable[[Dataset], torch.utils.data.Sampler]] = None,\n+        is_training: bool = False,\n+        dataloader_key: Optional[str] = None,\n+    ) -> DataLoader:\n+        \"\"\"Create a [`~torch.utils.data.DataLoader`] from the given dataset.\"\"\"\n \n-        train_dataset = self.train_dataset\n         data_collator = self.data_collator\n-        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n-            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n+        if is_datasets_available() and isinstance(dataset, datasets.Dataset):\n+            dataset = self._remove_unused_columns(dataset, description=description)\n         else:\n-            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n+            data_collator = self._get_collator_with_removed_columns(self.data_collator, description=description)\n \n         dataloader_params = {\n-            \"batch_size\": self._train_batch_size,\n+            \"batch_size\": batch_size,\n             \"collate_fn\": data_collator,\n             \"num_workers\": self.args.dataloader_num_workers,\n             \"pin_memory\": self.args.dataloader_pin_memory,\n             \"persistent_workers\": self.args.dataloader_persistent_workers,\n         }\n \n-        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n-            dataloader_params[\"sampler\"] = self._get_train_sampler()\n+        if not isinstance(dataset, torch.utils.data.IterableDataset):\n+            if sampler_fn is not None:\n+                dataloader_params[\"sampler\"] = sampler_fn(dataset)\n             dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n-            dataloader_params[\"worker_init_fn\"] = partial(\n-                seed_worker, num_workers=self.args.dataloader_num_workers, rank=self.args.process_index\n-            )\n             dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n+            if is_training:\n+                dataloader_params[\"worker_init_fn\"] = partial(\n+                    seed_worker, num_workers=self.args.dataloader_num_workers, rank=self.args.process_index\n+                )\n \n-        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n+        dataloader = DataLoader(dataset, **dataloader_params)\n+\n+        # Accelerator.free_memory() will destroy the references, so\n+        # we need to store the non-prepared version for eval dataloaders.\n+        if dataloader_key is not None and self.args.dataloader_persistent_workers:\n+            if hasattr(self, \"_eval_dataloaders\"):\n+                self._eval_dataloaders[dataloader_key] = dataloader\n+            else:\n+                self._eval_dataloaders = {dataloader_key: dataloader}\n+\n+        return self.accelerator.prepare(dataloader)\n+\n+    def get_train_dataloader(self) -> DataLoader:\n+        \"\"\"\n+        Returns the training [`~torch.utils.data.DataLoader`].\n+\n+        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n+        training if necessary) otherwise.\n+\n+        Subclass and override this method if you want to inject some custom behavior.\n+        \"\"\"\n+        if self.train_dataset is None:\n+            raise ValueError(\"Trainer: training requires a train_dataset.\")\n+\n+        return self._get_dataloader(\n+            dataset=self.train_dataset,\n+            description=\"Training\",\n+            batch_size=self._train_batch_size,\n+            sampler_fn=self._get_train_sampler,\n+            is_training=True,\n+        )\n \n     def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:\n         if eval_dataset is None or not has_length(eval_dataset):\n@@ -1111,36 +1141,14 @@ def get_eval_dataloader(self, eval_dataset: Optional[Union[str, Dataset]] = None\n             if eval_dataset is not None\n             else self.eval_dataset\n         )\n-        data_collator = self.data_collator\n-\n-        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n-            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n-        else:\n-            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n \n-        dataloader_params = {\n-            \"batch_size\": self.args.eval_batch_size,\n-            \"collate_fn\": data_collator,\n-            \"num_workers\": self.args.dataloader_num_workers,\n-            \"pin_memory\": self.args.dataloader_pin_memory,\n-            \"persistent_workers\": self.args.dataloader_persistent_workers,\n-        }\n-\n-        if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n-            dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n-            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n-            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n-\n-        # accelerator.free_memory() will destroy the references, so\n-        # we need to store the non-prepared version\n-        eval_dataloader = DataLoader(eval_dataset, **dataloader_params)\n-        if self.args.dataloader_persistent_workers:\n-            if hasattr(self, \"_eval_dataloaders\"):\n-                self._eval_dataloaders[dataloader_key] = eval_dataloader\n-            else:\n-                self._eval_dataloaders = {dataloader_key: eval_dataloader}\n-\n-        return self.accelerator.prepare(eval_dataloader)\n+        return self._get_dataloader(\n+            dataset=eval_dataset,\n+            description=\"Evaluation\",\n+            batch_size=self.args.eval_batch_size,\n+            sampler_fn=self._get_eval_sampler,\n+            dataloader_key=dataloader_key,\n+        )\n \n     def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n         \"\"\"\n@@ -1153,28 +1161,12 @@ def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n                 The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n                 `model.forward()` method are automatically removed. It must implement `__len__`.\n         \"\"\"\n-        data_collator = self.data_collator\n-\n-        if is_datasets_available() and isinstance(test_dataset, datasets.Dataset):\n-            test_dataset = self._remove_unused_columns(test_dataset, description=\"test\")\n-        else:\n-            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"test\")\n-\n-        dataloader_params = {\n-            \"batch_size\": self.args.eval_batch_size,\n-            \"collate_fn\": data_collator,\n-            \"num_workers\": self.args.dataloader_num_workers,\n-            \"pin_memory\": self.args.dataloader_pin_memory,\n-            \"persistent_workers\": self.args.dataloader_persistent_workers,\n-        }\n-\n-        if not isinstance(test_dataset, torch.utils.data.IterableDataset):\n-            dataloader_params[\"sampler\"] = self._get_eval_sampler(test_dataset)\n-            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n-            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n-\n-        # We use the same batch_size as for eval.\n-        return self.accelerator.prepare(DataLoader(test_dataset, **dataloader_params))\n+        return self._get_dataloader(\n+            dataset=test_dataset,\n+            description=\"test\",\n+            batch_size=self.args.eval_batch_size,\n+            sampler_fn=self._get_eval_sampler,\n+        )\n \n     def create_optimizer_and_scheduler(self, num_training_steps: int):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 152,
        "additions": 72,
        "deletions": 80
    }
}