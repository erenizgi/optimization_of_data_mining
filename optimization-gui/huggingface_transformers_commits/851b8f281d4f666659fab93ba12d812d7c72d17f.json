{
    "author": "ArthurZucker",
    "message": "[`kernels`] If flash attention2 is not installed / fails to import (cc on our cluster) default to kernels (#40178)\n\n* first step if flash not installed but you set to use it\n\n* try importing\n\n* now default to using it\n\n* update our tests as well\n\n* wow yesterday I was not awake\n\n* fixup\n\n* style\n\n* lol the fix was very very simple\n\n* `RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/kernels@main#egg=kernels\n` for updated dockers\n\n* push review comments\n\n* fix\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "851b8f281d4f666659fab93ba12d812d7c72d17f",
    "files": [
        {
            "sha": "64cd09b928a2b012d6a3b1e44c1dbe4b349eba4c",
            "filename": "docker/transformers-all-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/851b8f281d4f666659fab93ba12d812d7c72d17f/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/851b8f281d4f666659fab93ba12d812d7c72d17f/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-all-latest-gpu%2FDockerfile?ref=851b8f281d4f666659fab93ba12d812d7c72d17f",
            "patch": "@@ -44,6 +44,8 @@ RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/pef\n \n # For bettertransformer\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/optimum@main#egg=optimum\n+# For kernels\n+RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/kernels@main#egg=kernels\n \n # For video model testing\n RUN python3 -m pip install --no-cache-dir av"
        },
        {
            "sha": "30de59d8b50af76109b1fe33f4caf8ed35b6a586",
            "filename": "docker/transformers-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/851b8f281d4f666659fab93ba12d812d7c72d17f/docker%2Ftransformers-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/851b8f281d4f666659fab93ba12d812d7c72d17f/docker%2Ftransformers-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-gpu%2FDockerfile?ref=851b8f281d4f666659fab93ba12d812d7c72d17f",
            "patch": "@@ -17,6 +17,7 @@ RUN python3 -m pip install --no-cache-dir --upgrade pip && \\\n     jupyter \\\n     tensorflow \\\n     torch\n+RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/kernels@main#egg=kernels\n \n RUN git clone https://github.com/NVIDIA/apex\n RUN cd apex && \\"
        },
        {
            "sha": "4a96e9e537e2e43cff5ab6ec103a0e6acf3ea20a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 14,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/851b8f281d4f666659fab93ba12d812d7c72d17f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/851b8f281d4f666659fab93ba12d812d7c72d17f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=851b8f281d4f666659fab93ba12d812d7c72d17f",
            "patch": "@@ -2792,17 +2792,25 @@ def _check_and_adjust_attn_implementation(\n             `str`: The final attention implementation to use, including potential fallbacks from sdpa to eager, or from\n             None to sdpa (to potentially eager).\n         \"\"\"\n-        # Register kernel if relevant\n-        if attn_implementation is not None and re.match(\n-            r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", attn_implementation\n+        applicable_attn_implementation = attn_implementation\n+        # If FA not installed, do not fail but use kernels instead\n+        if (\n+            applicable_attn_implementation == \"flash_attention_2\"\n+            and self._supports_flash_attn\n+            and not is_flash_attn_2_available()\n+            and is_kernels_available()\n+        ):\n+            applicable_attn_implementation = \"kernels-community/flash-attn\"\n+        if applicable_attn_implementation is not None and re.match(\n+            r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", applicable_attn_implementation\n         ):\n             if not is_kernels_available():\n                 raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n             attention_wrapper = None\n             # FIXME: @ArthurZucker this is dirty, did not want to do a lof of extra work\n-            actual_attn_name = attn_implementation\n-            if \"|\" in attn_implementation:\n-                attention_wrapper, actual_attn_name = attn_implementation.split(\"|\")\n+            actual_attn_name = applicable_attn_implementation\n+            if \"|\" in applicable_attn_implementation:\n+                attention_wrapper, actual_attn_name = applicable_attn_implementation.split(\"|\")\n                 # `transformers` has wrapper for sdpa, paged, flash, flex etc.\n                 attention_wrapper = ALL_ATTENTION_FUNCTIONS.get(attention_wrapper)\n             # Extract repo_id and kernel_name from the string\n@@ -2826,27 +2834,37 @@ def _check_and_adjust_attn_implementation(\n                     lazy_import_flash_attention(kernel)\n                 elif kernel_name is not None:\n                     kernel_function = getattr(kernel, kernel_name)\n-                ALL_ATTENTION_FUNCTIONS.register(attn_implementation, kernel_function)\n+                ALL_ATTENTION_FUNCTIONS.register(applicable_attn_implementation, kernel_function)\n                 ALL_MASK_ATTENTION_FUNCTIONS.register(\n-                    attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"]\n+                    applicable_attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"]\n                 )\n+                # log that we used kernel fallback\n+                if attn_implementation == \"flash_attention_2\":\n+                    logger.warning_once(\n+                        \"You do not have `flash_attn` installed, using `kernels-community/flash-attn` from the `kernels` \"\n+                        \"library instead!\"\n+                    )\n             except Exception as e:\n+                if attn_implementation == \"flash_attention_2\":\n+                    self._flash_attn_2_can_dispatch()  # will fail as fa2 is not available but raise the proper exception\n                 logger.warning_once(\n                     f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n                     \"default attention implementation instead (sdpa if available, eager otherwise).\"\n                 )\n                 try:\n                     self._sdpa_can_dispatch(is_init_check)\n-                    attn_implementation = \"sdpa\"\n+                    applicable_attn_implementation = \"sdpa\"\n                 except (ValueError, ImportError) as e:\n-                    attn_implementation = \"eager\"\n+                    applicable_attn_implementation = \"eager\"\n         else:\n-            attn_implementation = self.get_correct_attn_implementation(attn_implementation, is_init_check)\n+            applicable_attn_implementation = self.get_correct_attn_implementation(\n+                applicable_attn_implementation, is_init_check\n+            )\n             # preload flash attention here to allow compile with fullgraph\n-            if attn_implementation.startswith(\"flash_attention\"):\n-                lazy_import_flash_attention(attn_implementation)\n+            if applicable_attn_implementation.startswith(\"flash_attention\"):\n+                lazy_import_flash_attention(applicable_attn_implementation)\n \n-        return attn_implementation\n+        return applicable_attn_implementation\n \n     def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n         applicable_attention = \"sdpa\" if requested_attention is None else requested_attention"
        },
        {
            "sha": "8a35f13829c37929b8d03733ebe055ae3946138f",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/851b8f281d4f666659fab93ba12d812d7c72d17f/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/851b8f281d4f666659fab93ba12d812d7c72d17f/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=851b8f281d4f666659fab93ba12d812d7c72d17f",
            "patch": "@@ -601,7 +601,16 @@ def require_flash_attn(test_case):\n     These tests are skipped when Flash Attention isn't installed.\n \n     \"\"\"\n-    return unittest.skipUnless(is_flash_attn_2_available(), \"test requires Flash Attention\")(test_case)\n+    flash_attn_available = is_flash_attn_2_available()\n+    kernels_available = is_kernels_available()\n+    try:\n+        from kernels import get_kernel\n+\n+        get_kernel(\"kernels-community/flash-attn\")\n+    except Exception as _:\n+        kernels_available = False\n+\n+    return unittest.skipUnless(kernels_available | flash_attn_available, \"test requires Flash Attention\")(test_case)\n \n \n def require_kernels(test_case):"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 45,
        "deletions": 15
    }
}