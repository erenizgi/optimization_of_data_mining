{
    "author": "ydwu4",
    "message": "Avoid aliasing in cond's branches for torch 2.8 (#39488)\n\nAvoid alaising in cond's branches\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "78ef84921bcacb31f130ae0cb880911c4aad07f6",
    "files": [
        {
            "sha": "07b34014465301a1ce9545780ac093ff67659d0e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/78ef84921bcacb31f130ae0cb880911c4aad07f6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78ef84921bcacb31f130ae0cb880911c4aad07f6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=78ef84921bcacb31f130ae0cb880911c4aad07f6",
            "patch": "@@ -507,21 +507,22 @@ def _cache_dependant_input_preparation_exporting(\n             #     else:\n             #         if input_ids.shape[1] != cache_position.shape[0]:\n             #             input_ids = input_ids[:, cache_position]\n+            # We need to clone the outputs to avoid aliasing.\n             def branch_1(inputs_embeds, cache_position):\n-                return inputs_embeds[:, -cache_position.shape[0] :]\n+                return inputs_embeds[:, -cache_position.shape[0] :].clone()\n \n             def branch_2(input_ids, cache_position):\n-                return input_ids[:, -cache_position.shape[0] :]\n+                return input_ids[:, -cache_position.shape[0] :].clone()\n \n             def branch_3(input_ids, cache_position):\n-                return input_ids[:, cache_position]\n+                return input_ids[:, cache_position].clone()\n \n             inputs_embeds, input_ids = torch.cond(\n                 input_ids.shape[1] == 0,\n                 (\n                     lambda input_ids, inputs_embeds, cache_position: (\n                         branch_1(inputs_embeds, cache_position),\n-                        input_ids,\n+                        input_ids.clone(),\n                     )\n                 ),\n                 (\n@@ -534,7 +535,7 @@ def branch_3(input_ids, cache_position):\n                                 torch.cond(\n                                     input_ids.shape[1] != cache_position.shape[0],\n                                     branch_3,\n-                                    (lambda input_ids, cache_position: input_ids),\n+                                    (lambda input_ids, cache_position: input_ids.clone()),\n                                     [input_ids, cache_position],\n                                 )\n                             ),"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 6,
        "deletions": 5
    }
}