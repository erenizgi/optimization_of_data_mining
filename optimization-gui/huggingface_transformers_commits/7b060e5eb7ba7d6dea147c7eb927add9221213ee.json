{
    "author": "cyyever",
    "message": "Add missing arguments to class constructors (#40068)\n\n* Add missing arguments\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Fix typos\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "7b060e5eb7ba7d6dea147c7eb927add9221213ee",
    "files": [
        {
            "sha": "ca216bd9694933cbc7368352e8e605bd2e3f9d58",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -1091,7 +1091,7 @@ class AriaSharedExpertsMLP(LlamaMLP):\n     \"\"\"\n \n     def __init__(self, config: AriaTextConfig):\n-        super().__init__(self)\n+        super().__init__(config)\n         self.intermediate_size = config.intermediate_size * config.moe_num_shared_experts\n \n \n@@ -1255,8 +1255,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class AriaTextAttention(LlamaAttention):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: AriaTextConfig, layer_idx: int):\n-        super().__init__()\n+    pass\n \n \n class AriaTextDecoderLayer(LlamaDecoderLayer):\n@@ -1273,7 +1272,7 @@ class AriaTextDecoderLayer(LlamaDecoderLayer):\n     \"\"\"\n \n     def __init__(self, config: AriaTextConfig, layer_idx: int):\n-        super().__init__(self)\n+        super().__init__(config, layer_idx)\n         self.mlp = AriaTextMoELayer(config)\n \n \n@@ -1306,7 +1305,7 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(module)\n+        LlamaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, AriaProjector):\n             nn.init.trunc_normal_(module.query, std=self.config.initializer_range)\n "
        },
        {
            "sha": "ea72b603f60f2d1dfdbbf0915e7adef1be710cd6",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -275,7 +275,7 @@ class Cohere2Attention(CohereAttention, nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)"
        },
        {
            "sha": "ff93462c5b811b27fefc0c998abde1352ffd7e07",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -114,7 +114,7 @@ def forward(self, hidden_states):\n \n class Data2VecAudioFeatureEncoder(Wav2Vec2FeatureEncoder, nn.Module):\n     def __init__(self, config):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.conv_layers = nn.ModuleList(\n             [Data2VecAudioConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)]\n         )"
        },
        {
            "sha": "d031c08af69c8a0d2fecfc243df81b346a6a2e97",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -507,7 +507,7 @@ class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(module)\n+        LlamaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV2MoEGate):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n "
        },
        {
            "sha": "e7a3b68a54429fafa983fac1030a98ad6908e3e0",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -344,7 +344,7 @@ class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(module)\n+        LlamaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV3TopkRouter):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n "
        },
        {
            "sha": "669390437de3d9ae594db3d327528b549e1c343f",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -111,7 +111,7 @@ class DiaSelfAttention(LlamaAttention, nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx: int, is_causal: bool = False):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n         self.layer_idx = layer_idx\n         self.hidden_size = config.hidden_size"
        },
        {
            "sha": "1b8536833350b8d6acbd52376577e92f2c5c0b8f",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -408,7 +408,7 @@ class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_attention_backend = False\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(module)\n+        LlamaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, DiffLlamaAttention):\n             module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n             module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)"
        },
        {
            "sha": "41a0104f9425cb632d46fde32cc8b57710f26340",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -576,7 +576,7 @@ class DogePreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        LlamaPreTrainedModel._init_weights(module)\n+        LlamaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n                 module.A.data.zero_()"
        },
        {
            "sha": "187bd58567c9d6f1fb61003a5701c36634be9461",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -224,7 +224,7 @@ class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        MixtralPreTrainedModel._init_weights(module)\n+        MixtralPreTrainedModel._init_weights(self, module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n             module.e_score_correction_bias.data.zero_()\n "
        },
        {
            "sha": "c91f481574515cbcce55be107b05b1d8e7e1b0b1",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -129,7 +129,7 @@ def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch\n \n class EvollaSaProtSelfAttention(EsmSelfAttention, nn.Module):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n \n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n@@ -781,7 +781,7 @@ class EvollaPreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n-        LlamaPreTrainedModel._init_weights(module)\n+        LlamaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, EvollaSequenceAlignerCrossAttention):\n             module.gate_attention.zero_()\n             module.gate_ffw.zero_()"
        },
        {
            "sha": "5c07d3012ea6766c22b0dade9c98004f4b416a78",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -530,7 +530,7 @@ class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     ]\n \n     def _init_weights(self, module):\n-        Gemma2PreTrainedModel._init_weights(module)\n+        Gemma2PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3MultiModalProjector):\n             module.mm_input_projection_weight.data.zero_()\n "
        },
        {
            "sha": "dffeba69eea1dfeb9c983f3c270b892bff859c08",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -1920,7 +1920,7 @@ class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n     def _init_weights(self, module):\n-        Gemma2PreTrainedModel._init_weights(module)\n+        Gemma2PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, Gemma3nAudioAttention):"
        },
        {
            "sha": "cf157ad9b26aba0b6afd3e856b535baf1232c031",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -257,7 +257,7 @@ def __init__(\n \n class Glm4MoeAttention(CohereAttention, nn.Module):\n     def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -289,7 +289,7 @@ class Glm4MoeMLP(DeepseekV3MLP):\n \n class Glm4MoeTopkRouter(DeepseekV3TopkRouter, nn.Module):\n     def __init__(self, config: Glm4MoeConfig):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n         self.top_k = config.num_experts_per_tok\n         self.n_routed_experts = config.n_routed_experts"
        },
        {
            "sha": "f285ef62f3890d505c1d9570ba3de537499b0e8f",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -291,7 +291,7 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n     _supports_flex_attn = False\n \n     def _init_weights(self, module):\n-        LlavaPreTrainedModel._init_weights(module)\n+        LlavaPreTrainedModel._init_weights(self, module)\n         if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n                 module.rel_pos_h.data.zero_()"
        },
        {
            "sha": "a81b7d0be3fc848ec77fa128def1dad995ed9d0e",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -584,7 +584,7 @@ def _init_weights(self, module):\n \n class Phi4MultimodalVisionEmbeddings(SiglipVisionEmbeddings, nn.Module):\n     def __init__(self, config: Phi4MultimodalVisionConfig):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n         self.patch_size = config.patch_size\n         self.num_patches_per_side = config.image_size // self.patch_size\n@@ -1449,7 +1449,7 @@ class Phi4MultimodalRotaryEmbedding(Phi3RotaryEmbedding):\n \n class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n     def _init_weights(self, module):\n-        Phi3PreTrainedModel._init_weights(module)\n+        Phi3PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Phi4MultimodalImageEmbedding):\n             module.global_img_feature_extensor.data.zero_()\n             module.sub_img_feature_extensor.data.zero_()"
        },
        {
            "sha": "b4555ef927e3b6fd30b485350d94c47d943510ca",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -131,7 +131,7 @@ def extra_repr(self):\n \n class Qwen2DecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: Qwen2Config, layer_idx: int):\n-        super().__init__()\n+        super().__init__(config=config, layer_idx=layer_idx)\n         self.attention_type = config.layer_types[layer_idx]\n \n "
        },
        {
            "sha": "e65d0597f197ba195981e5fc060f4d90c5b11059",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -2063,7 +2063,7 @@ def __init__(self, config: Qwen2_5OmniThinkerConfig, device=None):\n # Removes the value error as a workaround.\n class Qwen2_5OmniAttention(Qwen2_5_VLAttention, nn.Module):\n     def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = None):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         self.config = config\n         self.layer_idx = layer_idx\n         if layer_idx is None:"
        },
        {
            "sha": "9beca2934ed5181e81370f8766d9df438daa6022",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -142,7 +142,7 @@ def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> N\n \n class Qwen2_5_VLVisionAttention(VisionAttention):\n     def __init__(self, config: Qwen2_5_VLVisionConfig) -> None:\n-        super().__init__()\n+        super().__init__(config)\n         self.dim = config.hidden_size\n \n "
        },
        {
            "sha": "a1baf7de87670f914441ec50941c9f799b238eb6",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -72,7 +72,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n \n class Starcoder2Attention(MistralAttention):\n     def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n-        super().__init__()\n+        super().__init__(config=config, layer_idx=layer_idx)\n         self.residual_dropout = config.residual_dropout\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.use_bias)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n@@ -131,7 +131,7 @@ def forward(\n \n class Starcoder2DecoderLayer(MistralDecoderLayer):\n     def __init__(self, config: Starcoder2Config, layer_idx: int):\n-        super().__init__(self)\n+        super().__init__(config, layer_idx)\n         self.self_attn = Starcoder2Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Starcoder2MLP(config)\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)"
        },
        {
            "sha": "ebddaa5c4521b8a37071d1e38607d80f89466846",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -488,7 +488,7 @@ class T5GemmaPreTrainedModel(Gemma2PreTrainedModel):\n \n     def _init_weights(self, module):\n         # TODO: support intialization for encoders and decoders separately(?)\n-        Gemma2PreTrainedModel._init_weights(module)\n+        Gemma2PreTrainedModel._init_weights(self, module)\n         std = self.config.initializer_range\n         if isinstance(module, T5GemmaClassificationHead):\n             scale = module.out_proj.weight.shape[0] ** -0.5"
        },
        {
            "sha": "c78e5bf55072473f26a9d2cb440a8a17168310a1",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -66,7 +66,7 @@ def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Ten\n \n class Wav2Vec2BertRotaryPositionalEmbedding(Wav2Vec2ConformerRotaryPositionalEmbedding, nn.Module):\n     def __init__(self, config):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         dim = config.hidden_size // config.num_attention_heads\n         base = config.rotary_embedding_base\n \n@@ -98,7 +98,7 @@ def forward(self, hidden_states):\n \n class Wav2Vec2BertFeedForward(Wav2Vec2FeedForward, nn.Module):\n     def __init__(self, config, act_fn=None, hidden_size=None):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         act_fn = act_fn if act_fn is not None else config.hidden_act\n         hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n         self.intermediate_dropout = nn.Dropout(config.activation_dropout)\n@@ -188,7 +188,7 @@ class Wav2Vec2BertSelfAttention(Wav2Vec2ConformerSelfAttention, nn.Module):\n     \"\"\"\n \n     def __init__(self, config, is_adapter_attention=False):\n-        nn.Module.__init__()\n+        nn.Module.__init__(self)\n         hidden_size = config.hidden_size if not is_adapter_attention else config.output_hidden_size\n \n         self.head_size = hidden_size // config.num_attention_heads"
        },
        {
            "sha": "a10e0a507ef410de5042845ee8a5687665355008",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n import enum\n-from collections.abc import Iterable  # pylint: disable=g-importing-member\n+from collections.abc import Iterable\n from typing import Any, Optional, Union, overload\n \n from ..generation import GenerationConfig"
        },
        {
            "sha": "da579423d2d4f92944ab0bb7430dfdddb190f5ac",
            "filename": "src/transformers/pipelines/table_question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b060e5eb7ba7d6dea147c7eb927add9221213ee/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py?ref=7b060e5eb7ba7d6dea147c7eb927add9221213ee",
            "patch": "@@ -450,7 +450,7 @@ def postprocess(self, model_outputs):\n \n                 answers.append(answer)\n             if len(answer) == 0:\n-                raise PipelineException(\"Empty answer\")\n+                raise PipelineException(\"Table question answering\", self.model.name_or_path, \"Empty answer\")\n         else:\n             answers = [{\"answer\": answer} for answer in self.tokenizer.batch_decode(outputs, skip_special_tokens=True)]\n "
        }
    ],
    "stats": {
        "total": 65,
        "additions": 32,
        "deletions": 33
    }
}