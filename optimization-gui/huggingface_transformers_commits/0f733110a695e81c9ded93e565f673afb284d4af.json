{
    "author": "zucchini-nlp",
    "message": "Support `return_tensors` in audio chat templates (#34601)\n\n* add audio chat templates\n\n* update\n\n* update\n\n* nit\n\n* green ci\n\n* we dont care about the order anymore\n\n* clean up after rebase\n\n* overriden tests rename\n\n* rename shieldgemma also\n\n* one more rename\n\n* require_read_token\n\n* removde images/videos\n\n* retrigger CI flaky",
    "sha": "0f733110a695e81c9ded93e565f673afb284d4af",
    "files": [
        {
            "sha": "610c1e36d4bad3b1ac262e6e927e4806324863a0",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -16,10 +16,52 @@\n and remove unnecessary dependencies.\n \"\"\"\n \n+import os\n import warnings\n+from io import BytesIO\n from typing import List, Optional, Tuple, Union\n \n import numpy as np\n+import requests\n+\n+from .utils import is_librosa_available, requires_backends\n+\n+\n+if is_librosa_available():\n+    import librosa\n+\n+\n+def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:\n+    \"\"\"\n+    Loads `audio` to an np.ndarray object.\n+\n+    Args:\n+        audio (`str` or `np.ndarray`):\n+            The audio to be laoded to the numpy array format.\n+        sampling_rate (`int`, *optional*, defaults to 16000):\n+            The samlping rate to be used when loading the audio. It should be same as the\n+            sampling rate the model you will be using further was trained with.\n+        timeout (`float`, *optional*):\n+            The timeout value in seconds for the URL request.\n+\n+    Returns:\n+        `np.ndarray`: A numpy artay representing the audio.\n+    \"\"\"\n+    requires_backends(load_audio, [\"librosa\"])\n+\n+    if isinstance(audio, str):\n+        # Load audio from URL (e.g https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav)\n+        if audio.startswith(\"http://\") or audio.startswith(\"https://\"):\n+            audio = librosa.load(BytesIO(requests.get(audio, timeout=timeout).content), sr=sampling_rate)[0]\n+        elif os.path.isfile(audio):\n+            audio = librosa.load(audio, sr=sampling_rate)[0]\n+    elif isinstance(audio, np.ndarray):\n+        audio = audio\n+    else:\n+        raise TypeError(\n+            \"Incorrect format used for `audio`. Should be an url linking to an audio, a local path, or numpy array.\"\n+        )\n+    return audio\n \n \n AudioInput = Union["
        },
        {
            "sha": "9e65c6d6634a70ba3e75d06d915e428c0a727b8d",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 55,
            "deletions": 37,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -16,13 +16,24 @@\n Processor class for Qwen2Audio.\n \"\"\"\n \n-from typing import List, Optional, Union\n+import warnings\n+from typing import List, Union\n \n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils.deprecation import deprecate_kwarg\n+\n+\n+class Qwen2AudioProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"audio_kwargs\": {},\n+    }\n \n \n class Qwen2AudioProcessor(ProcessorMixin):\n@@ -49,6 +60,7 @@ class Qwen2AudioProcessor(ProcessorMixin):\n     \"\"\"\n \n     attributes = [\"feature_extractor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"audio_token\", \"audio_bos_token\", \"audio_eos_token\"]\n     feature_extractor_class = \"WhisperFeatureExtractor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n@@ -68,13 +80,13 @@ def __init__(\n         self.audio_eos_token = tokenizer.audio_eos_token if hasattr(tokenizer, \"audio_eos_token\") else audio_eos_token\n         super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n \n+    @deprecate_kwarg(\"audios\", version=\"4.54.0\", new_name=\"audio\")\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        audios: Union[np.ndarray, List[np.ndarray]] = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        sampling_rate: Optional[int] = None,\n-        **kwargs,\n+        audio: Union[np.ndarray, List[np.ndarray]] = None,\n+        audios=None,  # kept for BC\n+        **kwargs: Unpack[Qwen2AudioProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n@@ -88,43 +100,48 @@ def __call__(\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            audios (`np.ndarray`, `List[np.ndarray]`):\n+            audio (`np.ndarray`, `List[np.ndarray]`):\n                 The audio or batch of audios to be prepared. Each audio can be a NumPy array.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            sampling_rate (`int`, defaults to 16000):\n-                The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n         \"\"\"\n \n+        # Handle BC when user passes deprecared keyword argument\n+        if audios is not None and audio is None:\n+            audio = audios\n+            warnings.wanr(\n+                \"You may have used the keyword argument for the `audio` inputs. It is strongly recommended to pass inputs with keyword arguments \"\n+                \"with keys `audio` and `text`. From transformers v4.55 `audio` will be the onle acceptable keyword argument.\",\n+                FutureWarning,\n+            )\n+\n         if text is None:\n-            raise ValueError(\"You need to specify either a `text` input to process.\")\n+            raise ValueError(\"You need to specify `text` input to process.\")\n         elif isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-        # ensure we have as much audios as audio tokens\n-        num_audio_tokens = sum(sample.count(self.audio_token) for sample in text)\n-        num_audios = 1 if isinstance(audios, np.ndarray) else len(audios)\n-        if num_audio_tokens != num_audios:\n-            raise ValueError(\n-                f\"Found {num_audio_tokens} {self.audio_token} token{'s' if num_audio_tokens > 1 else ''} in provided text but received {num_audios} audio{'s' if num_audios > 1 else ''}\"\n-            )\n+        output_kwargs = self._merge_kwargs(\n+            Qwen2AudioProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n \n-        if audios is not None:\n-            audio_inputs = self.feature_extractor(\n-                audios, sampling_rate=sampling_rate, return_attention_mask=True, padding=\"max_length\", **kwargs\n-            )\n-            audio_inputs[\"feature_attention_mask\"] = audio_inputs.pop(\n-                \"attention_mask\"\n-            )  # rename attention_mask to prevent conflicts later on\n+        if audio is not None:\n+            # ensure we have as much audios as audio tokens\n+            num_audio_tokens = sum(sample.count(self.audio_token) for sample in text)\n+            num_audios = 1 if type(audio) == np.ndarray else len(audio)\n+            if num_audio_tokens != num_audios:\n+                raise ValueError(\n+                    f\"Found {num_audio_tokens} {self.audio_token} token{'s' if num_audio_tokens > 1 else ''} in provided text but received {num_audios} audio{'s' if num_audios > 1 else ''}\"\n+                )\n+\n+            # Some kwargs should not be changed so we can expand text with audio tokens below\n+            output_kwargs[\"audio_kwargs\"][\"return_attention_mask\"] = True\n+            output_kwargs[\"audio_kwargs\"][\"padding\"] = \"max_length\"\n+            audio_inputs = self.feature_extractor(audio, **output_kwargs[\"audio_kwargs\"])\n+\n+            # rename attention_mask to prevent conflicts later on\n+            audio_inputs[\"feature_attention_mask\"] = audio_inputs.pop(\"attention_mask\")\n \n             expanded_text = []\n             audio_lengths = audio_inputs[\"feature_attention_mask\"].sum(-1).tolist()\n@@ -162,9 +179,9 @@ def __call__(\n                 expanded_text.append(sample)\n             text = expanded_text\n \n-        inputs = self.tokenizer(text, padding=padding, **kwargs)\n+        inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n-        if audios is not None:\n+        if audio is not None:\n             inputs.update(audio_inputs)\n \n         return BatchFeature(data={**inputs})\n@@ -190,6 +207,7 @@ def model_input_names(self):\n         return list(dict.fromkeys(tokenizer_input_names + feature_extractor_input_names + [\"feature_attention_mask\"]))\n \n     @property\n+    # NOTE: we don't have default templates anymore, and the below is kept only because the hub config is not yet updated!\n     def default_chat_template(self):\n         \"\"\"\n         This default vicuna template formats inputs in the form of a chat history. For each message in the chat history:\n@@ -228,7 +246,7 @@ def default_chat_template(self):\n                     \"{{ message['content'] }}<|im_end|>\\n\"\n                 \"{% else %}\"\n                     \"{% for content in message['content'] %}\"\n-                        \"{% if 'audio' in content or 'audio_url' in content %}\"\n+                        \"{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' %}\"\n                             \"{% set audio_count.value = audio_count.value + 1 %}\"\n                             \"Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n\"\n                         \"{% elif 'text' in content %}\""
        },
        {
            "sha": "7551714773ef53ab13dd80eef6c7d308673c9058",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -28,6 +28,7 @@\n import numpy as np\n import typing_extensions\n \n+from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n from .image_utils import (\n     ChannelDimension,\n@@ -419,6 +420,7 @@ def sample_indices_fn(num_frames, fps, metadata, **kwargs):\n     num_frames: Optional[int] = None\n     video_load_backend: Optional[str] = \"pyav\"\n     video_fps: Optional[int] = None\n+    sampling_rate: Optional[int] = 16_000\n     sample_indices_fn: Optional[Callable] = None\n \n \n@@ -938,6 +940,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n             \"common_kwargs\": {},\n         }\n \n+        possible_modality_keywords = {\"text\", \"audio\", \"videos\", \"images\"}\n         used_keys = set()\n \n         # get defaults from set model processor kwargs if they exist\n@@ -995,7 +998,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                 if key not in used_keys:\n                     if key in ModelProcessorKwargs.__annotations__[\"common_kwargs\"].__annotations__.keys():\n                         output_kwargs[\"common_kwargs\"][key] = kwargs[key]\n-                    else:\n+                    elif key not in possible_modality_keywords:\n                         logger.warning_once(\n                             f\"Keyword argument `{key}` is not a valid argument for this processor and will be ignored.\"\n                         )\n@@ -1336,15 +1339,23 @@ def apply_chat_template(\n         tokenize = chat_template_kwargs.get(\"tokenize\")\n         return_dict = chat_template_kwargs.get(\"return_dict\")\n         sample_indices_fn = chat_template_kwargs.get(\"sample_indices_fn\")\n+        sampling_rate = chat_template_kwargs.pop(\"sampling_rate\")\n \n         if tokenize:\n             batch_images, batch_videos = [], []\n+            batch_audios = []\n             batch_video_metadata = []\n             for conversation in conversations:\n                 images, videos = [], []\n                 video_metadata = []\n                 for message in conversation:\n                     visuals = [content for content in message[\"content\"] if content[\"type\"] in [\"image\", \"video\"]]\n+                    audio_fnames = [\n+                        content[key]\n+                        for content in message[\"content\"]\n+                        for key in [\"audio\", \"url\", \"path\"]\n+                        if key in content and content[\"type\"] == \"audio\"\n+                    ]\n                     image_fnames = [\n                         vision_info[key]\n                         for vision_info in visuals\n@@ -1357,6 +1368,10 @@ def apply_chat_template(\n                         for key in [\"video\", \"url\", \"path\"]\n                         if key in vision_info and vision_info[\"type\"] == \"video\"\n                     ]\n+\n+                    # Audio models do not accept nested list of audios (yet!)\n+                    for fname in audio_fnames:\n+                        batch_audios.append(load_audio(fname, sampling_rate=sampling_rate))\n                     for fname in image_fnames:\n                         images.append(load_image(fname))\n                     for fname in video_fnames:\n@@ -1423,6 +1438,7 @@ def apply_chat_template(\n                 text=prompt,\n                 images=batch_images if batch_images else None,\n                 videos=batch_videos if batch_videos else None,\n+                audios=batch_audios if batch_audios else None,\n                 **kwargs,\n             )\n             if return_dict:"
        },
        {
            "sha": "679d2467b4920c40c193559933a27306bc3e8e1f",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -238,7 +238,7 @@ def test_apply_chat_template(self):\n         self.assertEqual(rendered, expected_rendered)\n \n     # Override as AriaImageProcessor doesn't accept `do_rescale`\n-    def test_chat_template_accepts_processing_kwargs(self):\n+    def test_image_chat_template_accepts_processing_kwargs(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")"
        },
        {
            "sha": "315257249115fb0860bbcb6037f98b0b61b97d0d",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -116,7 +116,7 @@ def test_model_input_names(self):\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n \n-    def test_chat_template_single(self):\n+    def test_image_chat_template_single(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n@@ -154,7 +154,7 @@ def test_chat_template_single(self):\n         self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n         self.assertEqual(len(out_dict[self.images_input_name]), 71280)\n \n-    def test_chat_template_batched(self):\n+    def test_image_chat_template_batched(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")"
        },
        {
            "sha": "409d47e8866a1e6de720f3307d036afacb5e46c7",
            "filename": "tests/models/qwen2_audio/test_processor_qwen2_audio.py",
            "status": "modified",
            "additions": 72,
            "deletions": 3,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -11,20 +11,63 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import shutil\n import tempfile\n import unittest\n+from typing import Optional\n \n from transformers import AutoProcessor, AutoTokenizer, Qwen2AudioProcessor, WhisperFeatureExtractor\n from transformers.testing_utils import require_torch, require_torchaudio\n+from transformers.utils import is_torch_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available:\n+    pass\n \n \n @require_torch\n @require_torchaudio\n-class Qwen2AudioProcessorTest(unittest.TestCase):\n+class Qwen2AudioProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Qwen2AudioProcessor\n+\n     def setUp(self):\n         self.checkpoint = \"Qwen/Qwen2-Audio-7B-Instruct\"\n         self.tmpdirname = tempfile.mkdtemp()\n \n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = Qwen2AudioProcessor.from_pretrained(self.checkpoint, **processor_kwargs)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_audio_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).audio_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_processor_dict(self):\n+        return {\n+            \"chat_template\": \"{% set audio_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' %}{% set audio_count.value = audio_count.value + 1 %}Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\n+        }\n+\n+    # Override as Qwen2AudioProcessor needs audio tokens in prompts\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer <|AUDIO|>\"\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [\"lower newer <|AUDIO|>\"]\n+        return [\"lower newer <|AUDIO|>\", \"<|AUDIO|> upper older longer string\"] + [\"<|AUDIO|> lower newer\"] * (\n+            batch_size - 2\n+        )\n+\n     def test_can_load_various_tokenizers(self):\n         processor = Qwen2AudioProcessor.from_pretrained(self.checkpoint)\n         tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n@@ -77,7 +120,7 @@ def test_tokenizer_integration(self):\n             \"assistant\",\n             \"ÄŠ\",\n         ]\n-        print(slow_tokenizer.tokenize(prompt))\n+\n         self.assertEqual(slow_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n         self.assertEqual(fast_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n \n@@ -110,5 +153,31 @@ def test_chat_template(self):\n             },\n         ]\n \n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        formatted_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    def test_chat_template_with_continue_final_message(self):\n+        processor = AutoProcessor.from_pretrained(self.checkpoint)\n+        expected_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\\nWhat's that sound?<|im_end|>\\n<|im_start|>assistant\\nIt is the sound of \"  # fmt: skip\n+        messages = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What's that sound?\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of \"}],\n+            },\n+        ]\n+        prompt = processor.apply_chat_template(messages, continue_final_message=True)\n+        self.assertEqual(expected_prompt, prompt)"
        },
        {
            "sha": "86e1c740eee64db2e86bad1596535eb7b73ed6fe",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -113,7 +113,7 @@ def test_model_input_names(self):\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n \n-    def test_chat_template_single(self):\n+    def test_image_chat_template_single(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n@@ -151,7 +151,7 @@ def test_chat_template_single(self):\n         self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n         self.assertEqual(len(out_dict[self.images_input_name]), 71280)\n \n-    def test_chat_template_batched(self):\n+    def test_image_chat_template_batched(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")"
        },
        {
            "sha": "a1c087be1fdecd7323cd0307547079179967446b",
            "filename": "tests/models/shieldgemma2/test_processing_shieldgemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -166,22 +166,22 @@ def test_with_multiple_images(self):\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n     @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_chat_template_accepts_processing_kwargs(self):\n+    def test_image_chat_template_accepts_processing_kwargs(self):\n         pass\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n     @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_chat_template_batched(self):\n+    def test_image_chat_template_batched(self):\n         pass\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n     @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_chat_template_dict_torch(self):\n+    def test_image_chat_template_dict_torch(self):\n         pass\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n     @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_chat_template_single(self):\n+    def test_image_chat_template_single(self):\n         pass\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2"
        },
        {
            "sha": "2d0e624d3722176c55d1406a8e155ba22b2d2e67",
            "filename": "tests/models/wav2vec2/test_processor_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fwav2vec2%2Ftest_processor_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fwav2vec2%2Ftest_processor_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_processor_wav2vec2.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -18,8 +18,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n-\n from transformers.models.wav2vec2 import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES\n from transformers.utils import FEATURE_EXTRACTOR_NAME\n@@ -30,6 +28,8 @@\n \n class Wav2Vec2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Wav2Vec2Processor\n+    audio_input_name = \"input_values\"\n+    text_input_name = \"labels\"\n \n     def setUp(self):\n         vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n@@ -132,22 +132,6 @@ def test_tokenizer(self):\n         for key in encoded_tok.keys():\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n-    def test_padding_argument_not_ignored(self):\n-        # padding, or any other overlap arg between audio extractor and tokenizer\n-        # should be passed to both text and audio and not ignored\n-\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Wav2Vec2Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-        batch_duration_in_seconds = [1, 3, 2, 6]\n-        input_features = [np.random.random(16_000 * s) for s in batch_duration_in_seconds]\n-\n-        # padding = True should not raise an error and will if the audio processor popped its value to None\n-        _ = processor(\n-            input_features, padding=True, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\"\n-        )\n-\n     def test_tokenizer_decode(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "6e98c434dea2c86c1f3b647906da2e6b105cdd4a",
            "filename": "tests/models/wav2vec2_bert/test_processor_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processor_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processor_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processor_wav2vec2_bert.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -18,8 +18,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n-\n from transformers.models.seamless_m4t import SeamlessM4TFeatureExtractor\n from transformers.models.wav2vec2 import Wav2Vec2CTCTokenizer\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES\n@@ -32,6 +30,7 @@\n \n class Wav2Vec2BertProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Wav2Vec2BertProcessor\n+    text_input_name = \"labels\"\n \n     def setUp(self):\n         vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n@@ -136,22 +135,6 @@ def test_tokenizer(self):\n         for key in encoded_tok.keys():\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n-    def test_padding_argument_not_ignored(self):\n-        # padding, or any other overlap arg between audio extractor and tokenizer\n-        # should be passed to both text and audio and not ignored\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Wav2Vec2BertProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-        batch_duration_in_seconds = [1, 3, 2, 6]\n-        input_features = [np.random.random(16_000 * s) for s in batch_duration_in_seconds]\n-\n-        # padding = True should not raise an error and will if the audio processor popped its value to None\n-        # processor(input_features, padding=True, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\")\n-        _ = processor(\n-            input_features, padding=True, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\"\n-        )\n-\n     def test_tokenizer_decode(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "0485fdf97d1c5c94570218f6d22109b57dba25db",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 205,
            "deletions": 74,
            "changes": 279,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f733110a695e81c9ded93e565f673afb284d4af/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=0f733110a695e81c9ded93e565f673afb284d4af",
            "patch": "@@ -29,6 +29,7 @@\n from transformers.testing_utils import (\n     check_json_file_has_correct_format,\n     require_av,\n+    require_librosa,\n     require_torch,\n     require_vision,\n )\n@@ -73,6 +74,7 @@ class ProcessorTesterMixin:\n     text_input_name = \"input_ids\"\n     images_input_name = \"pixel_values\"\n     videos_input_name = \"pixel_values_videos\"\n+    audio_input_name = \"input_features\"\n \n     def prepare_processor_dict(self):\n         return {}\n@@ -105,6 +107,8 @@ def get_processor(self):\n         processor = self.processor_class(**components, **self.prepare_processor_dict())\n         return processor\n \n+    # TODO: raushan unify all these special token LLMs under the general preparation. We can get audio/image token\n+    # from tokenizer, so we can generalize instead of overriding\n     def prepare_text_inputs(self, batch_size: Optional[int] = None):\n         if batch_size is None:\n             return \"lower newer\"\n@@ -363,101 +367,83 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n-    #  text + audio kwargs testing\n+    # text + audio kwargs testing\n     @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n         feature_extractor = self.get_component(\"feature_extractor\")\n-        if hasattr(self, \"get_tokenizer\"):\n-            tokenizer = self.get_tokenizer(max_length=117, padding=\"max_length\")\n-        elif hasattr(self, \"get_component\"):\n-            tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        else:\n-            self.assertTrue(False, \"Processor doesn't have get_tokenizer or get_component defined\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        tokenizer = self.get_component(\"tokenizer\", max_length=300, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+\n+        input_str = self.prepare_text_inputs(batch_size=3)\n         raw_speech = floats_list((3, 1000))\n+        raw_speech = [np.asarray(audio) for audio in raw_speech]\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\")\n-        if \"input_ids\" in inputs:\n-            self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n-        elif \"labels\" in inputs:\n-            self.assertEqual(len(inputs[\"labels\"][0]), 117)\n+        self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n \n     @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n         feature_extractor = self.get_component(\"feature_extractor\")\n-        if hasattr(self, \"get_tokenizer\"):\n-            tokenizer = self.get_tokenizer(max_length=117)\n-        elif hasattr(self, \"get_component\"):\n-            tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+\n+        input_str = self.prepare_text_inputs(batch_size=3)\n         raw_speech = floats_list((3, 1000))\n-        inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=112, padding=\"max_length\")\n-        if \"input_ids\" in inputs:\n-            self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n-        elif \"labels\" in inputs:\n-            self.assertEqual(len(inputs[\"labels\"][0]), 112)\n+        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n+\n+        self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n \n     @require_torch\n     def test_unstructured_kwargs_audio(self):\n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n         feature_extractor = self.get_component(\"feature_extractor\")\n-        if hasattr(self, \"get_tokenizer\"):\n-            tokenizer = self.get_tokenizer(max_length=117)\n-        elif hasattr(self, \"get_component\"):\n-            tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        tokenizer = self.get_component(\"tokenizer\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs(batch_size=3)\n         raw_speech = floats_list((3, 1000))\n-        inputs = processor(\n-            text=input_str,\n-            audio=raw_speech,\n-            return_tensors=\"pt\",\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n+        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n \n-        if \"input_ids\" in inputs:\n-            self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-        elif \"labels\" in inputs:\n-            self.assertEqual(len(inputs[\"labels\"][0]), 76)\n+        self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n \n     @require_torch\n     def test_doubly_passed_kwargs_audio(self):\n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n         feature_extractor = self.get_component(\"feature_extractor\")\n-        if hasattr(self, \"get_tokenizer\"):\n-            tokenizer = self.get_tokenizer()\n-        elif hasattr(self, \"get_component\"):\n-            tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        tokenizer = self.get_component(\"tokenizer\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\"]\n+        input_str = self.prepare_text_inputs(batch_size=3)\n         raw_speech = floats_list((3, 1000))\n+        raw_speech = [np.asarray(audio) for audio in raw_speech]\n         with self.assertRaises(ValueError):\n             _ = processor(\n                 text=input_str,\n                 audio=raw_speech,\n-                audio_kwargs={\"padding\": \"max_length\"},\n+                text_kwargs={\"padding\": \"max_length\"},\n                 padding=\"max_length\",\n             )\n \n@@ -466,31 +452,27 @@ def test_doubly_passed_kwargs_audio(self):\n     def test_structured_kwargs_audio_nested(self):\n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n         feature_extractor = self.get_component(\"feature_extractor\")\n-        if hasattr(self, \"get_tokenizer\"):\n-            tokenizer = self.get_tokenizer()\n-        elif hasattr(self, \"get_component\"):\n-            tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\"]\n+        input_str = self.prepare_text_inputs(batch_size=3)\n         raw_speech = floats_list((3, 1000))\n+        raw_speech = [np.asarray(audio) for audio in raw_speech]\n \n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-            \"audio_kwargs\": {\"padding\": \"max_length\", \"max_length\": 66},\n+            \"audio_kwargs\": {\"padding\": \"max_length\", \"max_length\": 300},\n         }\n \n         inputs = processor(text=input_str, audio=raw_speech, **all_kwargs)\n-        if \"input_ids\" in inputs:\n-            self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-        elif \"labels\" in inputs:\n-            self.assertEqual(len(inputs[\"labels\"][0]), 76)\n+        self.assertEqual(len(inputs[self.text_input_name][0]), 76)\n \n     def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n         if \"video_processor\" not in self.processor_class.attributes:\n@@ -680,9 +662,10 @@ def test_structured_kwargs_nested_from_dict_video(self):\n \n     # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n     # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n-    def test_overlapping_text_kwargs_handling(self):\n+    def test_overlapping_text_image_kwargs_handling(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n@@ -699,6 +682,28 @@ def test_overlapping_text_kwargs_handling(self):\n                 text_kwargs={\"padding\": \"do_not_pad\"},\n             )\n \n+    def test_overlapping_text_audio_kwargs_handling(self):\n+        \"\"\"\n+        Checks that `padding`, or any other overlap arg between audio extractor and tokenizer\n+        is be passed to only text and ignored for audio for BC purposes\n+        \"\"\"\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n+        feature_extractor = self.get_component(\"feature_extractor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs(batch_size=3)\n+        audio_lengths = [4000, 8000, 16000, 32000]\n+        raw_speech = [np.asarray(audio)[:length] for audio, length in zip(floats_list((3, 32_000)), audio_lengths)]\n+\n+        # padding = True should not raise an error and will if the audio processor popped its value to None\n+        _ = processor(text=input_str, audio=raw_speech, padding=True, return_tensors=\"pt\")\n+\n     def test_prepare_and_validate_optional_call_args(self):\n         processor = self.get_processor()\n         optional_call_args_name = getattr(processor, \"optional_call_args\", [])\n@@ -752,11 +757,14 @@ def test_chat_template_save_loading(self):\n             # the reloaded tokenizer should get the chat template as well\n             self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)\n \n-    def test_chat_template_single(self):\n+    def test_image_chat_template_single(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n         messages = [\n             [\n                 {\n@@ -797,11 +805,14 @@ def test_chat_template_single(self):\n         self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n         self.assertEqual(len(out_dict[self.images_input_name]), 1)\n \n-    def test_chat_template_batched(self):\n+    def test_image_chat_template_batched(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n         batched_messages = [\n             [\n                 {\n@@ -864,11 +875,14 @@ def test_chat_template_batched(self):\n         self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n         self.assertEqual(len(out_dict[self.images_input_name]), 2)\n \n-    def test_chat_template_accepts_processing_kwargs(self):\n+    def test_image_chat_template_accepts_processing_kwargs(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n         messages = [\n             [\n                 {\n@@ -915,11 +929,14 @@ def test_chat_template_accepts_processing_kwargs(self):\n         self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)\n \n     @require_torch\n-    def test_chat_template_dict_torch(self):\n+    def test_image_chat_template_dict_torch(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -1171,3 +1188,117 @@ def _process_messages_for_chat_template(\n         self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 243)\n+\n+    @require_librosa\n+    def test_audio_chat_template_single(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n+        messages = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What's that sound?\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of glass shattering.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"How about this one?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template([messages], add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)  # batch size=1\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n+        )\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        messages[1][\"content\"][0][\"audio\"] = (\n+            \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n+        )\n+        messages[3][\"content\"][0][\"audio\"] = (\n+            \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n+        )\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertTrue(self.audio_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 1)  # batch-size=1\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)  # batch-size=1\n+        self.assertEqual(len(out_dict[self.audio_input_name]), 2)  # 2 audios in the conversation\n+\n+    @require_torch\n+    @require_librosa\n+    def test_audio_chat_template_dict_torch(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n+        messages = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What's that sound?\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of glass shattering.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"How about this one?\"},\n+                ],\n+            },\n+        ]\n+\n+        out_dict_tensors = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+\n+        self.assertTrue(self.audio_input_name in out_dict_tensors)\n+        for k in out_dict_tensors:\n+            self.assertIsInstance(out_dict_tensors[k], torch.Tensor)"
        }
    ],
    "stats": {
        "total": 563,
        "additions": 403,
        "deletions": 160
    }
}