{
    "author": "MekkCyber",
    "message": "[Quantization] torchao serialization (#43127)\n\nfix",
    "sha": "c8208185a738f5159d365d696a12cf740b678439",
    "files": [
        {
            "sha": "e3c263347b0b9bcb41ea62974fc33bfc64ef081f",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8208185a738f5159d365d696a12cf740b678439/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8208185a738f5159d365d696a12cf740b678439/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=c8208185a738f5159d365d696a12cf740b678439",
            "patch": "@@ -146,7 +146,7 @@ def get_state_dict_and_metadata(self, model):\n         We flatten the state dict of tensor subclasses so that it is compatible with the safetensors format.\n         \"\"\"\n         if TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n-            return flatten_tensor_state_dict(model.state_dict()), {}\n+            return flatten_tensor_state_dict(model.state_dict())\n         else:\n             raise RuntimeError(\n                 f\"In order to use safetensors with torchao, please use torchao version >= 0.15.0. Current version: {TORCHAO_VERSION}\""
        },
        {
            "sha": "8745da5a2b7ee47b77880c2f3f84c0210a8179ce",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8208185a738f5159d365d696a12cf740b678439/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8208185a738f5159d365d696a12cf740b678439/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=c8208185a738f5159d365d696a12cf740b678439",
            "patch": "@@ -529,7 +529,7 @@ def test_fqn_to_config_non_weight_param(self):\n         quant_config = TorchAoConfig(quant_type=config)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             \"jcaip/Llama-4-Scout-17B-two-layers-only-testing\",\n-            device_map=\"auto\",\n+            device_map=\"cuda\",\n             dtype=torch.bfloat16,\n             quantization_config=quant_config,\n         )"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}