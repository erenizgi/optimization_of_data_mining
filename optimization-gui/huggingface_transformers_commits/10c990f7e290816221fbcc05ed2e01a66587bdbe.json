{
    "author": "jiqing-feng",
    "message": "enable triton backend on awq xpu (#39443)\n\n* enable triton backend on awq xpu\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* Update src/transformers/quantizers/quantizer_awq.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* fix dtype check\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix check\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "10c990f7e290816221fbcc05ed2e01a66587bdbe",
    "files": [
        {
            "sha": "8146c99678834a1024c2eea046ef0a59b38e3aae",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/10c990f7e290816221fbcc05ed2e01a66587bdbe/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10c990f7e290816221fbcc05ed2e01a66587bdbe/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=10c990f7e290816221fbcc05ed2e01a66587bdbe",
            "patch": "@@ -52,8 +52,12 @@ def validate_environment(self, device_map, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n \n-        if self.quantization_config.version == AWQLinearVersion.GEMM and not torch.cuda.is_available():\n-            logger.warning_once(\"No CUDA found, replace GEMM with IPEX version to support non-cuda AWQ model.\")\n+        if (\n+            self.quantization_config.version == AWQLinearVersion.GEMM\n+            and not torch.cuda.is_available()\n+            and not torch.xpu.is_available()\n+        ):\n+            logger.warning_once(\"No CUDA or XPU found, consider switching to the IPEX version for CPU-only execution.\")\n             self.quantization_config.version = AWQLinearVersion.IPEX\n \n         if self.quantization_config.version == AWQLinearVersion.IPEX:\n@@ -71,14 +75,14 @@ def validate_environment(self, device_map, **kwargs):\n                     \" This is not supported. Please make sure only cpu and xpu in the device_map.\"\n                 )\n         else:\n-            if not torch.cuda.is_available():\n+            if not torch.cuda.is_available() and not torch.xpu.is_available():\n                 raise RuntimeError(\n                     \"GPU is required to run AWQ quantized model. You can use IPEX version AWQ if you have an Intel CPU\"\n                 )\n \n             if device_map is None:\n                 logger.warning_once(\n-                    \"You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set \"\n+                    \"You have loaded an AWQ model on CPU and have a CUDA/XPU device available, make sure to set \"\n                     \"your model on a GPU device in order to run your model.\"\n                 )\n             elif device_map is not None:\n@@ -94,11 +98,15 @@ def update_torch_dtype(self, torch_dtype):\n         if torch_dtype is None:\n             torch_dtype = torch.float16\n             logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `torch_dtype` manually.\")\n-        elif torch_dtype == torch.bfloat16 and torch.cuda.is_available():\n-            logger.warning(\"`torch.bfloat16` is not supported for AWQ CUDA kernels yet. Casting to `torch.float16`.\")\n+        elif torch_dtype == torch.bfloat16 and (torch.cuda.is_available() or torch.xpu.is_available()):\n+            logger.warning(\n+                \"`torch.bfloat16` is not supported for AWQ CUDA/XPU kernels yet. Casting to `torch.float16`.\"\n+            )\n             torch_dtype = torch.float16\n-        elif torch_dtype != torch.float16 and torch.cuda.is_available():\n-            logger.warning(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency on CUDA with AWQ.\")\n+        elif torch_dtype != torch.float16 and (torch.cuda.is_available() or torch.xpu.is_available()):\n+            logger.warning(\n+                \"We suggest you to set `torch_dtype=torch.float16` for better efficiency on CUDA/XPU with AWQ.\"\n+            )\n         return torch_dtype\n \n     def _process_model_before_weight_loading("
        }
    ],
    "stats": {
        "total": 24,
        "additions": 16,
        "deletions": 8
    }
}