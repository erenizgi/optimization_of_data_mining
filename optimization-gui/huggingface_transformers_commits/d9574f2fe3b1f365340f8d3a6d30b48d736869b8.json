{
    "author": "sbucaille",
    "message": "docs: update SuperGlue docs (#39406)\n\n* docs: update SuperGlue docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "d9574f2fe3b1f365340f8d3a6d30b48d736869b8",
    "files": [
        {
            "sha": "e4f1c8931eb3272b0138379044a85c86dd785953",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 106,
            "deletions": 80,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9574f2fe3b1f365340f8d3a6d30b48d736869b8/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9574f2fe3b1f365340f8d3a6d30b48d736869b8/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=d9574f2fe3b1f365340f8d3a6d30b48d736869b8",
            "patch": "@@ -10,40 +10,31 @@ specific language governing permissions and limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n-\n -->\n \n-# SuperGlue\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\" >\n+    </div>\n </div>\n \n-## Overview\n+# SuperGlue\n \n-The SuperGlue model was proposed in [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://huggingface.co/papers/1911.11763) by Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n+[SuperGlue](https://huggingface.co/papers/1911.11763) is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. SuperGlue introduces a flexible context aggregation mechanism based on attention, enabling it to reason about the underlying 3D scene and feature assignments jointly. Paired with the [SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and estimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\n \n-This model consists of matching two sets of interest points detected in an image. Paired with the \n-[SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and \n-estimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\n+You can find all the original SuperGlue checkpoints under the [Magic Leap Community](https://huggingface.co/magic-leap-community) organization.\n \n-The abstract from the paper is the following:\n+> [!TIP]\n+> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n+>\n+> Click on the SuperGlue models in the right sidebar for more examples of how to apply SuperGlue to different computer vision tasks.\n \n-*This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences \n-and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs \n-are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling \n-SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, \n-our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image \n-pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in \n-challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and \n-can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at this [URL](https://github.com/magicleap/SuperGluePretrainedNetwork).*\n+The example below demonstrates how to match keypoints between two images with the [`AutoModel`] class.\n \n-## How to use\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n \n-Here is a quick example of using the model. Since this model is an image matching model, it requires pairs of images to be matched. \n-The raw outputs contain the list of keypoints detected by the keypoint detector as well as the list of matches with their corresponding \n-matching scores.\n-```python\n+```py\n from transformers import AutoImageProcessor, AutoModel\n import torch\n from PIL import Image\n@@ -52,7 +43,7 @@ import requests\n url_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n image1 = Image.open(requests.get(url_image1, stream=True).raw)\n url_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n-image_2 = Image.open(requests.get(url_image2, stream=True).raw)\n+image2 = Image.open(requests.get(url_image2, stream=True).raw)\n \n images = [image1, image2]\n \n@@ -62,67 +53,97 @@ model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n inputs = processor(images, return_tensors=\"pt\")\n with torch.no_grad():\n     outputs = model(**inputs)\n-```\n-\n-You can use the `post_process_keypoint_matching` method from the `SuperGlueImageProcessor` to get the keypoints and matches in a more readable format:\n \n-```python\n+# Post-process to get keypoints and matches\n image_sizes = [[(image.height, image.width) for image in images]]\n-outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-for i, output in enumerate(outputs):\n-    print(\"For the image pair\", i)\n-    for keypoint0, keypoint1, matching_score in zip(\n-            output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n-    ):\n-        print(\n-            f\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n+processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Notes\n+\n+- SuperGlue performs feature matching between two images simultaneously, requiring pairs of images as input.\n+\n+    ```python\n+    from transformers import AutoImageProcessor, AutoModel\n+    import torch\n+    from PIL import Image\n+    import requests\n+    \n+    processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+    model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+    \n+    # SuperGlue requires pairs of images\n+    images = [image1, image2]\n+    inputs = processor(images, return_tensors=\"pt\")\n+    outputs = model(**inputs)\n+    \n+    # Extract matching information\n+    keypoints0 = outputs.keypoints0  # Keypoints in first image\n+    keypoints1 = outputs.keypoints1  # Keypoints in second image\n+    matches = outputs.matches        # Matching indices\n+    matching_scores = outputs.matching_scores  # Confidence scores\n+    ```\n+\n+- The model outputs matching indices, keypoints, and confidence scores for each match.\n+- For better visualization and analysis, use the [`SuperGlueImageProcessor.post_process_keypoint_matching`] method to get matches in a more readable format.\n+\n+    ```py\n+    # Process outputs for visualization\n+    image_sizes = [[(image.height, image.width) for image in images]]\n+    processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+    \n+    for i, output in enumerate(processed_outputs):\n+        print(f\"For the image pair {i}\")\n+        for keypoint0, keypoint1, matching_score in zip(\n+                output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n+        ):\n+            print(f\"Keypoint at {keypoint0.numpy()} matches with keypoint at {keypoint1.numpy()} with score {matching_score}\")\n+    ```\n+\n+- The example below demonstrates how to visualize matches between two images.\n+\n+    ```py\n+    import matplotlib.pyplot as plt\n+    import numpy as np\n+\n+    # Create side by side image\n+    merged_image = np.zeros((max(image1.height, image2.height), image1.width + image2.width, 3))\n+    merged_image[: image1.height, : image1.width] = np.array(image1) / 255.0\n+    merged_image[: image2.height, image1.width :] = np.array(image2) / 255.0\n+    plt.imshow(merged_image)\n+    plt.axis(\"off\")\n+\n+    # Retrieve the keypoints and matches\n+    output = processed_outputs[0]\n+    keypoints0 = output[\"keypoints0\"]\n+    keypoints1 = output[\"keypoints1\"]\n+    matching_scores = output[\"matching_scores\"]\n+\n+    # Plot the matches\n+    for keypoint0, keypoint1, matching_score in zip(keypoints0, keypoints1, matching_scores):\n+        plt.plot(\n+            [keypoint0[0], keypoint1[0] + image1.width],\n+            [keypoint0[1], keypoint1[1]],\n+            color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n+            alpha=0.9,\n+            linewidth=0.5,\n         )\n+        plt.scatter(keypoint0[0], keypoint0[1], c=\"black\", s=2)\n+        plt.scatter(keypoint1[0] + image1.width, keypoint1[1], c=\"black\", s=2)\n \n-```\n+    plt.savefig(\"matched_image.png\", dpi=300, bbox_inches='tight')\n+    ```\n \n-From the outputs, you can visualize the matches between the two images using the following code:\n-```python\n-import matplotlib.pyplot as plt\n-import numpy as np\n-\n-# Create side by side image\n-merged_image = np.zeros((max(image1.height, image2.height), image1.width + image2.width, 3))\n-merged_image[: image1.height, : image1.width] = np.array(image1) / 255.0\n-merged_image[: image2.height, image1.width :] = np.array(image2) / 255.0\n-plt.imshow(merged_image)\n-plt.axis(\"off\")\n-\n-# Retrieve the keypoints and matches\n-output = outputs[0]\n-keypoints0 = output[\"keypoints0\"]\n-keypoints1 = output[\"keypoints1\"]\n-matching_scores = output[\"matching_scores\"]\n-keypoints0_x, keypoints0_y = keypoints0[:, 0].numpy(), keypoints0[:, 1].numpy()\n-keypoints1_x, keypoints1_y = keypoints1[:, 0].numpy(), keypoints1[:, 1].numpy()\n-\n-# Plot the matches\n-for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n-        keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, matching_scores\n-):\n-    plt.plot(\n-        [keypoint0_x, keypoint1_x + image1.width],\n-        [keypoint0_y, keypoint1_y],\n-        color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n-        alpha=0.9,\n-        linewidth=0.5,\n-    )\n-    plt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\n-    plt.scatter(keypoint1_x + image1.width, keypoint1_y, c=\"black\", s=2)\n-\n-# Save the plot\n-plt.savefig(\"matched_image.png\", dpi=300, bbox_inches='tight')\n-plt.close()\n-```\n+<div class=\"flex justify-center\">\n+    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/01ZYaLB1NL5XdA8u7yCo4.png\">\n+</div>\n \n-![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/01ZYaLB1NL5XdA8u7yCo4.png)\n+## Resources\n \n-This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n-The original code can be found [here](https://github.com/magicleap/SuperGluePretrainedNetwork).\n+- Refer to the [original SuperGlue repository](https://github.com/magicleap/SuperGluePretrainedNetwork) for more examples and implementation details.\n \n ## SuperGlueConfig\n \n@@ -133,10 +154,15 @@ The original code can be found [here](https://github.com/magicleap/SuperGluePret\n [[autodoc]] SuperGlueImageProcessor\n \n - preprocess\n+- post_process_keypoint_matching\n \n+<frameworkcontent>\n+<pt>\n ## SuperGlueForKeypointMatching\n \n [[autodoc]] SuperGlueForKeypointMatching\n \n - forward\n-- post_process_keypoint_matching\n\\ No newline at end of file\n+\n+</pt>\n+</frameworkcontent>\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 186,
        "additions": 106,
        "deletions": 80
    }
}