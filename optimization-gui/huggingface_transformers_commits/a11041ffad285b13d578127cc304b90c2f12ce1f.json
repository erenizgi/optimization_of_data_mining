{
    "author": "MekkCyber",
    "message": "Fix : add require_read_token for gemma2 gated model (#35687)\n\nfix gemma2 gated model test",
    "sha": "a11041ffad285b13d578127cc304b90c2f12ce1f",
    "files": [
        {
            "sha": "1cd9c2d4a83e3944e9d7258ce2be164521b94be6",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a11041ffad285b13d578127cc304b90c2f12ce1f/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a11041ffad285b13d578127cc304b90c2f12ce1f/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=a11041ffad285b13d578127cc304b90c2f12ce1f",
            "patch": "@@ -18,6 +18,7 @@\n from transformers import AddedToken, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n from transformers.testing_utils import (\n     require_gguf,\n+    require_read_token,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -880,6 +881,7 @@ def test_gemma2_fp32(self):\n         EXPECTED_TEXT = \"Hello! ðŸ‘‹\\n\\nI'm a large language model\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    @require_read_token\n     def test_gemma2_weights_conversion_fp32(self):\n         original_model = AutoModelForCausalLM.from_pretrained(\n             self.original_gemma2_model_id,"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}