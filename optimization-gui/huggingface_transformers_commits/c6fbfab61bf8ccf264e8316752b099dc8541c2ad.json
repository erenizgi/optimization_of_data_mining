{
    "author": "cyr0930",
    "message": "[fix] batch inference for llava_onevision (#40021)\n\n* [fix] llava onevision batch inference\n\n* style\n\n* cannot pass inconsistent list & handle text-only case",
    "sha": "c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
    "files": [
        {
            "sha": "935feaf8f01862534777d78c19ab89dcc54088e7",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -1048,7 +1048,7 @@\n       - local: model_doc/llama4\n         title: Llama4\n       - local: model_doc/llava\n-        title: Llava\n+        title: LLaVA\n       - local: model_doc/llava_next\n         title: LLaVA-NeXT\n       - local: model_doc/llava_next_video"
        },
        {
            "sha": "4d15e2a621d5237db806f039329571d7876ce92c",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -38,7 +38,7 @@ yielding new emerging capabilities. In particular, strong video understanding an\n cross-scenario capabilities are demonstrated through task transfer from images to\n videos.*\n \n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava-ov-acrhitecture.png\"\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava-ov-architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n <small> LLaVA-OneVision architecture. Taken from the <a href=\"https://huggingface.co/papers/2408.03326\">original paper.</a> </small>\n@@ -165,20 +165,20 @@ conversation_1 = [\n         \"content\": [\n             {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-            ],\n+        ],\n     },\n     {\n         \"role\": \"assistant\",\n         \"content\": [\n             {\"type\": \"text\", \"text\": \"There is a red stop sign in the image.\"},\n-            ],\n+        ],\n     },\n     {\n         \"role\": \"user\",\n         \"content\": [\n             {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n             {\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n-            ],\n+        ],\n     },\n ]\n \n@@ -188,7 +188,7 @@ conversation_2 = [\n         \"content\": [\n             {\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-            ],\n+        ],\n     },\n ]\n \n@@ -198,13 +198,14 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     padding=True,\n-    return_tensors=\"pt\"\n+    padding_side=\"left\",\n+    return_tensors=\"pt\",\n ).to(model.device, torch.float16)\n \n # Generate\n generate_ids = model.generate(**inputs, max_new_tokens=30)\n processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-['user\\n\\nWhat is shown in this image?\\nassistant\\nThere is a red stop sign in the image.\\nuser\\n\\nWhat about this image? How many cats do you see?\\nassistant\\ntwo', 'user\\n\\nWhat is shown in this image?\\nassistant\\n']\n+['user\\n\\nWhat is shown in this image?\\nassistant\\nThere is a red stop sign in the image.\\nuser\\n\\nWhat about this image? How many cats do you see?\\nassistant\\ntwo', 'user\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a whimsical scene of a snowman sitting by a campfire. The snowman is anthropomorphized, wearing a hat and']\n ```\n \n ### Video inference\n@@ -312,10 +313,6 @@ model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaOnevisionVideoProcessor\n \n-## LlavaOnevisionVideoProcessor\n-\n-[[autodoc]] LlavaOnevisionVideoProcessor\n-\n ## LlavaOnevisionModel\n \n [[autodoc]] LlavaOnevisionModel"
        },
        {
            "sha": "8729c7444d299ec52145f379649f528eb3bb28ec",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -682,6 +682,7 @@ def preprocess(\n \n         if isinstance(images, (tuple, list)) and isinstance(images[0], (tuple, list)):\n             # if the first element is a list, we assume that all elements are lists\n+            images = [x for x in images if x]  # handle text-only case\n             batch_num_images = [len(x) for x in images]\n         elif isinstance(images, (tuple, list)):\n             # treat this as a single-image case for backward compatibility"
        },
        {
            "sha": "0e2e5434dab277a2976c9885efb56720edcc6c45",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -394,7 +394,6 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None, v\n                 image_feature = image_feature[0]\n                 if image_newline is not None:\n                     image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n-                image_feature = image_feature.flatten(0, 1)\n             new_image_features.append(image_feature)\n             feature_lens.append(image_feature.size(0))\n         feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)"
        },
        {
            "sha": "ef32f4b8625fcbc56961139c06e56f43a8222880",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -320,7 +320,6 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None, v\n                 image_feature = image_feature[0]\n                 if image_newline is not None:\n                     image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n-                image_feature = image_feature.flatten(0, 1)\n             new_image_features.append(image_feature)\n             feature_lens.append(image_feature.size(0))\n         feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)"
        },
        {
            "sha": "7deadb9131b19a1460dc8506bfe81460b932a685",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -215,14 +215,15 @@ def _expand_image_tokens(\n         max_num_vision_tokens = 0\n         for sample in text:\n             if special_token in sample:\n-                is_multi_image = next(batch_num_images) != 1\n+                num_images = next(batch_num_images)  # should consume iterable\n+                is_multi_image = num_images != 1\n             else:\n                 is_multi_image = False\n             while special_token in sample:\n+                original_size = next(image_sizes)  # should consume iterable\n                 if is_multi_image:\n                     num_image_tokens = self.num_image_tokens + 1  # one for image_newline\n                 else:\n-                    original_size = next(image_sizes)\n                     if not isinstance(original_size, (list, tuple)):\n                         # cast to list to avoid numerical precision errors when calculating unpadding\n                         original_size = original_size.tolist()"
        },
        {
            "sha": "9fd4845513cb43bd55419009d82c7bd4d8a63950",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6fbfab61bf8ccf264e8316752b099dc8541c2ad/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=c6fbfab61bf8ccf264e8316752b099dc8541c2ad",
            "patch": "@@ -446,20 +446,25 @@ def test_small_model_integration_test_multi_image_nested(self):\n \n         url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n-        prompt = (\n-            \"user\\n<image><image>\\nWhat is the difference between these images?<|im_end|>\\n<|im_start|>assistant\\n\"\n-        )\n-        images_nested = [[self.image, image]]\n-        inputs = self.processor(text=prompt, images=images_nested, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        prompts = [\n+            \"user\\nTell me about the french revolution.<|im_end|>\\n<|im_start|>assistant\\n\",  # text-only case\n+            \"user\\n<image><image>\\nWhat is the difference between these images?<|im_end|>\\n<|im_start|>assistant\\n\",\n+            self.prompt_image,\n+        ]\n+        images_nested = [[], [image, self.image], [self.image]]\n+        inputs = self.processor(\n+            text=prompts,\n+            images=images_nested,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.float16)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=40)\n-        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe first image is a radar chart showing the performance of different models in a specific task, while the second image is a street scene with a stop sign in the foreground.\"  # fmt: skip\n+        EXPECTED_DECODED_TEXT = [\"user\\nTell me about the french revolution.\\nassistant\\nThe French Revolution! A pivotal event in modern history that had a profound impact on the course of Western civilization. Here's a brief overview:\\n\\n**Background**\\n\\nIn the late 18th century,\", \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe first image shows a stop sign with a traditional Chinese architectural background, while the second image displays a radar chart with various algorithms and models, including BLIP-2, InstructBLIP, Q\", \"user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different\"]  # fmt: skip\n+        DECODED_TEXT = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        self.assertEqual(\n-            self.processor.decode(output[0], skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        self.assertListEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_bitsandbytes"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 28,
        "deletions": 26
    }
}