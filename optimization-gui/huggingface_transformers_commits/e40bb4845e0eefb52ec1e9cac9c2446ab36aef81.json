{
    "author": "zucchini-nlp",
    "message": "Load and save video-processor from separate folder (#33562)\n\n* load and save from video-processor folder\r\n\r\n* Update src/transformers/models/llava_onevision/processing_llava_onevision.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "e40bb4845e0eefb52ec1e9cac9c2446ab36aef81",
    "files": [
        {
            "sha": "2047557208372a73ae6dc75007cdf32e29d2323f",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=e40bb4845e0eefb52ec1e9cac9c2446ab36aef81",
            "patch": "@@ -621,6 +621,7 @@ def preprocess(\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n         image_grid_pinpoints = image_grid_pinpoints if image_grid_pinpoints is not None else self.image_grid_pinpoints\n         resample = resample if resample is not None else self.resample\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale"
        },
        {
            "sha": "d4ae02e0bb154c18f042108ca78112c5b19292fb",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 51,
            "deletions": 2,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=e40bb4845e0eefb52ec1e9cac9c2446ab36aef81",
            "patch": "@@ -17,6 +17,7 @@\n \"\"\"\n \n import math\n+import os\n import sys\n from typing import Iterable, List, Union\n \n@@ -34,6 +35,11 @@\n     ProcessorMixin,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ..auto import AutoImageProcessor\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n class LlavaOnevisionProcessorKwargs(ProcessingKwargs, total=False):\n@@ -96,7 +102,7 @@ def __init__(\n         chat_template=None,\n         image_token=\"<image>\",\n         video_token=\"<video>\",\n-        **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n+        **kwargs,\n     ):\n         self.num_image_tokens = num_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n@@ -109,7 +115,7 @@ def __call__(\n         images: ImageInput = None,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         videos: VideoInput = None,\n-        **kwargs,\n+        **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -272,3 +278,46 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+    # override to save video-config in a separate config file\n+    def save_pretrained(self, save_directory, **kwargs):\n+        if os.path.isfile(save_directory):\n+            raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n+        os.makedirs(save_directory, exist_ok=True)\n+        video_processor_path = os.path.join(save_directory, \"video_processor\")\n+        self.video_processor.save_pretrained(video_processor_path)\n+\n+        video_processor_present = \"video_processor\" in self.attributes\n+        if video_processor_present:\n+            self.attributes.remove(\"video_processor\")\n+\n+        outputs = super().save_pretrained(save_directory, **kwargs)\n+\n+        if video_processor_present:\n+            self.attributes += [\"video_processor\"]\n+        return outputs\n+\n+    # override to load video-config from a separate config file\n+    @classmethod\n+    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n+        processor = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n+\n+        # if return_unused_kwargs a tuple is returned where the second element is 'unused_kwargs'\n+        if isinstance(processor, tuple):\n+            processor = processor[0]\n+\n+        try:\n+            video_processor = AutoImageProcessor.from_pretrained(\n+                pretrained_model_name_or_path, subfolder=\"video_processor\"\n+            )\n+            processor.video_processor = video_processor\n+        except EnvironmentError:\n+            # this means users are using prev version of saved processor where we had only one preprocessor_config.json\n+            # for loading back that should work and load a LlavaOnevisionVideoProcessor class\n+            logger.info(\n+                \"You are loading `LlavaOnevisionProcessor` but the indicated `path` doesn't contain a folder called \"\n+                \"`video_processor`. It is strongly recommended to load and save the processor again so the video processor is saved \"\n+                \"in a separate config.\"\n+            )\n+\n+        return processor"
        },
        {
            "sha": "1f998ca4bc04df9d9866b98b6bfb670f84441c8b",
            "filename": "tests/models/llava_onevision/test_processing_llava_onevision.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py?ref=e40bb4845e0eefb52ec1e9cac9c2446ab36aef81",
            "patch": "@@ -58,15 +58,16 @@ def get_video_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n \n     def prepare_processor_dict(self):\n-        return {\"chat_template\": \"dummy_template\"}\n+        return {\"chat_template\": \"dummy_template\", \"num_image_tokens\": 6, \"vision_feature_select_strategy\": \"default\"}\n \n-    @unittest.skip(\n-        \"Skip because the model has no processor kwargs except for chat template and\"\n-        \"chat template is saved as a separate file. Stop skipping this test when the processor\"\n-        \"has new kwargs saved in config file.\"\n-    )\n     def test_processor_to_json_string(self):\n-        pass\n+        processor = self.get_processor()\n+        obj = json.loads(processor.to_json_string())\n+        for key, value in self.prepare_processor_dict().items():\n+            # chat_tempalate are tested as a separate test because they are saved in separate files\n+            if key != \"chat_template\":\n+                self.assertEqual(obj[key], value)\n+                self.assertEqual(getattr(processor, key, None), value)\n \n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n@@ -191,7 +192,7 @@ def test_unstructured_kwargs_batched(self):\n             max_length=76,\n         )\n         self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 5)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 4)\n \n     @require_torch\n     @require_vision\n@@ -282,7 +283,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=112)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 2)\n \n     @require_vision\n     @require_torch\n@@ -299,4 +300,4 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 2)"
        },
        {
            "sha": "306f3100fb8d74e9974fc46a0b35fe05c4db91d5",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e40bb4845e0eefb52ec1e9cac9c2446ab36aef81/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=e40bb4845e0eefb52ec1e9cac9c2446ab36aef81",
            "patch": "@@ -112,6 +112,14 @@ def test_processor_from_and_save_pretrained(self):\n \n                 self.assertEqual(processor_second.to_dict(), processor_first.to_dict())\n \n+                for attribute in processor_first.attributes:\n+                    attribute_first = getattr(processor_first, attribute)\n+                    attribute_second = getattr(processor_second, attribute)\n+\n+                    # tokenizer repr contains model-path from where we loaded\n+                    if \"tokenizer\" not in attribute:\n+                        self.assertEqual(repr(attribute_first), repr(attribute_second))\n+\n     # These kwargs-related tests ensure that processors are correctly instantiated.\n     # they need to be applied only if an image_processor exists.\n "
        }
    ],
    "stats": {
        "total": 83,
        "additions": 71,
        "deletions": 12
    }
}