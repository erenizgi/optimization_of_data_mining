{
    "author": "yao-matrix",
    "message": "fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8 (#39116)\n\n* fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* zamba2\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* xx\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* internvl\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* tp cases\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "2100ee654569d323bfb77266cd3a75070abfda97",
    "files": [
        {
            "sha": "0f8739bb66c69c2e6ab9224d4998d8ea10f76efb",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -24,6 +24,7 @@\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n+    cleanup,\n     is_flash_attn_2_available,\n     require_flash_attn,\n     require_read_token,\n@@ -136,6 +137,9 @@ def test_generate_continue_from_inputs_embeds(self):\n class Cohere2IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n \n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def test_model_bf16(self):\n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_TEXTS = ["
        },
        {
            "sha": "2afe3f0ef3802bcead7501a11b65bf4ee72c1e34",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 54,
            "deletions": 25,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -29,6 +29,7 @@\n )\n from transformers.file_utils import cached_property\n from transformers.testing_utils import (\n+    Expectations,\n     is_flaky,\n     require_timm,\n     require_torch,\n@@ -804,34 +805,62 @@ def test_grounding_dino_loss(self):\n         with torch.no_grad():\n             outputs = model(**text_inputs, **image_inputs)\n \n-        # Loss differs by CPU and GPU, also this can be changed in future.\n-        expected_loss_dict = {\n-            \"loss_ce\": torch.tensor(1.1147),\n-            \"loss_bbox\": torch.tensor(0.2031),\n-            \"loss_giou\": torch.tensor(0.5819),\n-            \"loss_ce_0\": torch.tensor(1.1941),\n-            \"loss_bbox_0\": torch.tensor(0.1978),\n-            \"loss_giou_0\": torch.tensor(0.5524),\n-            \"loss_ce_1\": torch.tensor(1.1621),\n-            \"loss_bbox_1\": torch.tensor(0.1909),\n-            \"loss_giou_1\": torch.tensor(0.5892),\n-            \"loss_ce_2\": torch.tensor(1.1641),\n-            \"loss_bbox_2\": torch.tensor(0.1892),\n-            \"loss_giou_2\": torch.tensor(0.5626),\n-            \"loss_ce_3\": torch.tensor(1.1943),\n-            \"loss_bbox_3\": torch.tensor(0.1941),\n-            \"loss_giou_3\": torch.tensor(0.5607),\n-            \"loss_ce_4\": torch.tensor(1.0956),\n-            \"loss_bbox_4\": torch.tensor(0.2008),\n-            \"loss_giou_4\": torch.tensor(0.5836),\n-            \"loss_ce_enc\": torch.tensor(16226.3164),\n-            \"loss_bbox_enc\": torch.tensor(0.3063),\n-            \"loss_giou_enc\": torch.tensor(0.7380),\n-        }\n+        # Loss differs by CPU and accelerator, also this can be changed in future.\n+        expected_loss_dicts = Expectations(\n+            {\n+                 (\"xpu\", 3): {\n+                                    \"loss_ce\": torch.tensor(1.1147),\n+                                    \"loss_bbox\": torch.tensor(0.2031),\n+                                    \"loss_giou\": torch.tensor(0.5819),\n+                                    \"loss_ce_0\": torch.tensor(1.1941),\n+                                    \"loss_bbox_0\": torch.tensor(0.1978),\n+                                    \"loss_giou_0\": torch.tensor(0.5524),\n+                                    \"loss_ce_1\": torch.tensor(1.1621),\n+                                    \"loss_bbox_1\": torch.tensor(0.1909),\n+                                    \"loss_giou_1\": torch.tensor(0.5892),\n+                                    \"loss_ce_2\": torch.tensor(1.1641),\n+                                    \"loss_bbox_2\": torch.tensor(0.1892),\n+                                    \"loss_giou_2\": torch.tensor(0.5626),\n+                                    \"loss_ce_3\": torch.tensor(1.1943),\n+                                    \"loss_bbox_3\": torch.tensor(0.1941),\n+                                    \"loss_giou_3\": torch.tensor(0.5592),\n+                                    \"loss_ce_4\": torch.tensor(1.0956),\n+                                    \"loss_bbox_4\": torch.tensor(0.2037),\n+                                    \"loss_giou_4\": torch.tensor(0.5813),\n+                                    \"loss_ce_enc\": torch.tensor(16226.3164),\n+                                    \"loss_bbox_enc\": torch.tensor(0.3063),\n+                                    \"loss_giou_enc\": torch.tensor(0.7380),\n+                },\n+                (\"cuda\", None): {\n+                                    \"loss_ce\": torch.tensor(1.1147),\n+                                    \"loss_bbox\": torch.tensor(0.2031),\n+                                    \"loss_giou\": torch.tensor(0.5819),\n+                                    \"loss_ce_0\": torch.tensor(1.1941),\n+                                    \"loss_bbox_0\": torch.tensor(0.1978),\n+                                    \"loss_giou_0\": torch.tensor(0.5524),\n+                                    \"loss_ce_1\": torch.tensor(1.1621),\n+                                    \"loss_bbox_1\": torch.tensor(0.1909),\n+                                    \"loss_giou_1\": torch.tensor(0.5892),\n+                                    \"loss_ce_2\": torch.tensor(1.1641),\n+                                    \"loss_bbox_2\": torch.tensor(0.1892),\n+                                    \"loss_giou_2\": torch.tensor(0.5626),\n+                                    \"loss_ce_3\": torch.tensor(1.1943),\n+                                    \"loss_bbox_3\": torch.tensor(0.1941),\n+                                    \"loss_giou_3\": torch.tensor(0.5607),\n+                                    \"loss_ce_4\": torch.tensor(1.0956),\n+                                    \"loss_bbox_4\": torch.tensor(0.2008),\n+                                    \"loss_giou_4\": torch.tensor(0.5836),\n+                                    \"loss_ce_enc\": torch.tensor(16226.3164),\n+                                    \"loss_bbox_enc\": torch.tensor(0.3063),\n+                                    \"loss_giou_enc\": torch.tensor(0.7380),\n+                },\n+            }\n+        )  # fmt: skip\n+        expected_loss_dict = expected_loss_dicts.get_expectation()\n \n         expected_loss = torch.tensor(32482.2305)\n \n         for key in expected_loss_dict:\n-            self.assertTrue(torch.allclose(outputs.loss_dict[key], expected_loss_dict[key], atol=1e-3))\n+            torch.testing.assert_close(outputs.loss_dict[key], expected_loss_dict[key], rtol=1e-5, atol=1e-3)\n \n         self.assertTrue(torch.allclose(outputs.loss, expected_loss, atol=1e-3))"
        },
        {
            "sha": "66621fc0fe5d683171925cdc3c92b6054c5c3918",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 23,
            "deletions": 7,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -30,6 +30,8 @@\n     InstructBlipVisionConfig,\n )\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n@@ -722,6 +724,9 @@ def prepare_img():\n @require_torch\n @slow\n class InstructBlipModelIntegrationTest(unittest.TestCase):\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=False)\n+\n     @require_bitsandbytes\n     @require_accelerate\n     def test_inference_vicuna_7b(self):\n@@ -739,13 +744,24 @@ def test_inference_vicuna_7b(self):\n         outputs = model.generate(**inputs, max_new_tokens=30)\n         generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n \n-        expected_outputs = [32001] * 32 + [2, 1724, 338, 22910, 1048, 445, 1967, 29973, 450, 22910, 9565, 310, 445, 1967, 338, 393, 263, 767, 338, 13977, 292, 22095, 373, 278, 1250, 310, 263, 13328, 20134, 29963, 1550, 19500, 373, 263, 19587, 4272, 11952, 29889]  # fmt: off\n-\n-        self.assertEqual(outputs[0].tolist(), expected_outputs)\n-        self.assertEqual(\n-            generated_text,\n-            \"What is unusual about this image? The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV while driving on a busy city street.\",\n-        )\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): [32001] * 32 + [2, 1724, 338, 22910, 1048, 445, 1967, 29973, 450, 22910, 9565, 310, 445, 1967, 338, 393, 263, 767, 338, 13977, 292, 22095, 373, 278, 1250, 310, 263, 13328, 20134, 29963, 1550, 19500, 1623, 263, 19587, 4272, 11952, 29889],\n+                (\"cuda\", None): [32001] * 32 + [2, 1724, 338, 22910, 1048, 445, 1967, 29973, 450, 22910, 9565, 310, 445, 1967, 338, 393, 263, 767, 338, 13977, 292, 22095, 373, 278, 1250, 310, 263, 13328, 20134, 29963, 1550, 19500, 373, 263, 19587, 4272, 11952, 29889],\n+            }\n+        )  # fmt: off\n+        expected_output = expected_outputs.get_expectation()\n+\n+        expected_texts = Expectations(\n+            {\n+                (\"xpu\", 3): \"What is unusual about this image? The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV while driving down a busy city street.\",\n+                (\"cuda\", None): \"What is unusual about this image? The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV while driving on a busy city street.\",\n+            }\n+        )  # fmt: off\n+        expected_text = expected_texts.get_expectation()\n+\n+        self.assertEqual(outputs[0].tolist(), expected_output)\n+        self.assertEqual(generated_text, expected_text)\n \n     def test_inference_flant5_xl(self):\n         processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")"
        },
        {
            "sha": "19eb3cc4c71781e355e0472210240a2db12a043c",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -430,7 +430,7 @@ def test_qwen2_small_model_integration_batched_generate(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate\"',\n+                (\"xpu\", 3): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of',\n                 (\"cuda\", 7): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of',\n             }\n         )  # fmt: skip\n@@ -793,7 +793,7 @@ def test_llama_small_model_integration_batched_generate(self):\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\",\n+                (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n                 (\"cuda\", 7): 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.',\n                 (\"cuda\", 8): 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.',\n             }"
        },
        {
            "sha": "5ecc4732a2abd0db1494eccab3a2661bbe996d82",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -17,6 +17,8 @@\n \n from transformers import is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n     require_read_token,\n     require_torch_large_accelerator,\n     slow,\n@@ -78,10 +80,17 @@ def setUp(self):\n             },\n         ]\n \n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def test_model_17b_16e_fp16(self):\n-        EXPECTED_TEXT = [\n-                'system\\n\\nYou are a helpful assistant.user\\n\\nWhat is shown in this image?assistant\\n\\nThe image shows a cow standing on a beach, with a blue sky and a body of water in the background. The cow is brown with a white'\n-        ]  # fmt: skip\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): ['system\\n\\nYou are a helpful assistant.user\\n\\nWhat is shown in this image?assistant\\n\\nThe image shows a cow standing on a beach with a blue sky and a body of water in the background. The cow is brown with a white face'],\n+                (\"cuda\", None): ['system\\n\\nYou are a helpful assistant.user\\n\\nWhat is shown in this image?assistant\\n\\nThe image shows a cow standing on a beach, with a blue sky and a body of water in the background. The cow is brown with a white'],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         inputs = self.processor.apply_chat_template(\n             self.messages_1, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True"
        },
        {
            "sha": "3f35a54acb66c2db69288d392a43ef75f092ada0",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import AutoTokenizer, Zamba2Config, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -678,14 +679,23 @@ def test_simple_batched_generate_with_padding(self, torch_device):\n             ]\n             , dtype=torch.float32)  # fmt: skip\n \n-        EXPECTED_LOGITS_NO_GRAD_1 = torch.tensor(\n-            [\n-               0.1966,  6.3449,  3.8350, -5.7291, -6.5106, -6.5104, -6.5103, -6.5104,\n-               -6.5103, -6.5104, -6.5106, -6.5105,  7.8700, 13.5434, -6.5104, -6.5096,\n-               -6.5106, -6.5102, -6.5106, -6.5106, -6.5105, -6.5106, -6.5104, -6.5106,\n-               -6.5105, -6.5106, -6.5106, -6.5113, -6.5102, -6.5105, -6.5108, -6.5105,\n-               -6.5104, -6.5106, -6.5106, -6.5104, -6.5106, -6.5107, -6.5103, -6.5105          ]\n-            , dtype=torch.float32)  # fmt: skip\n+        EXPECTED_LOGITS_NO_GRAD_1S = Expectations(\n+            {\n+                (\"xpu\", 3): torch.tensor([0.2027,  6.3481,  3.8392, -5.7279, -6.5090, -6.5088, -6.5087, -6.5088,\n+                                          -6.5087, -6.5088, -6.5090, -6.5089,  7.8796, 13.5483, -6.5088, -6.5080,\n+                                          -6.5090, -6.5086, -6.5090, -6.5090, -6.5089, -6.5090, -6.5088, -6.5090,\n+                                          -6.5089, -6.5090, -6.5090, -6.5097, -6.5086, -6.5089, -6.5092, -6.5089,\n+                                          -6.5088, -6.5090, -6.5090, -6.5088, -6.5090, -6.5091, -6.5087, -6.5089],\n+                                         dtype=torch.float32),\n+                (\"cuda\", None): torch.tensor([0.1966,  6.3449,  3.8350, -5.7291, -6.5106, -6.5104, -6.5103, -6.5104,\n+                                              -6.5103, -6.5104, -6.5106, -6.5105,  7.8700, 13.5434, -6.5104, -6.5096,\n+                                              -6.5106, -6.5102, -6.5106, -6.5106, -6.5105, -6.5106, -6.5104, -6.5106,\n+                                              -6.5105, -6.5106, -6.5106, -6.5113, -6.5102, -6.5105, -6.5108, -6.5105,\n+                                              -6.5104, -6.5106, -6.5106, -6.5104, -6.5106, -6.5107, -6.5103, -6.5105],\n+                                             dtype=torch.float32),\n+            }\n+        )  # fmt: skip\n+        EXPECTED_LOGITS_NO_GRAD_1 = EXPECTED_LOGITS_NO_GRAD_1S.get_expectation()\n \n         torch.testing.assert_close(logits[0, -1, :40].cpu(), EXPECTED_LOGITS_NO_GRAD_0, rtol=1e-3, atol=1e-3)\n         torch.testing.assert_close("
        },
        {
            "sha": "9dc0bc396d938ad629ea4fd396beaca94f16d803",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -520,14 +520,14 @@ def test_pipeline(self):\n \n @require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n-class Bnb4bitTestMultiGpu(Base4bitTest):\n+class Bnb4bitTestMultiAccelerator(Base4bitTest):\n     def setUp(self):\n         super().setUp()\n \n-    def test_multi_gpu_loading(self):\n+    def test_multi_accelerator_loading(self):\n         r\"\"\"\n-        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.\n-        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice\n+        This tests that the model has been loaded and can be used correctly on a multi-accelerator setup.\n+        Let's just try to load a model on 2 accelerators and see if it works. The model we test has ~2GB of total, 3GB should suffice\n         \"\"\"\n         device_map = {\n             \"transformer.word_embeddings\": 0,"
        },
        {
            "sha": "69abd550e51d999b123e9f50c5d4831b844e92ee",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2100ee654569d323bfb77266cd3a75070abfda97/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=2100ee654569d323bfb77266cd3a75070abfda97",
            "patch": "@@ -24,7 +24,7 @@\n     backend_device_count,\n     get_torch_dist_unique_port,\n     require_huggingface_hub_greater_or_equal,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     torch_device,\n )\n \n@@ -168,6 +168,6 @@ def test_model_save(self):\n                     del non_tp_tensor, tp_tensor\n \n \n-@require_torch_multi_gpu\n-class TestTensorParallelCuda(TestTensorParallel):\n+@require_torch_multi_accelerator\n+class TestTensorParallelAccelerator(TestTensorParallel):\n     nproc_per_node = backend_device_count(torch_device)"
        }
    ],
    "stats": {
        "total": 172,
        "additions": 120,
        "deletions": 52
    }
}