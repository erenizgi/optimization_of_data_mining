{
    "author": "gante",
    "message": "Generate: move `logits` to same device as `input_ids` (#34076)\n\ntmp commit",
    "sha": "d314ce70bffc6d9df137ef14f490423fbacfba85",
    "files": [
        {
            "sha": "83a489bb13f36cc404d507dcbdee2631c73d893d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d314ce70bffc6d9df137ef14f490423fbacfba85/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d314ce70bffc6d9df137ef14f490423fbacfba85/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d314ce70bffc6d9df137ef14f490423fbacfba85",
            "patch": "@@ -2623,6 +2623,7 @@ def _dola_decoding(\n             next_token_logits = _dola_select_contrast(\n                 candidate_premature_layers, candidate_premature_logits, final_logits\n             )\n+            next_token_logits = next_token_logits.to(input_ids.device)\n             # pre-process distribution\n             next_token_scores = logits_processor(input_ids, next_token_logits)\n \n@@ -2794,6 +2795,7 @@ def _contrastive_search(\n                 # (the clone itself is always small)\n                 # .float() is needed to retain precision for later logits manipulations\n                 logit_for_next_step = outputs.logits[:, -1, :].clone().float()\n+                logit_for_next_step = logit_for_next_step.to(input_ids.device)\n \n                 model_kwargs = self._update_model_kwargs_for_generation(\n                     outputs,\n@@ -2988,6 +2990,7 @@ def _contrastive_search(\n                     next_past_key_values = tuple(new_key_values)\n \n             logit_for_next_step = torch.stack(torch.split(logits, top_k))[range(batch_size), selected_idx, :]\n+            logit_for_next_step = logit_for_next_step.to(input_ids.device)\n \n             # Rebuilds the relevant parts of the model output for the selected token, for use in the next iteration\n             if self.config.is_encoder_decoder:\n@@ -3184,6 +3187,7 @@ def _sample(\n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n             next_token_logits = outputs.logits.clone()[:, -1, :].float()\n+            next_token_logits = next_token_logits.to(input_ids.device)\n \n             # pre-process distribution\n             next_token_scores = logits_processor(input_ids, next_token_logits)\n@@ -3434,6 +3438,7 @@ def _beam_search(\n             # (the clone itself is always small)\n             # .float() is needed to retain precision for later logits manipulations\n             next_token_logits = outputs.logits[:, -1, :].clone().float()\n+            next_token_logits = next_token_logits.to(input_ids.device)\n             next_token_scores = nn.functional.log_softmax(\n                 next_token_logits, dim=-1\n             )  # (batch_size * num_beams, vocab_size)\n@@ -3691,6 +3696,7 @@ def _group_beam_search(\n                 # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n                 # (the clone itself is always small)\n                 raw_logit_score = outputs.logits[:, -1, :].clone()\n+                raw_logit_score = raw_logit_score.to(input_ids.device)\n \n             for beam_group_idx in range(num_beam_groups):\n                 group_start_idx = beam_group_idx * num_sub_beams\n@@ -3710,6 +3716,7 @@ def _group_beam_search(\n                 # No need to clone() the logits here as they will not retain outputs.logits at the end of the loop\n                 # .float() is needed to retain precision for later logits manipulations\n                 next_token_logits = outputs.logits[batch_group_indices, -1, :].float()\n+                next_token_logits = next_token_logits.to(input_ids.device)\n \n                 next_token_scores = nn.functional.log_softmax(\n                     next_token_logits, dim=-1\n@@ -3967,6 +3974,7 @@ def _constrained_beam_search(\n             # (the clone itself is always small)\n             # .float() is needed to retain precision for later logits manipulations\n             next_token_logits = outputs.logits[:, -1, :].clone().float()\n+            next_token_logits = next_token_logits.to(input_ids.device)\n             next_token_scores = nn.functional.log_softmax(\n                 next_token_logits, dim=-1\n             )  # (batch_size * num_beams, vocab_size)\n@@ -4215,6 +4223,7 @@ def _assisted_decoding(\n             # 2.3. Process the new logits\n             # .float() is needed to retain precision for later logits manipulations\n             new_logits = outputs.logits[:, -candidate_length - 1 :].float()  # excludes the input prompt if present\n+            new_logits = new_logits.to(input_ids.device)\n             next_token_logits = new_logits.clone()\n             if len(logits_processor) > 0:\n                 for i in range(candidate_length + 1):"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 9,
        "deletions": 0
    }
}