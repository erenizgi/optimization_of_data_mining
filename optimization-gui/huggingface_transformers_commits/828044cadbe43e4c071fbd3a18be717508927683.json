{
    "author": "cyyever",
    "message": "Add Optional typing (#40686)\n\n* Add Optional typing\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Fix typing\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Format\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "828044cadbe43e4c071fbd3a18be717508927683",
    "files": [
        {
            "sha": "1fa0570ee81fd8754c9f2d5e7a5f073a1ca29118",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1091,7 +1091,7 @@ def _get_logits_processor(\n         self,\n         generation_config: GenerationConfig,\n         input_ids_seq_length: Optional[int] = None,\n-        encoder_input_ids: torch.LongTensor = None,\n+        encoder_input_ids: Optional[torch.LongTensor] = None,\n         prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n         logits_processor: Optional[LogitsProcessorList] = None,\n         device: Optional[str] = None,"
        },
        {
            "sha": "071348cb43307e146a06b59bd662c99ad569209e",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -243,7 +243,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "c305b71b7eba42471907cc9892323526bb5932a2",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1,3 +1,5 @@\n+from typing import Optional\n+\n import torch\n \n from ..generation.continuous_batching import PagedAttentionCache\n@@ -16,7 +18,7 @@ def paged_attention_forward(\n     q: torch.Tensor,\n     k: torch.Tensor,\n     v: torch.Tensor,\n-    attention_mask: torch.Tensor = None,\n+    attention_mask: Optional[torch.Tensor] = None,\n     cache: PagedAttentionCache = None,\n     cu_seq_lens_q=None,\n     cu_seq_lens_k=None,"
        },
        {
            "sha": "4233a5bf5fb89aad5d2e5a97efb8fa33a571cd96",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -14,6 +14,7 @@\n \"HIGGS through FLUTE (Flexible Lookup Table Engine for LUT-quantized LLMs) integration file\"\n \n from math import sqrt\n+from typing import Optional\n \n from ..utils import (\n     is_flute_available,\n@@ -496,8 +497,8 @@ def __init__(\n         out_features: int,\n         num_bits: int,\n         bias=True,\n-        dtype: torch.dtype = None,\n-        device: torch.device = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n         group_size: int = 256,\n         hadamard_size: int = 1024,\n     ):"
        },
        {
            "sha": "c19a06dfad0e7699ed396fd8095dd532cdfe471f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1719,7 +1719,11 @@ def create_extended_attention_mask_for_decoder(input_shape, attention_mask, devi\n         return extended_attention_mask\n \n     def get_extended_attention_mask(\n-        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n+        self,\n+        attention_mask: Tensor,\n+        input_shape: tuple[int],\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n     ) -> Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored."
        },
        {
            "sha": "5cc8efea0fb4d9d41883304fea0c41a5d9c2b551",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Image/Text processor class for ALIGN\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -72,7 +72,7 @@ def __init__(self, image_processor, tokenizer):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "1280b256d4a09ba6774153f862fe404002a20010",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Image/Text processor class for AltCLIP\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -58,7 +58,7 @@ def __init__(self, image_processor=None, tokenizer=None):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "3e6aa4ea12d14013029df4aec24c9a3fda005687",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -153,7 +153,7 @@ def preprocess(\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "bccb7dff9e9217510004d33acd6d007622c68c1d",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1005,9 +1005,9 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_mask: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1134,9 +1134,9 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_mask: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "a1e4a8bebef53ef59e5722bf841ebab27896c13f",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -539,7 +539,7 @@ def preprocess(\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -1392,9 +1392,9 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_mask: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1463,9 +1463,9 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_mask: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "5ccb074399f522e844cb596a3227985440adce7b",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -269,8 +269,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "f76e046e0b948f0e4b33e361082bbd7332d254d1",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -166,8 +166,8 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "ee04f019ba29c42e436d7bb8bd217d46108fc629",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -88,7 +88,9 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+    def forward(\n+        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n         if position_ids is None:"
        },
        {
            "sha": "c25880bcfadacdfc7a7a5663ad1766d7230f1df3",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -183,7 +183,7 @@ def _preprocess(\n         do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -215,7 +215,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -260,7 +260,7 @@ def _preprocess_segmentation_map(\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_reduce_labels: Optional[bool] = None,\n@@ -308,7 +308,7 @@ def preprocess(\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "959202e866edf8dd3ad9fac59e2838d75707e07f",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -81,7 +81,9 @@ class BigBirdPegasusLearnedPositionalEmbedding(nn.Embedding):\n     def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n         if position_ids is None:"
        },
        {
            "sha": "3d32752edca86aa5105edbbc7dc566d71f79ab8f",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -178,7 +178,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "78a152374fd0089846f4431c8b408da4c5c9ff54",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -162,7 +162,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "5cc4334a974cb8e47f1b94b180e1f5cf8f553513",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -65,7 +65,7 @@ def __init__(self, image_processor, tokenizer, **kwargs):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "a1c89f7f460a2954f4b8638cc51bf3bc6f248e84",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -79,7 +79,7 @@ def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "28145b337a68ae67902abb91195f33c645ed1059",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -378,7 +378,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "64610ec4462aecbba307e4f34f63b02f67c69ed3",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -137,7 +137,7 @@ def resize(\n         image: \"torch.Tensor\",\n         size: SizeDict,\n         size_divisor: int = 32,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "9cae9d7bdd34d8b7a2292059ae1e92b587891272",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -170,7 +170,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "421c4ea9837418ea8a34f1df0341262ca93630b6",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Chameleon.\"\"\"\n \n+from typing import Optional\n+\n import numpy as np\n \n from ...image_processing_utils_fast import BaseImageProcessorFast\n@@ -87,7 +89,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\""
        },
        {
            "sha": "c55805f28913dca050f48ee24f8fa7050f974513",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -171,7 +171,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "bb3a93d93f2eb7cc60df678ff97b55b10ed83955",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -17,7 +17,7 @@\n \"\"\"\n \n import warnings\n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -69,7 +69,7 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[ChineseClipProcessorKwargs],"
        },
        {
            "sha": "ea17e4a65ff49fe6e0ce1a9ef8ef1db84ec36fca",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -204,7 +204,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "ddb0f360c6d66c4e95ec45cb34144de9bed9f818",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -219,8 +219,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "36f5d0b71ce071fe4a1cc911f28264b815eeccd3",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -115,8 +115,8 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "0988b0f7aafbd78431bb6239b21da32123b9a160",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -89,7 +89,7 @@ def query_augmentation_token(self) -> str:\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n@@ -208,7 +208,7 @@ def __call__(\n \n     def process_images(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "e6f72cd60b66dd145a1143af6a3cc1d67aa5a9ac",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -133,7 +133,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n@@ -279,7 +279,7 @@ def query_augmentation_token(self) -> str:\n \n     def process_images(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "2c268248856bf1bc295e5f0630c51c6f73503617",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -92,7 +92,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "1609f6e182dad154f3b85a2a5835dc027024550e",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -92,7 +92,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n@@ -260,7 +260,7 @@ def query_augmentation_token(self) -> str:\n \n     def process_images(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "06ef3f431050f6f943d9a19c3122351cf319e039",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -385,7 +385,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -441,7 +441,7 @@ def resize_annotation(\n         orig_size: tuple[int, int],\n         target_size: tuple[int, int],\n         threshold: float = 0.5,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resizes an annotation to a target size."
        },
        {
            "sha": "af89274500ddb33160e92b5591bc9ac83c55f24c",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -192,7 +192,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         crop_pct: Optional[float] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "7cfa9039701023e8fd97be5158e4dde9469180ff",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -85,12 +85,12 @@ class CsmOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_loss: Optional[torch.FloatTensor] = None\n-    depth_decoder_logits: torch.FloatTensor = None\n+    depth_decoder_logits: Optional[torch.FloatTensor] = None\n     depth_decoder_past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     depth_decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -415,7 +415,7 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -546,7 +546,7 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -925,7 +925,7 @@ def prepare_inputs_for_generation(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         input_values: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         input_values_cutoffs: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "f83a1abd5ae8d772a3813c380e662bb1a4577ca7",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -84,12 +84,12 @@ class CsmOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_loss: Optional[torch.FloatTensor] = None\n-    depth_decoder_logits: torch.FloatTensor = None\n+    depth_decoder_logits: Optional[torch.FloatTensor] = None\n     depth_decoder_past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     depth_decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -162,7 +162,7 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -312,7 +312,7 @@ def prepare_inputs_for_generation(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -603,7 +603,7 @@ def prepare_inputs_for_generation(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         input_values: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         input_values_cutoffs: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "a6c35f6be0d5cd526903f6338c88b10559a4db41",
            "filename": "src/transformers/models/deepseek_vl/configuration_deepseek_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -18,6 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import (\n@@ -67,8 +68,8 @@ class DeepseekVLConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        text_config: AutoConfig = None,\n-        vision_config: AutoConfig = None,\n+        text_config: Optional[AutoConfig] = None,\n+        vision_config: Optional[AutoConfig] = None,\n         image_token_id: int = 100015,\n         **kwargs,\n     ):"
        },
        {
            "sha": "02b39db51e884a78ec7c324aab3e8dc3235f3751",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -208,7 +208,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "59a86c89921de75f927bc956c8e8fac355c0353e",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -77,7 +77,7 @@ def resize(\n         image: \"torch.Tensor\",\n         size: SizeDict,\n         min_size: int,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "2a34ce84a93e7177714aeb9f74d975c2fdec7b98",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -205,8 +205,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -278,8 +278,8 @@ def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "9c3f4f39bdc1ac1693d6a1f7d5a94442b9aadd8e",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...configuration_utils import PretrainedConfig\n from ...image_processing_utils import BatchFeature\n@@ -79,8 +79,8 @@ class DeepseekVLConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        text_config: AutoConfig = None,\n-        vision_config: AutoConfig = None,\n+        text_config: Optional[AutoConfig] = None,\n+        vision_config: Optional[AutoConfig] = None,\n         image_token_id: int = 100015,\n         **kwargs,\n     ):\n@@ -243,7 +243,7 @@ def __init__(\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[DeepseekVLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "8abb3f1b4ad6e88f5d7d1eaa8b4253c64b4e6a92",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -18,7 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -72,7 +72,7 @@ def __init__(\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[DeepseekVLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "9fd82dbfefdfb48be2a2e5a44177d117a4c67cff",
            "filename": "src/transformers/models/deepseek_vl_hybrid/configuration_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -18,8 +18,12 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PretrainedConfig\n-from ...utils import logging\n+from ...utils import (\n+    logging,\n+)\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n@@ -66,9 +70,9 @@ class DeepseekVLHybridConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        text_config: AutoConfig = None,\n-        vision_config: AutoConfig = None,\n-        high_res_vision_config: AutoConfig = None,\n+        text_config: Optional[AutoConfig] = None,\n+        vision_config: Optional[AutoConfig] = None,\n+        high_res_vision_config: Optional[AutoConfig] = None,\n         image_token_id: int = 100015,\n         **kwargs,\n     ):"
        },
        {
            "sha": "e3f0b54d65d1df2ab1d13342a64b49651b45c4b8",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -240,8 +240,8 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         high_res_size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n-        high_res_resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n+        high_res_resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "37a2f9d78a6fdfab471b394fec6500c170a78b7f",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -111,7 +111,7 @@ def resize(\n         image: \"torch.Tensor\",\n         size: SizeDict,\n         min_size: int,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "65c5c8024e09027458b83d92135b9ae12558b565",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -309,9 +309,9 @@ def get_placeholder_mask(\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        high_res_pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        high_res_pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -420,9 +420,9 @@ def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        high_res_pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        high_res_pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "ba6637e1b2693e4614e38bb56c24a6c60fd4f822",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -130,9 +130,9 @@ class DeepseekVLHybridConfig(DeepseekVLConfig):\n \n     def __init__(\n         self,\n-        text_config: AutoConfig = None,\n-        vision_config: AutoConfig = None,\n-        high_res_vision_config: AutoConfig = None,\n+        text_config: Optional[AutoConfig] = None,\n+        vision_config: Optional[AutoConfig] = None,\n+        high_res_vision_config: Optional[AutoConfig] = None,\n         image_token_id: int = 100015,\n         **kwargs,\n     ):\n@@ -295,9 +295,9 @@ def get_image_features(self, pixel_values, high_res_pixel_values):\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        high_res_pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        high_res_pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -358,9 +358,9 @@ class DeepseekVLHybridForConditionalGeneration(DeepseekVLForConditionalGeneratio\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        high_res_pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        high_res_pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -546,8 +546,8 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         high_res_size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n-        high_res_resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n+        high_res_resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -920,7 +920,7 @@ class DeepseekVLHybridProcessor(DeepseekVLProcessor):\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[DeepseekVLHybridProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "465945033eec87e54dafcd867e778c97b64064ab",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -18,7 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_processing_utils_fast import BatchFeature\n from ...image_utils import ImageInput\n@@ -72,7 +72,7 @@ def __init__(\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         **kwargs: Unpack[DeepseekVLHybridProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "b6cd0a7075f323873a0fd0a72c2d135160e281d7",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -376,7 +376,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -432,7 +432,7 @@ def resize_annotation(\n         orig_size: tuple[int, int],\n         target_size: tuple[int, int],\n         threshold: float = 0.5,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resizes an annotation to a target size."
        },
        {
            "sha": "a2dd1281e920fdea3b4c51ef79c09bc86c605921",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -180,7 +180,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "284a99b559f475eb6805ac2dc3bd581515ab744c",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -719,12 +719,12 @@ class RealmReaderOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     retriever_loss: Optional[torch.FloatTensor] = None\n     reader_loss: Optional[torch.FloatTensor] = None\n-    retriever_correct: torch.BoolTensor = None\n-    reader_correct: torch.BoolTensor = None\n+    retriever_correct: Optional[torch.BoolTensor] = None\n+    reader_correct: Optional[torch.BoolTensor] = None\n     block_idx: Optional[torch.LongTensor] = None\n     candidate: Optional[torch.LongTensor] = None\n-    start_pos: torch.int32 = None\n-    end_pos: torch.int32 = None\n+    start_pos: Optional[torch.IntTensor] = None\n+    end_pos: Optional[torch.IntTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -742,7 +742,7 @@ class RealmForOpenQAOutput(ModelOutput):\n             Predicted answer ids.\n     \"\"\"\n \n-    reader_output: dict = None\n+    reader_output: Optional[dict] = None\n     predicted_answer_ids: Optional[torch.LongTensor] = None\n \n "
        },
        {
            "sha": "19c3fb0bd4855a6ab785e02951371a2d080368fb",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -618,7 +618,7 @@ class TransfoXLModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n-    mems: list[torch.FloatTensor] = None\n+    mems: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -652,7 +652,7 @@ class TransfoXLSequenceClassifierOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    mems: list[torch.FloatTensor] = None\n+    mems: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -688,7 +688,7 @@ class TransfoXLLMHeadModelOutput(ModelOutput):\n \n     losses: Optional[torch.FloatTensor] = None\n     prediction_scores: Optional[torch.FloatTensor] = None\n-    mems: list[torch.FloatTensor] = None\n+    mems: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "c0e1a33f091b55378103ff05dadf48a8fa73d881",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -222,7 +222,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -281,7 +281,7 @@ def preprocess(\n         size: Optional[dict[str, int]] = None,\n         patch_size: Optional[list[int]] = None,\n         num_frames: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "92d518363b2c34b5b30fd04dc0dbe982c8551e3d",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -194,7 +194,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "9877729434e15620f6955e2076b7046b740de0e6",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -397,7 +397,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -453,7 +453,7 @@ def resize_annotation(\n         orig_size: tuple[int, int],\n         target_size: tuple[int, int],\n         threshold: float = 0.5,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resizes an annotation to a target size."
        },
        {
            "sha": "bf18c775eed6de578da318700994ca219c8f23e1",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -45,7 +45,7 @@ def _get_logits_processor(\n         self,\n         generation_config: GenerationConfig,\n         input_ids_seq_length: Optional[int] = None,\n-        encoder_input_ids: torch.LongTensor = None,\n+        encoder_input_ids: Optional[torch.LongTensor] = None,\n         prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n         logits_processor: Optional[LogitsProcessorList] = None,\n         device: Optional[str] = None,"
        },
        {
            "sha": "570981decf614c4f348ad59656f390fbf07b8e22",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -314,7 +314,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_thumbnail: Optional[bool] = None,\n         do_align_long_axis: Optional[bool] = None,\n         do_pad: Optional[bool] = None,"
        },
        {
            "sha": "b84d0ed949d3f1fbc9c83f4721b16addc5e8f822",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -76,7 +76,7 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "4e02ae10144ce13cabf3d438f5cabe2b80c5b6f0",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -299,7 +299,7 @@ def _preprocess(\n         do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n@@ -340,7 +340,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n@@ -391,7 +391,7 @@ def _preprocess_segmentation_map(\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n         do_reduce_labels: Optional[bool] = None,\n@@ -442,7 +442,7 @@ def preprocess(\n         size: Optional[int] = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "05ee807ce8e557cfb287f738b9de29b4ef0c8d4b",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -313,7 +313,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         ensure_multiple_of: Optional[int] = 1,\n         keep_aspect_ratio: bool = False,"
        },
        {
            "sha": "e49fa04ea2ebc591f5db8f86349fc9540bfd645e",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -140,7 +140,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         ensure_multiple_of: Optional[int] = 1,\n         keep_aspect_ratio: bool = False,"
        },
        {
            "sha": "58ce0e96f5b8664720505d89ddd1f15ace7e5e21",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -224,7 +224,7 @@ def preprocess(\n         images,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_grayscale: Optional[bool] = None,"
        },
        {
            "sha": "93ad821051f3bb8fd9d68fdbed1ca9369f7ea3ff",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -143,7 +143,7 @@ def _preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -285,7 +285,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "5e791d1042f6b5316b34a67f30477bdd0b83d825",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1410,9 +1410,9 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        image_sizes: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1508,9 +1508,9 @@ def decode_image_tokens(self, **kwargs):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        image_sizes: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "5dd8d02f61aa5e7192424e3c261530cd67ed6110",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -995,9 +995,9 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        image_sizes: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1093,9 +1093,9 @@ def decode_image_tokens(self, **kwargs):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        image_sizes: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "93a440693dee8afd41a543232da82d90937b922c",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -409,7 +409,7 @@ def _preprocess_images(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_split_image: Optional[bool] = None,\n         do_pad: Optional[bool] = None,\n         do_rescale: Optional[bool] = None,\n@@ -470,7 +470,7 @@ def _preprocess_mask(\n         do_resize: Optional[bool] = False,\n         do_pad: Optional[bool] = False,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         data_format: Union[str, ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n@@ -510,7 +510,7 @@ def preprocess(\n         do_split_image: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -647,7 +647,7 @@ def preprocess(\n     def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n-        segmentation_maps: ImageInput = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n         ignore_index: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "d95567491fe19fcc9081b17719bdac908f1269a2",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -640,7 +640,7 @@ def get_extended_attention_mask(\n         self,\n         attention_mask: Tensor,\n         input_shape: tuple[int],\n-        device: torch.device = None,\n+        device: Optional[torch.device] = None,\n         dtype: Optional[torch.dtype] = None,\n     ) -> Tensor:\n         \"\"\"\n@@ -810,7 +810,7 @@ def forward(self, embeds, mask):\n @dataclass\n @auto_docstring\n class EvollaProteinEncoderModelOutput(ModelOutput):\n-    sequence_compressor_output: torch.FloatTensor = None\n+    sequence_compressor_output: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -1401,7 +1401,7 @@ def set_input_embeddings(self, value):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1519,11 +1519,11 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,  # text input ids\n+        input_ids: Optional[torch.LongTensor] = None,  # text input ids\n         attention_mask: Optional[torch.Tensor] = None,  # text attention mask\n         inputs_embeds: Optional[torch.FloatTensor] = None,  # text input embeddings\n         labels: Optional[torch.LongTensor] = None,\n-        protein_input_ids: torch.LongTensor = None,\n+        protein_input_ids: Optional[torch.LongTensor] = None,\n         protein_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         **kwargs,"
        },
        {
            "sha": "a58a3e7b7341c1f5051b8bfcad54ef74a9847a4d",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -272,7 +272,7 @@ def get_extended_attention_mask(\n         self,\n         attention_mask: Tensor,\n         input_shape: tuple[int],\n-        device: torch.device = None,\n+        device: Optional[torch.device] = None,\n         dtype: Optional[torch.dtype] = None,\n     ) -> Tensor:\n         \"\"\"\n@@ -442,7 +442,7 @@ def forward(self, embeds, mask):\n @dataclass\n @auto_docstring\n class EvollaProteinEncoderModelOutput(ModelOutput):\n-    sequence_compressor_output: torch.FloatTensor = None\n+    sequence_compressor_output: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -840,7 +840,7 @@ def set_input_embeddings(self, value):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -958,11 +958,11 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,  # text input ids\n+        input_ids: Optional[torch.LongTensor] = None,  # text input ids\n         attention_mask: Optional[torch.Tensor] = None,  # text attention mask\n         inputs_embeds: Optional[torch.FloatTensor] = None,  # text input embeddings\n         labels: Optional[torch.LongTensor] = None,\n-        protein_input_ids: torch.LongTensor = None,\n+        protein_input_ids: Optional[torch.LongTensor] = None,\n         protein_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         **kwargs,"
        },
        {
            "sha": "34eca44936a051b0f61e8c231dfb0177f49c11a9",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -355,7 +355,7 @@ def __init__(self, config: Exaone4Config):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "604dc9b8f9cbeade0ce4a226d14e789d0d8e03c5",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -368,7 +368,7 @@ def __init__(self, config: Exaone4Config):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "865daf384b49805c486419705521ef899fdb4ab4",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1240,7 +1240,7 @@ def __init__(self, config: FalconH1Config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[FalconHybridMambaAttentionDynamicCache] = None,\n@@ -1480,7 +1480,7 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[FalconHybridMambaAttentionDynamicCache] = None,"
        },
        {
            "sha": "8b00de3ab97fa3f8d7d0f0aa23e3b1c1dcb6c3cb",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1021,7 +1021,7 @@ def __init__(self, config: FalconH1Config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[FalconHybridMambaAttentionDynamicCache] = None,\n@@ -1245,7 +1245,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class FalconH1ForCausalLM(LlamaForCausalLM):\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[FalconHybridMambaAttentionDynamicCache] = None,"
        },
        {
            "sha": "7b4db246a8fa4195efd8d9840e351aae43af2a53",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -394,7 +394,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -459,7 +459,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "d5edfadc3ffcd9a07cd424e1ac9608063e948599",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -174,9 +174,10 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        image_patches: torch.Tensor = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n-        image_patches_indices: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n+        image_patches: Optional[torch.Tensor] = None,\n+        image_patches_indices: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -280,9 +281,10 @@ def get_decoder(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        image_patches: torch.Tensor = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n-        image_patches_indices: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n+        image_patches: Optional[torch.Tensor] = None,\n+        image_patches_indices: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "debbcb23aac1f9df63f052bdb39883a22efc71b9",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -485,7 +485,7 @@ def get_sample_encoding(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "8addbbfd378ce2e9656f40a923fa7fa5faf72a7b",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -242,7 +242,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "2e0f6a53053d56e028cfe0be7628024b69a82da8",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -844,8 +844,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n@@ -1029,8 +1029,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n@@ -1252,7 +1252,7 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "6901e0182ba7e9ecace277843d31c20e85a67d37",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -787,8 +787,8 @@ def _update_causal_mask(self, **super_kwargs):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n@@ -899,8 +899,8 @@ class Gemma3ForConditionalGeneration(PaliGemmaForConditionalGeneration):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n@@ -1125,7 +1125,7 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "791c47833a4edfa3896f30a3cdae642cd7e20454",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -79,7 +79,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         videos=None,\n         audio=None,"
        },
        {
            "sha": "e2c2c3ae10f8ab79f2c18d95028f57c28f6f9150",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -98,7 +98,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio: Optional[Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]]] = None,\n         videos=None,"
        },
        {
            "sha": "8293545deee239d1b5d19355086cb58fb06b8b1a",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -162,7 +162,7 @@ def _preprocess(\n         images: Union[ImageInput, VideoInput],\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -296,10 +296,10 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "c85df51f1dbf27c6616d0e18a689be02490444c9",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -665,7 +665,7 @@ class Glm4vModelOutputWithPast(ModelOutput):\n         The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -1153,8 +1153,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -1194,7 +1194,7 @@ def get_placeholder_mask(\n     @can_return_tuple\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n@@ -1374,7 +1374,7 @@ def visual(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,"
        },
        {
            "sha": "ccc8dc9c7e3aab14f22d78ccf9954f9212b23412",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1150,8 +1150,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -1191,7 +1191,7 @@ def get_placeholder_mask(\n     @can_return_tuple\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n@@ -1304,7 +1304,7 @@ class Glm4vForConditionalGeneration(Qwen2_5_VLForConditionalGeneration):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n@@ -1539,9 +1539,9 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Glm4vProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "817da3630d52e79657ac25c1ef4acd93f86b26b9",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -94,9 +94,9 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Glm4vProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "ccb97dc5d7c407de2450f85cc7fbf67496b28b44",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -799,7 +799,7 @@ class Glm4vMoeModelOutputWithPast(ModelOutput):\n         The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -1269,8 +1269,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -1310,7 +1310,7 @@ def get_placeholder_mask(\n     @can_return_tuple\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n@@ -1490,7 +1490,7 @@ def visual(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,"
        },
        {
            "sha": "209ac88ea2fbecc5ddd2c4d82b92830992caac18",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -259,7 +259,7 @@ def preprocess(\n         crop_to_patches: Optional[bool] = None,\n         min_patches: Optional[int] = None,\n         max_patches: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -422,7 +422,7 @@ def crop_image_to_patches(\n         max_patches: int,\n         use_thumbnail: bool = True,\n         patch_size: Optional[Union[tuple, int, dict]] = None,\n-        data_format: ChannelDimension = None,\n+        data_format: Optional[ChannelDimension] = None,\n     ):\n         \"\"\"\n         Crop the image to patches and return a list of cropped images."
        },
        {
            "sha": "788ac69d931aa62c2a953a860a39450626beec2e",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -596,8 +596,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -717,8 +717,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "0ecf39fcd03b577b406868a639d3ce8ee9425e3d",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -322,8 +322,8 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -382,8 +382,8 @@ class GotOcr2ForConditionalGeneration(LlavaForConditionalGeneration):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "e4a6ad1c41f4e857b1f034bf39672ae81d2ce23e",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -54,7 +54,7 @@ class GraniteSpeechCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -356,8 +356,8 @@ def get_audio_features(self, input_features: torch.Tensor) -> torch.Tensor:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        input_features: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n         input_features_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "e3a1e69fc861391296afa0c80def7c3fd1327e51",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1312,7 +1312,7 @@ def __init__(self, config: GraniteMoeHybridConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "4de1ff25391426936ca1db847b496a82c894af22",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -192,7 +192,7 @@ def __init__(self, config: GraniteMoeHybridConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "9869e8eb48013f2324aae5ff5d6a512f0afb86b9",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -407,7 +407,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -463,7 +463,7 @@ def resize_annotation(\n         orig_size: tuple[int, int],\n         target_size: tuple[int, int],\n         threshold: float = 0.5,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resizes an annotation to a target size."
        },
        {
            "sha": "3e2cfcd86160e1b5ed53145f4bed0d1c6668cb5c",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -150,7 +150,7 @@ def __init__(self, image_processor, tokenizer):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "69aa24b9f8f0c0280fc354a57a4c3a707eda3416",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -87,7 +87,7 @@ class HieraModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    bool_masked_pos: torch.BoolTensor = None\n+    bool_masked_pos: Optional[torch.BoolTensor] = None\n     ids_restore: Optional[torch.LongTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -156,7 +156,7 @@ class HieraForPreTrainingOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    bool_masked_pos: torch.BoolTensor = None\n+    bool_masked_pos: Optional[torch.BoolTensor] = None\n     ids_restore: Optional[torch.LongTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "3f0db7644563a22d78c849f2154f779193d36030",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -397,7 +397,7 @@ def preprocess(\n         do_convert_rgb: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "1ed120350813811c40e6ae11fba26e21010de2c5",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -858,7 +858,9 @@ def inputs_merger(\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_hidden_states)\n         return inputs_embeds\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         \"\"\"\n         Encodes images into continuous embeddings that can be forwarded to the language model.\n \n@@ -1038,7 +1040,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.text_model.set_input_embeddings(value)\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n \n     @can_return_tuple"
        },
        {
            "sha": "e460a041965ab1315f992b3378df40dc3af7d735",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -608,7 +608,7 @@ def preprocess(\n         do_convert_rgb: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_image_splitting: Optional[bool] = None,\n         do_rescale: Optional[bool] = None,\n         max_image_size: Optional[dict[str, int]] = None,"
        },
        {
            "sha": "a6047ba77a87c08cfc6179b9ef06384855aaa28d",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -215,7 +215,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n@@ -254,7 +254,7 @@ def split_images(\n         self,\n         images: torch.Tensor,\n         max_image_size: dict[str, int],\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Split an image into squares of side max_image_size and the original image resized to max_image_size.\n@@ -313,7 +313,7 @@ def resize_for_vision_encoder(\n         self,\n         image: torch.Tensor,\n         vision_encoder_max_size: int,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resize images to be multiples of `vision_encoder_max_size` while preserving the aspect ratio."
        },
        {
            "sha": "24429672da28160eab82a5cb1a195f30c566d1ae",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -608,7 +608,9 @@ def inputs_merger(\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_hidden_states)\n         return inputs_embeds\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         \"\"\"\n         Encodes images into continuous embeddings that can be forwarded to the language model.\n \n@@ -801,7 +803,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.text_model.set_input_embeddings(value)\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n \n     @can_return_tuple"
        },
        {
            "sha": "9168ecaceff2dde5a8baf01d3bcc68574cb83946",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -181,7 +181,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_normalize: Optional[bool] = None,\n         do_color_quantize: Optional[bool] = None,\n         clusters: Optional[Union[list[list[int]], np.ndarray]] = None,"
        },
        {
            "sha": "122fc11622ff51e72467b4f72bf46091ebb7c20c",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -17,7 +17,7 @@\n \"\"\"\n \n import os\n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -83,7 +83,7 @@ def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_toke\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "56391b59dbdd8c04b8995708c1c471ee9f07d75e",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -162,10 +162,10 @@ def resize(\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,\n-        images: VideoInput = None,\n+        images: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -285,10 +285,10 @@ def preprocess(\n     # Ignore copy\n     def _preprocess_image(\n         self,\n-        image: ImageInput = None,\n+        image: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "ee4e843e2f330ad969c84144b9223cb053ad7ec4",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -71,7 +71,7 @@ def __init__(self, video_processor, tokenizer, qformer_tokenizer, num_query_toke\n \n     def __call__(\n         self,\n-        images: VideoInput = None,\n+        images: Optional[VideoInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "3168546635ff2f351041d58e050152406cb2fd2c",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -632,8 +632,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -820,8 +820,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "bcef3a2ccbb0f464f23f09c8a82454cd77c5a6a2",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -548,8 +548,8 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "f604ffd3b72e49f7e7311e224b775e433345f9e5",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -619,7 +619,7 @@ def __init__(self, config: JambaConfig, layer_idx):\n     def cuda_kernels_forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cache_params: HybridMambaAttentionDynamicCache = None,\n+        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n         batch_size, seq_len, _ = hidden_states.shape\n@@ -723,7 +723,7 @@ def cuda_kernels_forward(\n         return contextualized_states\n \n     # fmt: off\n-    def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCache = None, attention_mask: Optional[torch.LongTensor] = None):\n+    def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttentionDynamicCache] = None, attention_mask: Optional[torch.LongTensor] = None):\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated MLP's linear projection\n@@ -811,7 +811,7 @@ def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCa\n     def forward(\n         self,\n         hidden_states,\n-        cache_params: HybridMambaAttentionDynamicCache = None,\n+        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n         if self.use_fast_kernels:"
        },
        {
            "sha": "33f073a2b91cfacfb0d11b205471df875f34e0fe",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -205,7 +205,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "deb13f66e9f372fd651fb55e89e6a3cd8678ebd1",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -83,7 +83,7 @@ def resize(\n         image: \"torch.Tensor\",\n         size: SizeDict,\n         min_size: int,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "9fa2ba354dd0b2423bb4c7bd09c41240b10cdb99",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -83,7 +83,7 @@ class JanusVQVAEOutput(ModelOutput):\n     \"\"\"\n \n     decoded_pixel_values: Optional[torch.FloatTensor] = None\n-    embedding_loss: torch.FloatTensor = None\n+    embedding_loss: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -1130,8 +1130,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1205,8 +1205,8 @@ def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1299,7 +1299,7 @@ def decode_image_tokens(self, image_tokens: torch.Tensor):\n     @torch.no_grad\n     def generate(\n         self,\n-        inputs: torch.Tensor = None,\n+        inputs: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         logits_processor: Optional[LogitsProcessorList] = None,\n         **kwargs,"
        },
        {
            "sha": "eef6f57d0d1dd425ba8bd8a44c52fd6b99dcee06",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -410,7 +410,7 @@ class JanusVQVAEOutput(ModelOutput):\n     \"\"\"\n \n     decoded_pixel_values: Optional[torch.FloatTensor] = None\n-    embedding_loss: torch.FloatTensor = None\n+    embedding_loss: Optional[torch.FloatTensor] = None\n \n \n class JanusBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):\n@@ -934,8 +934,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1009,8 +1009,8 @@ def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1103,7 +1103,7 @@ def decode_image_tokens(self, image_tokens: torch.Tensor):\n     @torch.no_grad\n     def generate(\n         self,\n-        inputs: torch.Tensor = None,\n+        inputs: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         logits_processor: Optional[LogitsProcessorList] = None,\n         **kwargs,"
        },
        {
            "sha": "0563edb5bd41392a0efa9e6c48253fa222e8bdc5",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for Janus.\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -80,7 +80,7 @@ def __init__(self, image_processor, tokenizer, chat_template=None, use_default_s\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         videos=None,\n         audio=None,\n         **kwargs: Unpack[JanusProcessorKwargs],"
        },
        {
            "sha": "58b3dff1e07a545973f53d03b49f284ca5a3cee2",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -134,7 +134,7 @@ def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwa\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, list[TextInput]] = None,\n         audio=None,\n         videos=None,\n@@ -342,7 +342,7 @@ def _preprocess_single_example(self, text, image, bboxes, img_info_tokens):\n     def preprocess_examples(\n         self,\n         texts: Union[TextInput, list[TextInput]],\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         bboxes: BboxInput = None,\n         num_image_tokens: Optional[int] = 64,\n     ) -> Union[str, list[str]]:"
        },
        {
            "sha": "c51d9109b48b05d05a7e6123f7d1270d944b705b",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -283,7 +283,7 @@ class Kosmos2_5ModelOutput(ModelOutput):\n             input) to speed up sequential decoding.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -346,7 +346,7 @@ class Kosmos2_5ForConditionalGenerationModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "0e3c70c80234ae8fc4961be806fd3d7cdeda1cc6",
            "filename": "src/transformers/models/kosmos2_5/processing_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -79,7 +79,7 @@ def __init__(self, image_processor, tokenizer):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, list[TextInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "de2e7361a6d3f5abe3a9e4f6c0d864db62837717",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -202,7 +202,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         apply_ocr: Optional[bool] = None,\n         ocr_lang: Optional[str] = None,\n         tesseract_config: Optional[str] = None,"
        },
        {
            "sha": "5bf03b39e4b9d97440c5e284e16b13e026f4bd25",
            "filename": "src/transformers/models/levit/image_processing_levit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -180,7 +180,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "096c846234daa7d29387a88ba85b987d6c645aa9",
            "filename": "src/transformers/models/levit/image_processing_levit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for LeViT.\"\"\"\n \n+from typing import Optional\n+\n from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict\n from ...image_transforms import (\n     ChannelDimension,\n@@ -51,7 +53,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\""
        },
        {
            "sha": "400475b76c77e6f078f53ad31fe4dcffde5bef8e",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -225,7 +225,7 @@ def preprocess(\n         images,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_grayscale: Optional[bool] = None,"
        },
        {
            "sha": "fd460e54d393b437175932ab4b038daaf43b169f",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -690,7 +690,7 @@ def _match_image_pair(\n         descriptors: torch.Tensor,\n         height: int,\n         width: int,\n-        mask: torch.Tensor = None,\n+        mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple, tuple]:"
        },
        {
            "sha": "64c36f21fef9213606210f103f817f5c0ff5a946",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -848,7 +848,7 @@ def _match_image_pair(\n         descriptors: torch.Tensor,\n         height: int,\n         width: int,\n-        mask: torch.Tensor = None,\n+        mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple, tuple]:"
        },
        {
            "sha": "223d4a1078066d26271878190ae5754c182dd61f",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -495,7 +495,7 @@ def __init__(self, config: Llama4TextConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -583,7 +583,7 @@ def __init__(self, config: Llama4TextConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n@@ -668,7 +668,7 @@ class Llama4CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -1227,8 +1227,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1242,7 +1242,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Llama4CausalLMOutputWithPast]:\n         r\"\"\""
        },
        {
            "sha": "ae8956d4df70d9cf8c6d7f11b129d0fddec8c026",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -245,16 +245,16 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, LlavaModelOutputWithPast]:\n         vision_feature_layer = (\n@@ -369,8 +369,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "b3e95ff8f2527e17f3095ba2adb55d4ae2b5aa41",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for Llava.\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -92,7 +92,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "98f608fc6bf594bc5b996de58a15e774bf305052",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -319,7 +319,7 @@ def _preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n@@ -553,7 +553,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         image_grid_pinpoints: Optional[list] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "a319afec03372f31280ba8379e9c33a5f8b21a1a",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -453,8 +453,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -605,8 +605,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "f8e90d6303cd10471d659429f70b082428a5b321",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for LLaVa-NeXT.\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -102,7 +102,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "ba1cd30a1133fe285baf1b1faa3ee9aac0c97591",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -180,7 +180,7 @@ def _preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n@@ -280,7 +280,7 @@ def preprocess(\n         images: VideoInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "3845c301cc8fdaae1b8c26b053082007edff7ee9",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -480,8 +480,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -520,9 +520,9 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -746,9 +746,9 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "f4802930f78477a548e3d2fa26681a02cfba5535",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -401,8 +401,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -439,9 +439,9 @@ def get_placeholder_mask(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -544,9 +544,9 @@ def get_video_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "fa639bd1929ee83afb0b79c2f4cde9ec98c1139c",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for LLaVa-NeXT-Video.\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -114,10 +114,10 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[LlavaNextVideoProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "6f523e30463bec435977fd329509ddbb8a677837",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -527,7 +527,7 @@ def _preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -603,7 +603,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         image_grid_pinpoints: Optional[list] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "204cd157c3fd80e629af61e1d41a8037335810cb",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -455,8 +455,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -495,10 +495,10 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -738,10 +738,10 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "9d6d3a53f7c8b591bfe8f93fd1d23359a1ae0b4f",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -477,10 +477,10 @@ def get_video_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -590,10 +590,10 @@ class LlavaOnevisionForConditionalGeneration(LlavaNextVideoForConditionalGenerat\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "663e1531d713d413dab76be13cebcc670efb8fe0",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -18,7 +18,7 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -112,10 +112,10 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "a0c369722b540635f8a84d83637ba518e06dcd81",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -605,7 +605,7 @@ def _preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -629,7 +629,7 @@ def _preprocess_image(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -711,7 +711,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -901,7 +901,7 @@ def pad(\n     def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n-        segmentation_maps: ImageInput = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,"
        },
        {
            "sha": "b94f0d8c308c27321c3873c45ed9d6d75e7085a2",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -194,7 +194,7 @@ def resize(\n         image: torch.Tensor,\n         size: SizeDict,\n         size_divisor: int = 0,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\""
        },
        {
            "sha": "e8c3d2344b8d674a6255636d2239f2ed0ae3a8ba",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -65,7 +65,7 @@ class Mask2FormerPixelDecoderOutput(ModelOutput):\n         or when `config.output_attentions=True`\n     \"\"\"\n \n-    multi_scale_features: tuple[torch.FloatTensor] = None\n+    multi_scale_features: Optional[tuple[torch.FloatTensor]] = None\n     mask_features: Optional[torch.FloatTensor] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -98,8 +98,8 @@ class Mask2FormerMaskedAttentionDecoderOutput(BaseModelOutputWithCrossAttentions\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[torch.FloatTensor] = None\n-    masks_queries_logits: tuple[torch.FloatTensor] = None\n-    intermediate_hidden_states: tuple[torch.FloatTensor] = None\n+    masks_queries_logits: Optional[tuple[torch.FloatTensor]] = None\n+    intermediate_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -132,7 +132,7 @@ class Mask2FormerPixelLevelModuleOutput(ModelOutput):\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    decoder_hidden_states: tuple[torch.FloatTensor] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -178,8 +178,8 @@ class Mask2FormerModelOutput(ModelOutput):\n     encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     pixel_decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     transformer_decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    transformer_decoder_intermediate_states: tuple[torch.FloatTensor] = None\n-    masks_queries_logits: tuple[torch.FloatTensor] = None\n+    transformer_decoder_intermediate_states: Optional[tuple[torch.FloatTensor]] = None\n+    masks_queries_logits: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n "
        },
        {
            "sha": "9ce33846170e7372da34dd43aded47da26431086",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -608,7 +608,7 @@ def _preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -632,7 +632,7 @@ def _preprocess_image(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -714,7 +714,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -903,7 +903,7 @@ def pad(\n     def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n-        segmentation_maps: ImageInput = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,"
        },
        {
            "sha": "ad5cb946d38d8b5fa54a4c81b39b513ad171ae2a",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -195,7 +195,7 @@ def resize(\n         image: torch.Tensor,\n         size: SizeDict,\n         size_divisor: int = 0,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\""
        },
        {
            "sha": "d1846c2531f6b2ae0dbfa79af0b133aa30042c56",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -94,7 +94,9 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+    def forward(\n+        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n         if position_ids is None:"
        },
        {
            "sha": "8f65375a789520f5386567a69a3ec162e66c7e8f",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -91,7 +91,7 @@ class MgpstrModelOutput(ModelOutput):\n         heads.\n     \"\"\"\n \n-    logits: tuple[torch.FloatTensor] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     a3_attentions: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "ac5e0fe2a24c198de05dfeff752d875d38ce6c50",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -652,7 +652,7 @@ def __init__(self, config: MiniMaxConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[MiniMaxCache] = None,"
        },
        {
            "sha": "9026457e35cbd317a8cb7dbb3a3840589d88ec46",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -482,7 +482,7 @@ class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n class MiniMaxModel(MixtralModel):\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[MiniMaxCache] = None,"
        },
        {
            "sha": "ecfb3080ee96df2c2d9ee8fcc2ff8293c133913d",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -289,8 +289,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -301,7 +301,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Mistral3ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -419,8 +419,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "213ab98fe902661c8d48ffc937dd40bfb68e5d98",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -163,8 +163,8 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -175,7 +175,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Mistral3ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -244,8 +244,8 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "6fa3f443c53b1a315a917a8167b100128b45046f",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -172,7 +172,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "eb6e6388bff46f0f57d0b76b78ca5485a73d962f",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -206,7 +206,7 @@ def _preprocess(\n         do_center_crop: bool,\n         do_normalize: bool,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         rescale_factor: Optional[float] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,\n@@ -235,7 +235,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_center_crop: Optional[bool] = None,\n@@ -326,7 +326,7 @@ def preprocess(\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "5411023c31047251b94a97b429a4529ae4241672",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -217,7 +217,7 @@ def _preprocess(\n         do_center_crop: bool,\n         do_flip_channel_order: bool,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         rescale_factor: Optional[float] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -244,7 +244,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_center_crop: Optional[bool] = None,\n@@ -329,7 +329,7 @@ def preprocess(\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_center_crop: Optional[bool] = None,"
        },
        {
            "sha": "3860632d7306934f7ea217a2f81f0ff10c6cf8f5",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -85,7 +85,7 @@ class MusicgenUnconditionalInput(ModelOutput):\n         from the prompts) and the unconditional logits (predicted without prompts).\n     \"\"\"\n \n-    encoder_outputs: tuple[torch.FloatTensor] = None\n+    encoder_outputs: Optional[tuple[torch.FloatTensor]] = None\n     attention_mask: Optional[torch.LongTensor] = None\n     guidance_scale: Optional[float] = None\n \n@@ -1931,7 +1931,7 @@ def _prepare_decoder_input_ids_for_generation(\n         model_kwargs: dict[str, torch.Tensor],\n         decoder_start_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = None,\n-        device: torch.device = None,\n+        device: Optional[torch.device] = None,\n     ) -> tuple[torch.LongTensor, dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n "
        },
        {
            "sha": "2c5e53fd891017e9378df4ec0baecd9703502650",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1815,7 +1815,7 @@ def _prepare_decoder_input_ids_for_generation(\n         model_kwargs: dict[str, torch.Tensor],\n         decoder_start_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = None,\n-        device: torch.device = None,\n+        device: Optional[torch.device] = None,\n     ) -> tuple[torch.LongTensor, dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n "
        },
        {
            "sha": "8dd74ded9bdecabace9c1711f99d60e11ec35db3",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -77,7 +77,9 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+    def forward(\n+        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n         if position_ids is None:"
        },
        {
            "sha": "ecbea1b10b78afa22db6d63e364be524b348294f",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -371,7 +371,7 @@ def preprocess(\n         do_crop_margin: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_thumbnail: Optional[bool] = None,\n         do_align_long_axis: Optional[bool] = None,\n         do_pad: Optional[bool] = None,"
        },
        {
            "sha": "136d7f17157595e7b2b7a0571ec1f28abcb093c6",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -241,7 +241,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "66fd18abf32cd50047768c87aded250bf5de234b",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -59,7 +59,7 @@ class OmDetTurboEncoderOutput(ModelOutput):\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n-    extracted_states: tuple[torch.FloatTensor] = None\n+    extracted_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -92,7 +92,7 @@ class OmDetTurboDecoderOutput(ModelOutput):\n     decoder_coords: Optional[torch.FloatTensor] = None\n     decoder_classes: Optional[torch.FloatTensor] = None\n     encoder_coord_logits: Optional[torch.FloatTensor] = None\n-    encoder_class_logits: tuple[torch.FloatTensor] = None\n+    encoder_class_logits: Optional[tuple[torch.FloatTensor]] = None\n     init_reference_points: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: tuple[tuple[torch.FloatTensor]] = None\n \n@@ -147,7 +147,7 @@ class OmDetTurboObjectDetectionOutput(ModelOutput):\n     init_reference_points: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     encoder_coord_logits: Optional[torch.FloatTensor] = None\n-    encoder_class_logits: tuple[torch.FloatTensor] = None\n+    encoder_class_logits: Optional[tuple[torch.FloatTensor]] = None\n     encoder_extracted_states: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None"
        },
        {
            "sha": "0c4cfd40eb62a874d1cc399b45bd8353f27366f8",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -225,7 +225,7 @@ def __init__(self, image_processor, tokenizer):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Optional[Union[list[str], list[list[str]]]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "615c71593062b7c9c87d4e4eb4944e0182176ead",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -571,7 +571,7 @@ def _preprocess(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -592,7 +592,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -671,7 +671,7 @@ def preprocess(\n         instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -948,7 +948,7 @@ def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n         task_inputs: list[str],\n-        segmentation_maps: ImageInput = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,"
        },
        {
            "sha": "a5336f6fc490890c475bfe110c40c58faeef72f2",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -781,7 +781,7 @@ class OneFormerPixelDecoderOutput(ModelOutput):\n         or when `config.output_attentions=True`\n     \"\"\"\n \n-    multi_scale_features: tuple[torch.FloatTensor] = None\n+    multi_scale_features: Optional[tuple[torch.FloatTensor]] = None\n     mask_features: Optional[torch.FloatTensor] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -806,8 +806,8 @@ class OneFormerPixelLevelModuleOutput(ModelOutput):\n         1/4 scale features from the last Pixel Decoder Layer.\n     \"\"\"\n \n-    encoder_features: list[torch.FloatTensor] = None\n-    decoder_features: list[torch.FloatTensor] = None\n+    encoder_features: Optional[list[torch.FloatTensor]] = None\n+    decoder_features: Optional[list[torch.FloatTensor]] = None\n     decoder_last_feature: Optional[torch.FloatTensor] = None\n \n "
        },
        {
            "sha": "bd6d63e83914d3e12f06f926ed079a9f25b39e54",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -316,7 +316,7 @@ def preprocess(\n         crop_to_patches: Optional[bool] = None,\n         min_patches: Optional[int] = None,\n         max_patches: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -481,7 +481,7 @@ def crop_image_to_patches(\n         max_patches: int,\n         use_covering_area_grid: bool = True,\n         patch_size: Optional[Union[tuple, int, dict]] = None,\n-        data_format: ChannelDimension = None,\n+        data_format: Optional[ChannelDimension] = None,\n         covering_threshold: float = 0.9,\n     ):\n         \"\"\""
        },
        {
            "sha": "6f6e958916094e93d9dbeaf729bba27e28879e88",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -592,8 +592,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n@@ -713,8 +713,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,"
        },
        {
            "sha": "6856be8feb4ff84cfe8ddbaa74047966e29b5bca",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -253,8 +253,8 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n@@ -347,8 +347,8 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,"
        },
        {
            "sha": "662ff15d101a35deaaaa02f702908aa3b7ebbad4",
            "filename": "src/transformers/models/ovis2/processing_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Union\n+from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -78,7 +78,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[Ovis2ProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "cc9c6cfdeaa8aa0532539f03e3e8e1e1aa3d9619",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -307,7 +307,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "1ae480913ca19a7afde1788b5ce4d5b1cd1986e6",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -276,8 +276,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n@@ -436,8 +436,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,"
        },
        {
            "sha": "3bb81d86e1aa68d024e53c9efaff6bab17db4e35",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -152,7 +152,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "c66d7b51d463c250ad0922211691ca7a6d3a6e72",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -217,7 +217,7 @@ def preprocess(\n         crop_size: Optional[dict[str, int]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "036dc00aa55c7d16a484df483dcf1b636459d651",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -211,8 +211,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is"
        },
        {
            "sha": "ef259f889f4f3ab8868917ca98115d1516bc818c",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -172,8 +172,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is"
        },
        {
            "sha": "f61c54554d32c22b4f22ae8905233405c828f3da",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n \n from collections.abc import Iterable\n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -87,10 +87,10 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[PerceptionLMProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "c6c6fdb163ab71d6c2754d0babf3cd8b4cbfd7dd",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -322,7 +322,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         patch_size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "5d42bb097476e2d2955dc87a10225dd6fee6e903",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -89,7 +89,7 @@ def resize(\n         image: torch.Tensor,\n         size: SizeDict,\n         patch_size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\""
        },
        {
            "sha": "bf4d3a736367854c6a2b484627d27b10f41c90ac",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for Pixtral.\n \"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -118,7 +118,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "38085a72264c3cd0d94637470dbe38a8aa45465f",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -285,7 +285,9 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+    def forward(\n+        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n         if position_ids is None:"
        },
        {
            "sha": "ee5500c823ccee4b51431521a75fbb9841bc2f28",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -216,7 +216,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         crop_pct: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "8fefa80be4321afe85496b235c80247cd0241edd",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -89,7 +89,7 @@ def resize(\n         image: \"torch.Tensor\",\n         size: SizeDict,\n         crop_pct: Optional[float] = None,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "9f687fe7548ff92b40a455b07e185b43794190a1",
            "filename": "src/transformers/models/pvt/image_processing_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -151,7 +151,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "51f9440001d61e1f82bc69f5d69db9cff22a205f",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -1761,8 +1761,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -2069,12 +2069,12 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n-    thinker_reply_part: torch.FloatTensor = None\n+    thinker_reply_part: Optional[torch.FloatTensor] = None\n \n \n @auto_docstring\n@@ -2271,7 +2271,7 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "260ead04b76c02af4d8bd02df14045299b5d55da",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -2209,8 +2209,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -2517,12 +2517,12 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n-    thinker_reply_part: torch.FloatTensor = None\n+    thinker_reply_part: Optional[torch.FloatTensor] = None\n \n \n class Qwen2_5OmniTalkerModel(Qwen2_5_VLTextModel):\n@@ -2570,7 +2570,7 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "45d8cacddeb25fcb5f2abeb80dd15644e9428b69",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -113,9 +113,9 @@ def __init__(\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n-        videos: VideoInput = None,\n-        audio: AudioInput = None,\n+        images: Optional[ImageInput] = None,\n+        videos: Optional[VideoInput] = None,\n+        audio: Optional[AudioInput] = None,\n         **kwargs: Unpack[Qwen2_5OmniProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "bc48a879c800e7e6470493044348bc6bb9603c03",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -493,7 +493,7 @@ class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n         The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -1188,8 +1188,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -1227,7 +1227,7 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1417,7 +1417,7 @@ def visual(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "859f51cd6100fc50916b6ceb842c1e9fd4367a4f",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -540,7 +540,7 @@ def get_rope_index(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -661,7 +661,7 @@ class Qwen2_5_VLForConditionalGeneration(Qwen2VLForConditionalGeneration):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -885,9 +885,9 @@ def model_input_names(self):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "5cfb07353626bb06e592a59d941e89a3f1579b54",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -97,9 +97,9 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "552b289f58c0ac4921327723e238a34b8845a332",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -167,7 +167,7 @@ def _preprocess(\n         images: Union[ImageInput, VideoInput],\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -299,12 +299,12 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         min_pixels: Optional[int] = None,\n         max_pixels: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "83200bcb904a8635565d0459440aeec7d7e487f1",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -73,7 +73,7 @@ class Qwen2VLModelOutputWithPast(ModelOutput):\n         The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -1123,8 +1123,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -1162,7 +1162,7 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1302,7 +1302,7 @@ def visual(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "a065acc126d16d6728c359f1325c08c159b929ed",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -92,9 +92,9 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2VLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "9927a8d022097e1ff703c0c9f6b1da16552e18d5",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -194,7 +194,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -250,7 +250,7 @@ def resize_annotation(\n         orig_size: tuple[int, int],\n         target_size: tuple[int, int],\n         threshold: float = 0.5,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resizes an annotation to a target size."
        },
        {
            "sha": "33a3661c5e6da694a179b2016bd6c62afc0c6235",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -267,7 +267,7 @@ def _preprocess(\n         do_rescale: bool,\n         do_normalize: bool,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         rescale_factor: Optional[float] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n@@ -295,7 +295,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "20ea1d5e623011c1ced693ece559bc0e822ffcc8",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -74,7 +74,7 @@ class Sam2VisionEncoderOutput(ModelOutput):\n         the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     fpn_hidden_states: Optional[torch.FloatTensor] = None\n     fpn_position_encoding: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -106,9 +106,9 @@ class Sam2ImageSegmentationOutput(ModelOutput):\n         Attentions weights of the mask decoder.\n     \"\"\"\n \n-    iou_scores: torch.FloatTensor = None\n-    pred_masks: torch.FloatTensor = None\n-    object_score_logits: torch.FloatTensor = None\n+    iou_scores: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n     image_embeddings: tuple[torch.FloatTensor, ...] = None\n     vision_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     vision_attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "647acde0dee93beebaa1e6ad5bed3a2b332c0dad",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -314,7 +314,7 @@ class Sam2VisionEncoderOutput(ModelOutput):\n         the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     fpn_hidden_states: Optional[torch.FloatTensor] = None\n     fpn_position_encoding: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -346,9 +346,9 @@ class Sam2ImageSegmentationOutput(ModelOutput):\n         Attentions weights of the mask decoder.\n     \"\"\"\n \n-    iou_scores: torch.FloatTensor = None\n-    pred_masks: torch.FloatTensor = None\n-    object_score_logits: torch.FloatTensor = None\n+    iou_scores: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n     image_embeddings: tuple[torch.FloatTensor, ...] = None\n     vision_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     vision_attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "5f147aab8dfadaeaa6790246e2275a76b0dd67be",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -62,8 +62,8 @@ def __init__(self, image_processor, target_size: Optional[int] = None, point_pad\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n-        segmentation_maps: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n         input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n         input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n         input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,"
        },
        {
            "sha": "6982921fbef55cf614b32109879def9556e66d77",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -125,7 +125,7 @@ class Sam2VideoInferenceSession:\n \n     def __init__(\n         self,\n-        video: torch.FloatTensor = None,\n+        video: Optional[torch.FloatTensor] = None,\n         video_height: Optional[int] = None,\n         video_width: Optional[int] = None,\n         inference_device: Union[torch.device, str] = \"cpu\",\n@@ -628,16 +628,16 @@ class Sam2VideoImageSegmentationOutput(ModelOutput):\n         A tensor representing the object pointer, used for tracking in videos. Only used for Sam2VideoModel.\n     \"\"\"\n \n-    iou_scores: torch.FloatTensor = None\n-    pred_masks: torch.FloatTensor = None\n-    object_score_logits: torch.FloatTensor = None\n+    iou_scores: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n     image_embeddings: tuple[torch.FloatTensor, ...] = None\n     vision_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     vision_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     mask_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n-    high_res_masks: torch.FloatTensor = None\n-    object_pointer: torch.FloatTensor = None\n+    high_res_masks: Optional[torch.FloatTensor] = None\n+    object_pointer: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -650,8 +650,8 @@ class Sam2VideoSegmentationOutput(ModelOutput):\n         The frame index of the video.\n     \"\"\"\n \n-    pred_masks: torch.FloatTensor = None\n-    frame_idx: int = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    frame_idx: Optional[int] = None\n \n \n @auto_docstring\n@@ -1123,7 +1123,7 @@ class Sam2VideoVisionEncoderOutput(ModelOutput):\n         the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     fpn_hidden_states: Optional[torch.FloatTensor] = None\n     fpn_position_encoding: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "bc569aec2811aa466db3dd0cff8e2be4f2ffa2f7",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -401,7 +401,7 @@ class Sam2VideoInferenceSession:\n \n     def __init__(\n         self,\n-        video: torch.FloatTensor = None,\n+        video: Optional[torch.FloatTensor] = None,\n         video_height: Optional[int] = None,\n         video_width: Optional[int] = None,\n         inference_device: Union[torch.device, str] = \"cpu\",\n@@ -974,8 +974,8 @@ class Sam2VideoImageSegmentationOutput(Sam2ImageSegmentationOutput):\n         A tensor representing the object pointer, used for tracking in videos. Only used for Sam2VideoModel.\n     \"\"\"\n \n-    high_res_masks: torch.FloatTensor = None\n-    object_pointer: torch.FloatTensor = None\n+    high_res_masks: Optional[torch.FloatTensor] = None\n+    object_pointer: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -988,8 +988,8 @@ class Sam2VideoSegmentationOutput(ModelOutput):\n         The frame index of the video.\n     \"\"\"\n \n-    pred_masks: torch.FloatTensor = None\n-    frame_idx: int = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    frame_idx: Optional[int] = None\n \n \n @auto_docstring"
        },
        {
            "sha": "7588cf256788f3e39796bc837814c5075f2f64d1",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -66,8 +66,8 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n-        segmentation_maps: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n         input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n         input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n         input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,"
        },
        {
            "sha": "46e66babe4deaf9e6f45a88ecad3672e831547a1",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -186,7 +186,7 @@ def _preprocess(\n         do_rescale: bool,\n         do_normalize: bool,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         rescale_factor: Optional[float] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n@@ -211,7 +211,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -299,7 +299,7 @@ def preprocess(\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "ffadfaf85edbe7ca4098d0ba73702e8117b7797c",
            "filename": "src/transformers/models/seggpt/image_processing_seggpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -245,7 +245,7 @@ def _preprocess_step(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -392,7 +392,7 @@ def preprocess(\n         prompt_masks: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "4341d087361ec87fce8beb1fe6550bbd241edef5",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -85,7 +85,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text=None,\n         videos=None,\n         audio=None,"
        },
        {
            "sha": "0ffed5258de5a09d84d88b6800fba3e934495c7e",
            "filename": "src/transformers/models/siglip/image_processing_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -113,7 +113,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "c5e5ded611136b73658dd0da73b8187db3c619be",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -49,7 +49,7 @@ def __init__(self, image_processor, tokenizer):\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         padding: Union[bool, str, PaddingStrategy] = False,\n         truncation: Union[bool, str, TruncationStrategy] = None,\n         max_length: Optional[int] = None,"
        },
        {
            "sha": "c08339b817325ace5d4e37767d315a5b69d7164d",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -605,7 +605,7 @@ def preprocess(\n         do_convert_rgb: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_image_splitting: Optional[bool] = None,\n         do_rescale: Optional[bool] = None,\n         max_image_size: Optional[dict[str, int]] = None,"
        },
        {
            "sha": "6f4bbd209bcaaa5095723088e2624befaa270b1c",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -205,7 +205,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n@@ -244,7 +244,7 @@ def split_images(\n         self,\n         images: torch.Tensor,\n         max_image_size: dict[str, int],\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Split an image into squares of side max_image_size and the original image resized to max_image_size.\n@@ -303,7 +303,7 @@ def resize_for_vision_encoder(\n         self,\n         image: torch.Tensor,\n         vision_encoder_max_size: int,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resize images to be multiples of `vision_encoder_max_size` while preserving the aspect ratio."
        },
        {
            "sha": "f0928c2cccdc7d1f41b96b36251e1a748548a824",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -576,7 +576,9 @@ def inputs_merger(\n         merged_embeds = torch.where(image_mask.unsqueeze(-1), image_embeds, inputs_embeds)\n         return merged_embeds\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         \"\"\"\n         Encodes images into continuous embeddings that can be forwarded to the language model.\n \n@@ -799,7 +801,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.text_model.set_input_embeddings(value)\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n \n     @can_return_tuple"
        },
        {
            "sha": "25d55b1a974a46dd32dbe5e849905c6528bebd2d",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -195,7 +195,9 @@ def inputs_merger(\n         merged_embeds = torch.where(image_mask.unsqueeze(-1), image_embeds, inputs_embeds)\n         return merged_embeds\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, pixel_attention_mask: Optional[torch.LongTensor] = None\n+    ):\n         \"\"\"\n         Encodes images into continuous embeddings that can be forwarded to the language model.\n "
        },
        {
            "sha": "97f0eaa9e7b29a79057970a618f655d69fd32b5a",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -249,7 +249,7 @@ def __call__(\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n         audio=None,\n-        videos: VideoInput = None,\n+        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[SmolVLMProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\""
        },
        {
            "sha": "5ad70d870c63dcfdbdb898580c080945fe3c16aa",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -156,7 +156,7 @@ def resize(\n         self,\n         video: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "bde3355d78edbb375bfb87bec4d1bc094509bea9",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -226,7 +226,7 @@ def preprocess(\n         images,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_grayscale: Optional[bool] = None,"
        },
        {
            "sha": "4c895b035feb41dd419e3364be6cc9b15c185497",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -187,7 +187,7 @@ def preprocess(\n         images,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_grayscale: Optional[bool] = None,"
        },
        {
            "sha": "153e297852894e61a1d7b5b3b31277b4287d39e1",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -206,7 +206,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "41b201a5c4ee911d162808ad7bdf1db4aa6e86a0",
            "filename": "src/transformers/models/textnet/image_processing_textnet_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -87,7 +87,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         size_divisor: int = 32,\n         **kwargs,"
        },
        {
            "sha": "83eb51b43444aac71dc4f7424dfcd390d86f34a2",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -51,7 +51,9 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+    def forward(\n+        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n         if position_ids is None:"
        },
        {
            "sha": "d6469b70c1a3fab1dee53d962a2b3a79f62d9181",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -18,7 +18,7 @@\n \n import warnings\n from contextlib import contextmanager\n-from typing import Union\n+from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -71,7 +71,7 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n \n     def __call__(\n         self,\n-        images: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,"
        },
        {
            "sha": "7fa758b6f4849bd6dee61f8089996481b2490be2",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -269,7 +269,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -343,7 +343,7 @@ def preprocess(\n         videos: Union[ImageInput, list[ImageInput], list[list[ImageInput]]],\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "a3bad696c36d2f9a66d73bffef5e3a82224734d0",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -164,7 +164,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "1ed8f911af8e8bbf2f7921f8d560db4ece9f0e76",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -175,7 +175,7 @@ def preprocess(\n         videos: Optional[list[VideoInput]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n@@ -326,10 +326,10 @@ def preprocess(\n \n     def _preprocess_image(\n         self,\n-        image: ImageInput = None,\n+        image: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "896c357e3cd2c3f4ce7869a683f533723618a643",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -58,7 +58,7 @@ class VideoLlavaModelOutputWithPast(ModelOutput):\n         video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[list[torch.FloatTensor]] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -286,8 +286,8 @@ def get_placeholder_mask(\n         self,\n         input_ids: torch.LongTensor,\n         inputs_embeds: torch.FloatTensor,\n-        image_features: torch.FloatTensor = None,\n-        video_features: torch.FloatTensor = None,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n@@ -326,9 +326,9 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values_images: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values_images: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -482,9 +482,9 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values_images: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values_images: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "8af47aeee3012e6b211f7cf8e04a6eae868d87ae",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -90,8 +90,8 @@ def __init__(\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: ImageInput = None,\n-        videos: ImageInput = None,\n+        images: Optional[ImageInput] = None,\n+        videos: Optional[ImageInput] = None,\n         padding: Union[bool, str, PaddingStrategy] = False,\n         truncation: Union[bool, str, TruncationStrategy] = None,\n         max_length=None,"
        },
        {
            "sha": "96545dc753112047713dcf1a97aa10cc25e3958a",
            "filename": "src/transformers/models/videomae/image_processing_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -182,7 +182,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -240,7 +240,7 @@ def preprocess(\n         videos: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "87abf7f7a7d694369902ae9119d6bf8edefeba7e",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -344,7 +344,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "6a8c6944bcb87cc69f89b88bb3b5de9b84d07093",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -213,8 +213,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -337,8 +337,8 @@ def multi_modal_projector(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "ec12cc6ee1bfce1a47ffe10950be91b453b47c06",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -106,8 +106,8 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -181,8 +181,8 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "16216e2eac90993f9cd0da01f91ac3cb09e6c53e",
            "filename": "src/transformers/models/vit/image_processing_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -156,7 +156,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "ab32d5b47eefde7182b6735829e9ccd0aaf53066",
            "filename": "src/transformers/models/vivit/image_processing_vivit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -229,7 +229,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n@@ -292,7 +292,7 @@ def preprocess(\n         videos: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "1dc382d6f68a5dab15d5b910732544612545325d",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -74,7 +74,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n     def __call__(\n         self,\n-        audio: AudioInput = None,\n+        audio: Optional[AudioInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         images=None,\n         videos=None,"
        },
        {
            "sha": "ead53edb101a749cb4e98f90ef16f65912edc93f",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -71,7 +71,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n     def __call__(\n         self,\n-        audio: AudioInput = None,\n+        audio: Optional[AudioInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         images=None,\n         videos=None,"
        },
        {
            "sha": "b77ec26d2b31022310ea6f989dc8964f58f66edb",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -69,12 +69,12 @@ def mlstm_chunkwise_recurrent_fw_C(\n         matV: torch.Tensor,\n         vecB: torch.Tensor,\n         vecI: torch.Tensor,\n-        matC_states: torch.Tensor = None,\n-        vecN_states: torch.Tensor = None,\n-        scaMinter_states: torch.Tensor = None,\n-        matC_initial: torch.Tensor = None,\n-        vecN_initial: torch.Tensor = None,\n-        scaMinter_initial: torch.Tensor = None,\n+        matC_states: Optional[torch.Tensor] = None,\n+        vecN_states: Optional[torch.Tensor] = None,\n+        scaMinter_states: Optional[torch.Tensor] = None,\n+        matC_initial: Optional[torch.Tensor] = None,\n+        vecN_initial: Optional[torch.Tensor] = None,\n+        scaMinter_initial: Optional[torch.Tensor] = None,\n         qk_scale: Optional[float] = None,\n         chunk_size: int = 64,\n         num_chunks: int = 1,\n@@ -237,9 +237,9 @@ def mlstm_chunkwise_fw(\n         value: torch.Tensor,\n         igate: torch.Tensor,\n         fgate: torch.Tensor,\n-        cstate: torch.Tensor = None,\n-        nstate: torch.Tensor = None,\n-        mstate: torch.Tensor = None,\n+        cstate: Optional[torch.Tensor] = None,\n+        nstate: Optional[torch.Tensor] = None,\n+        mstate: Optional[torch.Tensor] = None,\n         qk_scale: Optional[float] = None,\n         return_last_states: bool = False,\n         return_all_states: bool = False,\n@@ -318,9 +318,9 @@ def mlstm_chunkwise_native_autograd(\n         value: torch.Tensor,\n         igate: torch.Tensor,\n         fgate: torch.Tensor,\n-        c_initial: torch.Tensor = None,\n-        n_initial: torch.Tensor = None,\n-        m_initial: torch.Tensor = None,\n+        c_initial: Optional[torch.Tensor] = None,\n+        n_initial: Optional[torch.Tensor] = None,\n+        m_initial: Optional[torch.Tensor] = None,\n         return_last_states: bool = False,\n         eps: float = 1e-6,\n         chunk_size: int = 64,\n@@ -446,9 +446,9 @@ def mlstm_recurrent_sequence_native(\n         value: torch.Tensor,\n         igate: torch.Tensor,\n         fgate: torch.Tensor,\n-        c_initial: torch.Tensor = None,\n-        n_initial: torch.Tensor = None,\n-        m_initial: torch.Tensor = None,\n+        c_initial: Optional[torch.Tensor] = None,\n+        n_initial: Optional[torch.Tensor] = None,\n+        m_initial: Optional[torch.Tensor] = None,\n         return_last_states: bool = False,\n         eps: float = 1e-6,\n         dtype_state: torch.dtype = torch.float32,\n@@ -520,9 +520,9 @@ def wrap_chunkwise_pad_zeros(\n         value: torch.Tensor,\n         fgate: torch.Tensor,\n         igate: torch.Tensor,\n-        c_initial: torch.Tensor = None,\n-        n_initial: torch.Tensor = None,\n-        m_initial: torch.Tensor = None,\n+        c_initial: Optional[torch.Tensor] = None,\n+        n_initial: Optional[torch.Tensor] = None,\n+        m_initial: Optional[torch.Tensor] = None,\n         return_last_states: bool = False,\n         eps: float = 1e-6,\n         autocast_kernel_dtype: torch.dtype = torch.bfloat16,\n@@ -584,9 +584,9 @@ def wrap_chunkwise_arbitrary_sequence_length(\n         value: torch.Tensor,\n         fgate: torch.Tensor,\n         igate: torch.Tensor,\n-        c_initial: torch.Tensor = None,\n-        n_initial: torch.Tensor = None,\n-        m_initial: torch.Tensor = None,\n+        c_initial: Optional[torch.Tensor] = None,\n+        n_initial: Optional[torch.Tensor] = None,\n+        m_initial: Optional[torch.Tensor] = None,\n         return_last_states: bool = True,\n         eps: float = 1e-6,\n         autocast_kernel_dtype: torch.dtype = torch.bfloat16,\n@@ -773,9 +773,9 @@ def forward(\n             value: torch.Tensor,\n             igate: torch.Tensor,\n             fgate: torch.Tensor,\n-            c_initial: torch.Tensor = None,\n-            n_initial: torch.Tensor = None,\n-            m_initial: torch.Tensor = None,\n+            c_initial: Optional[torch.Tensor] = None,\n+            n_initial: Optional[torch.Tensor] = None,\n+            m_initial: Optional[torch.Tensor] = None,\n             return_last_states: bool = False,\n             mode: Optional[Literal[\"train\", \"inference\"]] = None,\n         ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]:"
        },
        {
            "sha": "4bea14b508ea439f81a0a88b3da4cce36fb7303d",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -424,7 +424,7 @@ def resize(\n         self,\n         image: torch.Tensor,\n         size: SizeDict,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -480,7 +480,7 @@ def resize_annotation(\n         orig_size: tuple[int, int],\n         target_size: tuple[int, int],\n         threshold: float = 0.5,\n-        interpolation: \"F.InterpolationMode\" = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n         Resizes an annotation to a target size."
        },
        {
            "sha": "08727ce5a8cca2c021b73c63f18b5f76112eb0ba",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -309,7 +309,7 @@ def preprocess(\n         size: Optional[int] = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "0b448678d5c7b68796e66ef77cdf9a7a2c5b224c",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -473,10 +473,10 @@ class MultiModalData:\n     and we might change its API in the future.\n     \"\"\"\n \n-    num_image_tokens: list[int] = None\n-    num_video_tokens: list[int] = None\n-    num_audio_tokens: list[int] = None\n-    num_image_patches: list[int] = None\n+    num_image_tokens: Optional[list[int]] = None\n+    num_video_tokens: Optional[list[int]] = None\n+    num_audio_tokens: Optional[list[int]] = None\n+    num_image_patches: Optional[list[int]] = None\n \n     def __contains__(self, key):\n         return hasattr(self, key) and getattr(self, key) is not None"
        },
        {
            "sha": "c72bdbb70bcd189582b8b69cd121be6a10c5c4e8",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -111,8 +111,8 @@ class TrainerState:\n     is_world_process_zero: bool = True\n     is_hyper_param_search: bool = False\n     trial_name: Optional[str] = None\n-    trial_params: dict[str, Union[str, float, int, bool]] = None\n-    stateful_callbacks: list[\"TrainerCallback\"] = None\n+    trial_params: Optional[dict[str, Union[str, float, int, bool]]] = None\n+    stateful_callbacks: Optional[list[\"TrainerCallback\"]] = None\n \n     def __post_init__(self):\n         if self.log_history is None:"
        },
        {
            "sha": "fc387772f092c28b17095b53af2efae4fbea25c6",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -54,8 +54,8 @@ class Seq2SeqTrainer(Trainer):\n     @deprecate_kwarg(\"tokenizer\", new_name=\"processing_class\", version=\"5.0.0\", raise_if_both_names=True)\n     def __init__(\n         self,\n-        model: Union[\"PreTrainedModel\", nn.Module] = None,\n-        args: \"TrainingArguments\" = None,\n+        model: Optional[Union[\"PreTrainedModel\", nn.Module]] = None,\n+        args: Optional[\"TrainingArguments\"] = None,\n         data_collator: Optional[\"DataCollator\"] = None,\n         train_dataset: Optional[Union[Dataset, \"IterableDataset\", \"datasets.Dataset\"]] = None,\n         eval_dataset: Optional[Union[Dataset, dict[str, Dataset]]] = None,\n@@ -66,7 +66,7 @@ def __init__(\n         compute_loss_func: Optional[Callable] = None,\n         compute_metrics: Optional[Callable[[\"EvalPrediction\"], dict]] = None,\n         callbacks: Optional[list[\"TrainerCallback\"]] = None,\n-        optimizers: tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n+        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),\n         preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n     ):\n         super().__init__("
        },
        {
            "sha": "304b5bd7c5b4233679f8f14f308fd907e565f2d6",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828044cadbe43e4c071fbd3a18be717508927683/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=828044cadbe43e4c071fbd3a18be717508927683",
            "patch": "@@ -66,7 +66,7 @@\n     list[\"np.ndarray\"],\n     list[\"torch.Tensor\"],\n     list[list[\"PIL.Image.Image\"]],\n-    list[list[\"np.ndarrray\"]],\n+    list[list[np.ndarray]],\n     list[list[\"torch.Tensor\"]],\n     URL,\n     list[URL],\n@@ -80,12 +80,12 @@\n @dataclass\n class VideoMetadata(Mapping):\n     total_num_frames: int\n-    fps: float = None\n-    width: int = None\n-    height: int = None\n-    duration: float = None\n-    video_backend: str = None\n-    frames_indices: list[int] = None\n+    fps: Optional[float] = None\n+    width: Optional[int] = None\n+    height: Optional[int] = None\n+    duration: Optional[float] = None\n+    video_backend: Optional[str] = None\n+    frames_indices: Optional[list[int]] = None\n \n     def __iter__(self):\n         return (f.name for f in fields(self))\n@@ -245,7 +245,7 @@ def make_batched_metadata(videos: VideoInput, video_metadata: Union[VideoMetadat\n     return video_metadata\n \n \n-def get_video_size(video: np.ndarray, channel_dim: ChannelDimension = None) -> tuple[int, int]:\n+def get_video_size(video: np.ndarray, channel_dim: Optional[ChannelDimension] = None) -> tuple[int, int]:\n     \"\"\"\n     Returns the (height, width) dimensions of the video.\n "
        }
    ],
    "stats": {
        "total": 1246,
        "additions": 645,
        "deletions": 601
    }
}