{
    "author": "jiqing-feng",
    "message": "enable torchao quantization on CPU (#36146)\n\n* enable torchao quantization on CPU\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix int4\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* enable CPU torchao tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix cuda tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix cpu tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix style\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix cuda tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix torchao available\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix torchao available\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix torchao config cannot convert to json\n\n* fix docs\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* rm to_dict to rebase\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* limited torchao version for CPU\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix skip\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* Update src/transformers/testing_utils.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* fix cpu test\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "9d6abf9778c90441f16b534895538d5e061d410c",
    "files": [
        {
            "sha": "61fc8bf322846d5011a1d476c9c91c535eb31cbb",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6abf9778c90441f16b534895538d5e061d410c/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6abf9778c90441f16b534895538d5e061d410c/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=9d6abf9778c90441f16b534895538d5e061d410c",
            "patch": "@@ -59,7 +59,7 @@ Use the table below to help you decide which quantization method to use.\n | [HQQ](./hqq.md)                               | 游릭                   | 游릭              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游릭              | 1/8         | 游릭               | 游댮                          | 游릭                      | https://github.com/mobiusml/hqq/            |\n | [optimum-quanto](./quanto.md)                 | 游릭                   | 游릭              | 游릭        | 游댮        | 游릭                                 | 游댮              | 游릭              | 2/4/8     | 游댮               | 游댮                          | 游릭                      | https://github.com/huggingface/optimum-quanto       |\n | [FBGEMM_FP8](./fbgemm_fp8.md)                 | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8             | 游댮               | 游릭                          | 游릭                      | https://github.com/pytorch/FBGEMM       |\n-| [torchao](./torchao.md)                       | 游릭                   |                 | 游릭        | 游댮        | 游리 <sub>5</sub> | 游댮              |                 | 4/8         |                  | 游릭游댮                        | 游릭                      | https://github.com/pytorch/ao       |\n+| [torchao](./torchao.md)                       | 游릭                   | 游릭               | 游릭        | 游댮        | 游리 <sub>5</sub> | 游댮              |                 | 4/8         |                  | 游릭游댮                        | 游릭                      | https://github.com/pytorch/ao       |\n | [VPTQ](./vptq.md)                             | 游댮                   | 游댮              |     游릭     | 游리        | 游댮                                 | 游댮              | 游릭              | 1/8         | 游댮               | 游릭                          | 游릭                      | https://github.com/microsoft/VPTQ            |\n | [SpQR](./spqr.md)                          | 游댮                       |  游댮   | 游릭        | 游댮              |    游댮    | 游댮         |         游릭              | 3              |              游댮                     | 游릭           | 游릭                      | https://github.com/Vahe1994/SpQR/       |\n | [FINEGRAINED_FP8](./finegrained_fp8.md)                 | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8             | 游댮               | 游릭                          | 游릭                      |        |"
        },
        {
            "sha": "c8116bf8eabbbb058005429e65b2c9a8b5095ae4",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6abf9778c90441f16b534895538d5e061d410c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6abf9778c90441f16b534895538d5e061d410c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=9d6abf9778c90441f16b534895538d5e061d410c",
            "patch": "@@ -22,9 +22,11 @@ pip install --upgrade torch torchao transformers\n \n By default, the weights are loaded in full precision (torch.float32) regardless of the actual data type the weights are stored in such as torch.float16. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a model's `config.json` file to automatically load the most memory-optimal data type.\n \n+\n ## Manually Choose Quantization Types and Settings\n \n `torchao` Provides many commonly used types of quantization, including different dtypes like int4, float8 and different flavors like weight only, dynamic quantization etc., only `int4_weight_only`, `int8_weight_only` and `int8_dynamic_activation_int8_weight` are integrated into hugigngface transformers currently, but we can add more when needed.\n+If you want to run the following codes on CPU even with GPU available, just change `device_map=\"cpu\"` and `quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128, layout=Int4CPULayout())` where `layout` comes from `from torchao.dtypes import Int4CPULayout` which is only available from torchao 0.8.0 and higher.\n \n Users can manually specify the quantization types and settings they want to use:\n \n@@ -40,7 +42,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_name)\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speedup\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -59,7 +61,7 @@ def benchmark_fn(func: Callable, *args, **kwargs) -> float:\n MAX_NEW_TOKENS = 1000\n print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n \n-bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n+bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n \n@@ -122,7 +124,7 @@ quantized_model.save_pretrained(output_dir, safe_serialization=False)\n \n # load quantized model\n ckpt_id = \"llama3-8b-int4wo-128\"  # or huggingface hub model id\n-loaded_quantized_model = AutoModelForCausalLM.from_pretrained(ckpt_id, device_map=\"cuda\")\n+loaded_quantized_model = AutoModelForCausalLM.from_pretrained(ckpt_id, device_map=\"auto\")\n \n \n # confirm the speedup"
        },
        {
            "sha": "4e06b11456bcfe2586113328402326a2b69b891f",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6abf9778c90441f16b534895538d5e061d410c/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6abf9778c90441f16b534895538d5e061d410c/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=9d6abf9778c90441f16b534895538d5e061d410c",
            "patch": "@@ -45,6 +45,7 @@\n import huggingface_hub.utils\n import urllib3\n from huggingface_hub import delete_repo\n+from packaging import version\n \n from transformers import logging as transformers_logging\n \n@@ -963,6 +964,18 @@ def require_torchao(test_case):\n     return unittest.skipUnless(is_torchao_available(), \"test requires torchao\")(test_case)\n \n \n+def require_torchao_version_greater_or_equal(torchao_version):\n+    def decorator(test_case):\n+        correct_torchao_version = is_torchao_available() and version.parse(\n+            version.parse(importlib.metadata.version(\"torchao\")).base_version\n+        ) >= version.parse(torchao_version)\n+        return unittest.skipUnless(\n+            correct_torchao_version, f\"Test requires torchao with the version greater than {torchao_version}.\"\n+        )(test_case)\n+\n+    return decorator\n+\n+\n def require_torch_tensorrt_fx(test_case):\n     \"\"\"Decorator marking a test that requires Torch-TensorRT FX\"\"\"\n     return unittest.skipUnless(is_torch_tensorrt_fx_available(), \"test requires Torch-TensorRT FX\")(test_case)"
        },
        {
            "sha": "9d91cfa593af9a7076402784a3793d0a7107a8d3",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6abf9778c90441f16b534895538d5e061d410c/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6abf9778c90441f16b534895538d5e061d410c/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=9d6abf9778c90441f16b534895538d5e061d410c",
            "patch": "@@ -1558,7 +1558,17 @@ def _get_torchao_quant_type_to_method(self):\n \n     def get_apply_tensor_subclass(self):\n         _STR_TO_METHOD = self._get_torchao_quant_type_to_method()\n-        return _STR_TO_METHOD[self.quant_type](**self.quant_type_kwargs)\n+        quant_type_kwargs = self.quant_type_kwargs.copy()\n+        if (\n+            not torch.cuda.is_available()\n+            and is_torchao_available()\n+            and self.quant_type == \"int4_weight_only\"\n+            and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+        ):\n+            from torchao.dtypes import Int4CPULayout\n+\n+            quant_type_kwargs[\"layout\"] = Int4CPULayout()\n+        return _STR_TO_METHOD[self.quant_type](**quant_type_kwargs)\n \n     def __repr__(self):\n         config_dict = self.to_dict()"
        },
        {
            "sha": "8e004500830ec8cb163e9ca4b5d88a50385da86d",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 94,
            "deletions": 64,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6abf9778c90441f16b534895538d5e061d410c/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6abf9778c90441f16b534895538d5e061d410c/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=9d6abf9778c90441f16b534895538d5e061d410c",
            "patch": "@@ -14,15 +14,18 @@\n # limitations under the License.\n \n import gc\n+import importlib.metadata\n import tempfile\n import unittest\n \n+from packaging import version\n+\n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n from transformers.testing_utils import (\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     require_torchao,\n-    torch_device,\n+    require_torchao_version_greater_or_equal,\n )\n from transformers.utils import is_torch_available, is_torchao_available\n \n@@ -38,13 +41,17 @@\n     )\n     from torchao.quantization.autoquant import AQMixin\n \n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\"):\n+        from torchao.dtypes import Int4CPULayout\n+\n \n-def check_torchao_quantized(test_module, qlayer, batch_size=1, context_size=1024):\n+def check_torchao_int4_wo_quantized(test_module, qlayer):\n     weight = qlayer.weight\n-    test_module.assertTrue(isinstance(weight, AffineQuantizedTensor))\n     test_module.assertEqual(weight.quant_min, 0)\n     test_module.assertEqual(weight.quant_max, 15)\n-    test_module.assertTrue(isinstance(weight._layout, TensorCoreTiledLayout))\n+    test_module.assertTrue(isinstance(weight, AffineQuantizedTensor))\n+    layout = Int4CPULayout if weight.device.type == \"cpu\" else TensorCoreTiledLayout\n+    test_module.assertTrue(isinstance(weight.tensor_impl._layout, layout))\n \n \n def check_autoquantized(test_module, qlayer):\n@@ -60,8 +67,8 @@ def check_forward(test_module, model, batch_size=1, context_size=1024):\n     test_module.assertEqual(out.shape[1], context_size)\n \n \n-@require_torch_gpu\n @require_torchao\n+@require_torchao_version_greater_or_equal(\"0.8.0\")\n class TorchAoConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n         \"\"\"\n@@ -102,15 +109,19 @@ def test_json_serializable(self):\n         quantization_config.to_json_string(use_diff=False)\n \n \n-@require_torch_gpu\n @require_torchao\n+@require_torchao_version_greater_or_equal(\"0.8.0\")\n class TorchAoTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 10\n-\n     EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n-\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+    device = \"cpu\"\n+    quant_scheme_kwargs = (\n+        {\"group_size\": 32, \"layout\": Int4CPULayout()}\n+        if is_torchao_available() and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+        else {\"group_size\": 32}\n+    )\n \n     def tearDown(self):\n         gc.collect()\n@@ -121,20 +132,20 @@ def test_int4wo_quant(self):\n         \"\"\"\n         Simple LLM model testing int4 weight only quantization\n         \"\"\"\n-        quant_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n+        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n \n         # Note: we quantize the bfloat16 model on the fly to int4\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             torch_dtype=torch.bfloat16,\n-            device_map=torch_device,\n+            device_map=self.device,\n             quantization_config=quant_config,\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n-        check_torchao_quantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n+        check_torchao_int4_wo_quantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n \n-        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n@@ -143,46 +154,51 @@ def test_int4wo_quant_bfloat16_conversion(self):\n         \"\"\"\n         Testing the dtype of model will be modified to be bfloat16 for int4 weight only quantization\n         \"\"\"\n-        quant_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n+        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n \n         # Note: we quantize the bfloat16 model on the fly to int4\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             torch_dtype=None,\n-            device_map=torch_device,\n+            device_map=self.device,\n             quantization_config=quant_config,\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n-        check_torchao_quantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n+        check_torchao_int4_wo_quantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n \n-        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n-    def test_int4wo_quant_multi_gpu(self):\n+    def test_int8_dynamic_activation_int8_weight_quant(self):\n         \"\"\"\n-        Simple test that checks if the quantized model int4 wieght only is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        Simple LLM model testing int8_dynamic_activation_int8_weight\n         \"\"\"\n+        quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n \n-        quant_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            torch_dtype=torch.bfloat16,\n-            device_map=\"auto\",\n+            device_map=self.device,\n             quantization_config=quant_config,\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n-        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n-\n-        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+\n+@require_torch_gpu\n+class TorchAoGPUTest(TorchAoTest):\n+    device = \"cuda\"\n+    quant_scheme_kwargs = {\"group_size\": 32}\n \n     def test_int4wo_offload(self):\n         \"\"\"\n@@ -228,32 +244,35 @@ def test_int4wo_offload(self):\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n-        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n         EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 2. What is the temperature outside\"\n \n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n-    def test_int8_dynamic_activation_int8_weight_quant(self):\n+    @require_torch_multi_gpu\n+    def test_int4wo_quant_multi_gpu(self):\n         \"\"\"\n-        Simple LLM model testing int8_dynamic_activation_int8_weight\n+        Simple test that checks if the quantized model int4 wieght only is working properly with multiple GPUs\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n         \"\"\"\n-        quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n \n-        # Note: we quantize the bfloat16 model on the fly to int4\n+        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            device_map=torch_device,\n+            torch_dtype=torch.bfloat16,\n+            device_map=\"auto\",\n             quantization_config=quant_config,\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n-        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n+        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     def test_autoquant(self):\n         \"\"\"\n@@ -264,11 +283,11 @@ def test_autoquant(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             torch_dtype=torch.bfloat16,\n-            device_map=torch_device,\n+            device_map=self.device,\n             quantization_config=quant_config,\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n-        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n         output = quantized_model.generate(\n             **input_ids, max_new_tokens=self.max_new_tokens, cache_implementation=\"static\"\n         )\n@@ -283,17 +302,22 @@ def test_autoquant(self):\n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n \n-@require_torch_gpu\n @require_torchao\n+@require_torchao_version_greater_or_equal(\"0.8.0\")\n class TorchAoSerializationTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 10\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n     # TODO: investigate why we don't have the same output as the original model for this test\n     SERIALIZED_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n-    quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32}\n-    device = \"cuda:0\"\n+    quant_scheme = \"int4_weight_only\"\n+    quant_scheme_kwargs = (\n+        {\"group_size\": 32, \"layout\": Int4CPULayout()}\n+        if is_torchao_available() and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+        else {\"group_size\": 32}\n+    )\n+    device = \"cpu\"\n \n     # called only once for all test in this class\n     @classmethod\n@@ -325,9 +349,9 @@ def check_serialization_expected_output(self, device, expected_output):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n             loaded_quantized_model = AutoModelForCausalLM.from_pretrained(\n-                self.model_name, torch_dtype=torch.bfloat16, device_map=self.device\n+                self.model_name, torch_dtype=torch.bfloat16, device_map=device\n             )\n-            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(device)\n \n             output = loaded_quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n@@ -336,46 +360,52 @@ def test_serialization_expected_output(self):\n         self.check_serialization_expected_output(self.device, self.SERIALIZED_EXPECTED_OUTPUT)\n \n \n-class TorchAoSerializationW8A8Test(TorchAoSerializationTest):\n+class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n-    device = \"cuda:0\"\n \n+    @require_torch_gpu\n+    def test_serialization_expected_output_on_cuda(self):\n+        \"\"\"\n+        Test if we can serialize on device (cpu) and load/infer the model on cuda\n+        \"\"\"\n+        self.check_serialization_expected_output(\"cuda\", self.SERIALIZED_EXPECTED_OUTPUT)\n \n-class TorchAoSerializationW8Test(TorchAoSerializationTest):\n+\n+class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+\n+    @require_torch_gpu\n+    def test_serialization_expected_output_on_cuda(self):\n+        \"\"\"\n+        Test if we can serialize on device (cpu) and load/infer the model on cuda\n+        \"\"\"\n+        self.check_serialization_expected_output(\"cuda\", self.SERIALIZED_EXPECTED_OUTPUT)\n+\n+\n+@require_torch_gpu\n+class TorchAoSerializationGPTTest(TorchAoSerializationTest):\n+    quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32}\n     device = \"cuda:0\"\n \n \n-class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n+@require_torch_gpu\n+class TorchAoSerializationW8A8GPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n-    device = \"cpu\"\n-\n-    def test_serialization_expected_output_cuda(self):\n-        \"\"\"\n-        Test if we can serialize on device (cpu) and load/infer the model on cuda\n-        \"\"\"\n-        new_device = \"cuda:0\"\n-        self.check_serialization_expected_output(new_device, self.SERIALIZED_EXPECTED_OUTPUT)\n+    device = \"cuda:0\"\n \n \n-class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n+@require_torch_gpu\n+class TorchAoSerializationW8GPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n-    device = \"cpu\"\n-\n-    def test_serialization_expected_output_cuda(self):\n-        \"\"\"\n-        Test if we can serialize on device (cpu) and load/infer the model on cuda\n-        \"\"\"\n-        new_device = \"cuda:0\"\n-        self.check_serialization_expected_output(new_device, self.SERIALIZED_EXPECTED_OUTPUT)\n+    device = \"cuda:0\"\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 193,
        "additions": 124,
        "deletions": 69
    }
}