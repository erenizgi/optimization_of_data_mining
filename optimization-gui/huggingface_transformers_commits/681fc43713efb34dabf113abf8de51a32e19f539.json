{
    "author": "Rocketknight1",
    "message": "Sync video classification pipeline with huggingface_hub spec (#34288)\n\n* Sync video classification pipeline\r\n\r\n* Add disclaimer",
    "sha": "681fc43713efb34dabf113abf8de51a32e19f539",
    "files": [
        {
            "sha": "057910098da20a1dfc02bf0d8b041e2d7af8cd09",
            "filename": "src/transformers/pipelines/video_classification.py",
            "status": "modified",
            "additions": 48,
            "deletions": 6,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/681fc43713efb34dabf113abf8de51a32e19f539/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/681fc43713efb34dabf113abf8de51a32e19f539/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py?ref=681fc43713efb34dabf113abf8de51a32e19f539",
            "patch": "@@ -1,3 +1,17 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import warnings\n from io import BytesIO\n from typing import List, Union\n \n@@ -42,7 +56,7 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, \"av\")\n         self.check_model_type(MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES)\n \n-    def _sanitize_parameters(self, top_k=None, num_frames=None, frame_sampling_rate=None):\n+    def _sanitize_parameters(self, top_k=None, num_frames=None, frame_sampling_rate=None, function_to_apply=None):\n         preprocess_params = {}\n         if frame_sampling_rate is not None:\n             preprocess_params[\"frame_sampling_rate\"] = frame_sampling_rate\n@@ -52,14 +66,23 @@ def _sanitize_parameters(self, top_k=None, num_frames=None, frame_sampling_rate=\n         postprocess_params = {}\n         if top_k is not None:\n             postprocess_params[\"top_k\"] = top_k\n+        if function_to_apply is not None:\n+            if function_to_apply not in [\"softmax\", \"sigmoid\", \"none\"]:\n+                raise ValueError(\n+                    f\"Invalid value for `function_to_apply`: {function_to_apply}. \"\n+                    \"Valid options are ['softmax', 'sigmoid', 'none']\"\n+                )\n+            postprocess_params[\"function_to_apply\"] = function_to_apply\n+        else:\n+            postprocess_params[\"function_to_apply\"] = \"softmax\"\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, videos: Union[str, List[str]], **kwargs):\n+    def __call__(self, inputs: Union[str, List[str]] = None, **kwargs):\n         \"\"\"\n         Assign labels to the video(s) passed as inputs.\n \n         Args:\n-            videos (`str`, `List[str]`):\n+            inputs (`str`, `List[str]`):\n                 The pipeline handles three types of videos:\n \n                 - A string containing a http link pointing to a video\n@@ -76,6 +99,11 @@ def __call__(self, videos: Union[str, List[str]], **kwargs):\n             frame_sampling_rate (`int`, *optional*, defaults to 1):\n                 The sampling rate used to select frames from the video. If not provided, will default to 1, i.e. every\n                 frame will be used.\n+            function_to_apply(`str`, *optional*, defaults to \"softmax\"):\n+                The function to apply to the model output. By default, the pipeline will apply the softmax function to\n+                the output of the model. Valid options: [\"softmax\", \"sigmoid\", \"none\"]. Note that passing Python's\n+                built-in `None` will default to \"softmax\", so you need to pass the string \"none\" to disable any\n+                post-processing.\n \n         Return:\n             A dictionary or a list of dictionaries containing result. If the input is a single video, will return a\n@@ -87,7 +115,16 @@ def __call__(self, videos: Union[str, List[str]], **kwargs):\n             - **label** (`str`) -- The label identified by the model.\n             - **score** (`int`) -- The score attributed by the model for that label.\n         \"\"\"\n-        return super().__call__(videos, **kwargs)\n+        # After deprecation of this is completed, remove the default `None` value for `images`\n+        if \"videos\" in kwargs:\n+            warnings.warn(\n+                \"The `videos` argument has been renamed to `inputs`. In version 5 of Transformers, `videos` will no longer be accepted\",\n+                FutureWarning,\n+            )\n+            inputs = kwargs.pop(\"videos\")\n+        if inputs is None:\n+            raise ValueError(\"Cannot call the video-classification pipeline without an inputs argument!\")\n+        return super().__call__(inputs, **kwargs)\n \n     def preprocess(self, video, num_frames=None, frame_sampling_rate=1):\n         if num_frames is None:\n@@ -114,12 +151,17 @@ def _forward(self, model_inputs):\n         model_outputs = self.model(**model_inputs)\n         return model_outputs\n \n-    def postprocess(self, model_outputs, top_k=5):\n+    def postprocess(self, model_outputs, top_k=5, function_to_apply=\"softmax\"):\n         if top_k > self.model.config.num_labels:\n             top_k = self.model.config.num_labels\n \n         if self.framework == \"pt\":\n-            probs = model_outputs.logits.softmax(-1)[0]\n+            if function_to_apply == \"softmax\":\n+                probs = model_outputs.logits[0].softmax(-1)\n+            elif function_to_apply == \"sigmoid\":\n+                probs = model_outputs.logits[0].sigmoid()\n+            else:\n+                probs = model_outputs.logits[0]\n             scores, ids = probs.topk(top_k)\n         else:\n             raise ValueError(f\"Unsupported framework: {self.framework}\")"
        },
        {
            "sha": "f1ed97ac13df1a3c6df198910ea78b7b77315e02",
            "filename": "tests/pipelines/test_pipelines_video_classification.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/681fc43713efb34dabf113abf8de51a32e19f539/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/681fc43713efb34dabf113abf8de51a32e19f539/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_video_classification.py?ref=681fc43713efb34dabf113abf8de51a32e19f539",
            "patch": "@@ -14,11 +14,12 @@\n \n import unittest\n \n-from huggingface_hub import hf_hub_download\n+from huggingface_hub import VideoClassificationOutputElement, hf_hub_download\n \n from transformers import MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING, VideoMAEFeatureExtractor\n from transformers.pipelines import VideoClassificationPipeline, pipeline\n from transformers.testing_utils import (\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n     require_av,\n@@ -76,6 +77,8 @@ def run_pipeline_test(self, video_classifier, examples):\n                     {\"score\": ANY(float), \"label\": ANY(str)},\n                 ],\n             )\n+            for element in outputs:\n+                compare_pipeline_output_to_hub_spec(element, VideoClassificationOutputElement)\n \n     @require_torch\n     def test_small_model_pt(self):\n@@ -93,6 +96,9 @@ def test_small_model_pt(self):\n             nested_simplify(outputs, decimals=4),\n             [{\"score\": 0.5199, \"label\": \"LABEL_0\"}, {\"score\": 0.4801, \"label\": \"LABEL_1\"}],\n         )\n+        for output in outputs:\n+            for element in output:\n+                compare_pipeline_output_to_hub_spec(element, VideoClassificationOutputElement)\n \n         outputs = video_classifier(\n             [\n@@ -108,6 +114,9 @@ def test_small_model_pt(self):\n                 [{\"score\": 0.5199, \"label\": \"LABEL_0\"}, {\"score\": 0.4801, \"label\": \"LABEL_1\"}],\n             ],\n         )\n+        for output in outputs:\n+            for element in output:\n+                compare_pipeline_output_to_hub_spec(element, VideoClassificationOutputElement)\n \n     @require_tf\n     @unittest.skip"
        },
        {
            "sha": "fe8a197237291aac12a4a909f5f1375ec5df4297",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/681fc43713efb34dabf113abf8de51a32e19f539/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/681fc43713efb34dabf113abf8de51a32e19f539/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=681fc43713efb34dabf113abf8de51a32e19f539",
            "patch": "@@ -34,6 +34,7 @@\n     ImageToTextInput,\n     ObjectDetectionInput,\n     QuestionAnsweringInput,\n+    VideoClassificationInput,\n     ZeroShotImageClassificationInput,\n )\n \n@@ -47,6 +48,7 @@\n     ImageToTextPipeline,\n     ObjectDetectionPipeline,\n     QuestionAnsweringPipeline,\n+    VideoClassificationPipeline,\n     ZeroShotImageClassificationPipeline,\n )\n from transformers.testing_utils import (\n@@ -132,6 +134,7 @@\n     \"image-to-text\": (ImageToTextPipeline, ImageToTextInput),\n     \"object-detection\": (ObjectDetectionPipeline, ObjectDetectionInput),\n     \"question-answering\": (QuestionAnsweringPipeline, QuestionAnsweringInput),\n+    \"video-classification\": (VideoClassificationPipeline, VideoClassificationInput),\n     \"zero-shot-image-classification\": (ZeroShotImageClassificationPipeline, ZeroShotImageClassificationInput),\n }\n "
        }
    ],
    "stats": {
        "total": 68,
        "additions": 61,
        "deletions": 7
    }
}