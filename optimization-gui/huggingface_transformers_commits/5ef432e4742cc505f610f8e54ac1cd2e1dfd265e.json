{
    "author": "vasqu",
    "message": "[`TF`] Fix Tensorflow XLA Generation on limited seq_len models (#33903)\n\n* fix tf xla generation on limited seq_len models\r\n\r\n* [run-slow] opt\r\n\r\n* [run-slow] opt",
    "sha": "5ef432e4742cc505f610f8e54ac1cd2e1dfd265e",
    "files": [
        {
            "sha": "eb328d83e9e7a46bc0fc9bb0036f16208d71ff33",
            "filename": "tests/test_modeling_tf_common.py",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ef432e4742cc505f610f8e54ac1cd2e1dfd265e/tests%2Ftest_modeling_tf_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ef432e4742cc505f610f8e54ac1cd2e1dfd265e/tests%2Ftest_modeling_tf_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_tf_common.py?ref=5ef432e4742cc505f610f8e54ac1cd2e1dfd265e",
            "patch": "@@ -1715,10 +1715,9 @@ def test_dataset_conversion(self):\n                 model.train_on_batch(test_batch, test_batch_labels)\n \n     def _test_xla_generate(self, **generate_kwargs):\n-        def _generate_and_check_results(model, inputs_dict):\n-            if \"input_ids\" in inputs_dict:\n-                inputs = inputs_dict[\"input_ids\"]\n-                # make sure there are no pad tokens in prompt, which may trigger unwanted behavior\n+        def _generate_and_check_results(model, inputs, is_input_ids):\n+            # make sure there are no pad tokens in prompt, which may trigger unwanted behavior\n+            if is_input_ids:\n                 if model.generation_config.pad_token_id is not None:\n                     if config.pad_token_id == 0:\n                         new_pad_token = model.generation_config.pad_token_id + 1\n@@ -1727,10 +1726,6 @@ def _generate_and_check_results(model, inputs_dict):\n                 else:\n                     new_pad_token = None\n                 inputs = tf.where(inputs != model.generation_config.pad_token_id, inputs, new_pad_token)\n-            elif \"input_features\" in inputs_dict:\n-                inputs = inputs_dict[\"input_features\"]\n-            else:\n-                raise ValueError(\"No valid generate input found in inputs_dict\")\n \n             generated = model.generate(inputs, **generate_kwargs).numpy()\n             generate_xla = tf.function(model.generate, jit_compile=True)\n@@ -1753,12 +1748,20 @@ def _generate_and_check_results(model, inputs_dict):\n             config.eos_token_id = None  # Generate until max length\n             config.do_sample = False\n \n+            # extract the input to the model\n+            is_input_ids = \"input_ids\" in inputs_dict\n+            is_input_features = \"input_features\" in inputs_dict\n+            if not (is_input_ids or is_input_features):\n+                raise ValueError(\"No valid generate input found in inputs_dict\")\n+            inputs = inputs_dict[\"input_ids\"] if is_input_ids else inputs_dict[\"input_features\"]\n+\n             # fix config for models with additional sequence-length limiting settings\n+            seq_len = inputs.get_shape()[1]\n             for var_name in [\"max_position_embeddings\", \"max_target_positions\"]:\n                 attr = getattr(config, var_name, None)\n-                if attr is not None and attr < generate_kwargs[\"max_new_tokens\"]:\n+                if attr is not None and attr < seq_len + generate_kwargs[\"max_new_tokens\"]:\n                     try:\n-                        setattr(config, var_name, generate_kwargs[\"max_new_tokens\"])\n+                        setattr(config, var_name, seq_len + generate_kwargs[\"max_new_tokens\"])\n                     except NotImplementedError:\n                         # xlnet will raise an exception when trying to set\n                         # max_position_embeddings.\n@@ -1767,10 +1770,10 @@ def _generate_and_check_results(model, inputs_dict):\n             model = model_class(config)\n \n             if model.supports_xla_generation:\n-                _generate_and_check_results(model, inputs_dict)\n+                _generate_and_check_results(model, inputs, is_input_ids)\n             else:\n                 with self.assertRaises(ValueError):\n-                    _generate_and_check_results(model, inputs_dict)\n+                    _generate_and_check_results(model, inputs, is_input_ids)\n \n     def test_xla_generate_fast(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 27,
        "additions": 15,
        "deletions": 12
    }
}