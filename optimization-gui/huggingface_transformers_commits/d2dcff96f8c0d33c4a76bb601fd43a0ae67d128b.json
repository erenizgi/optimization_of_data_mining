{
    "author": "amyeroberts",
    "message": "[InstructBLIP] qformer_tokenizer is required input (#33222)\n\n* [InstructBLIP] qformer_tokenizer is required input\r\n\r\n* Bit safer\r\n\r\n* Add to instructblipvideo processor\r\n\r\n* Fix up\r\n\r\n* Use video inputs\r\n\r\n* Update tests/models/instructblipvideo/test_processor_instructblipvideo.py",
    "sha": "d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b",
    "files": [
        {
            "sha": "e3251395a78153d663a62321d9851a3ab4721212",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b",
            "patch": "@@ -50,24 +50,23 @@ class InstructBlipProcessor(ProcessorMixin):\n             An instance of [`BlipImageProcessor`]. The image processor is a required input.\n         tokenizer (`AutoTokenizer`):\n             An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n-        qformer_tokenizer (`AutoTokenizer`, *optional*):\n+        qformer_tokenizer (`AutoTokenizer`):\n             An instance of ['PreTrainedTokenizer`]. The Q-Former tokenizer is a required input.\n         num_query_tokens (`int`, *optional*):\"\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n+    attributes = [\"image_processor\", \"tokenizer\", \"qformer_tokenizer\"]\n     valid_kwargs = [\"num_query_tokens\"]\n     image_processor_class = \"BlipImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n+    qformer_tokenizer_class = \"AutoTokenizer\"\n \n-    def __init__(self, image_processor, tokenizer, qformer_tokenizer=None, num_query_tokens=None, **kwargs):\n-        # add QFormer tokenizer\n-        self.qformer_tokenizer = qformer_tokenizer\n+    def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n         self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n         tokenizer.add_tokens([self.image_token], special_tokens=True)\n         self.num_query_tokens = num_query_tokens\n-        super().__init__(image_processor, tokenizer)\n+        super().__init__(image_processor, tokenizer, qformer_tokenizer)\n \n     def __call__(\n         self,\n@@ -205,7 +204,17 @@ def save_pretrained(self, save_directory, **kwargs):\n         os.makedirs(save_directory, exist_ok=True)\n         qformer_tokenizer_path = os.path.join(save_directory, \"qformer_tokenizer\")\n         self.qformer_tokenizer.save_pretrained(qformer_tokenizer_path)\n-        return super().save_pretrained(save_directory, **kwargs)\n+\n+        # We modify the attributes so that only the tokenizer and image processor are saved in the main folder\n+        qformer_present = \"qformer_tokenizer\" in self.attributes\n+        if qformer_present:\n+            self.attributes.remove(\"qformer_tokenizer\")\n+\n+        outputs = super().save_pretrained(save_directory, **kwargs)\n+\n+        if qformer_present:\n+            self.attributes += [\"qformer_tokenizer\"]\n+        return outputs\n \n     # overwrite to load the Q-Former tokenizer from a separate folder\n     @classmethod"
        },
        {
            "sha": "39bcc6a06c3595f89d43b27a6d838d5a5f8365bc",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b",
            "patch": "@@ -50,24 +50,23 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n             An instance of [`InstructBlipVideoImageProcessor`]. The image processor is a required input.\n         tokenizer (`AutoTokenizer`):\n             An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n-        qformer_tokenizer (`AutoTokenizer`, *optional*):\n+        qformer_tokenizer (`AutoTokenizer`):\n             An instance of ['PreTrainedTokenizer`]. The Q-Former tokenizer is a required input.\n         num_query_tokens (`int`, *optional*):\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n+    attributes = [\"image_processor\", \"tokenizer\", \"qformer_tokenizer\"]\n     valid_kwargs = [\"num_query_tokens\"]\n     image_processor_class = \"InstructBlipVideoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n+    qformer_tokenizer_class = \"AutoTokenizer\"\n \n-    def __init__(self, image_processor, tokenizer, qformer_tokenizer=None, num_query_tokens=None, **kwargs):\n-        # add QFormer tokenizer\n-        self.qformer_tokenizer = qformer_tokenizer\n+    def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n         self.video_token = AddedToken(\"<video>\", normalized=False, special=True)\n         tokenizer.add_tokens([self.video_token], special_tokens=True)\n         self.num_query_tokens = num_query_tokens\n-        super().__init__(image_processor, tokenizer)\n+        super().__init__(image_processor, tokenizer, qformer_tokenizer)\n \n     def __call__(\n         self,\n@@ -95,6 +94,9 @@ def __call__(\n \n         Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You have to specify at least one of images or text.\")\n+\n         encoding = BatchFeature()\n \n         if text is not None:\n@@ -204,7 +206,17 @@ def save_pretrained(self, save_directory, **kwargs):\n         os.makedirs(save_directory, exist_ok=True)\n         qformer_tokenizer_path = os.path.join(save_directory, \"qformer_tokenizer\")\n         self.qformer_tokenizer.save_pretrained(qformer_tokenizer_path)\n-        return super().save_pretrained(save_directory, **kwargs)\n+\n+        # We modify the attributes so that only the tokenizer and image processor are saved in the main folder\n+        qformer_present = \"qformer_tokenizer\" in self.attributes\n+        if qformer_present:\n+            self.attributes.remove(\"qformer_tokenizer\")\n+\n+        outputs = super().save_pretrained(save_directory, **kwargs)\n+\n+        if qformer_present:\n+            self.attributes += [\"qformer_tokenizer\"]\n+        return outputs\n \n     # overwrite to load the Q-Former tokenizer from a separate folder\n     @classmethod"
        },
        {
            "sha": "59198bcc5002c24e162474ea06178733b288582c",
            "filename": "tests/models/instructblip/test_processor_instructblip.py",
            "status": "modified",
            "additions": 236,
            "deletions": 2,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py?ref=d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b",
            "patch": "@@ -18,9 +18,11 @@\n import numpy as np\n import pytest\n \n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n     from PIL import Image\n@@ -36,7 +38,9 @@\n \n \n @require_vision\n-class InstructBlipProcessorTest(unittest.TestCase):\n+class InstructBlipProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = InstructBlipProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -189,3 +193,233 @@ def test_model_input_names(self):\n             list(inputs.keys()),\n             [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n         )\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", padding=\"longest\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", padding=\"longest\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(\n+            text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n+        )\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_doubly_passed_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\"]\n+        image_input = self.prepare_image_inputs()\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n+                size={\"height\": 214, \"width\": 214},\n+            )\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Override as InstructBlipProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)"
        },
        {
            "sha": "945fe004d12e02a0deb1dca1971c62673cb17a35",
            "filename": "tests/models/instructblipvideo/test_processor_instructblipvideo.py",
            "status": "added",
            "additions": 425,
            "deletions": 0,
            "changes": 425,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py?ref=d2dcff96f8c0d33c4a76bb601fd43a0ae67d128b",
            "patch": "@@ -0,0 +1,425 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import pytest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import (\n+        AutoProcessor,\n+        BertTokenizerFast,\n+        GPT2Tokenizer,\n+        InstructBlipVideoImageProcessor,\n+        InstructBlipVideoProcessor,\n+        PreTrainedTokenizerFast,\n+    )\n+\n+\n+@require_vision\n+# Copied from tests.models.instructblip.test_processor_instructblip.InstructBlipProcessorTest with InstructBlip->InstructBlipVideo, BlipImageProcessor->InstructBlipVideoImageProcessor\n+class InstructBlipVideoProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = InstructBlipVideoProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = InstructBlipVideoImageProcessor()\n+        tokenizer = GPT2Tokenizer.from_pretrained(\"hf-internal-testing/tiny-random-GPT2Model\")\n+        qformer_tokenizer = BertTokenizerFast.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n+\n+        processor = InstructBlipVideoProcessor(image_processor, tokenizer, qformer_tokenizer)\n+\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_qformer_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).qformer_tokenizer\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    # Ignore copy\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of list of PIL images\"\"\"\n+\n+        video_inputs = [\n+            [Image.fromarray(np.random.randint(255, size=(30, 400, 3), dtype=np.uint8)) for _ in range(5)]\n+            for _ in range(2)\n+        ]\n+        return video_inputs\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        processor = InstructBlipVideoProcessor(\n+            tokenizer=self.get_tokenizer(),\n+            image_processor=self.get_image_processor(),\n+            qformer_tokenizer=self.get_qformer_tokenizer(),\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS)\", eos_token=\"(EOS)\")\n+        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n+\n+        processor = InstructBlipVideoProcessor.from_pretrained(\n+            self.tmpdirname, bos_token=\"(BOS)\", eos_token=\"(EOS)\", do_normalize=False, padding_value=1.0\n+        )\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n+        self.assertIsInstance(processor.tokenizer, PreTrainedTokenizerFast)\n+\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.image_processor, InstructBlipVideoImageProcessor)\n+        self.assertIsInstance(processor.qformer_tokenizer, BertTokenizerFast)\n+\n+    def test_image_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        qformer_tokenizer = self.get_qformer_tokenizer()\n+\n+        processor = InstructBlipVideoProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n+        input_processor = processor(images=image_input, return_tensors=\"np\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_tokenizer(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        qformer_tokenizer = self.get_qformer_tokenizer()\n+\n+        processor = InstructBlipVideoProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+\n+        input_str = [\"lower newer\"]\n+\n+        encoded_processor = processor(text=input_str)\n+\n+        encoded_tokens = tokenizer(input_str, return_token_type_ids=False)\n+        encoded_tokens_qformer = qformer_tokenizer(input_str, return_token_type_ids=False)\n+\n+        for key in encoded_tokens.keys():\n+            self.assertListEqual(encoded_tokens[key], encoded_processor[key])\n+\n+        for key in encoded_tokens_qformer.keys():\n+            self.assertListEqual(encoded_tokens_qformer[key], encoded_processor[\"qformer_\" + key])\n+\n+    def test_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        qformer_tokenizer = self.get_qformer_tokenizer()\n+\n+        processor = InstructBlipVideoProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+\n+        self.assertListEqual(\n+            list(inputs.keys()),\n+            [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n+        )\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n+    def test_tokenizer_decode(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        qformer_tokenizer = self.get_qformer_tokenizer()\n+\n+        processor = InstructBlipVideoProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+\n+        predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n+\n+        decoded_processor = processor.batch_decode(predicted_ids)\n+        decoded_tok = tokenizer.batch_decode(predicted_ids)\n+\n+        self.assertListEqual(decoded_tok, decoded_processor)\n+\n+    def test_model_input_names(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        qformer_tokenizer = self.get_qformer_tokenizer()\n+\n+        processor = InstructBlipVideoProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+\n+        self.assertListEqual(\n+            list(inputs.keys()),\n+            [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n+        )\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", padding=\"longest\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", padding=\"longest\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(\n+            text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n+        )\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_doubly_passed_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\"]\n+        image_input = self.prepare_image_inputs()\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n+                size={\"height\": 214, \"width\": 214},\n+            )\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)"
        }
    ],
    "stats": {
        "total": 712,
        "additions": 696,
        "deletions": 16
    }
}