{
    "author": "YangKai0616",
    "message": "[XPU] Fix UT errors in the sam3 and lfm series model. (#42798)\n\n* Make sam3 tests pass on XPU\n\n* Update flm2 tests GT for XPU\n\n* Remove the skip tests of local mask for XPU\n\n* Pass position_ids to varlen FA2\n\n* Change modular also\n\n* Skip FA2 bwd tests\n\n* Make style\n\n* Increase rtol\n\n* Adapt to the main branch\n\n* fix cuda 1\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "f80b0485feccccb8102ee788b7609e3ec98a1567",
    "files": [
        {
            "sha": "cef19eb98301ce90f46eb35543fca8486c01c89c",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 12,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=f80b0485feccccb8102ee788b7609e3ec98a1567",
            "patch": "@@ -176,8 +176,8 @@ def test_model_1a8b_logits(self):\n         # Expected mean on dim = -1\n         EXPECTED_MEANS = Expectations(\n             {\n-                (\"cuda\", None): torch.tensor([[-1.3855, -0.5123, -1.3143, -1.2144, -1.0791, -1.2117, -1.4704, -0.7648, -0.6175, -1.2402, -1.1459, -1.0083, -1.0247, -0.8830, -1.5643, -1.7266, -1.6254,]]),\n-                (\"xpu\", None): torch.tensor([[-1.3863, -0.4653, -1.3246, -1.3199, -1.0940, -1.2254, -1.4716, -0.8852, -0.5920, -1.2182, -1.1782, -1.0268, -1.0114, -0.8816, -1.5774, -1.7408, -1.6147,]]),\n+                (\"cuda\", None): torch.tensor([[-1.3912, -0.4653, -1.3339, -1.3249, -1.0985, -1.2373, -1.4599, -0.7515, -0.6140, -1.2329, -1.1481, -1.0081, -0.9937, -0.8875, -1.5539, -1.7283, -1.6284]]),\n+                (\"xpu\", None): torch.tensor([[-1.3879, -0.4730, -1.3193, -1.3139, -1.0826, -1.2129, -1.4744, -0.7485, -0.6004, -1.2353, -1.1602, -1.0432, -1.0180, -0.9099, -1.5949, -1.7487, -1.5991]]),\n             }\n         )\n         # fmt: on\n@@ -188,8 +188,8 @@ def test_model_1a8b_logits(self):\n         # Expected portion of the logits\n         EXPECTED_SLICES = Expectations(\n             {\n-                (\"cuda\", None): torch.tensor([-1.2656, 2.4844, 5.5000, -1.3359, -1.3203, -1.3438, 1.9375, 5.8438, -0.6523, -1.2891]),\n-                (\"xpu\", None): torch.tensor([-1.2656, 2.4531, 5.4375, -1.3438, -1.3203, -1.3516, 1.9297, 5.7812, -0.6719, -1.3203]),\n+                (\"cuda\", None): torch.tensor([-1.2734, 2.4844, 5.5000, -1.3438, -1.3281, -1.3516, 1.9375, 5.8438, -0.6641, -1.2969]),\n+                (\"xpu\", None): torch.tensor([-1.2734,  2.4531, 5.4688, -1.3438, -1.3281, -1.3516, 1.9297, 5.7812, -0.6719, -1.3125]),\n             }\n         )\n         # fmt: on\n@@ -219,14 +219,16 @@ def test_model_1a8b_batched_chat_generation(self):\n         # fmt: off\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n-                (\"cuda\", None): [\"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n-                                 \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n-                                 \"The Meji Restoration in Japan ended or the Meiji Restoration (1868â€“1912) marked a pivotal\",\n-                                ],\n-                (\"xpu\", None): ['Who are you? (AI) designed to assist?  \\nI am an AI assistant developed to',\n-                                'Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor',\n-                                'The Meji Restoration in Japan ended**  \\n* **Key Event:** The overthrow of the Tokugawa'\n-                               ],\n+                (\"cuda\", None): [\n+                    \"Who are you? (AI) designed to assist?  \\nI am an AI assistant developed to\",\n+                    \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum.\",\n+                    \"The Meji Restoration in Japan ended**  \\n**A.** The shogunate was abolished, and imperial\"\n+                ],\n+                (\"xpu\", None): [\n+                    \"Who are you? (AI) designed to assist?  \\nI am an AI language model developed\",\n+                    \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n+                    \"The Meji Restoration in Japan ended, which occurred in 1868, marked the:  \\nA) Establish\"\n+                ],\n             }\n         )\n         # fmt: on"
        },
        {
            "sha": "8bf48d9b0f1e507893bf35810f14a0ef4ec766fa",
            "filename": "tests/models/sam3/test_modeling_sam3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py?ref=f80b0485feccccb8102ee788b7609e3ec98a1567",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers.testing_utils import (\n     backend_empty_cache,\n+    require_deterministic_for_xpu,\n     require_torch,\n     slow,\n     torch_device,\n@@ -1448,6 +1449,7 @@ def test_semantic_segmentation_output(self):\n         # Check that semantic seg has same spatial size as pred_masks\n         self.assertEqual(outputs.semantic_seg.shape[-2:], outputs.pred_masks.shape[-2:])\n \n+    @require_deterministic_for_xpu\n     def test_efficient_multi_prompt_single_image(self):\n         \"\"\"Test efficient inference with multiple prompts on a single image using get_vision_features.\"\"\"\n         raw_image = prepare_coco_cat_image()\n@@ -1491,6 +1493,7 @@ def test_efficient_multi_prompt_single_image(self):\n         torch.testing.assert_close(outputs_with_embeds.pred_boxes, outputs_direct.pred_boxes, atol=1e-5, rtol=1e-5)\n         torch.testing.assert_close(outputs_with_embeds.pred_masks, outputs_direct.pred_masks, atol=1e-5, rtol=1e-5)\n \n+    @require_deterministic_for_xpu\n     def test_efficient_single_prompt_multi_images(self):\n         \"\"\"Test efficient inference with same prompt on multiple images using get_text_features.\"\"\"\n         raw_image1 = prepare_coco_cat_image()"
        },
        {
            "sha": "7e066bd84ee52d5f82a36bcca9e449dbcd68df6d",
            "filename": "tests/models/sam3_tracker_video/test_modeling_sam3_tracker_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py?ref=f80b0485feccccb8102ee788b7609e3ec98a1567",
            "patch": "@@ -428,7 +428,7 @@ def test_inference_mask_generation_video_batched_bb(self):\n                 ]\n             ).to(torch_device),\n             atol=1e-4,\n-            rtol=1e-4,\n+            rtol=1e-3,\n         )\n \n     def test_inference_propagate_video_from_mask_input(self):"
        },
        {
            "sha": "5dc7c0d390cc3eefe5e886e054de16779c26dc8a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f80b0485feccccb8102ee788b7609e3ec98a1567/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f80b0485feccccb8102ee788b7609e3ec98a1567",
            "patch": "@@ -2749,12 +2749,6 @@ def flash_attn_inference_equivalence(\n             if getattr(config, \"sliding_window\", None):\n                 config.sliding_window = 2\n \n-                if torch_device == \"xpu\" and (\n-                    attn_implementation == \"kernels-community/flash-attn2\"\n-                    or attn_implementation == \"flash_attention_2\"\n-                ):\n-                    self.skipTest(\"XPU does not support sliding window attention with Flash-Attention-2 currently.\")\n-\n             model = model_class(config)\n             if not all(\n                 submodel._supports_flash_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n@@ -3386,6 +3380,9 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n         if not is_torch_fp16_available_on_device(torch_device):\n             self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n \n+        if torch_device == \"xpu\":\n+            self.skipTest(\"XPU FA2 currently does not support backward.\")\n+\n         torch.compiler.reset()\n         dtype = torch.float16\n "
        }
    ],
    "stats": {
        "total": 40,
        "additions": 21,
        "deletions": 19
    }
}