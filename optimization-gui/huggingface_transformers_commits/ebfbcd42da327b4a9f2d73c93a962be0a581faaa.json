{
    "author": "remi-or",
    "message": "Test fixes for Aria (and some Expectation for llava_next_video) (#39131)\n\n* Expectations for llava_next_video\n\n* Updated image src in aria\n\n* Fix test_small_model_integration_test\n\n* Fix small model integration llama\n\n* Fix a bunch of tests\n\n* Style\n\n* Shortened generation in test from 900 to 90",
    "sha": "ebfbcd42da327b4a9f2d73c93a962be0a581faaa",
    "files": [
        {
            "sha": "cdab28a3a7d6b1e47b2914f7f8f24cd6488f6968",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 119,
            "deletions": 54,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebfbcd42da327b4a9f2d73c93a962be0a581faaa/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebfbcd42da327b4a9f2d73c93a962be0a581faaa/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=ebfbcd42da327b4a9f2d73c93a962be0a581faaa",
            "patch": "@@ -25,6 +25,7 @@\n     AriaTextConfig,\n     AutoProcessor,\n     AutoTokenizer,\n+    BitsAndBytesConfig,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -52,6 +53,9 @@\n if is_vision_available():\n     from PIL import Image\n \n+# Used to be https://aria-vl.github.io/static/images/view.jpg but it was removed, llava-vl has the same image\n+IMAGE_OF_VIEW_URL = \"https://llava-vl.github.io/static/images/view.jpg\"\n+\n \n class AriaVisionText2TextModelTester:\n     def __init__(\n@@ -262,23 +266,38 @@ def tearDown(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         # Let's make sure we test the preprocessing to replace what is used\n-        model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            \"rhymes-ai/Aria\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n+        )\n \n-        prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n-        image_file = \"https://aria-vl.github.io/static/images/view.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\")\n+        prompt = \"<|img|>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n+        raw_image = Image.open(requests.get(IMAGE_OF_VIEW_URL, stream=True).raw)\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(model.device, model.dtype)\n \n-        EXPECTED_INPUT_IDS = torch.tensor([[1, 32000, 28705, 13, 11123, 28747, 1824, 460, 272, 1722,315, 1023, 347, 13831, 925, 684, 739, 315, 3251, 456,1633, 28804, 13, 4816, 8048, 12738, 28747]])  # fmt: skip\n+        non_img_tokens = [\n+            109, 3905, 2000, 93415, 4551, 1162, 901, 3894, 970, 2478, 1017, 19312, 2388, 1596, 1809, 970, 5449, 1235,\n+            3333, 93483, 109, 61081, 11984, 14800, 93415\n+        ]  # fmt: skip\n+        EXPECTED_INPUT_IDS = torch.tensor([[9] * 256 + non_img_tokens]).to(inputs[\"input_ids\"].device)\n         self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n \n         output = model.generate(**inputs, max_new_tokens=20)\n-        EXPECTED_DECODED_TEXT = \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\"  # fmt: skip\n+        decoded_output = self.processor.decode(output[0], skip_special_tokens=True)\n \n-        self.assertEqual(\n-            self.processor.decode(output[0], skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        expected_output = Expectations(\n+            {\n+                (\n+                    \"cuda\",\n+                    None,\n+                ): \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\",\n+                (\n+                    \"rocm\",\n+                    (9, 5),\n+                ): \"\\n USER: What are the things I should be cautious about when I visit this place?\\n ASSISTANT: When you visit this place, you should be cautious about the following things:\\n\\n- The\",\n+            }\n+        ).get_expectation()\n+        self.assertEqual(decoded_output, expected_output)\n \n     @slow\n     @require_torch_large_accelerator\n@@ -287,20 +306,29 @@ def test_small_model_integration_test_llama_single(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"rhymes-ai/Aria\"\n \n-        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            model_id,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n-        prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? ASSISTANT:\"\n-        image_file = \"https://aria-vl.github.io/static/images/view.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        prompt = \"USER: <|img|>\\nWhat are the things I should be cautious about when I visit this place? ASSISTANT:\"\n+        raw_image = Image.open(requests.get(IMAGE_OF_VIEW_URL, stream=True).raw)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(model.device, model.dtype)\n \n-        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n-        EXPECTED_DECODED_TEXT = \"USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Finally, be respectful of the environment and other visitors, and follow any posted rules or guidelines for the area.\"  # fmt: skip\n+        output = model.generate(**inputs, max_new_tokens=90, do_sample=False)\n+        EXPECTED_DECODED_TEXT = Expectations(\n+            {\n+                (\"cuda\", (8, 0)): \"USER: \\n What are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this beautiful location, it's important to be mindful of a few things to ensure both your safety and the preservation of the environment. Firstly, always be cautious when walking on the wooden pier, as it can be slippery, especially during or after rain. Secondly, be aware of the local wildlife and do not feed or disturb them. Lastly, respect the natural surroundings by not littering and sticking to\",\n+                (\"rocm\", (9, 5)): \"USER: \\n What are the things I should be cautious about when I visit this place? ASSISTANT: \\n\\nWhen visiting this place, you should be cautious about the following:\\n\\n1. **Weather Conditions**: The weather can be unpredictable, so it's important to check the forecast and dress in layers. Sudden changes in weather can occur, so be prepared for rain or cold temperatures.\\n\\n2. **Safety on the Dock**: The dock may be slippery, especially when\",\n+            }\n+        ).get_expectation()  # fmt: off\n \n+        decoded_output = processor.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n         self.assertEqual(\n-            processor.decode(output[0], skip_special_tokens=True),\n+            decoded_output,\n             EXPECTED_DECODED_TEXT,\n+            f\"Expected: {repr(EXPECTED_DECODED_TEXT)}\\nActual: {repr(decoded_output)}\",\n         )\n \n     @slow\n@@ -310,53 +338,77 @@ def test_small_model_integration_test_llama_batched(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"rhymes-ai/Aria\"\n \n-        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            model_id,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         prompts = [\n-            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT:\",\n-            \"USER: <image>\\nWhat is this? ASSISTANT:\",\n+            \"USER: <|img|>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT:\",\n+            \"USER: <|img|>\\nWhat is this? ASSISTANT:\",\n         ]\n-        image1 = Image.open(requests.get(\"https://aria-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image1 = Image.open(requests.get(IMAGE_OF_VIEW_URL, stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n+        inputs = processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True).to(\n+            model.device, model.dtype\n+        )\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, you', 'USER:  \\nWhat is this? ASSISTANT: The image features two cats lying down on a pink couch. One cat is located on']  # fmt: skip\n+        EXPECTED_DECODED_TEXT = Expectations(\n+            {\n+                (\"cuda\", None): [\n+                    \"USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, you\",\n+                    \"USER:  \\nWhat is this? ASSISTANT: The image features two cats lying down on a pink couch. One cat is located on\",\n+                ],\n+                (\"rocm\", (9, 5)): [\n+                    \"USER: \\n What are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT: \\n\\nWhen visiting this place, you should be cautious about the weather conditions, as it\",\n+                    \"USER: \\n What is this? ASSISTANT: This is a picture of two cats sleeping on a couch. USER: What is the color of\",\n+                ],\n+            }\n+        ).get_expectation()\n \n-        self.assertEqual(\n-            processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n         # Let's make sure we test the preprocessing to replace what is used\n-        model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            \"rhymes-ai/Aria\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n+        )\n         # The first batch is longer in terms of text, but only has 1 image. The second batch will be padded in text, but the first will be padded because images take more space!.\n         prompts = [\n-            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n-            \"USER: <image>\\nWhat is this?\\nASSISTANT:\",\n+            \"USER: <|img|>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n+            \"USER: <|img|>\\nWhat is this?\\nASSISTANT:\",\n         ]\n-        image1 = Image.open(requests.get(\"https://aria-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image1 = Image.open(requests.get(IMAGE_OF_VIEW_URL, stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True).to(\n+            model.device, model.dtype\n+        )\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = [\n-            'USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, there are a few things to be cautious about and items to bring.',\n-            'USER:  \\nWhat is this?\\nASSISTANT: Cats'\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        EXPECTED_DECODED_TEXT = Expectations({\n+            (\"cuda\", None): [\n+                'USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, there are a few things to be cautious about and items to bring.',\n+                'USER:  \\nWhat is this?\\nASSISTANT: Cats',\n+            ],\n+            (\"rocm\", (9, 5)): [\n+                'USER: \\n What are the things I should be cautious about when I visit this place? What should I bring with me?\\n ASSISTANT: \\n\\nWhen visiting this place, you should be cautious about the following:\\n\\n-',\n+                'USER: \\n What is this?\\n ASSISTANT: This is a picture of two cats sleeping on a couch. The couch is red, and the cats',\n+            ],\n+        }).get_expectation()  # fmt: skip\n+\n+        decoded_output = self.processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_torch_large_accelerator\n@@ -366,26 +418,31 @@ def test_small_model_integration_test_llama_batched_regression(self):\n         model_id = \"rhymes-ai/Aria\"\n \n         # Multi-image & multi-prompt (e.g. 3 images and 2 prompts now fails with SDPA, this tests if \"eager\" works as before)\n-        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True, attn_implementation=\"eager\")\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            model_id,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n+        )\n         processor = AutoProcessor.from_pretrained(model_id, pad_token=\"<pad>\")\n \n         prompts = [\n-            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n-            \"USER: <image>\\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER: <image>\\nAnd this?\\nASSISTANT:\",\n+            \"USER: <|img|>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n+            \"USER: <|img|>\\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER: <|img|>\\nAnd this?\\nASSISTANT:\",\n         ]\n-        image1 = Image.open(requests.get(\"https://aria-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image1 = Image.open(requests.get(IMAGE_OF_VIEW_URL, stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n         inputs = processor(images=[image1, image2, image1], text=prompts, return_tensors=\"pt\", padding=True)\n+        inputs = inputs.to(model.device, model.dtype)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, which appears to be a dock or pier extending over a body of water', 'USER:  \\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER:  \\nAnd this?\\nASSISTANT: A cat sleeping on a bed.']  # fmt: skip\n+        EXPECTED_DECODED_TEXT = Expectations({\n+            (\"cuda\", None): ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, which appears to be a dock or pier extending over a body of water', 'USER:  \\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER:  \\nAnd this?\\nASSISTANT: A cat sleeping on a bed.'],\n+            (\"rocm\", (9, 5)): ['USER: \\n What are the things I should be cautious about when I visit this place? What should I bring with me?\\n ASSISTANT: \\n\\nWhen visiting this place, you should be cautious about the weather conditions, as it', 'USER: \\n What is this?\\n ASSISTANT: Two cats lying on a bed!\\n USER: \\n And this?\\n ASSISTANT: A serene lake scene with a wooden dock extending into the water.\\n USER: \\n']\n+        }).get_expectation()  # fmt: skip\n \n-        self.assertEqual(\n-            processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_torch_large_accelerator\n@@ -395,7 +452,8 @@ def test_batched_generation(self):\n         # Skip multihead_attn for 4bit because MHA will read the original weight without dequantize.\n         # See https://github.com/huggingface/transformers/pull/37444#discussion_r2045852538.\n         model = AriaForConditionalGeneration.from_pretrained(\n-            \"rhymes-ai/Aria\", load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]\n+            \"rhymes-ai/Aria\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n         )\n         processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n \n@@ -447,6 +505,10 @@ def test_batched_generation(self):\n                     \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n                     \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a patch of ground with some dry grass. The\",\n                 ],\n+                (\"rocm\", (9, 5)): [\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image shows a cute golden retriever puppy sitting on a paved surface with a stick\",\n+                    '<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young llama standing on a patch of ground with some dry grass and dirt. The'\n+                ],\n             }\n         )  # fmt: skip\n         EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n@@ -480,9 +542,12 @@ def test_tokenizer_integration(self):\n     @require_bitsandbytes\n     def test_generation_no_images(self):\n         model_id = \"rhymes-ai/Aria\"\n-        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = AriaForConditionalGeneration.from_pretrained(\n+            model_id,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n-\n+        assert model.device.type == \"cuda\", \"This test is only supported on CUDA\"  # TODO: remove this\n         # Prepare inputs with no images\n         inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n "
        },
        {
            "sha": "2cddb1ecfd361a00463bd443f23bfb090216512d",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 29,
            "deletions": 15,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebfbcd42da327b4a9f2d73c93a962be0a581faaa/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebfbcd42da327b4a9f2d73c93a962be0a581faaa/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=ebfbcd42da327b4a9f2d73c93a962be0a581faaa",
            "patch": "@@ -29,6 +29,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_bitsandbytes,\n     require_torch,\n@@ -378,12 +379,16 @@ def test_small_model_integration_test(self):\n \n         # verify generation\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=40)\n-        EXPECTED_DECODED_TEXT = (\n-            \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while another child is attempting to read the same book. The child who is reading the book seems\",  # cuda output\n-            \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while wearing a pair of glasses that are too large for them. The glasses are\",  # xpu output\n-        )\n+        expected_decoded_text = Expectations(\n+            {\n+                (\"cuda\", None): \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while another child is attempting to read the same book. The child who is reading the book seems\",\n+                (\"xpu\", None): \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while wearing a pair of glasses that are too large for them. The glasses are\",\n+                (\"rocm\", (9, 5)): \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and adorable behavior of the young child. The child is seen reading a book, but instead of turning the pages like one would typically do, they\",\n+            }\n+        ).get_expectation()  # fmt: off\n \n-        self.assertTrue(self.processor.decode(output[0], skip_special_tokens=True) in EXPECTED_DECODED_TEXT)\n+        decoded_text = self.processor.decode(output[0], skip_special_tokens=True)\n+        self.assertEqual(decoded_text, expected_decoded_text)\n \n     @slow\n     @require_bitsandbytes\n@@ -400,15 +405,17 @@ def test_small_model_integration_test_batch(self):\n         ).to(torch_device)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n+        decoded_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_DECODED_TEXT = [\n-            'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a',\n-            'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a'\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        expected_decoded_text = Expectations(\n+            {\n+                (\"cuda\", None): \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a\",\n+                (\"rocm\", (9, 5)): \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and adorable behavior of the young child. The\",\n+            }\n+        ).get_expectation()  # fmt: off\n+        EXPECTED_DECODED_TEXT = [expected_decoded_text, expected_decoded_text]\n+\n+        self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_bitsandbytes\n@@ -435,8 +442,15 @@ def test_small_model_integration_test_batch_different_vision_types(self):\n \n         # verify generation\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=50)\n-        EXPECTED_DECODED_TEXT = 'USER: \\nWhat is shown in this image? ASSISTANT: The image appears to be a graphical representation of a machine learning model\\'s performance on a task, likely related to natural language processing or text understanding. It shows a scatter plot with two axes, one labeled \"BLIP-2\"'  # fmt: skip\n-        self.assertEqual(self.processor.decode(output[0], skip_special_tokens=True), EXPECTED_DECODED_TEXT)\n+        EXPECTED_DECODED_TEXT = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): \"USER: \\nWhat is shown in this image? ASSISTANT: The image displays a chart that appears to be a comparison of different models or versions of a machine learning (ML) model, likely a neural network, based on their performance on a task or dataset. The chart is a scatter plot with axes labeled\",\n+                (\"cuda\", None): 'USER: \\nWhat is shown in this image? ASSISTANT: The image appears to be a graphical representation of a machine learning model\\'s performance on a task, likely related to natural language processing or text understanding. It shows a scatter plot with two axes, one labeled \"BLIP-2\"',\n+            }\n+        ).get_expectation()  # fmt: off\n+\n+        decoded_text = self.processor.decode(output[0], skip_special_tokens=True)\n+        self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_bitsandbytes"
        }
    ],
    "stats": {
        "total": 217,
        "additions": 148,
        "deletions": 69
    }
}