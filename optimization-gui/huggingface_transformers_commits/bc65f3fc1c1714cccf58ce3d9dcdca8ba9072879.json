{
    "author": "Cyrilvallez",
    "message": "[modular] Do not track imports in functions (#36279)\n\n* Add check\n\n* just check for function\n\n* Update examples",
    "sha": "bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
    "files": [
        {
            "sha": "febd1b886752c03819025276f6bac360fb7b714e",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -140,6 +140,11 @@ class MyNewModelConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "a5364a85d53720605bf35b228a20568606be4beb",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -43,6 +43,11 @@ class MyNewModel2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "ba05b4ea51b2c5d455ce51ef10d1fb1ee47228a7",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -79,6 +79,20 @@ class NewModelConfig(PretrainedConfig):\n \n     model_type = \"new_model\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "bde80dd096417ce3785aa230d36f2e5186340041",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -19,7 +19,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -221,8 +221,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n-\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "7ade13e977f133263cca96ea00c1741bbf444fb5",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -356,6 +356,7 @@ class DummyPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -698,7 +699,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "ee649f9286477e94ab38896e2bde28ad8317c288",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -356,6 +356,7 @@ class Multimodal1TextPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -698,7 +699,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "854d280663ff72c540a2b1695f1261b596215b6a",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 10,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -356,6 +356,7 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -491,6 +492,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwarg for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -703,7 +705,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n@@ -787,17 +791,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a01094737cd6b17b35aa4759832a09a2c0034866",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 22,
            "deletions": 14,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -19,6 +19,7 @@\n     add_start_docstrings_to_model_forward,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_new_task_model import NewTaskModelConfig\n \n@@ -254,8 +255,7 @@ def _update_causal_mask(\n         token_type_ids,\n         past_key_values,\n         cache_position,\n-        input_ids=None,\n-        inputs_embeds=None,\n+        input_tensor,\n         is_training: bool = False,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n@@ -265,8 +265,7 @@ def _update_causal_mask(\n \n         using_static_cache = isinstance(past_key_values, StaticCache)\n         min_dtype = torch.finfo(self.dtype).min\n-        inputs_lead_dim = input_ids.shape[0] if input_ids is not None else inputs_embeds.shape[0]\n-        sequence_length = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+        inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         elif isinstance(past_key_values, HybridCache):\n@@ -297,16 +296,20 @@ def _update_causal_mask(\n         if attention_mask is not None:\n             causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n             mask_length = attention_mask.shape[-1]\n+\n+            # First unmask prefix tokens during training\n+            if is_training:\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n+                )\n+\n+            # Then apply padding mask (will mask pad tokens)\n             padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n             padding_mask = padding_mask == 0\n             causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                 padding_mask, min_dtype\n             )\n-            # we are training thus we need to create a full mask on the image + prefix but causal on suffix\n-            if is_training:\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n-                )\n+\n         return causal_mask\n \n     def get_image_features(self, pixel_values: torch.FloatTensor):\n@@ -325,6 +328,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(NEW_TASK_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=NewTaskModelCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -351,10 +355,12 @@ def forward(\n                 config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -418,7 +424,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         token_type_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         labels=None,\n         **kwargs,\n     ):\n@@ -431,7 +437,7 @@ def prepare_inputs_for_generation(\n             position_ids=position_ids,\n             cache_position=cache_position,\n             use_cache=use_cache,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n             **kwargs,\n         )\n@@ -445,10 +451,12 @@ def prepare_inputs_for_generation(\n             model_inputs[\"pixel_values\"] = pixel_values\n         is_training = token_type_ids is not None and labels is not None\n         if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n+            input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n             causal_mask = self._update_causal_mask(\n-                attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n+                attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n             )\n             model_inputs[\"attention_mask\"] = causal_mask\n+\n         return model_inputs\n \n     def resize_token_embeddings("
        },
        {
            "sha": "d618cd54e90ad8e5fcaf608b6df2c92b07817119",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -356,6 +356,7 @@ class SuperPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -620,7 +621,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "6c85a1e7a1c07488fd3a8e8982016236ab73a54f",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=bc65f3fc1c1714cccf58ce3d9dcdca8ba9072879",
            "patch": "@@ -649,9 +649,11 @@ def leave_FunctionDef(self, node):\n             self.current_function = None\n \n     def visit_If(self, node):\n-        for stmt in node.body.body:\n-            if m.matches(stmt, m.SimpleStatementLine(body=[m.ImportFrom() | m.Import()])):\n-                self.imports.append(node)\n+        # If we are inside a function, do not add the import to the list of imports\n+        if self.current_function is None:\n+            for stmt in node.body.body:\n+                if m.matches(stmt, m.SimpleStatementLine(body=[m.ImportFrom() | m.Import()])):\n+                    self.imports.append(node)\n \n     def visit_ClassDef(self, node: ClassDef) -> None:\n         \"\"\"Record class nodes to create their dependencies at the end.\"\"\""
        }
    ],
    "stats": {
        "total": 115,
        "additions": 82,
        "deletions": 33
    }
}