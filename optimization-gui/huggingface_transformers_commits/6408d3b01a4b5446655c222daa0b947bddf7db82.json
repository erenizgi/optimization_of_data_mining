{
    "author": "vasqu",
    "message": "[`PEFT`] Fix prefix tuning (#41696)\n\n* fix\n\n* simplify\n\n* add my 2 cents",
    "sha": "6408d3b01a4b5446655c222daa0b947bddf7db82",
    "files": [
        {
            "sha": "d3b2cc7f0dc33aa4936c567c8c7be4fe15365084",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6408d3b01a4b5446655c222daa0b947bddf7db82/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6408d3b01a4b5446655c222daa0b947bddf7db82/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=6408d3b01a4b5446655c222daa0b947bddf7db82",
            "patch": "@@ -733,9 +733,19 @@ def _preprocess_mask_arguments(\n     # If using a cache, it can give all information about mask sizes based on seen tokens\n     if past_key_values is not None:\n         kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n-    # Otherwise, the sizes are simply the input sizes\n+    # Otherwise, we infer based on our input\n     else:\n-        kv_length, kv_offset = input_embeds.shape[1], 0\n+        # 1. Rely on input directly\n+        if attention_mask is None:\n+            kv_length, kv_offset = input_embeds.shape[1], 0\n+        # 2. Rely on the mask instead - needed for special cases like prefix tuning in PEFT\n+        #\n+        # This is a very unique and special case where an encoder utilizes a cache and expects its length\n+        # to be accounted for (usually, they should never use a cache). In general, the mask should always\n+        # match with the input sizes nonetheless (i.e. it does not affect others).\n+        # Conclusion: \"prefix tuning is evil\"\n+        else:\n+            kv_length, kv_offset = attention_mask.shape[-1], 0\n \n     # We check the position_ids for potential packed sequence format (only if the 2D attention mask is explicitly None,\n     # and we don't have past_key_values, i.e. generally a training setup)"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 12,
        "deletions": 2
    }
}