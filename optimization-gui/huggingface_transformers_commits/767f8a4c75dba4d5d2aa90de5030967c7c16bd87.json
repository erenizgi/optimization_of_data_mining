{
    "author": "cyyever",
    "message": "Fix typoes in src and tests (#40845)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
    "files": [
        {
            "sha": "8d6e057be84a7f04f3638bad859af47910ba5ca9",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -79,7 +79,7 @@ class PagedAttentionCache:\n         layer group, and the shape of the cache tensor is `[num_blocks * block_size, num_heads, head_size]`.\n \n     Grouping layers into groups is useful because when we allocate one block to a group N, the block allocated is the\n-        same for all layers in group N, equivalently it is allocated accross all cache tensors. This allows us to\n+        same for all layers in group N, equivalently it is allocated across all cache tensors. This allows us to\n         efficiently allocate and free blocks, and to efficiently read and write key and value states.\n \n     For instance, imagine we have 8 blocks of cache and a model with two layer groups: a full-attention group with 3\n@@ -349,7 +349,7 @@ class PagedAttentionMemoryHandler:\n     The memory footprint consists of three main components:\n     - Cache memory: the space needed to store the cache tensors:\n         2 * layer_group_size * [num_pages, page_size] * cache_dtype\n-    - Activation memory: the space temporarly taken by the largest activation during the model forward pass:\n+    - Activation memory: the space temporarily taken by the largest activation during the model forward pass:\n         peak_activation_per_token * max_tokens_per_batch * activation_dtype_size\n     - Static tensors: the space taken by the input/output buffers and metadata tensors for batch processing, sum of:\n         - inputs_ids + outputs_ids + position_ids + logits_indices: 4 * max_tokens_per_batch * int32_size"
        },
        {
            "sha": "a25c412e688ac1fbb04cc20e6550ed5f25a31682",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -1108,14 +1108,14 @@ def _get_single_block_row_attention(\n         if block_id == to_end_block_id - 2:\n             illegal_blocks.append(1)\n \n-        selected_random_blokcs = []\n+        selected_random_blocks = []\n \n         for i in range(to_end_block_id - to_start_block_id):\n             if perm_block[i] not in illegal_blocks:\n-                selected_random_blokcs.append(perm_block[i])\n-            if len(selected_random_blokcs) == num_rand_blocks:\n+                selected_random_blocks.append(perm_block[i])\n+            if len(selected_random_blocks) == num_rand_blocks:\n                 break\n-        return np.array(selected_random_blokcs, dtype=np.int32)\n+        return np.array(selected_random_blocks, dtype=np.int32)\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->BigBird"
        },
        {
            "sha": "e36e4b06dbef0ad74328aa606412312a6335b91f",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -1086,14 +1086,14 @@ def _get_single_block_row_attention(\n         if block_id == to_end_block_id - 2:\n             illegal_blocks.append(1)\n \n-        selected_random_blokcs = []\n+        selected_random_blocks = []\n \n         for i in range(to_end_block_id - to_start_block_id):\n             if perm_block[i] not in illegal_blocks:\n-                selected_random_blokcs.append(perm_block[i])\n-            if len(selected_random_blokcs) == num_rand_blocks:\n+                selected_random_blocks.append(perm_block[i])\n+            if len(selected_random_blocks) == num_rand_blocks:\n                 break\n-        return np.array(selected_random_blokcs, dtype=np.int32)\n+        return np.array(selected_random_blocks, dtype=np.int32)\n \n \n class BigBirdPegasusEncoderAttention(nn.Module):"
        },
        {
            "sha": "15881a64eb3799cc23bf95c5bece4079717af0a8",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -351,7 +351,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        cache_postion: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Args:\n@@ -492,16 +492,16 @@ def _position_bucket(self, relative_position, num_buckets=32, max_distance=128):\n         relative_position = torch.abs(relative_position)\n         max_exact = num_buckets // 2\n         is_small = relative_position < max_exact\n-        relative_postion_if_large = max_exact + (\n+        relative_position_if_large = max_exact + (\n             torch.log(relative_position.float() / max_exact)\n             / math.log(max_distance / max_exact)\n             * (num_buckets - max_exact)\n         ).to(torch.int32)\n-        relative_postion_if_large = torch.min(\n-            relative_postion_if_large,\n-            torch.full_like(relative_postion_if_large, num_buckets - 1),\n+        relative_position_if_large = torch.min(\n+            relative_position_if_large,\n+            torch.full_like(relative_position_if_large, num_buckets - 1),\n         )\n-        relative_buckets += torch.where(is_small, relative_position.to(torch.int32), relative_postion_if_large)\n+        relative_buckets += torch.where(is_small, relative_position.to(torch.int32), relative_position_if_large)\n         return relative_buckets\n \n "
        },
        {
            "sha": "aefd9648d3fe3fd10a8502d654c9d5ddb8b053e1",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -439,9 +439,9 @@ def convert_transformer_weights(\n         decoder_block_start = path.find(_TRANSFORMER_DECODER_BLOCK)\n         decoder_block_offset = decoder_block_start + _TRANSFORMER_DECODER_BLOCK_LEN\n         decoder_block_path = path[decoder_block_offset:]\n-        next_path_seperator_idx = decoder_block_path.find(\"/\")\n-        layer_idx = decoder_block_path[:next_path_seperator_idx]\n-        decoder_block_path = decoder_block_path[next_path_seperator_idx:]\n+        next_path_separator_idx = decoder_block_path.find(\"/\")\n+        layer_idx = decoder_block_path[:next_path_separator_idx]\n+        decoder_block_path = decoder_block_path[next_path_separator_idx:]\n \n         base_path = f\"language_model.model.layers.{layer_idx}\"\n "
        },
        {
            "sha": "82a1d5e451cadb7ef943dba35cc5fcf61166c950",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -950,7 +950,7 @@ def __init__(self, config):\n         self.visual_projection = GitProjection(config)\n \n         if config.num_image_with_embedding is not None:\n-            self.img_temperal_embedding = nn.ParameterList(\n+            self.img_temporal_embedding = nn.ParameterList(\n                 nn.Parameter(torch.zeros(1, 1, config.vision_config.hidden_size))\n                 for _ in range(config.num_image_with_embedding)\n             )\n@@ -1115,7 +1115,7 @@ def forward(\n                     visual_features_frame = self.image_encoder(\n                         pixel_values[:, frame_idx, :, :], interpolate_pos_encoding=interpolate_pos_encoding\n                     ).last_hidden_state\n-                    visual_features_frame += self.img_temperal_embedding[frame_idx]\n+                    visual_features_frame += self.img_temporal_embedding[frame_idx]\n                     visual_features.append(visual_features_frame)\n \n                 # finally, concatenate all features along sequence dimension"
        },
        {
            "sha": "598845750da29b148ce3c17f60df74a5a51ffc2d",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -74,7 +74,7 @@ def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim\n         y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n         ret = y_hard - y_soft.detach() + y_soft\n     else:\n-        # Reparametrization trick.\n+        # Reparameterization trick.\n         ret = y_soft\n     return ret\n \n@@ -662,7 +662,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "ee8fe04771b70f0d188c04e185691efb7770c2a9",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -242,7 +242,7 @@ def preprocess(\n             raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n         # Here, normalize() is using a constant factor to divide pixel values.\n-        # hence, the method does not need iamge_mean and image_std.\n+        # hence, the method does not need image_mean and image_std.\n         validate_preprocess_arguments(\n             do_resize=do_resize,\n             size=size,"
        },
        {
            "sha": "028ccf6bf8a2a6b409bc45251f6dfe6c91acb886",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -34,7 +34,7 @@\n # Similar to transformers.models.pix2struct.image_processing_pix2struct.torch_extract_patches but dealing with a batch of images directly.\n def torch_extract_patches(image_tensor, patch_height, patch_width):\n     \"\"\"\n-    Utiliy function to extract patches from a given tensor representing a batch of images. Returns a tensor of shape\n+    Utility function to extract patches from a given tensor representing a batch of images. Returns a tensor of shape\n     (batch_size, `rows`, `columns`, `num_channels` x `patch_height` x `patch_width`).\n \n     Args:"
        },
        {
            "sha": "8fa8f0a788755b7310ddd7f97ecc465c1994586a",
            "filename": "src/transformers/models/kyutai_speech_to_text/feature_extraction_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Ffeature_extraction_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Ffeature_extraction_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Ffeature_extraction_kyutai_speech_to_text.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -203,7 +203,7 @@ def __call__(\n             if padding:\n                 padded_inputs[\"padding_mask\"] = padded_inputs.pop(\"attention_mask\")\n \n-        # now let's padd left and right\n+        # now let's pad left and right\n         pad_left = int(self.audio_silence_prefix_seconds * self.sampling_rate)\n         pad_right = int((self.audio_delay_seconds + 1.0) * self.sampling_rate)\n         padded_inputs[\"input_values\"] = np.pad("
        },
        {
            "sha": "77c636570d588f8388698d684d2525d1af58293e",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -1078,7 +1078,7 @@ def __init__(self, config):\n         self.codec_model = AutoModel.from_config(config.codec_config)\n \n         # we are in an edge case where for the codec_model self.can_generate is False, setting self.codec_model.generation_config to None\n-        # yet the codec_model needs a generation config to initalize it's cache for streaming inference\n+        # yet the codec_model needs a generation config to initialize it's cache for streaming inference\n         # we therefore initialize a generation config for the codec model\n         self.codec_model.generation_config = GenerationConfig.from_model_config(config.codec_config)\n "
        },
        {
            "sha": "af8c182f226e45d284e537969f7c894257c1ee9e",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -182,7 +182,7 @@ def __call__(\n             if padding:\n                 padded_inputs[\"padding_mask\"] = padded_inputs.pop(\"attention_mask\")\n \n-        # now let's padd left and right\n+        # now let's pad left and right\n         pad_left = int(self.audio_silence_prefix_seconds * self.sampling_rate)\n         pad_right = int((self.audio_delay_seconds + 1.0) * self.sampling_rate)\n         padded_inputs[\"input_values\"] = np.pad(\n@@ -258,7 +258,7 @@ def __init__(self, config):\n         self.codec_model = AutoModel.from_config(config.codec_config)\n \n         # we are in an edge case where for the codec_model self.can_generate is False, setting self.codec_model.generation_config to None\n-        # yet the codec_model needs a generation config to initalize it's cache for streaming inference\n+        # yet the codec_model needs a generation config to initialize it's cache for streaming inference\n         # we therefore initialize a generation config for the codec model\n         self.codec_model.generation_config = GenerationConfig.from_model_config(config.codec_config)\n "
        },
        {
            "sha": "dc44ad67f71f876fc253cd29a0d1b5758e908c47",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -2882,7 +2882,7 @@ def forward(\n             Task inputs. Task inputs can be obtained using [`AutoImageProcessor`]. See [`OneFormerProcessor.__call__`]\n             for details.\n         text_inputs (`list[torch.Tensor]`, *optional*):\n-            Tensor fof shape `(num_queries, sequence_length)` to be fed to a model\n+            Tensor of shape `(num_queries, sequence_length)` to be fed to a model\n \n         Example:\n \n@@ -3068,7 +3068,7 @@ def forward(\n             Task inputs. Task inputs can be obtained using [`AutoImageProcessor`]. See [`OneFormerProcessor.__call__`]\n             for details.\n         text_inputs (`list[torch.Tensor]`, *optional*):\n-            Tensor fof shape `(num_queries, sequence_length)` to be fed to a model\n+            Tensor of shape `(num_queries, sequence_length)` to be fed to a model\n         mask_labels (`list[torch.Tensor]`, *optional*):\n             List of mask labels of shape `(num_labels, height, width)` to be fed to a model\n         class_labels (`list[torch.LongTensor]`, *optional*):"
        },
        {
            "sha": "c26132a484397067ad7354ea50705e493f063ff1",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -190,7 +190,7 @@ def _fit_image_to_canvas(self, img_width: int, img_height: int, tile_size: int):\n                         target_width=n_w * tile_size,\n                         target_height=n_h * tile_size,\n                     )\n-                    # Llama3V dynamic tiling. Priortize biggest canvas.\n+                    # Llama3V dynamic tiling. Prioritize biggest canvas.\n                     if (scale < 1.0 and (image_width_height[0] >= optimal_image_width_height[0])) or (\n                         scale >= 1.0 and (image_width_height[1] >= optimal_image_width_height[1])\n                     ):"
        },
        {
            "sha": "3c1fdb8b0a8c05e3ca341814ff08ba6ba4ba9e9c",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -137,7 +137,7 @@ class Phi4MultimodalAudioConfig(PretrainedConfig):\n             The dropout ratio.\n         ext_pw_out_channel (`int`, *optional*, defaults to 1024):\n             Number of out channels in the point-wise conv modules.\n-        depthwise_seperable_out_channel (`int`, *optional*, defaults to 1024):\n+        depthwise_separable_out_channel (`int`, *optional*, defaults to 1024):\n             Number of out channels in the depth-wise separable conv modules.\n         depthwise_multiplier (`int`, *optional*, defaults to 1):\n             Input size multiplier for the depth-wise separable conv modules.\n@@ -190,7 +190,7 @@ def __init__(\n         left_chunk: int = 18,\n         dropout_rate: float = 0.0,\n         ext_pw_out_channel: int = 1024,\n-        depthwise_seperable_out_channel: int = 1024,\n+        depthwise_separable_out_channel: int = 1024,\n         depthwise_multiplier: int = 1,\n         kernel_size: int = 3,\n         conv_activation: str = \"swish\",\n@@ -217,7 +217,7 @@ def __init__(\n         self.num_blocks = num_blocks\n         self.dropout_rate = dropout_rate\n         self.ext_pw_out_channel = ext_pw_out_channel\n-        self.depthwise_seperable_out_channel = depthwise_seperable_out_channel\n+        self.depthwise_separable_out_channel = depthwise_separable_out_channel\n         self.depthwise_multiplier = depthwise_multiplier\n         self.kernel_size = kernel_size\n         self.conv_activation = conv_activation"
        },
        {
            "sha": "ad2ef3e07124223702b0b9e67014592c0d72f9db",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -746,7 +746,7 @@ def forward(\n         return attn_output\n \n \n-class Phi4MultimodalAudioDepthWiseSeperableConv1d(nn.Module):\n+class Phi4MultimodalAudioDepthWiseSeparableConv1d(nn.Module):\n     def __init__(self, config: Phi4MultimodalAudioConfig, padding: int = 0):\n         super().__init__()\n         self.dw_conv = nn.Conv1d(\n@@ -758,7 +758,7 @@ def __init__(self, config: Phi4MultimodalAudioConfig, padding: int = 0):\n             groups=config.hidden_size,\n         )\n         self.pw_conv = nn.Conv1d(\n-            config.hidden_size * config.depthwise_multiplier, config.depthwise_seperable_out_channel, 1, 1, 0\n+            config.hidden_size * config.depthwise_multiplier, config.depthwise_separable_out_channel, 1, 1, 0\n         )\n \n     def forward(self, hidden_states):\n@@ -794,7 +794,7 @@ def __init__(self, config: Phi4MultimodalAudioConfig):\n \n         self.layer_norm = nn.LayerNorm(config.hidden_size)\n         self.glu = Phi4MultimodalAudioGluPointWiseConv(config)\n-        self.dw_sep_conv_1d = Phi4MultimodalAudioDepthWiseSeperableConv1d(config, padding=config.kernel_size - 1)\n+        self.dw_sep_conv_1d = Phi4MultimodalAudioDepthWiseSeparableConv1d(config, padding=config.kernel_size - 1)\n         self.act = ACT2FN[config.conv_activation]\n         self.ext_pw_conv_1d = nn.Conv1d(config.hidden_size, config.ext_pw_out_channel, kernel_size=1, stride=1)\n         self.dropout = nn.Dropout(config.dropout_rate)"
        },
        {
            "sha": "0514136cad85ee710f6d392301e4de7057413299",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -174,7 +174,7 @@ class Phi4MultimodalAudioConfig(PretrainedConfig):\n             The dropout ratio.\n         ext_pw_out_channel (`int`, *optional*, defaults to 1024):\n             Number of out channels in the point-wise conv modules.\n-        depthwise_seperable_out_channel (`int`, *optional*, defaults to 1024):\n+        depthwise_separable_out_channel (`int`, *optional*, defaults to 1024):\n             Number of out channels in the depth-wise separable conv modules.\n         depthwise_multiplier (`int`, *optional*, defaults to 1):\n             Input size multiplier for the depth-wise separable conv modules.\n@@ -227,7 +227,7 @@ def __init__(\n         left_chunk: int = 18,\n         dropout_rate: float = 0.0,\n         ext_pw_out_channel: int = 1024,\n-        depthwise_seperable_out_channel: int = 1024,\n+        depthwise_separable_out_channel: int = 1024,\n         depthwise_multiplier: int = 1,\n         kernel_size: int = 3,\n         conv_activation: str = \"swish\",\n@@ -254,7 +254,7 @@ def __init__(\n         self.num_blocks = num_blocks\n         self.dropout_rate = dropout_rate\n         self.ext_pw_out_channel = ext_pw_out_channel\n-        self.depthwise_seperable_out_channel = depthwise_seperable_out_channel\n+        self.depthwise_separable_out_channel = depthwise_separable_out_channel\n         self.depthwise_multiplier = depthwise_multiplier\n         self.kernel_size = kernel_size\n         self.conv_activation = conv_activation\n@@ -930,7 +930,7 @@ def forward(\n         return attn_output\n \n \n-class Phi4MultimodalAudioDepthWiseSeperableConv1d(nn.Module):\n+class Phi4MultimodalAudioDepthWiseSeparableConv1d(nn.Module):\n     def __init__(self, config: Phi4MultimodalAudioConfig, padding: int = 0):\n         super().__init__()\n         self.dw_conv = nn.Conv1d(\n@@ -942,7 +942,7 @@ def __init__(self, config: Phi4MultimodalAudioConfig, padding: int = 0):\n             groups=config.hidden_size,\n         )\n         self.pw_conv = nn.Conv1d(\n-            config.hidden_size * config.depthwise_multiplier, config.depthwise_seperable_out_channel, 1, 1, 0\n+            config.hidden_size * config.depthwise_multiplier, config.depthwise_separable_out_channel, 1, 1, 0\n         )\n \n     def forward(self, hidden_states):\n@@ -978,7 +978,7 @@ def __init__(self, config: Phi4MultimodalAudioConfig):\n \n         self.layer_norm = nn.LayerNorm(config.hidden_size)\n         self.glu = Phi4MultimodalAudioGluPointWiseConv(config)\n-        self.dw_sep_conv_1d = Phi4MultimodalAudioDepthWiseSeperableConv1d(config, padding=config.kernel_size - 1)\n+        self.dw_sep_conv_1d = Phi4MultimodalAudioDepthWiseSeparableConv1d(config, padding=config.kernel_size - 1)\n         self.act = ACT2FN[config.conv_activation]\n         self.ext_pw_conv_1d = nn.Conv1d(config.hidden_size, config.ext_pw_out_channel, kernel_size=1, stride=1)\n         self.dropout = nn.Dropout(config.dropout_rate)"
        },
        {
            "sha": "33044a4d1271d354dfdba3c1f5118017b962e408",
            "filename": "src/transformers/models/rwkv/convert_rwkv_checkpoint_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Frwkv%2Fconvert_rwkv_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Frwkv%2Fconvert_rwkv_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fconvert_rwkv_checkpoint_to_hf.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -36,7 +36,7 @@\n     \"14B\": 40,\n }\n \n-HIDEN_SIZE_MAPPING = {\n+HIDDEN_SIZE_MAPPING = {\n     \"169M\": 768,\n     \"430M\": 1024,\n     \"1B5\": 2048,\n@@ -106,7 +106,7 @@ def convert_rmkv_checkpoint_to_hf_format(\n     config = RwkvConfig(\n         vocab_size=vocab_size,\n         num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size],\n-        hidden_size=HIDEN_SIZE_MAPPING[size],\n+        hidden_size=HIDDEN_SIZE_MAPPING[size],\n     )\n     config.save_pretrained(output_dir)\n "
        },
        {
            "sha": "0c4f1118d30fcb79083c6368f82463edf29f165e",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -2190,7 +2190,7 @@ def __init__(self, config):\n             kernel_size=kernel_size,\n             padding=(kernel_size - 1) // 2,\n         )\n-        self.activation_fuction = nn.ReLU()\n+        self.activation_function = nn.ReLU()\n         self.ln1 = nn.LayerNorm(embed_dim)\n         self.dropout_module = nn.Dropout(p=var_pred_dropout)\n         self.conv2 = nn.Conv1d(\n@@ -2205,10 +2205,10 @@ def __init__(self, config):\n     def forward(self, hidden_states: Tensor) -> Tensor:\n         # Input: B x T x C; Output: B x T\n         hidden_states = self.conv1(hidden_states.transpose(1, 2))\n-        hidden_states = self.activation_fuction(hidden_states).transpose(1, 2)\n+        hidden_states = self.activation_function(hidden_states).transpose(1, 2)\n         hidden_states = self.dropout_module(self.ln1(hidden_states))\n         hidden_states = self.conv2(hidden_states.transpose(1, 2))\n-        hidden_states = self.activation_fuction(hidden_states).transpose(1, 2)\n+        hidden_states = self.activation_function(hidden_states).transpose(1, 2)\n         hidden_states = self.dropout_module(self.ln2(hidden_states))\n         return self.proj(hidden_states).squeeze(dim=2)\n "
        },
        {
            "sha": "352dc20011afa46b3b05fc34765c3a1960e7e7c6",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -2383,7 +2383,7 @@ def __init__(self, embed_dim, hidden_dim, kernel_size, var_pred_dropout):\n             kernel_size=kernel_size,\n             padding=\"same\",\n         )\n-        self.activation_fuction = nn.ReLU()\n+        self.activation_function = nn.ReLU()\n         self.ln1 = nn.LayerNorm(hidden_dim)\n         self.dropout_module = nn.Dropout(p=var_pred_dropout)\n         self.conv2 = nn.Conv1d(\n@@ -2400,12 +2400,12 @@ def forward(self, hidden_states: Tensor, padding_mask: Optional[Tensor] = None)\n         if padding_mask is not None:\n             hidden_states = hidden_states.masked_fill(~padding_mask.bool().unsqueeze(-1), 0.0)\n         hidden_states = self.conv1(hidden_states.transpose(1, 2))\n-        hidden_states = self.activation_fuction(hidden_states).transpose(1, 2)\n+        hidden_states = self.activation_function(hidden_states).transpose(1, 2)\n         hidden_states = self.dropout_module(self.ln1(hidden_states))\n         if padding_mask is not None:\n             hidden_states = hidden_states.masked_fill(~padding_mask.bool().unsqueeze(-1), 0.0)\n         hidden_states = self.conv2(hidden_states.transpose(1, 2))\n-        hidden_states = self.activation_fuction(hidden_states).transpose(1, 2)\n+        hidden_states = self.activation_function(hidden_states).transpose(1, 2)\n         hidden_states = self.dropout_module(self.ln2(hidden_states))\n         return self.proj(hidden_states).squeeze(dim=2)\n "
        },
        {
            "sha": "1b812ba60a4b5fe5093b876021a84f3580bf51b5",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -88,7 +88,7 @@ def __init__(\n \n         super().__init__(feature_extractor, tokenizer)\n \n-    def _retreive_input_features(self, audio, max_source_positions, **kwargs):\n+    def _retrieve_input_features(self, audio, max_source_positions, **kwargs):\n         \"\"\"\n         Handles specific logic of Voxtral expected input features: audio arrays should be padded to next multiple of 480000 (duration is a multiple of 30s), see VoxtralProcessorKwargs' default audio_kwargs.\n         Then mel input features are extracted and stacked along batch dimension, splitting into chunks of max_source_positions.\n@@ -222,7 +222,7 @@ def apply_chat_template(\n                 data = dict(encoded_instruct_inputs)\n                 if audio is not None:\n                     max_source_positions = audio_kwargs.pop(\"max_source_positions\")\n-                    data[\"input_features\"] = self._retreive_input_features(audio, max_source_positions, **audio_kwargs)\n+                    data[\"input_features\"] = self._retrieve_input_features(audio, max_source_positions, **audio_kwargs)\n \n                 return BatchFeature(data=data, tensor_type=return_tensors)\n \n@@ -421,7 +421,7 @@ def apply_transcription_request(\n \n                 # extract the input features\n                 max_source_positions = audio_kwargs.pop(\"max_source_positions\")\n-                data[\"input_features\"] = self._retreive_input_features(\n+                data[\"input_features\"] = self._retrieve_input_features(\n                     audio_arrays, max_source_positions, **audio_kwargs\n                 )\n "
        },
        {
            "sha": "451b98193d43c61c4e5bed28cecfaa49b2e07f11",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -850,7 +850,7 @@ def wrapper(self, *args, **kwargs):\n         }\n \n         # We let cross attentions to be saved separately because some models add `cross-attn` layer\n-        # when certain condtions are met. Let's output cross attention if attentions are requested (for BC)\n+        # when certain conditions are met. Let's output cross attention if attentions are requested (for BC)\n         if \"output_attentions\" in recordable_keys:\n             recordable_keys[\"output_cross_attentions\"] = recordable_keys[\"output_attentions\"]\n "
        },
        {
            "sha": "33623b385ce33865fd0897225fb12ef6073f0c4f",
            "filename": "src/transformers/utils/metrics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Futils%2Fmetrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/src%2Ftransformers%2Futils%2Fmetrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmetrics.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -339,7 +339,7 @@ def record_kv_cache_memory_metrics(self, cache) -> None:\n             page_size = cache.head_dim * cache.num_key_value_heads\n             page_mem_in_bytes = page_size * cache.dtype.itemsize\n             # When a block is allocated, it is for both K and V, so we multiply by 2\n-            # It's also allocated accross all cache tensors, so we multiply by the nb of tensors: len(cache.key_cache)\n+            # It's also allocated across all cache tensors, so we multiply by the nb of tensors: len(cache.key_cache)\n             block_mem_in_bytes = 2 * len(cache.key_cache) * cache.block_size * page_mem_in_bytes\n \n             # Retrieve the number of used and free blocks"
        },
        {
            "sha": "e8cdd43ff5f77979543759e297122e194f6a583c",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -446,7 +446,7 @@ def test_batch_generation(self):\n \n     @slow\n     @require_torch_accelerator\n-    def test_batch_generation_padd(self):\n+    def test_batch_generation_padding(self):\n         path_560m = \"bigscience/bloom-560m\"\n         model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n         model = model.eval()"
        },
        {
            "sha": "788a60021a889cf8d1505bf3b436ea70b8cbaef3",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -393,7 +393,7 @@ def create_and_check_model(self, config, input_ids, attention_mask, pixel_values\n             result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n         )\n \n-    def create_and_check_model_for_image_segmentation(self, config, input_ids, attention_maks, pixel_values):\n+    def create_and_check_model_for_image_segmentation(self, config, input_ids, attention_mask, pixel_values):\n         model = CLIPSegForImageSegmentation(config).to(torch_device).eval()\n         with torch.no_grad():\n             result = model(input_ids, pixel_values)"
        },
        {
            "sha": "5f97cfad359d11da54dad76204809e2ddd485263",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -379,7 +379,7 @@ def test_batch_generation(self):\n         model.config.pad_token_id = model.config.eos_token_id\n \n         # use different length sentences to test batching\n-        sentences = [\"def hellow_world():\", \"def greet(name):\"]\n+        sentences = [\"def hello_world():\", \"def greet(name):\"]\n \n         inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n         input_ids = inputs[\"input_ids\"].to(torch_device)\n@@ -415,7 +415,7 @@ def test_batch_generation(self):\n         padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n \n         expected_output_sentence = [\n-            'def hellow_world():\\n    print(\"Hello World\")\\n\\nhellow_world()',\n+            'def hello_world():\\n    print(\"Hello World\")\\n\\nhellow_world()',\n             'def greet(name):\\n    print(f\"Hello {name}\")\\n\\ng',\n         ]\n         self.assertListEqual(expected_output_sentence, batch_out_sentence)"
        },
        {
            "sha": "4619c7a7f19d8af1d105c0f001eb4635cc5b98d3",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -241,7 +241,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n \n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n-            self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n+            self.skipTest(reason=\"Intel XPU doesn't support flash_attention_2 as of now.\")\n \n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_COMPLETIONS = ["
        },
        {
            "sha": "204ef79831f39ad65577f7607589b1d87d667f6b",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -362,7 +362,7 @@ def _load_conversation(self):\n     def test_1b_model_integration_generate(self):\n         \"\"\"\n         Tests the generated tokens match the ones from the original model implementation.\n-        Such tokens are to be retreived using https://gist.github.com/eustlb/d25577a357ddcf8f4a8cd0d00baca551, which is a script that infers the original model.\n+        Such tokens are to be retrieved using https://gist.github.com/eustlb/d25577a357ddcf8f4a8cd0d00baca551, which is a script that infers the original model.\n         \"\"\"\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         prompt = \"<|begin_of_text|>[0]What are you working on?<|end_of_text|><|AUDIO|><|audio_eos|><|begin_of_text|>[1]I'm figuring out my budget.<|end_of_text|>\"\n@@ -406,7 +406,7 @@ def test_1b_model_integration_generate(self):\n     def test_1b_model_integration_generate_no_audio(self):\n         \"\"\"\n         Tests the generated tokens match the ones from the original model implementation.\n-        Such tokens are to be retreived using https://gist.github.com/eustlb/aed822f765e928b9612e01b0d8836d69, which is a script that infers the original model.\n+        Such tokens are to be retrieved using https://gist.github.com/eustlb/aed822f765e928b9612e01b0d8836d69, which is a script that infers the original model.\n         \"\"\"\n \n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n@@ -467,7 +467,7 @@ def test_1b_model_integration_generate_no_audio(self):\n     def test_1b_model_integration_generate_multiple_audio(self):\n         \"\"\"\n         Test the generated tokens match the ones from the original model implementation.\n-        Such tokens are to be retreived using https://gist.github.com/eustlb/0c94de002e1325abb61d32217f74c0f8, which is a script that infers the original model.\n+        Such tokens are to be retrieved using https://gist.github.com/eustlb/0c94de002e1325abb61d32217f74c0f8, which is a script that infers the original model.\n         \"\"\"\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n \n@@ -526,7 +526,7 @@ def test_1b_model_integration_generate_multiple_audio(self):\n     def test_1b_model_integration_generate_batched(self):\n         \"\"\"\n         Test the generated tokens match the ones from the original model implementation.\n-        Such tokens are to be retreived using https://gist.github.com/eustlb/bcc532b53161bc31da3d66cb07ae193f, which is a script that infers the original model.\n+        Such tokens are to be retrieved using https://gist.github.com/eustlb/bcc532b53161bc31da3d66cb07ae193f, which is a script that infers the original model.\n         \"\"\"\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n "
        },
        {
            "sha": "df97dc4a0af478085af1996dfc6c3481545cf542",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -326,7 +326,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "a22d229a04058ba2cec5765e5b4b99e23ec6b483",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -383,7 +383,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n \n @require_torch\n-class DistilBertModelIntergrationTest(unittest.TestCase):\n+class DistilBertModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_inference_no_head_absolute_embedding(self):\n         model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
        },
        {
            "sha": "b518c0db956d017df33ad37a1693b9e0ee233a00",
            "filename": "tests/models/evolla/test_modeling_evolla.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -257,7 +257,7 @@ def test_generate_multiple_proteins(self):\n     def test_saprot_output(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True\n-        protein_informations = {\n+        protein_information = {\n             \"input_ids\": inputs_dict[\"protein_input_ids\"],\n             \"attention_mask\": inputs_dict[\"protein_attention_mask\"],\n         }\n@@ -267,13 +267,13 @@ def test_saprot_output(self):\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n-            protein_encoder_outputs = model.protein_encoder.model(**protein_informations, return_dict=True)\n+            protein_encoder_outputs = model.protein_encoder.model(**protein_information, return_dict=True)\n             print(model_class, protein_encoder_outputs)\n \n     def test_protein_encoder_output(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True\n-        protein_informations = {\n+        protein_information = {\n             \"input_ids\": inputs_dict[\"protein_input_ids\"],\n             \"attention_mask\": inputs_dict[\"protein_attention_mask\"],\n         }\n@@ -283,7 +283,7 @@ def test_protein_encoder_output(self):\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n-            protein_encoder_outputs = model.protein_encoder(**protein_informations, return_dict=True)\n+            protein_encoder_outputs = model.protein_encoder(**protein_information, return_dict=True)\n             print(model_class, protein_encoder_outputs)\n \n     def test_single_forward(self):"
        },
        {
            "sha": "95c33187eb7c780fd32803625b4db534e9ecd27d",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -814,8 +814,8 @@ def test_dynamic_sliding_window_is_default(self):\n         prompt = \"What is the capital of France?\"\n         model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n \n-        foward_outputs = model(**model_inputs)\n-        self.assertIn(\"DynamicSlidingWindowLayer\", str(foward_outputs.past_key_values))\n+        forward_outputs = model(**model_inputs)\n+        self.assertIn(\"DynamicSlidingWindowLayer\", str(forward_outputs.past_key_values))\n \n         generate_outputs = model.generate(\n             **model_inputs, max_new_tokens=2, do_sample=False, return_dict_in_generate=True"
        },
        {
            "sha": "adb925934548a9c326564b959511e1bd91733e82",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -127,7 +127,7 @@ def __init__(\n         self.audio_token_index = audio_token_index\n         self.tie_word_embeddings = tie_word_embeddings\n         self.initializer_range = initializer_range\n-        self.has_lora_adapater = has_lora_adapter\n+        self.has_lora_adapter = has_lora_adapter\n         self.downsample_rate = downsample_rate\n         self.window_size = window_size\n         self.is_training = is_training\n@@ -152,7 +152,7 @@ def get_config(self):\n             audio_token_index=self.audio_token_index,\n             tie_word_embeddings=self.tie_word_embeddings,\n             initializer_range=self.initializer_range,\n-            has_lora_adapter=self.has_lora_adapater,\n+            has_lora_adapter=self.has_lora_adapter,\n         )\n \n     def prepare_config_and_inputs(self):"
        },
        {
            "sha": "c2c98882ef0296a4d50a11b9e3a5f6d5f420e568",
            "filename": "tests/models/kosmos2/test_processing_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -97,7 +97,7 @@ def get_image_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    def test_image_procesor_load_save_reload(self):\n+    def test_image_processor_load_save_reload(self):\n         # make sure load from Hub repo. -> save -> reload locally work\n         image_processor = CLIPImageProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n         with TemporaryDirectory() as tmp_dir:"
        },
        {
            "sha": "8325c0f699ed994ae9f209ec13935979c04e47f5",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -717,7 +717,7 @@ def test_generation(self):\n         reproduce test expected outputs using original codebase: https://gist.github.com/eustlb/7a9aa6139d11e0103c6b65bac103da52\n \n         DISCLAIMER: we are testing for pretty short inputs. Indeed, reproducing correct expected outputs for longer is not possible\n-        as implementation choices (qkv matrix in one linear for original code vs three for hf) create growing divergence with context lenght,\n+        as implementation choices (qkv matrix in one linear for original code vs three for hf) create growing divergence with context length,\n         ultimately giving different outputs.\n         \"\"\"\n         processor = KyutaiSpeechToTextProcessor.from_pretrained(self.model_checkpoint)\n@@ -747,7 +747,7 @@ def test_generation_batched(self):\n         reproduce test expected outputs using original codebase: https://gist.github.com/eustlb/b58c217c75124d405ec1c13877c7ece8\n \n         DISCLAIMER: we are testing for pretty short inputs. Indeed, reproducing correct expected outputs for longer is not possible\n-        as implementation choices (qkv matrix in one linear for original code vs three for hf) create growing divergence with context lenght,\n+        as implementation choices (qkv matrix in one linear for original code vs three for hf) create growing divergence with context length,\n         ultimately giving different outputs.\n         \"\"\"\n         processor = KyutaiSpeechToTextProcessor.from_pretrained(self.model_checkpoint)"
        },
        {
            "sha": "5ca0499805ef33acec818698b23cae900eefd2a3",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -1697,11 +1697,11 @@ def test_added_token_with_space_before(self):\n         words_without_space = tokens_to_add + list(tokenizer_s.added_tokens_encoder.keys())\n         boxes = [[i, i, i, i] for i in range(len(words_with_space))]\n \n-        tokens_to_add_formated = [\n+        tokens_to_add_formatted = [\n             AddedToken(token, rstrip=True, lstrip=True, single_word=False) for token in tokens_to_add\n         ]\n-        tokenizer_s.add_tokens(tokens_to_add_formated)\n-        tokenizer_f.add_tokens(tokens_to_add_formated)\n+        tokenizer_s.add_tokens(tokens_to_add_formatted)\n+        tokenizer_f.add_tokens(tokens_to_add_formatted)\n \n         ids_s = tokenizer_s(words_with_space, boxes=boxes).input_ids\n         ids_f = tokenizer_f(words_with_space, boxes=boxes).input_ids"
        },
        {
            "sha": "32c5edd3071f6e4f55bc4e6ce02c8915a20ca0c7",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -117,7 +117,7 @@ def prepare_config_and_inputs(self):\n         # all pad tokens have pos id = 2 and rest are between 2..seq_length\n         # and the seq_length here is seq_length - num_pad_tokens\n         # but when using past, there is no way of knowing if the past input ids had\n-        # pad tokens in them, which results in incorrect seq_lenth and which in turn results in\n+        # pad tokens in them, which results in incorrect seq_length and which in turn results in\n         # position_ids being off by num_pad_tokens in past input\n         input_ids = input_ids.clamp(self.pad_token_id + 1)\n         decoder_input_ids = decoder_input_ids.clamp(self.pad_token_id + 1)"
        },
        {
            "sha": "e9acdddcd0c35d48deea816be587d67cba80bb39",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 24,
            "deletions": 8,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -274,12 +274,14 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n             [self.image_token_id, self.bos_token_id, 2028, 374, 264, 1296, 11914, 13],\n             [self.bos_token_id, 2028, 374, 264, 1296, 11914, 13, self.image_token_id, self.image_token_id, 2028, 374, 264, 1296, 11914, 13],\n         ]\n-        # fmt: onn\n+        # fmt: on\n         images = [[self.image1], [self.image1, self.image2]]\n         inputs = processor(text=text, images=images, padding=True, size={\"width\": 256, \"height\": 256})\n \n         self.assertEqual(inputs[\"pixel_values\"].shape, (2, 2, 4, 3, 256, 256))\n-        for input_ids_i, attention_mask_i, expected_ids_i in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], expected_ids):\n+        for input_ids_i, attention_mask_i, expected_ids_i in zip(\n+            inputs[\"input_ids\"], inputs[\"attention_mask\"], expected_ids\n+        ):\n             pad_ids = [id for id, m in zip(input_ids_i, attention_mask_i) if m == 0]\n             input_ids = [id for id, m in zip(input_ids_i, attention_mask_i) if m == 1]\n             self.assertEqual(input_ids, expected_ids_i)\n@@ -291,24 +293,38 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n         # Check that only first tile of first sample is attended to all text tokens\n         first_sample_mask = cross_attention_mask[0].copy()\n         first_image_first_tile_attention = first_sample_mask[:, :1, :1]  # text tokens, images, tiles\n-        self.assertTrue(np.all(first_image_first_tile_attention == 1), f\"Cross attention mask is not all ones: {first_image_first_tile_attention}\")\n+        self.assertTrue(\n+            np.all(first_image_first_tile_attention == 1),\n+            f\"Cross attention mask is not all ones: {first_image_first_tile_attention}\",\n+        )\n \n         # zero out first tile of first image\n         first_image_first_tile_attention[:, :1, :1] = 0\n-        self.assertTrue(np.all(first_image_first_tile_attention == 0), f\"Cross attention mask is not all zeros: {first_image_first_tile_attention}\")\n+        self.assertTrue(\n+            np.all(first_image_first_tile_attention == 0),\n+            f\"Cross attention mask is not all zeros: {first_image_first_tile_attention}\",\n+        )\n \n         # second sample\n         second_sample_mask = cross_attention_mask[1].copy()\n         first_image_first_tile_attention = second_sample_mask[7:, :1, :1]  # text tokens, images, tiles\n-        self.assertTrue(np.all(first_image_first_tile_attention == 1), f\"Cross attention mask is not all ones: {first_image_first_tile_attention}\")\n+        self.assertTrue(\n+            np.all(first_image_first_tile_attention == 1),\n+            f\"Cross attention mask is not all ones: {first_image_first_tile_attention}\",\n+        )\n \n         second_image_two_tiles_attention = second_sample_mask[8:, 1:2, :2]  # text tokens, images, tiles\n-        self.assertTrue(np.all(second_image_two_tiles_attention == 1), f\"Cross attention mask is not all ones: {second_image_two_tiles_attention}\")\n+        self.assertTrue(\n+            np.all(second_image_two_tiles_attention == 1),\n+            f\"Cross attention mask is not all ones: {second_image_two_tiles_attention}\",\n+        )\n \n         # zero out both images masks\n         second_sample_mask[7:, :1, :1] = 0\n         second_sample_mask[8:, 1:2, :2] = 0\n-        self.assertTrue(np.all(second_sample_mask == 0), f\"Cross attention mask is not all zeros: {second_sample_mask}\")\n+        self.assertTrue(\n+            np.all(second_sample_mask == 0), f\"Cross attention mask is not all zeros: {second_sample_mask}\"\n+        )\n \n     def test_process_interleaved_images_prompts_image_error(self):\n         text = [\n@@ -406,6 +422,6 @@ def test_special_mm_token_truncation(self):\n                 max_length=3,\n             )\n \n-    @unittest.skip(\"Mllama can't process inouts with no image ttogether with multimodal inputs\")\n+    @unittest.skip(\"Mllama can't process inputs with no image ttogether with multimodal inputs\")\n     def test_processor_text_has_no_visual(self):\n         pass"
        },
        {
            "sha": "b1f0ce468a38a6985e4b475275478f077e58abb5",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -402,7 +402,7 @@ def test_saved_config_excludes_reference_compile(self):\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test\n-    def test_flash_attention_dispatches_by_defaul(self):\n+    def test_flash_attention_dispatches_by_default(self):\n         \"ModernBert should dispatch to FA2 by default, not SDPA\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "b8e3232dc0055d75faa7684d16700c20b61eab22",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -85,7 +85,7 @@ def __init__(\n             hidden_size=32,\n             num_attention_heads=8,\n             intermediate_size=48,\n-            depthwise_seperable_out_channel=128,\n+            depthwise_separable_out_channel=128,\n             nemo_conv_channels=128,\n             initializer_range=1e-5,\n         ),"
        },
        {
            "sha": "689256de2d0d6402eade0b592fa0bff0e0aa2225",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -328,7 +328,7 @@ def test_initialization(self):\n         for model_class in self.all_model_classes:\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n-                # This is an excepton in the module, it's initialized with xavier_uniform without using initializer_range\n+                # This is an exception in the module, it's initialized with xavier_uniform without using initializer_range\n                 if name.endswith(\"patch_embeddings.projection.weight\"):\n                     continue\n                 if param.requires_grad:"
        },
        {
            "sha": "4bf85697c4cc1b8e9d4780e25232685d66571797",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -160,7 +160,7 @@ def create_and_test_config_from_pretrained_custom_kwargs(self):\n         for composite configs. We should overwrite only the requested keys, keeping all values of the\n         subconfig that are loaded from the checkpoint.\n         \"\"\"\n-        # Check only composite configs. We can't know which attributes each type fo config has so check\n+        # Check only composite configs. We can't know which attributes each type of config has so check\n         # only text config because we are sure that all text configs have a `vocab_size`\n         config = self.config_class(**self.inputs_dict)\n         if config.get_text_config() is config or not hasattr(self.parent.model_tester, \"get_config\"):"
        },
        {
            "sha": "5ba84bab5501564c231b39695ab9b429f1b6e32d",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -481,7 +481,7 @@ def test_phi4_with_all_processors(self):\n                 Phi4MultimodalAudioAttention,\n                 Phi4MultimodalAudioConformerEncoderLayer,\n                 Phi4MultimodalAudioConvModule,\n-                Phi4MultimodalAudioDepthWiseSeperableConv1d,\n+                Phi4MultimodalAudioDepthWiseSeparableConv1d,\n                 Phi4MultimodalAudioEmbedding,\n                 Phi4MultimodalAudioGluPointWiseConv,\n                 Phi4MultimodalAudioMeanVarianceNormLayer,\n@@ -567,7 +567,7 @@ class MyTest2AudioAttention(Phi4MultimodalAudioAttention):\n                 pass\n \n \n-            class MyTest2AudioDepthWiseSeperableConv1d(Phi4MultimodalAudioDepthWiseSeperableConv1d):\n+            class MyTest2AudioDepthWiseSeparableConv1d(Phi4MultimodalAudioDepthWiseSeparableConv1d):\n                 pass\n \n "
        },
        {
            "sha": "2bb00776af98a9bb0e689e057180feacaeb3f232",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/767f8a4c75dba4d5d2aa90de5030967c7c16bd87/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=767f8a4c75dba4d5d2aa90de5030967c7c16bd87",
            "patch": "@@ -504,7 +504,7 @@ def find_code_and_splits(object_name: str, base_path: str, buffer: Optional[dict\n         code (`str`):\n             The object's code.\n         code_splits (`List[Tuple[str, int, int]]`):\n-            `code` splitted into blocks. See `split_code_into_blocks`.\n+            `code` split into blocks. See `split_code_into_blocks`.\n     \"\"\"\n     if buffer is None:\n         buffer = {}"
        }
    ],
    "stats": {
        "total": 214,
        "additions": 116,
        "deletions": 98
    }
}