{
    "author": "gorkachea",
    "message": "ðŸ“š docs(granite-speech): add comprehensive usage examples (#42125)\n\n* ðŸ“š docs(granite-speech): add comprehensive usage examples\n\nResolves the TODO (@alex-jw-brooks) by adding complete usage documentation\nfor Granite Speech model now that it's released and compatible with transformers.\n\nAdded examples for:\n- Basic speech transcription\n- Speech-to-text with additional context\n- Batch processing multiple audio files\n- Tips for best results (audio format, LoRA adapter, memory optimization)\n\nThis helps users get started with the Granite Speech multimodal model\nby providing practical, copy-paste-ready code examples for common use cases.\n\nReplaces TODO comment on line 44 with ~100 lines of comprehensive\ndocumentation following the patterns used in other multimodal model docs.\n\n* Address review feedback: add chat template usage and move model-specific tips\n\n- Added proper chat template formatting in the second example (per @zucchini-nlp feedback)\n- Removed generic LLM tips (temperature, batch size, memory)\n- Moved Granite Speech-specific tips (audio format, LoRA adapter) to Usage tips section\n\nThis keeps the documentation focused on model-specific features rather than general LLM knowledge.\n\n* docs: use datasets library for working audio examples\n\nAddress review feedback by replacing placeholder audio paths with real\nexamples using hf-internal-testing/librispeech_asr_dummy dataset. This\nmakes all code examples copy-paste ready and reproducible.\n\n- Add datasets import to all three examples\n- Replace 'path/to/audio.wav' with actual dataset loading\n- Ensure proper audio sampling rate handling\n\nCo-authored-by: eustlb <eustache.leblond@gmail.com>\n\n* ðŸ“š docs: use modern chat template pattern with tokenize=True for audio\n\n---------\n\nCo-authored-by: eustlb <eustache.leblond@gmail.com>",
    "sha": "45d273d52c2c9ecbebfd213a54ededd0b609b13c",
    "files": [
        {
            "sha": "9ad8b2f890fce32296df332fbd305844c1922001",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 108,
            "deletions": 1,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/45d273d52c2c9ecbebfd213a54ededd0b609b13c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/45d273d52c2c9ecbebfd213a54ededd0b609b13c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=45d273d52c2c9ecbebfd213a54ededd0b609b13c",
            "patch": "@@ -40,8 +40,115 @@ This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9\n ## Usage tips\n \n - This model bundles its own LoRA adapter, which will be automatically loaded and enabled/disabled as needed during inference calls. Be sure to install [PEFT](https://github.com/huggingface/peft) to ensure the LoRA is correctly applied!\n+- The model expects 16kHz sampling rate audio. The processor will automatically resample if needed.\n+- The LoRA adapter is automatically enabled when audio features are present and disabled for text-only inputs, so you don't need to manage it manually.\n+\n+## Usage example\n+\n+Granite Speech is a multimodal speech-to-text model that can transcribe audio and respond to text prompts. Here's how to use it:\n+\n+### Basic Speech Transcription\n+\n+```python\n+from transformers import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+# Load model and processor\n+model = GraniteSpeechForConditionalGeneration.from_pretrained(\n+    \"ibm-granite/granite-3.2-8b-speech\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+processor = GraniteSpeechProcessor.from_pretrained(\"ibm-granite/granite-3.2-8b-speech\")\n+\n+# Load audio from dataset (16kHz sampling rate required)\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+audio = ds['audio'][0]['array']\n+\n+# Process audio\n+inputs = processor(audio=audio, return_tensors=\"pt\").to(model.device)\n+\n+# Generate transcription\n+generated_ids = model.generate(**inputs, max_new_tokens=256)\n+transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+print(transcription)\n+```\n+\n+### Speech-to-Text with Chat Template\n+\n+For instruction-following with audio, use the chat template with audio directly in the conversation format:\n+\n+```python\n+from transformers import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+model = GraniteSpeechForConditionalGeneration.from_pretrained(\n+    \"ibm-granite/granite-3.2-8b-speech\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+processor = GraniteSpeechProcessor.from_pretrained(\"ibm-granite/granite-3.2-8b-speech\")\n+\n+# Load audio from dataset\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+audio = ds['audio'][0]\n+\n+# Prepare conversation with audio and text\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"audio\", \"audio\": audio},\n+            {\"type\": \"text\", \"text\": \"Transcribe the following audio:\"},\n+        ],\n+    }\n+]\n+\n+# Apply chat template with audio - processor handles both tokenization and audio processing\n+inputs = processor.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\").to(model.device)\n+\n+# Generate transcription\n+generated_ids = model.generate(**inputs, max_new_tokens=512)\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+print(output_text)\n+```\n+\n+### Batch Processing\n+\n+Process multiple audio files efficiently:\n+\n+```python\n+from transformers import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+model = GraniteSpeechForConditionalGeneration.from_pretrained(\n+    \"ibm-granite/granite-3.2-8b-speech\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+processor = GraniteSpeechProcessor.from_pretrained(\"ibm-granite/granite-3.2-8b-speech\")\n+\n+# Load multiple audio samples from dataset\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+audio_samples = [ds['audio'][i]['array'] for i in range(3)]\n+\n+# Process batch\n+inputs = processor(audio=audio_samples, return_tensors=\"pt\", padding=True).to(model.device)\n+\n+# Generate for all inputs\n+generated_ids = model.generate(**inputs, max_new_tokens=256)\n+transcriptions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+for i, transcription in enumerate(transcriptions):\n+    print(f\"Audio {i+1}: {transcription}\")\n+```\n \n-<!-- TODO (@alex-jw-brooks) Add an example here once the model compatible with the transformers implementation is released -->\n \n ## GraniteSpeechConfig\n "
        }
    ],
    "stats": {
        "total": 109,
        "additions": 108,
        "deletions": 1
    }
}