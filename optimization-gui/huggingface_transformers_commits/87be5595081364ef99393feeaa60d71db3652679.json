{
    "author": "zucchini-nlp",
    "message": "Fix attention mask in mamba layers (#41790)\n\n* not all mamba models are like LFM\n\n* compile friendly\n\n* adjust slow tests expectation\n\n* naming",
    "sha": "87be5595081364ef99393feeaa60d71db3652679",
    "files": [
        {
            "sha": "9285068292ad37a414a6625b76a1fe501eb44b3e",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -486,20 +486,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "79a1b0e5ea1535504b942bcf300bf49fba2a522a",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -36,6 +36,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -203,17 +204,6 @@ class BambaRMSNormGated(MambaRMSNormGated):\n     pass\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "28117b49d52b5ad9dfba8c0b7e95efe0625cd4bd",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -521,20 +521,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "62cbab82c3e6e626cbc8becac1a8eb54652a1a1d",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -39,6 +39,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -285,17 +286,6 @@ def forward(self, hidden_states, gate=None):\n         return hidden_states.to(input_dtype)\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "947d250cd13483605d62f34a4cfcada85479dc34",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -322,20 +322,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class GraniteMoeHybridMambaLayer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "e8f8cf4e40e5b0c863d17eb15ec0953c129d9ef7",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -422,6 +422,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -665,15 +666,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
        },
        {
            "sha": "f05386b2bb16e6c0c5ea2cf01811c85757d33e3f",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -473,15 +473,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
        },
        {
            "sha": "d5919c1b967fbbf455c19032b7af98f5f58130e9",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -485,6 +485,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -732,15 +733,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
        },
        {
            "sha": "ae1dcef0f39f68e0a6d3da2e1f05d79b59a2c7a8",
            "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -180,15 +180,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
        },
        {
            "sha": "6f1f31b9002ca10d238aa7e653248226ffcc41b0",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -117,6 +117,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
        },
        {
            "sha": "e8d267fa030f43b7bf0f33282d55aa569eaa96a8",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -430,6 +430,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
        },
        {
            "sha": "126a19be941198e36e842933b2331641645f3566",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -220,9 +220,9 @@ def test_model_1a8b_generation(self):\n     def test_model_1a8b_batched_chat_generation(self):\n         prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n         EXPECTED_TEXT_COMPLETIONS = [\n-            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n             \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n-            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+            \"The Meji Restoration in Japan ended or the Meiji Restoration (1868â€“1912) marked a pivotal\",\n         ]\n         set_seed(1789)\n         tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)"
        },
        {
            "sha": "4785525f4e9654b39135e274ba41899d7f2c9736",
            "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/87be5595081364ef99393feeaa60d71db3652679/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87be5595081364ef99393feeaa60d71db3652679/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py?ref=87be5595081364ef99393feeaa60d71db3652679",
            "patch": "@@ -300,7 +300,7 @@ def test_integration_test_batched(self):\n         )\n \n         # Create inputs\n-        text = [\"<image>In this image, we see\", \"<image>In this image, we see\"]\n+        text = [\"<image>In this image, we see\", \"<image>In this image, we see a cat\"]\n         images = [[self.image2], [self.image]]\n         inputs = self.processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n         inputs.to(device=torch_device, dtype=torch.bfloat16)\n@@ -310,6 +310,6 @@ def test_integration_test_batched(self):\n \n         expected_generated_text = [\n             \"In this image, we see a panoramic view of the New York City skyline. The iconic Statics and the New York\",\n-            \"In this image, there is a cat on a bed with a cat on a bed with a cat on a bed with a cat on a bed\",\n+            \"In this image, we see a cat that is lying on its side on a cat bed.\",\n         ]\n         self.assertListEqual(generated_texts, expected_generated_text)"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 38,
        "deletions": 39
    }
}