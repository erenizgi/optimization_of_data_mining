{
    "author": "xadupre",
    "message": "Remove unnecessary masked_fill in deberta models (#35182)",
    "sha": "5fba3f99c0a44f7613aeaa0550f7786919780663",
    "files": [
        {
            "sha": "c9a85bcad1bd6fe5d4b277ecd8a5266d7ddc2d0c",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fba3f99c0a44f7613aeaa0550f7786919780663/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fba3f99c0a44f7613aeaa0550f7786919780663/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=5fba3f99c0a44f7613aeaa0550f7786919780663",
            "patch": "@@ -290,7 +290,6 @@ def forward(\n         attention_scores = attention_scores.masked_fill(~(attention_mask), torch.finfo(query_layer.dtype).min)\n         # bsz x height x length x dimension\n         attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-        attention_probs.masked_fill(attention_mask, 0)\n \n         attention_probs = self.dropout(attention_probs)\n         if self.head_weights_proj is not None:"
        },
        {
            "sha": "7d2f25603a6f96543f3766af855f94e32cc35fd8",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fba3f99c0a44f7613aeaa0550f7786919780663/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fba3f99c0a44f7613aeaa0550f7786919780663/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=5fba3f99c0a44f7613aeaa0550f7786919780663",
            "patch": "@@ -267,7 +267,6 @@ def forward(\n         attention_scores = attention_scores.masked_fill(~(attention_mask), torch.finfo(query_layer.dtype).min)\n         # bsz x height x length x dimension\n         attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-        attention_probs.masked_fill(attention_mask, 0)\n \n         attention_probs = self.dropout(attention_probs)\n         context_layer = torch.bmm("
        }
    ],
    "stats": {
        "total": 2,
        "additions": 0,
        "deletions": 2
    }
}