{
    "author": "zucchini-nlp",
    "message": "Bart config doesn't need generation parameters (#42337)\n\n* fix bart and mvp\n\n* nits and fixes\n\n* style\n\n---------\n\nCo-authored-by: vasqu <antonprogamer@gmail.com>",
    "sha": "d2825a0219113897eef6487a7bdfd3012388ff3d",
    "files": [
        {
            "sha": "3485e8e1edde0d8b38942ab2dc986b78e9947eff",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2825a0219113897eef6487a7bdfd3012388ff3d/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2825a0219113897eef6487a7bdfd3012388ff3d/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=d2825a0219113897eef6487a7bdfd3012388ff3d",
            "patch": "@@ -436,6 +436,13 @@ def __init__(self, **kwargs):\n         self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n         self.transformers_version = kwargs.pop(\"transformers_version\", __version__)\n \n+        # Ensure backward compatibility for models that use `forced_bos_token_id` within their config\n+        if self._from_model_config and kwargs.get(\"force_bos_token_to_be_generated\", False):\n+            self.forced_bos_token_id = self.bos_token_id\n+            logger.warning_once(\n+                f\"Please make sure the generation config includes `forced_bos_token_id={self.bos_token_id}`. \"\n+            )\n+\n         # Additional attributes without default values\n         if not self._from_model_config:\n             # we don't want to copy values from the model config if we're initializing a `GenerationConfig` from a"
        },
        {
            "sha": "5548654112727560043d210c45845f0a858380ed",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2825a0219113897eef6487a7bdfd3012388ff3d/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2825a0219113897eef6487a7bdfd3012388ff3d/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=d2825a0219113897eef6487a7bdfd3012388ff3d",
            "patch": "@@ -14,8 +14,6 @@\n # limitations under the License.\n \"\"\"BART model configuration\"\"\"\n \n-import warnings\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n@@ -80,9 +78,6 @@ class BartConfig(PreTrainedConfig):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n         num_labels (`int`, *optional*, defaults to 3):\n             The number of labels to use in [`BartForSequenceClassification`].\n-        forced_eos_token_id (`int`, *optional*, defaults to 2):\n-            The id of the token to force as the last generated token when `max_length` is reached. Usually set to\n-            `eos_token_id`.\n \n     Example:\n \n@@ -130,7 +125,6 @@ def __init__(\n         eos_token_id=2,\n         is_encoder_decoder=True,\n         decoder_start_token_id=2,\n-        forced_eos_token_id=2,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -161,16 +155,9 @@ def __init__(\n             eos_token_id=eos_token_id,\n             is_encoder_decoder=is_encoder_decoder,\n             decoder_start_token_id=decoder_start_token_id,\n-            forced_eos_token_id=forced_eos_token_id,\n             **kwargs,\n         )\n         self.tie_encoder_decoder = True\n-        # ensure backward compatibility for BART CNN models\n-        if kwargs.get(\"force_bos_token_to_be_generated\", False):\n-            self.forced_bos_token_id = self.bos_token_id\n-            warnings.warn(\n-                f\"Please make sure the generation config includes `forced_bos_token_id={self.bos_token_id}`. \"\n-            )\n \n \n __all__ = [\"BartConfig\"]"
        },
        {
            "sha": "c006e19f42a0415a818b4d42d40042da1afb8e6b",
            "filename": "src/transformers/models/mvp/configuration_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2825a0219113897eef6487a7bdfd3012388ff3d/src%2Ftransformers%2Fmodels%2Fmvp%2Fconfiguration_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2825a0219113897eef6487a7bdfd3012388ff3d/src%2Ftransformers%2Fmodels%2Fmvp%2Fconfiguration_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fconfiguration_mvp.py?ref=d2825a0219113897eef6487a7bdfd3012388ff3d",
            "patch": "@@ -14,8 +14,6 @@\n # limitations under the License.\n \"\"\"MVP model configuration\"\"\"\n \n-import warnings\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n@@ -78,9 +76,6 @@ class MvpConfig(PreTrainedConfig):\n             Scale embeddings by diving by sqrt(d_model).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n-        forced_eos_token_id (`int`, *optional*, defaults to 2):\n-            The id of the token to force as the last generated token when `max_length` is reached. Usually set to\n-            `eos_token_id`.\n         use_prompt (`bool`, *optional*, defaults to `False`):\n             Whether or not to use prompt.\n         prompt_length (`int`, *optional*, defaults to 100):\n@@ -132,7 +127,6 @@ def __init__(\n         eos_token_id=2,\n         is_encoder_decoder=True,\n         decoder_start_token_id=2,\n-        forced_eos_token_id=2,\n         use_prompt=False,\n         prompt_length=100,\n         prompt_mid_dim=800,\n@@ -168,16 +162,8 @@ def __init__(\n             eos_token_id=eos_token_id,\n             is_encoder_decoder=is_encoder_decoder,\n             decoder_start_token_id=decoder_start_token_id,\n-            forced_eos_token_id=forced_eos_token_id,\n             **kwargs,\n         )\n \n-        if kwargs.get(\"force_bos_token_to_be_generated\", False):\n-            self.forced_bos_token_id = self.bos_token_id\n-            warnings.warn(\n-                f\"Please make sure the generated config includes `forced_bos_token_id={self.bos_token_id}` . \"\n-                \"The config can simply be saved and uploaded again to be fixed.\"\n-            )\n-\n \n __all__ = [\"MvpConfig\"]"
        },
        {
            "sha": "3ac771067dd2be6790388a9359edbb4c9790de5a",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2825a0219113897eef6487a7bdfd3012388ff3d/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2825a0219113897eef6487a7bdfd3012388ff3d/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=d2825a0219113897eef6487a7bdfd3012388ff3d",
            "patch": "@@ -1279,9 +1279,7 @@ def test_contrastive_search_bart(self):\n \n     @slow\n     def test_decoder_attention_mask(self):\n-        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0).to(\n-            torch_device\n-        )\n+        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(torch_device)\n         tokenizer = self.default_tokenizer\n         sentence = \"UN Chief Says There Is No <mask> in Syria\"\n         input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids.to(torch_device)\n@@ -1302,6 +1300,7 @@ def test_decoder_attention_mask(self):\n             max_new_tokens=20,\n             decoder_input_ids=decoder_input_ids,\n             decoder_attention_mask=decoder_attention_mask,\n+            forced_bos_token_id=0,\n         )\n         generated_sentence = tokenizer.batch_decode(generated_ids)[0]\n         expected_sentence = \"</s><pad><pad><pad><s>UN Chief Says There Is No Plan B for Peace in Syria</s>\""
        }
    ],
    "stats": {
        "total": 39,
        "additions": 9,
        "deletions": 30
    }
}