{
    "author": "winglian",
    "message": "fix document masking for chunked attention (#37429)\n\n* fix document masking for chunked attention\n\n* remove accidental debugging sum",
    "sha": "121f7037c7acd26eec6db405f2e2cb6d70e4931c",
    "files": [
        {
            "sha": "56c35e8d195626ea2ff0dd3a424d5a5303da46ae",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/121f7037c7acd26eec6db405f2e2cb6d70e4931c/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/121f7037c7acd26eec6db405f2e2cb6d70e4931c/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=121f7037c7acd26eec6db405f2e2cb6d70e4931c",
            "patch": "@@ -122,7 +122,7 @@ def make_flex_block_causal_mask(\n \n     if attention_chunk_size is not None:\n         # we create an arange, then we just // by chunk size to get [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n-        document_ids = (document_ids.fill_(1).cumsum(-1) - 1) // (attention_chunk_size)\n+        chunk_idxs = (document_ids.clone().fill_(1).cumsum(-1) - 1) // (attention_chunk_size)\n \n     # Instead of passing a tensor mask, flex attention requires a mask_mod function\n     # that determines which elements of QK^T should be included in the attention\n@@ -143,16 +143,26 @@ def causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n         final_mask = causal_mask & padding_mask & document_mask\n         return final_mask\n \n+    def chunk_causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n+        \"\"\"\n+        Combines the chunk mask with the causal mask for chunked attention.\n+        \"\"\"\n+        chunk_mask = chunk_idxs[batch_idx, q_idx] == chunk_idxs[batch_idx, kv_idx]\n+        causal_doc_mask = causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx)\n+        return chunk_mask & causal_doc_mask\n+\n+    mask_mod_maybe_combined = causal_mask_mod if attention_chunk_size is None else chunk_causal_mask_mod\n+\n     if offsets is not None:\n         q_offset = offsets[0]\n         kv_offset = offsets[1]\n \n         def mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n             offset_q = q_idx + q_offset\n             offset_kv = kv_idx + kv_offset\n-            return causal_mask_mod(batch_idx, head_idx, offset_q, offset_kv)\n+            return mask_mod_maybe_combined(batch_idx, head_idx, offset_q, offset_kv)\n     else:\n-        mask_mod = causal_mask_mod\n+        mask_mod = mask_mod_maybe_combined\n     return create_block_causal_mask_flex(\n         mask_mod=mask_mod,\n         B=batch_size,"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 13,
        "deletions": 3
    }
}