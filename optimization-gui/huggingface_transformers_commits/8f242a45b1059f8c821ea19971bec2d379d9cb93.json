{
    "author": "stas00",
    "message": "[deepspeed tests fixes] (#41925)\n\n* [deepspeed tests fixes]\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* simplify\n\n---------\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\nCo-authored-by: Stas Bekman <stas.bekman@snowflake.com>",
    "sha": "8f242a45b1059f8c821ea19971bec2d379d9cb93",
    "files": [
        {
            "sha": "ca9e7b03ac4c359179b48465e571c41a11bfde3b",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f242a45b1059f8c821ea19971bec2d379d9cb93/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f242a45b1059f8c821ea19971bec2d379d9cb93/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=8f242a45b1059f8c821ea19971bec2d379d9cb93",
            "patch": "@@ -756,13 +756,9 @@ def test_gradient_accumulation(self, stage, dtype):\n             self.assertNotEqual(yes_grad_accum_a, a)\n \n         # training with half the batch size but accumulation steps as 2 should give the same\n-        # weights, but sometimes get a slight difference still of 1e-6\n-        if torch_device == \"hpu\":\n-            self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, delta=1e-4)\n-            self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, delta=1e-4)\n-        else:\n-            self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, places=5)\n-            self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, places=5)\n+        # weights, but sometimes get a slight difference still\n+        self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, delta=1e-4)\n+        self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, delta=1e-4)\n \n         # Relative difference. See the note above how to get identical loss on a small bs\n         self.assertTrue((no_grad_accum_loss - yes_grad_accum_loss) / (no_grad_accum_loss + 1e-15) <= 1e-3)"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 3,
        "deletions": 7
    }
}