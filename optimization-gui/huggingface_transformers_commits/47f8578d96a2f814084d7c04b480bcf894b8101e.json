{
    "author": "sergiopaniego",
    "message": "Pass `eps` to `Mistral3RMSNorm` (#38026)\n\nPass eps to Mistral3RMSNorm\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "47f8578d96a2f814084d7c04b480bcf894b8101e",
    "files": [
        {
            "sha": "a74a4663fec05703bb8ae8d7b57d4175325407ee",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/47f8578d96a2f814084d7c04b480bcf894b8101e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47f8578d96a2f814084d7c04b480bcf894b8101e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=47f8578d96a2f814084d7c04b480bcf894b8101e",
            "patch": "@@ -104,7 +104,7 @@ def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor) -> to\n class Mistral3MultiModalProjector(nn.Module):\n     def __init__(self, config: Mistral3Config):\n         super().__init__()\n-        self.norm = Mistral3RMSNorm(config.vision_config.hidden_size)\n+        self.norm = Mistral3RMSNorm(config.vision_config.hidden_size, eps=config.text_config.rms_norm_eps)\n         self.patch_merger = Mistral3PatchMerger(config)\n         # We have hidden_size * the number of vision feature layers\n         num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)"
        },
        {
            "sha": "69232f1ec868e531911bd0b6febf05874e90df7c",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/47f8578d96a2f814084d7c04b480bcf894b8101e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47f8578d96a2f814084d7c04b480bcf894b8101e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=47f8578d96a2f814084d7c04b480bcf894b8101e",
            "patch": "@@ -82,7 +82,7 @@ def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor) -> to\n class Mistral3MultiModalProjector(nn.Module):\n     def __init__(self, config: Mistral3Config):\n         super().__init__()\n-        self.norm = Mistral3RMSNorm(config.vision_config.hidden_size)\n+        self.norm = Mistral3RMSNorm(config.vision_config.hidden_size, eps=config.text_config.rms_norm_eps)\n         self.patch_merger = Mistral3PatchMerger(config)\n         # We have hidden_size * the number of vision feature layers\n         num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}