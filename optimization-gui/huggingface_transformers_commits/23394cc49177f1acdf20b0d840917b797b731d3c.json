{
    "author": "yonigozlan",
    "message": "Simplify using custom resolution for sam3 and sam3_video inference (#42787)\n\n* simplify using custom resolution for sam3 and sam3_video inference\n\n* revert auto format\n\n* use setters and properties\n\n* Fix docstring\n\n* update dict to correctly save image_size to file for backward compatibility",
    "sha": "23394cc49177f1acdf20b0d840917b797b731d3c",
    "files": [
        {
            "sha": "bd63875eb1638152149cd4591a0343eb12d94305",
            "filename": "docs/source/en/model_doc/sam3.md",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -354,6 +354,21 @@ When running the same text prompt on multiple images, pre-compute text embedding\n ...     print(f\"Image {i+1}: {len(results['masks'])} '{text_prompt}' objects found\")\n ```\n \n+### Custom Resolution Inference\n+\n+<div class=\"warning\">\n+⚠️ **Performance Note**: Custom resolutions may degrade accuracy. The model is meant to be used at 1008px resolution.\n+</div>\n+\n+For faster inference or lower memory usage:\n+\n+```python\n+>>> config = Sam3Config.from_pretrained(\"facebook/sam3\")\n+>>> config.image_size = 560\n+>>> model = Sam3Model.from_pretrained(\"facebook/sam3\", config=config).to(device)\n+>>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\", size={\"height\": 560, \"width\": 560})\n+```\n+\n ### Prompt Label Conventions\n \n SAM3 uses the following label conventions:"
        },
        {
            "sha": "39cbe304e4c274e57099ffb57025fb3b6c7a12f8",
            "filename": "docs/source/en/model_doc/sam3_video.md",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -188,6 +188,21 @@ For real-time applications, SAM3 Video supports processing video frames as they\n >>> print(f\"Masks are at original video resolution: {frame_0_outputs['masks'].shape}\")\n ```\n \n+#### Custom Resolution Inference\n+\n+<div class=\"warning\">\n+⚠️ **Performance Note**: Custom resolutions may degrade accuracy. The model is meant to be used at 1008px resolution.\n+</div>\n+\n+For faster inference or lower memory usage:\n+\n+```python\n+>>> config = Sam3VideoConfig.from_pretrained(\"facebook/sam3\")\n+>>> config.image_size = 560\n+>>> model = Sam3VideoModel.from_pretrained(\"facebook/sam3\", config=config).to(device, dtype=torch.bfloat16)\n+>>> processor = Sam3VideoProcessor.from_pretrained(\"facebook/sam3\", size={\"height\": 560, \"width\": 560})\n+```\n+\n ## Sam3VideoConfig\n \n [[autodoc]] Sam3VideoConfig"
        },
        {
            "sha": "3f91d49880a5fd7029e10bb84a87ae35242f14eb",
            "filename": "src/transformers/models/sam3/configuration_sam3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -179,6 +179,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         super().__init__(**kwargs)\n \n+    @property\n+    def image_size(self):\n+        \"\"\"Image size for the vision encoder.\"\"\"\n+        return self.backbone_config.image_size\n+\n+    @image_size.setter\n+    def image_size(self, value):\n+        \"\"\"Set the image size and propagate to backbone.\"\"\"\n+        self.backbone_config.image_size = value\n+\n \n class Sam3GeometryEncoderConfig(PreTrainedConfig):\n     r\"\"\"\n@@ -506,6 +516,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         super().__init__(**kwargs)\n \n+    @property\n+    def image_size(self):\n+        \"\"\"Image size for the SAM3 model.\"\"\"\n+        return self.vision_config.image_size\n+\n+    @image_size.setter\n+    def image_size(self, value):\n+        \"\"\"Set the image size and propagate to vision config.\"\"\"\n+        self.vision_config.image_size = value\n+\n \n __all__ = [\n     \"Sam3Config\","
        },
        {
            "sha": "4e828c68ae0640594f6fb041df740cbe73443f75",
            "filename": "src/transformers/models/sam3_tracker_video/configuration_sam3_tracker_video.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fconfiguration_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fconfiguration_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fconfiguration_sam3_tracker_video.py?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -397,5 +397,30 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @property\n+    def image_size(self):\n+        \"\"\"Image size for the tracker video model.\"\"\"\n+        return self.vision_config.image_size\n+\n+    @image_size.setter\n+    def image_size(self, value):\n+        \"\"\"Set the image size and propagate to sub-configs. Calculates feature sizes based on patch_size.\"\"\"\n+        self.prompt_encoder_config.image_size = value\n+        self.vision_config.image_size = value\n+\n+        patch_size = self.vision_config.backbone_config.patch_size\n+        self.vision_config.backbone_feature_sizes = [\n+            [4 * value // patch_size, 4 * value // patch_size],\n+            [2 * value // patch_size, 2 * value // patch_size],\n+            [value // patch_size, value // patch_size],\n+        ]\n+        self.memory_attention_rope_feat_sizes = [\n+            value // patch_size,\n+            value // patch_size,\n+        ]\n+\n+        # keep the image_size in the __dict__ to save the value in the config file (backward compatibility)\n+        self.__dict__[\"image_size\"] = value\n+\n \n __all__ = [\"Sam3TrackerVideoMaskDecoderConfig\", \"Sam3TrackerVideoPromptEncoderConfig\", \"Sam3TrackerVideoConfig\"]"
        },
        {
            "sha": "5dac5b52dc16259efd82c2a8a843a6cf5129cf16",
            "filename": "src/transformers/models/sam3_tracker_video/modular_sam3_tracker_video.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -353,6 +353,31 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @property\n+    def image_size(self):\n+        \"\"\"Image size for the tracker video model.\"\"\"\n+        return self.vision_config.image_size\n+\n+    @image_size.setter\n+    def image_size(self, value):\n+        \"\"\"Set the image size and propagate to sub-configs. Calculates feature sizes based on patch_size.\"\"\"\n+        self.prompt_encoder_config.image_size = value\n+        self.vision_config.image_size = value\n+\n+        patch_size = self.vision_config.backbone_config.patch_size\n+        self.vision_config.backbone_feature_sizes = [\n+            [4 * value // patch_size, 4 * value // patch_size],\n+            [2 * value // patch_size, 2 * value // patch_size],\n+            [value // patch_size, value // patch_size],\n+        ]\n+        self.memory_attention_rope_feat_sizes = [\n+            value // patch_size,\n+            value // patch_size,\n+        ]\n+\n+        # keep the image_size in the __dict__ to save the value in the config file (backward compatibility)\n+        self.__dict__[\"image_size\"] = value\n+\n \n class Sam3TrackerVideoInferenceCache(Sam2VideoInferenceCache):\n     pass"
        },
        {
            "sha": "a9ec3271db7479016ec65570b67a013a926fac5c",
            "filename": "src/transformers/models/sam3_video/configuration_sam3_video.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconfiguration_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconfiguration_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconfiguration_sam3_video.py?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -96,6 +96,9 @@ class Sam3VideoConfig(PreTrainedConfig):\n     >>> # Initializing a SAM3 Video configuration with default detector and tracker\n     >>> configuration = Sam3VideoConfig()\n \n+    >>> # Changing image size for custom resolution inference (automatically propagates to all nested configs)\n+    >>> configuration.image_size = 560\n+\n     >>> # Initializing a model from the configuration\n     >>> model = Sam3VideoModel(configuration)\n \n@@ -225,5 +228,16 @@ def __init__(\n         self.high_conf_thresh = high_conf_thresh\n         self.high_iou_thresh = high_iou_thresh\n \n+    @property\n+    def image_size(self):\n+        \"\"\"Image size for the video model.\"\"\"\n+        return self.detector_config.image_size\n+\n+    @image_size.setter\n+    def image_size(self, value):\n+        \"\"\"Recursively propagate the image size to detector and tracker configs.\"\"\"\n+        self.detector_config.image_size = value\n+        self.tracker_config.image_size = value\n+\n \n __all__ = [\"Sam3VideoConfig\"]"
        },
        {
            "sha": "d99a4773f7161aa73e97e3d4533813c4f7228fde",
            "filename": "tests/models/sam3/test_modeling_sam3.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -855,6 +855,29 @@ def test_forward_with_both_pixel_values_and_vision_embeds_raises_error(self):\n             with self.assertRaises(ValueError):\n                 model(**inputs_with_both)\n \n+    def test_custom_image_size(self):\n+        \"\"\"Test that custom image size can be set and propagates correctly through nested configs.\"\"\"\n+        config = self.model_tester.get_config()\n+        config.image_size = 560\n+\n+        self.assertEqual(config.image_size, 560)\n+        self.assertEqual(config.vision_config.image_size, 560)\n+        self.assertEqual(config.vision_config.backbone_config.image_size, 560)\n+\n+        # Verify model works with custom size\n+        model = Sam3Model(config=config).to(torch_device).eval()\n+        pixel_values = floats_tensor([self.model_tester.batch_size, self.model_tester.num_channels, 560, 560]).to(\n+            torch_device\n+        )\n+        input_ids = torch.randint(0, 1000, (self.model_tester.batch_size, 16), device=torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=torch.ones_like(input_ids))\n+\n+        self.assertIsNotNone(outputs.pred_masks)\n+        self.assertIsNotNone(outputs.pred_boxes)\n+        self.assertIsNotNone(outputs.pred_logits)\n+\n     @unittest.skip(reason=\"SAM3 model can't be compiled dynamic yet\")\n     def test_sdpa_can_compile_dynamic(self):\n         pass"
        },
        {
            "sha": "8efa46d8173ec36a433479e26bf06b135376cac5",
            "filename": "tests/models/sam3_video/test_modeling_sam3_video.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/23394cc49177f1acdf20b0d840917b797b731d3c/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23394cc49177f1acdf20b0d840917b797b731d3c/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py?ref=23394cc49177f1acdf20b0d840917b797b731d3c",
            "patch": "@@ -532,3 +532,19 @@ def test_inference_video_multi_prompt(self):\n         # All prompts in prompt_to_obj_ids should be from our original prompts\n         for prompt in prompt_to_obj_ids.keys():\n             self.assertIn(prompt, prompts)\n+\n+    def test_custom_image_size(self):\n+        \"\"\"Test that custom image size can be set and propagates correctly to detector and tracker configs.\"\"\"\n+        from transformers import Sam3VideoConfig\n+\n+        config = Sam3VideoConfig.from_pretrained(\"facebook/sam3\")\n+        config.image_size = 560\n+\n+        self.assertEqual(config.image_size, 560)\n+        self.assertEqual(config.detector_config.image_size, 560)\n+        self.assertEqual(config.tracker_config.image_size, 560)\n+        self.assertEqual(config.detector_config.vision_config.image_size, 560)\n+        self.assertEqual(config.detector_config.vision_config.backbone_config.image_size, 560)\n+\n+        model = Sam3VideoModel.from_pretrained(\"facebook/sam3\", config=config).to(torch_device).eval()\n+        self.assertEqual(model.config.image_size, 560)"
        }
    ],
    "stats": {
        "total": 153,
        "additions": 153,
        "deletions": 0
    }
}