{
    "author": "zucchini-nlp",
    "message": "[video processors] decode only sampled videos -> less RAM and faster processing (#39600)\n\n* draft update two models for now\n\n* batch update all VLMs first\n\n* update some more image processors\n\n* update\n\n* fix a few tests\n\n* just make CI green for now\n\n* fix copies\n\n* update once more\n\n* update\n\n* unskip the test\n\n* fix these two\n\n* fix torchcodec audio loading\n\n* maybe\n\n* yay, i fixed torchcodec installation and now can actually test it\n\n* fix copies deepseek\n\n* make sure the metadata is returrned when users request it\n\n* add docs\n\n* update\n\n* fixup\n\n* Update src/transformers/audio_utils.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/glm4v/video_processing_glm4v.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* update\n\n* what if we set some metadata attr to `None`\n\n* fix CI\n\n* fix one test\n\n* fix 4 channel test\n\n* fix glm timestemps\n\n* rebase gone wrong\n\n* raise warning once\n\n* fixup\n\n* typo\n\n* fix copies\n\n* ifx smolvlm test\n\n* this is why torch's official benchmark was faster, set threads to `0`\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "f690a2a1e09e8a8c7b04cc050ef24838c609060b",
    "files": [
        {
            "sha": "7dc9de60571f1730f3b430bc0135261f1fc418ea",
            "filename": "docs/source/en/main_classes/image_processor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -16,8 +16,7 @@ rendered properly in your Markdown viewer.\n \n # Image Processor\n \n-An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to Numpy and PyTorch tensors. It may also include model specific post-processing such as converting logits to segmentation masks.\n-\n+An image processor is in charge of loading images (optionally), preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.\n Fast image processors are available for a few models and more will be added in the future. They are based on the [torchvision](https://pytorch.org/vision/stable/index.html) library and provide a significant speed-up, especially when processing on GPU.\n They have the same API as the base image processors and can be used as drop-in replacements.\n To use a fast image processor, you need to install the `torchvision` library, and set the `use_fast` argument to `True` when instantiating the image processor:"
        },
        {
            "sha": "ee69030ab1a1366b306427822fac168a47700abe",
            "filename": "docs/source/en/main_classes/video_processor.md",
            "status": "modified",
            "additions": 42,
            "deletions": 2,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -14,10 +14,9 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-\n # Video Processor\n \n-A **Video Processor** is a utility responsible for preparing input features for video models, as well as handling the post-processing of their outputs. It provides transformations such as resizing, normalization, and conversion into PyTorch. \n+A **Video Processor** is a utility responsible for preparing input features for video models, as well as handling the post-processing of their outputs. It provides transformations such as resizing, normalization, and conversion into PyTorch. Along ith transformations the `VideoProcessor` class handles video decoding from local paths or URLs (requires [`torchcodec`](https://pypi.org/project/torchcodec/)) and frame sampling according to model-specific strategies.\n \n The video processor extends the functionality of image processors by allowing Vision Large Language Models (VLMs) to handle videos with a distinct set of arguments compared to images. It serves as the bridge between raw video data and the model, ensuring that input features are optimized for the VLM.\n \n@@ -48,6 +47,47 @@ processor = torch.compile(processor)\n processed_video = processor(video, return_tensors=\"pt\")\n ```\n \n+#### Sampling behavior\n+\n+The video processor can also sample video frames using the technique best suited for the given model. Sampling behavior is controlled with the `do_sample_frames` argument and can be configured through model-specific parameters such as `num_frames` or `fps` (the rate at which the video will be sampled). If the input video is given as a local path or URL (`str`), the processor will decode it automatically. To obtain metadata about the decoded video, such as sampled frame indices, original dimensions, duration, and fps, pass `return_metadata=True` to the processor.\n+\n+<Tip warning={false}>\n+\n+- Specifying `num_frames` does not guarantee the output will contain exactly that number of frames. Depending on the model, the sampler may enforce minimum or maximum frame limits.\n+\n+- The default decoder is [`torchcodec`](https://pypi.org/project/torchcodec/), which must be installed.\n+\n+</Tip>\n+\n+\n+```python\n+from transformers import AutoVideoProcessor\n+\n+processor = AutoVideoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device=\"cuda\")\n+processed_video_inputs = processor(videos=[\"video_path.mp4\"], return_metadata=True, do_sample_frames=True, return_tensors=\"pt\")\n+video_metadata = processed_video_inputs[\"video_metadata\"]\n+\n+# See how many frames the original video had and what was the original FPS\n+print(video_metadata.total_num_frames, video_metadata.fps)\n+```\n+\n+If you pass an already decoded video array but still want to enable model-specific frame sampling, it is strongly recommended to provide video_metadata. This allows the sampler to know the original videoâ€™s duration and FPS. You can pass metadata as a `VideoMetadata` object or as a plain dict.\n+\n+```python\n+from transformers import AutoVideoProcessor\n+from transformers.video_utils import VideoMetadata\n+\n+processor = AutoVideoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device=\"cuda\")\n+my_decodec_video = torch.randint(0, 255, size=(100, 3, 1280, 1280)) # short video of 100 frames\n+video_metadata = VideoMetadata(\n+    total_num_frames=100,\n+    fps=24,\n+    duration=4.1, # in seconds\n+)\n+processed_video_inputs = processor(videos=[\"video_path.mp4\"], video_metadata=video_metadata, do_sample_frames=True, num_frames=10, return_tensors=\"pt\")\n+print(processed_video_inputs.pixel_values_videos.shape)\n+>>> [10, 3, 384, 384]\n+```\n \n ## BaseVideoProcessor\n "
        },
        {
            "sha": "0564fdd1b25fa5d70d90ec1b54fad076a98a03f8",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 63,
            "deletions": 13,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -21,7 +21,7 @@\n import os\n import warnings\n from io import BytesIO\n-from typing import Any, Optional, Union\n+from typing import Any, Optional, Sequence, Union\n \n import numpy as np\n import requests\n@@ -31,6 +31,7 @@\n     is_numpy_array,\n     is_soundfile_available,\n     is_torch_tensor,\n+    is_torchcodec_available,\n     requires_backends,\n )\n \n@@ -44,6 +45,12 @@\n     # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n     import soxr\n \n+if is_torchcodec_available():\n+    from torchcodec.decoders import AudioDecoder\n+\n+\n+AudioInput = Union[np.ndarray, \"torch.Tensor\", Sequence[np.ndarray], Sequence[\"torch.Tensor\"]]  # noqa: F821\n+\n \n def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:\n     \"\"\"\n@@ -61,14 +68,14 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n     Returns:\n         `np.ndarray`: A numpy array representing the audio.\n     \"\"\"\n-    requires_backends(load_audio, [\"librosa\"])\n-\n     if isinstance(audio, str):\n-        # Load audio from URL (e.g https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav)\n-        if audio.startswith(\"http://\") or audio.startswith(\"https://\"):\n-            audio = librosa.load(BytesIO(requests.get(audio, timeout=timeout).content), sr=sampling_rate)[0]\n-        elif os.path.isfile(audio):\n-            audio = librosa.load(audio, sr=sampling_rate)[0]\n+        # Try to load with `torchcodec` but do not enforce users to install it. If not found\n+        # fallback to `librosa`. If using an audio-only model, most probably `torchcodec` won't be\n+        # needed.\n+        if is_torchcodec_available():\n+            audio = load_audio_torchcodec(audio, sampling_rate=sampling_rate)\n+        else:\n+            audio = load_audio_librosa(audio, sampling_rate=sampling_rate, timeout=timeout)\n     elif isinstance(audio, np.ndarray):\n         audio = audio\n     else:\n@@ -78,6 +85,54 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n     return audio\n \n \n+def load_audio_torchcodec(audio: Union[str, np.ndarray], sampling_rate=16000) -> np.ndarray:\n+    \"\"\"\n+    Loads `audio` to an np.ndarray object using `torchcodec`.\n+\n+    Args:\n+        audio (`str` or `np.ndarray`):\n+            The audio to be loaded to the numpy array format.\n+        sampling_rate (`int`, *optional*, defaults to 16000):\n+            The sampling rate to be used when loading the audio. It should be same as the\n+            sampling rate the model you will be using further was trained with.\n+\n+    Returns:\n+        `np.ndarray`: A numpy array representing the audio.\n+    \"\"\"\n+    requires_backends(load_audio, [\"torchcodec\"])\n+\n+    # Set `num_channels` to `1` which is what most models expects and the default in librosa\n+    decoder = AudioDecoder(audio, sample_rate=sampling_rate, num_channels=1)\n+    audio = decoder.get_all_samples().data[0].numpy()  # NOTE: feature extractors don't accept torch tensors\n+    return audio\n+\n+\n+def load_audio_librosa(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:\n+    \"\"\"\n+    Loads `audio` to an np.ndarray object using `librosa`.\n+\n+    Args:\n+        audio (`str` or `np.ndarray`):\n+            The audio to be loaded to the numpy array format.\n+        sampling_rate (`int`, *optional*, defaults to 16000):\n+            The sampling rate to be used when loading the audio. It should be same as the\n+            sampling rate the model you will be using further was trained with.\n+        timeout (`float`, *optional*):\n+            The timeout value in seconds for the URL request.\n+\n+    Returns:\n+        `np.ndarray`: A numpy array representing the audio.\n+    \"\"\"\n+    requires_backends(load_audio, [\"librosa\"])\n+\n+    # Load audio from URL (e.g https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav)\n+    if audio.startswith(\"http://\") or audio.startswith(\"https://\"):\n+        audio = librosa.load(BytesIO(requests.get(audio, timeout=timeout).content), sr=sampling_rate)[0]\n+    elif os.path.isfile(audio):\n+        audio = librosa.load(audio, sr=sampling_rate)[0]\n+    return audio\n+\n+\n def load_audio_as(\n     audio: str,\n     return_format: str,\n@@ -157,11 +212,6 @@ def load_audio_as(\n         raise ValueError(f\"Error loading audio: {e}\")\n \n \n-AudioInput = Union[\n-    np.ndarray, \"torch.Tensor\", list[np.ndarray], tuple[np.ndarray], list[\"torch.Tensor\"], tuple[\"torch.Tensor\"]  # noqa: F821\n-]\n-\n-\n def is_valid_audio(audio):\n     return is_numpy_array(audio) or is_torch_tensor(audio)\n "
        },
        {
            "sha": "899f4ea746b6db7d61059c2e6e3b3c8a0f7c4c74",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -17,14 +17,13 @@\n import json\n import os\n import warnings\n-from io import BytesIO\n from typing import Any, Optional, TypeVar, Union\n \n import numpy as np\n-import requests\n \n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature as BaseBatchFeature\n+from .image_utils import is_valid_image, load_image\n from .utils import (\n     IMAGE_PROCESSOR_NAME,\n     PushToHubMixin,\n@@ -33,15 +32,10 @@\n     download_url,\n     is_offline_mode,\n     is_remote_url,\n-    is_vision_available,\n     logging,\n )\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n ImageProcessorType = TypeVar(\"ImageProcessorType\", bound=\"ImageProcessingMixin\")\n \n \n@@ -514,25 +508,19 @@ def register_for_auto_class(cls, auto_class=\"AutoImageProcessor\"):\n \n         cls._auto_class = auto_class\n \n-    def fetch_images(self, image_url_or_urls: Union[str, list[str]]):\n+    def fetch_images(self, image_url_or_urls: Union[str, list[str], list[list[str]]]):\n         \"\"\"\n         Convert a single or a list of urls into the corresponding `PIL.Image` objects.\n \n         If a single url is passed, the return value will be a single object. If a list is passed a list of objects is\n         returned.\n         \"\"\"\n-        headers = {\n-            \"User-Agent\": (\n-                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0\"\n-                \" Safari/537.36\"\n-            )\n-        }\n         if isinstance(image_url_or_urls, list):\n             return [self.fetch_images(x) for x in image_url_or_urls]\n         elif isinstance(image_url_or_urls, str):\n-            response = requests.get(image_url_or_urls, stream=True, headers=headers)\n-            response.raise_for_status()\n-            return Image.open(BytesIO(response.content))\n+            return load_image(image_url_or_urls)\n+        elif is_valid_image(image_url_or_urls):\n+            return image_url_or_urls\n         else:\n             raise TypeError(f\"only a single or a list of entries is supported but got type={type(image_url_or_urls)}\")\n "
        },
        {
            "sha": "670fb5ece4cd1982455c431c2eb1f80896c8ea69",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -85,7 +85,7 @@ def validate_fast_preprocess_arguments(\n     crop_size: Optional[SizeDict] = None,\n     do_resize: Optional[bool] = None,\n     size: Optional[SizeDict] = None,\n-    resample: Optional[\"PILImageResampling\"] = None,\n+    interpolation: Optional[\"F.InterpolationMode\"] = None,\n     return_tensors: Optional[Union[str, TensorType]] = None,\n     data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n ):\n@@ -105,7 +105,7 @@ def validate_fast_preprocess_arguments(\n         crop_size=crop_size,\n         do_resize=do_resize,\n         size=size,\n-        resample=resample,\n+        interpolation=interpolation,\n     )\n     # Extra checks for ImageProcessorFast\n     if return_tensors is not None and return_tensors != \"pt\":\n@@ -469,6 +469,8 @@ def _prepare_images_structure(\n         Returns:\n             `ImageInput`: The images with a valid nesting.\n         \"\"\"\n+        # Checks for `str` in case of URL/local path and optionally loads images\n+        images = self.fetch_images(images)\n         return make_flat_list_of_images(images, expected_ndims=expected_ndims)\n \n     def _process_image(\n@@ -582,11 +584,19 @@ def _further_process_kwargs(\n \n         kwargs[\"size\"] = size\n         kwargs[\"crop_size\"] = crop_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"data_format\"] = data_format\n \n+        # torch resize uses interpolation instead of resample\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n         return kwargs\n \n     def _validate_preprocess_kwargs(\n@@ -600,7 +610,7 @@ def _validate_preprocess_kwargs(\n         size: Optional[SizeDict] = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[SizeDict] = None,\n-        resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = None,\n         **kwargs,\n@@ -618,7 +628,7 @@ def _validate_preprocess_kwargs(\n             size=size,\n             do_center_crop=do_center_crop,\n             crop_size=crop_size,\n-            resample=resample,\n+            interpolation=interpolation,\n             return_tensors=return_tensors,\n             data_format=data_format,\n         )\n@@ -646,18 +656,7 @@ def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImag\n         # Validate kwargs\n         self._validate_preprocess_kwargs(**kwargs)\n \n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-\n-        # Check if resample is an int before checking if it's an instance of PILImageResampling\n-        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n-        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (int, PILImageResampling)) else resample\n-        )\n-\n         # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n         kwargs.pop(\"data_format\")\n \n         return self._preprocess_image_like_inputs("
        },
        {
            "sha": "d55bf710c06ca3f1454d2a405594524c80684657",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -535,6 +535,7 @@ def validate_preprocess_arguments(\n     do_resize: Optional[bool] = None,\n     size: Optional[dict[str, int]] = None,\n     resample: Optional[\"PILImageResampling\"] = None,\n+    interpolation: Optional[\"InterpolationMode\"] = None,\n ):\n     \"\"\"\n     Checks validity of typically used arguments in an `ImageProcessor` `preprocess` method.\n@@ -559,8 +560,13 @@ def validate_preprocess_arguments(\n     if do_center_crop and crop_size is None:\n         raise ValueError(\"`crop_size` must be specified if `do_center_crop` is `True`.\")\n \n-    if do_resize and (size is None or resample is None):\n-        raise ValueError(\"`size` and `resample` must be specified if `do_resize` is `True`.\")\n+    if interpolation is not None and resample is not None:\n+        raise ValueError(\n+            \"Only one of `interpolation` and `resample` should be specified, depending on image processor type.\"\n+        )\n+\n+    if do_resize and not (size is not None and (resample is not None or interpolation is not None)):\n+        raise ValueError(\"`size` and `resample/interpolation` must be specified if `do_resize` is `True`.\")\n \n \n # In the future we can add a TF implementation here when we have TF models."
        },
        {
            "sha": "6146f9b32bd2fcf0d5fda8833dbea01b0a3be2d5",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -228,6 +228,7 @@ def preprocess(\n         if max_image_size not in [490, 980]:\n             raise ValueError(\"max_image_size must be either 490 or 980\")\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "d91807fcbaf199dd9f7ac5d644412ca4cd1dcc62",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -614,6 +614,7 @@ def preprocess(\n         if max_image_size not in [490, 980]:\n             raise ValueError(\"max_image_size must be either 490 or 980\")\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "7045c967046da163fa2d7daef04f5c6e6bebc794",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -189,6 +189,7 @@ def __call__(\n         # Process images\n         image_inputs = {}\n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             images = make_flat_list_of_images(images)\n             image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n             num_patches = image_inputs.pop(\"num_patches\")"
        },
        {
            "sha": "b932cb1453f2d23c1e21f1c16a9641d88672e08c",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -231,6 +231,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "7e047284aa2f292b0d1e390d8741f80a16bf09ec",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -465,6 +465,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "651fd63b7e44d0534f3c842176e8843277991f17",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -248,6 +248,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "25709a3d5462d31b4d4ee293cae2c731ab790a6b",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -285,6 +285,7 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "3b86a0ee111619d0a5c42af03c2af58e792f9555",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -19,7 +19,7 @@\n from transformers.models.paligemma.processing_paligemma import IMAGE_TOKEN, PaliGemmaProcessor, build_string_from_input\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n+from ...image_utils import ImageInput, make_flat_list_of_images\n from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torch_available, logging\n@@ -147,13 +147,8 @@ def __call__(\n             raise ValueError(\"Only one of text or images can be processed at a time\")\n \n         if images is not None:\n-            if is_valid_image(images):\n-                images = [images]\n-            elif isinstance(images, list) and is_valid_image(images[0]):\n-                pass\n-            elif not (isinstance(images, list) and isinstance(images[0], list) and is_valid_image(images[0][0])):\n-                raise ValueError(\"images must be an image, list of images or list of list of images\")\n-\n+            images = self.image_processor.fetch_images(images)\n+            images = make_flat_list_of_images(images)\n             texts_doc = [self.visual_prompt_prefix] * len(images)\n             images = [image.convert(\"RGB\") for image in images]\n \n@@ -167,7 +162,6 @@ def __call__(\n                 )\n                 for prompt, image_list in zip(texts_doc, images)\n             ]\n-            images = make_flat_list_of_images(images)\n             pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n             # max_length has to account for the image tokens"
        },
        {
            "sha": "429856ec30cbd92f72b0d281c72fa2adf9383dcb",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -23,7 +23,7 @@\n from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n+from ...image_utils import ImageInput, make_flat_list_of_images\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n from ...utils import is_torch_available\n@@ -191,13 +191,8 @@ def __call__(\n             raise ValueError(\"Only one of text or images can be processed at a time\")\n \n         if images is not None:\n-            if is_valid_image(images):\n-                images = [images]\n-            elif isinstance(images, list) and is_valid_image(images[0]):\n-                pass\n-            elif not (isinstance(images, list) and isinstance(images[0], list) and is_valid_image(images[0][0])):\n-                raise ValueError(\"images must be an image, list of images or list of list of images\")\n-\n+            images = self.image_processor.fetch_images(images)\n+            images = make_flat_list_of_images(images)\n             texts_doc = [self.visual_prompt_prefix] * len(images)\n             images = [image.convert(\"RGB\") for image in images]\n \n@@ -211,7 +206,6 @@ def __call__(\n                 )\n                 for prompt, image_list in zip(texts_doc, images)\n             ]\n-            images = make_flat_list_of_images(images)\n             pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n             # max_length has to account for the image tokens"
        },
        {
            "sha": "8a68d434837f28b95ec37314d2afb226d48d73ec",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -277,6 +277,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "6d9b7709eae671ca931d37906a0cd3cc8e3c19cd",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -16,10 +16,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...image_processing_utils import BatchFeature\n-from ...image_utils import (\n-    ImageInput,\n-    make_flat_list_of_images,\n-)\n+from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import (\n     PreTokenizedInput,\n@@ -302,7 +299,6 @@ def __call__(\n \n         # process images if pixel_values are provided\n         if images is not None:\n-            images = make_flat_list_of_images(images)\n             data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n         return BatchFeature(data=data)"
        },
        {
            "sha": "ada14ab87b908b80215390ddb74a40f8afaa0db5",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Union\n \n from ...image_processing_utils import BatchFeature\n-from ...image_utils import ImageInput, make_flat_list_of_images\n+from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n@@ -128,7 +128,6 @@ def __call__(\n \n         # process images if pixel_values are provided\n         if images is not None:\n-            images = make_flat_list_of_images(images)\n             data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n         return BatchFeature(data=data)"
        },
        {
            "sha": "4589a0deeca5a2d805a7171174f5e21661c53cca",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -34,7 +34,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -327,7 +327,8 @@ def preprocess(\n         high_res_size = high_res_size if high_res_size is not None else self.high_res_size\n         high_res_size_dict = get_size_dict(high_res_size)\n \n-        images = make_list_of_images(images)\n+        images = self.fetch_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "14a99c56a049cb8e15a289114285c46083e8caef",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -312,9 +312,15 @@ def _further_process_kwargs(\n             else high_res_resample\n         )\n \n+        low_res_resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[low_res_resample]\n+            if isinstance(low_res_resample, (int, PILImageResampling))\n+            else low_res_resample\n+        )\n+\n         kwargs[\"size\"] = size\n         kwargs[\"high_res_size\"] = high_res_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"high_res_image_mean\"] = high_res_image_mean"
        },
        {
            "sha": "b6ada0c8f1e5cb0863673338640f0a393e102265",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -36,7 +36,6 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_flat_list_of_images,\n-    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -633,7 +632,8 @@ def preprocess(\n         high_res_size = high_res_size if high_res_size is not None else self.high_res_size\n         high_res_size_dict = get_size_dict(high_res_size)\n \n-        images = make_list_of_images(images)\n+        images = self.fetch_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -804,9 +804,15 @@ def _further_process_kwargs(\n             else high_res_resample\n         )\n \n+        low_res_resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[low_res_resample]\n+            if isinstance(low_res_resample, (int, PILImageResampling))\n+            else low_res_resample\n+        )\n+\n         kwargs[\"size\"] = size\n         kwargs[\"high_res_size\"] = high_res_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"high_res_image_mean\"] = high_res_image_mean\n@@ -969,7 +975,6 @@ def __call__(\n \n         # process images if pixel_values are provided\n         if images is not None:\n-            images = make_flat_list_of_images(images)\n             inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             data[\"pixel_values\"] = inputs[\"pixel_values\"]\n             data[\"high_res_pixel_values\"] = inputs[\"high_res_pixel_values\"]"
        },
        {
            "sha": "914c59ad205c294f148303ef3d0e6ce252c861ad",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Union\n \n from ...image_processing_utils_fast import BatchFeature\n-from ...image_utils import ImageInput, make_flat_list_of_images\n+from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n@@ -128,7 +128,6 @@ def __call__(\n \n         # process images if pixel_values are provided\n         if images is not None:\n-            images = make_flat_list_of_images(images)\n             inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             data[\"pixel_values\"] = inputs[\"pixel_values\"]\n             data[\"high_res_pixel_values\"] = inputs[\"high_res_pixel_values\"]"
        },
        {
            "sha": "5a480351307ab7b346d17d85d6072ee2cd32eb09",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -381,6 +381,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n \n         if images is not None:\n+            images = self.fetch_images(images)\n             images = make_batched_images(images)\n \n         if images is not None and not valid_images(images):"
        },
        {
            "sha": "05131c543d83b3d0276b68e6d66cfb943de1b7fb",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -578,6 +578,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         ignore_index = ignore_index if ignore_index is not None else self.ignore_index\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "5dcc5326d968c9276cf8b454c1224660a230646e",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -337,7 +337,6 @@ def _further_process_kwargs(\n \n         kwargs[\"size\"] = size\n         kwargs[\"crop_size\"] = crop_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"codebook_size\"] = codebook_size\n@@ -351,6 +350,15 @@ def _further_process_kwargs(\n             else codebook_resample\n         )\n \n+        # torch resize uses interpolation instead of resample\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n         return kwargs\n \n     def _preprocess_image("
        },
        {
            "sha": "f7bd414dbb9181c131962e9f51facc56df83b448",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -334,6 +334,7 @@ def preprocess(\n             else self.pan_and_scan_min_ratio_to_activate\n         )\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "4c27053e1a6f1779690df6da0efd49315f214438",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -101,8 +101,9 @@ def __call__(\n \n         image_inputs = {}\n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             batched_images = make_nested_list_of_images(images)\n-            image_inputs = self.image_processor(batched_images, **output_kwargs[\"images_kwargs\"])\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n \n             # Create empty text to be replaced with placeholders\n             if not text:"
        },
        {
            "sha": "19274fece4c13b329ede684d3f4d590ffabae114",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -134,6 +134,7 @@ def __call__(\n             audio_inputs = {}\n \n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             batched_images = make_nested_list_of_images(images)\n             image_inputs = self.image_processor(batched_images, **output_kwargs[\"images_kwargs\"])\n "
        },
        {
            "sha": "db2691c38dc7fe9d4bc6ddb0e779547eabf3d6d2",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -370,7 +370,6 @@ def preprocess(\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n \n         \"\"\"\n-\n         if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n             raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n         elif size is None:\n@@ -390,6 +389,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         if images is not None:\n+            images = self.fetch_images(images)\n             images = make_flat_list_of_images(images)\n \n         if images is not None and not valid_images(images):\n@@ -461,9 +461,9 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n             height=height,\n             width=width,\n             factor=factor,\n-            temporal_factor=self.temporal_patch_size,\n             min_pixels=size[\"shortest_edge\"],\n             max_pixels=size[\"longest_edge\"],\n+            temporal_factor=self.temporal_patch_size,\n         )\n         grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n         return grid_h * grid_w"
        },
        {
            "sha": "f39ba8e3824a64737215a9b5fdaf16f6be202c3b",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -1510,6 +1510,7 @@ class Glm4vProcessorKwargs(Qwen2_VLProcessorKwargs):\n             \"padding\": False,\n             \"return_mm_token_type_ids\": False,\n         },\n+        \"videos_kwargs\": {\"return_metadata\": True},\n     }\n \n \n@@ -1591,11 +1592,14 @@ def __call__(\n \n         if videos is not None:\n             videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n-            timestamps = videos_inputs.pop(\"timestamps\")\n+            # If user has not requested video metadata, pop it\n+            if \"return_metadata\" not in kwargs:\n+                video_metadata = videos_inputs.pop(\"video_metadata\")\n+            else:\n+                video_metadata = videos_inputs[\"video_metadata\"]\n             video_grid_thw = videos_inputs[\"video_grid_thw\"]\n         else:\n             videos_inputs = {}\n-            timestamps = []\n             video_grid_thw = None\n \n         if not isinstance(text, list):\n@@ -1620,22 +1624,27 @@ def __call__(\n                     num_frames = video_grid_thw[video_index][0]\n                     video_structure = \"\"\n \n-                    if hasattr(timestamps, \"tolist\"):\n-                        timestamps_list = timestamps.tolist()[0]\n-                    else:\n-                        timestamps_list = timestamps[0] if isinstance(timestamps[0], list) else timestamps\n+                    metadata = video_metadata[i]\n+                    if metadata.fps is None:\n+                        logger.warning_once(\n+                            \"SmolVLM requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                            \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                            \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n+                        )\n+                    metadata.fps = 24 if metadata.fps is None else metadata.fps\n+                    timestamps = metadata.timestamps[::2]  # mrope\n \n                     unique_timestamps = []\n-                    for idx in range(0, len(timestamps_list)):\n-                        unique_timestamps.append(timestamps_list[idx])\n+                    for idx in range(0, len(timestamps)):\n+                        unique_timestamps.append(timestamps[idx])\n \n                     selected_timestamps = unique_timestamps[:num_frames]\n                     while len(selected_timestamps) < num_frames:\n                         selected_timestamps.append(selected_timestamps[-1] if selected_timestamps else 0)\n \n                     for frame_idx in range(num_frames):\n                         timestamp_sec = selected_timestamps[frame_idx]\n-                        frame_structure = f\"<|begin_of_image|>{self.image_token}<|end_of_image|>{timestamp_sec}\"\n+                        frame_structure = f\"<|begin_of_image|>{self.image_token}<|end_of_image|>{int(timestamp_sec)}\"\n                         video_structure += frame_structure\n \n                     text[i] = text[i].replace(self.video_token, video_structure, 1)"
        },
        {
            "sha": "3cec2c8972587cc41dee3cd947e59f2613497900",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 22,
            "deletions": 9,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -26,9 +26,13 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n from ...video_utils import VideoInput\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class Glm4vVideosProcessorKwargs(VideosKwargs, total=False):\n     fps: Union[list[float], float]\n \n@@ -46,6 +50,7 @@ class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n             \"padding\": False,\n             \"return_mm_token_type_ids\": False,\n         },\n+        \"videos_kwargs\": {\"return_metadata\": True},\n     }\n     videos_kwargs: Glm4vVideosProcessorKwargs\n \n@@ -142,11 +147,14 @@ def __call__(\n \n         if videos is not None:\n             videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n-            timestamps = videos_inputs.pop(\"timestamps\")\n+            # If user has not requested video metadata, pop it\n+            if \"return_metadata\" not in kwargs:\n+                video_metadata = videos_inputs.pop(\"video_metadata\")\n+            else:\n+                video_metadata = videos_inputs[\"video_metadata\"]\n             video_grid_thw = videos_inputs[\"video_grid_thw\"]\n         else:\n             videos_inputs = {}\n-            timestamps = []\n             video_grid_thw = None\n \n         if not isinstance(text, list):\n@@ -171,22 +179,27 @@ def __call__(\n                     num_frames = video_grid_thw[video_index][0]\n                     video_structure = \"\"\n \n-                    if hasattr(timestamps, \"tolist\"):\n-                        timestamps_list = timestamps.tolist()[0]\n-                    else:\n-                        timestamps_list = timestamps[0] if isinstance(timestamps[0], list) else timestamps\n+                    metadata = video_metadata[i]\n+                    if metadata.fps is None:\n+                        logger.warning_once(\n+                            \"SmolVLM requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                            \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                            \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n+                        )\n+                    metadata.fps = 24 if metadata.fps is None else metadata.fps\n+                    timestamps = metadata.timestamps[::2]  # mrope\n \n                     unique_timestamps = []\n-                    for idx in range(0, len(timestamps_list)):\n-                        unique_timestamps.append(timestamps_list[idx])\n+                    for idx in range(0, len(timestamps)):\n+                        unique_timestamps.append(timestamps[idx])\n \n                     selected_timestamps = unique_timestamps[:num_frames]\n                     while len(selected_timestamps) < num_frames:\n                         selected_timestamps.append(selected_timestamps[-1] if selected_timestamps else 0)\n \n                     for frame_idx in range(num_frames):\n                         timestamp_sec = selected_timestamps[frame_idx]\n-                        frame_structure = f\"<|begin_of_image|>{self.image_token}<|end_of_image|>{timestamp_sec}\"\n+                        frame_structure = f\"<|begin_of_image|>{self.image_token}<|end_of_image|>{int(timestamp_sec)}\"\n                         video_structure += frame_structure\n \n                     text[i] = text[i].replace(self.video_token, video_structure, 1)"
        },
        {
            "sha": "a327ac200507dda3d086252f651b0b57e3c7c289",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 34,
            "deletions": 43,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -120,27 +120,42 @@ def _further_process_kwargs(\n \n     def sample_frames(\n         self,\n-        video: torch.Tensor,\n-        metadata: Union[VideoMetadata, dict],\n+        metadata: VideoMetadata,\n+        fps: Optional[Union[int, float]] = None,\n+        **kwargs,\n     ):\n-        total_frames = video.shape[0]\n-        video_fps = getattr(metadata, \"fps\", 2.0)\n-        meta_frames = getattr(metadata, \"total_num_frames\", total_frames)\n-        max_frame_idx = meta_frames - 1\n-        duration = getattr(metadata, \"duration\", None)\n-        if duration is None:\n-            duration = round(max_frame_idx / video_fps) + 1\n+        \"\"\"\n+        Args:\n+            metadata (`VideoMetadata`):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            fps (`int` or `float`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+        Returns:\n+            np.ndarray:\n+                Indices to sample video frames.\n+        \"\"\"\n+        if metadata is None or getattr(metadata, \"fps\", None) is None:\n+            raise ValueError(\n+                \"Asked to sample frames per second but no video metadata was provided which is required when sampling in GLM4V. \"\n+                \"Please pass in `VideoMetadata` object or set `do_sample_frames=False`\"\n+            )\n+\n+        total_frames = metadata.total_num_frames\n+        requested_fps = fps if fps is not None else self.fps\n+\n+        max_frame_idx = total_frames - 1\n+        duration = metadata.duration or round(max_frame_idx / metadata.fps) + 1\n \n         if duration <= self.max_duration:\n-            n = int(math.floor(duration * self.fps))\n-            frame_indices = [min(max_frame_idx, int(math.ceil(i * video_fps / self.fps))) for i in range(n)]\n+            n = int(math.floor(duration * requested_fps))\n+            frame_indices = [min(max_frame_idx, int(math.ceil(i * metadata.fps / requested_fps))) for i in range(n)]\n         else:\n-            num_samples = int(self.max_duration * self.fps)\n-            if num_samples >= meta_frames:\n-                frame_indices = list(range(meta_frames))\n+            num_samples = int(self.max_duration * requested_fps)\n+            if num_samples >= total_frames:\n+                frame_indices = list(range(total_frames))\n             else:\n                 target_seconds = np.linspace(0, duration, num_samples, endpoint=True)\n-                frame_indices = [min(max_frame_idx, int(math.ceil(t * video_fps))) for t in target_seconds]\n+                frame_indices = [min(max_frame_idx, int(math.ceil(t * metadata.fps))) for t in target_seconds]\n \n         seen, uniq = set(), []\n         for idx in frame_indices:\n@@ -151,23 +166,18 @@ def sample_frames(\n         if len(uniq) & 1:\n             uniq.append(uniq[-1])\n \n-        frame_indices = uniq\n-        sampled_video = video[frame_indices]\n-        full_second_idxs = [int(idx / video_fps) for idx in frame_indices]\n-        second_idxs = full_second_idxs[::2]  # mrope\n-        return sampled_video, second_idxs\n+        return np.array(uniq)\n \n     def _preprocess(\n         self,\n         videos: list[torch.Tensor],\n-        video_metadata: Optional[Union[list[VideoMetadata], list[dict]]] = None,\n+        do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: bool = SizeDict,\n+        size: Optional[SizeDict] = None,\n         interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n         do_normalize: bool = True,\n-        do_sample_frames: bool = True,\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         patch_size: Optional[int] = None,\n@@ -176,25 +186,7 @@ def _preprocess(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ):\n-        timestamps_list = []\n-        if do_sample_frames:\n-            if video_metadata is None or (isinstance(video_metadata, list) and video_metadata[0] is None):\n-                raise ValueError(\n-                    \"Frame sampling is enabled but no video metadata was found. \"\n-                    \"Please pass in `VideoMetadata` object per each input video or set `do_sample_frames=False`\"\n-                )\n-            processed_videos = []\n-            for video, metadata in zip(videos, video_metadata):\n-                video, timestamps = self.sample_frames(video, metadata)\n-                timestamps_list.append(timestamps)\n-                processed_videos.append(video)\n-        else:\n-            # Assume 24 fps by default and prepare timestamps for the whole video when all frames are sampled\n-            processed_videos = videos\n-            timestamps_list = [[idx // 24 for idx in range(len(video))] for video in videos]\n-            timestamps_list = timestamps_list[::2]  # mrope\n-\n-        grouped_videos, grouped_videos_index = group_videos_by_shape(processed_videos)\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}\n \n         for shape, stacked_videos in grouped_videos.items():\n@@ -271,7 +263,6 @@ def _preprocess(\n         data = {\n             \"pixel_values_videos\": pixel_values_videos,\n             \"video_grid_thw\": video_grid_thw,\n-            \"timestamps\": timestamps_list,\n         }\n \n         return BatchFeature(data=data, tensor_type=return_tensors)"
        },
        {
            "sha": "a1a48fa6cf7b9951edcc2023a1a9af6bdc23de28",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -339,6 +339,8 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "190e1d31dc7861891d71eda0a87f5b2c51bafbbe",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -151,6 +151,7 @@ def preprocess(\n         if isinstance(images, list) and len(images) == 0:\n             return []\n \n+        images = self.fetch_images(images)\n         images = make_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "2e564708a0788f89be543b83fbe741fcfde1f0af",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -472,6 +472,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         do_image_splitting = do_image_splitting if do_image_splitting is not None else self.do_image_splitting\n \n+        images = self.fetch_images(images)\n         images_list = make_nested_list_of_images(images)\n \n         if not valid_images(images_list[0]):"
        },
        {
            "sha": "feae13ab2e45f94de33aa5bcf6950f1fe73b3235",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -687,6 +687,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n         do_pad = do_pad if do_pad is not None else self.do_pad\n \n+        images = self.fetch_images(images)\n         images_list = make_nested_list_of_images(images)\n \n         if not valid_images(images_list[0]):"
        },
        {
            "sha": "805ecda064974df70bd90eb2eff9a1bd7bcf94af",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -35,7 +35,7 @@\n )\n from ...utils.import_utils import requires\n from ...video_processing_utils import BaseVideoProcessor\n-from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n+from ...video_utils import group_videos_by_shape, reorder_videos\n \n \n if is_vision_available():\n@@ -76,7 +76,6 @@ def __init__(self, **kwargs: Unpack[InstructBlipVideoVideoProcessorInitKwargs]):\n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n-        video_metadata: Union[list[VideoMetadata], list[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -88,24 +87,11 @@ def _preprocess(\n         do_pad: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        do_sample_frames: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        fps: Optional[Union[int, float]] = None,\n-        num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        device: Optional[\"torch.Tensor\"] = None,\n+        **kwargs,\n     ) -> BatchFeature:\n-        if do_sample_frames:\n-            videos = [\n-                self.sample_frames(video, metadata, num_frames, fps) for video, metadata in zip(videos, video_metadata)\n-            ]\n-\n-        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n-        # moving the whole video incurs high GPU mem usage for long videos\n-        if device is not None:\n-            videos = [video.to(device) for video in videos]\n-\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "179dccb63eb3edee5412d879660ddef78d5c6d61",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput, concatenate_list, make_flat_list_of_images\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n class InternVLImagesKwargs(ImagesKwargs, total=False):\n@@ -216,13 +216,13 @@ def __call__(\n         video_patch_indices = np.array([0])\n         video_num_patches_indices = np.array([0])\n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             images = make_flat_list_of_images(images)\n             image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n             image_num_patches = image_inputs.pop(\"num_patches\")\n             image_pixel_values = image_inputs.pop(\"pixel_values\")\n             image_num_patches_indices = np.cumsum(image_num_patches)\n         if videos is not None:\n-            videos = make_batched_videos(videos)\n             video_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n             video_pixel_values = video_inputs.pop(\"pixel_values_videos\")\n \n@@ -246,7 +246,7 @@ def __call__(\n             )\n             if images is not None and image_index != len(images):\n                 raise ValueError(\"Number of image placeholders in the prompt does not match the number of images.\")\n-            if videos is not None and video_index != len(videos):\n+            if videos is not None and video_index != len(num_frames_per_video):\n                 raise ValueError(\"Number of video placeholders in the prompt does not match the number of videos.\")\n \n             # Concatenate the interleaved image and video patches (function agnostic to the patches type (list, numpy array, torch tensor))"
        },
        {
            "sha": "2fc5729119e96e99e90e0130a563670be35739eb",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 30,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -73,21 +73,19 @@ def __init__(self, **kwargs: Unpack[InternVLVideoProcessorInitKwargs]):\n \n     def sample_frames(\n         self,\n-        video: \"torch.Tensor\",\n-        metadata: Optional[Union[VideoMetadata, dict]] = None,\n+        metadata: VideoMetadata,\n         num_frames: Optional[int] = None,\n         fps: Optional[Union[int, float]] = None,\n         initial_shift: Optional[Union[bool, float, int]] = None,\n+        **kwargs,\n     ):\n         \"\"\"\n         Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n         If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n         and `fps` are mutually exclusive.\n \n         Args:\n-            video (`torch.Tensor`):\n-                Video that need to be sampled.\n-            metadata (`VideoMetadata`, *optional*):\n+            metadata (`VideoMetadata`):\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n@@ -97,21 +95,21 @@ def sample_frames(\n                 The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n \n         Returns:\n-            torch.Tensor:\n-                Sampled video frames.\n+            np.ndarray:\n+                Indices to sample video frames.\n         \"\"\"\n         num_frames = num_frames if num_frames is not None else self.num_frames\n         initial_shift = initial_shift if initial_shift is not None else self.initial_shift\n-        total_num_frames = video.shape[0]\n+        total_num_frames = metadata.total_num_frames\n \n         # If num_frames is not given but fps is, calculate num_frames from fps\n         if num_frames is None and fps is not None:\n-            if metadata is None:\n+            if metadata is None or metadata.fps is None:\n                 raise ValueError(\n                     \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n                     \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n                 )\n-            num_frames = int(total_num_frames / metadata[\"fps\"] * fps)\n+            num_frames = int(total_num_frames / metadata.fps * fps)\n \n         if initial_shift is True:\n             initial_shift = total_num_frames / num_frames / 2\n@@ -122,13 +120,11 @@ def sample_frames(\n             )\n \n         indices = torch.arange(initial_shift, total_num_frames, total_num_frames / num_frames).int()\n-        video = video[indices].contiguous()\n-        return video\n+        return indices\n \n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n-        video_metadata: Union[list[VideoMetadata], list[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -142,25 +138,9 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        do_sample_frames: Optional[bool] = None,\n-        fps: Optional[Union[int, float]] = None,\n-        num_frames: Optional[int] = None,\n-        initial_shift: Optional[Union[bool, float, int]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        device: Optional[\"torch.Tensor\"] = None,\n+        **kwargs,\n     ) -> BatchFeature:\n-        if do_sample_frames:\n-            # Sample video frames\n-            videos = [\n-                self.sample_frames(video, metadata, fps=fps, num_frames=num_frames, initial_shift=initial_shift)\n-                for video, metadata in zip(videos, video_metadata)\n-            ]\n-\n-        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n-        # moving the whole video incurs high GPU mem usage for long videos\n-        if device is not None:\n-            videos = [video.to(device) for video in videos]\n-\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "ac2012c62b041863d67c1d320cc2ad304a46694f",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -280,6 +280,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "ce590bc6f40bb9bcbb7283ecb78256d0e56361bf",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -188,6 +188,7 @@ def __call__(\n         # Process images\n         image_inputs = {}\n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             images = make_flat_list_of_images(images)\n             image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n             image_height, image_width = image_inputs[\"pixel_values\"][0].shape[-2:]"
        },
        {
            "sha": "a18a9e66ed8a705f662cb4b3c16022d8e0688373",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -367,6 +367,7 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n+        images = self.fetch_images(images)\n         images = make_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "87ac7f1ce61abc3e0bd8b9ce7bf1a11d6b7f6aea",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -640,6 +640,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "5508f5dcdd4bd2e049bce75e0e3dac6ac352f3dd",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -356,6 +356,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n+        images = self.fetch_images(images)\n         images = make_batched_videos(images)\n         logger.warning(\n             \"`LlavaNextVideoImageProcessor` is deprecated and will be removed in v5.0. \""
        },
        {
            "sha": "091073b1f4c0a6841125dff993286c82497d26ec",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -692,6 +692,7 @@ def preprocess(\n         # only single image patching is supported\n         need_patching = [n == 1 for n in batch_num_images for _ in range(n)]\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "ba1a596aa459fc797a665c9cd964c4916693749c",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -692,6 +692,7 @@ def preprocess(\n         # extra validation\n         _validate_mllama_preprocess_arguments(do_resize, size, do_pad, max_image_tiles)\n \n+        images = self.fetch_images(images)\n         images_list = make_nested_list_of_images(images)\n \n         if self.do_convert_rgb:"
        },
        {
            "sha": "0dae7c83430339f443ac3c25f691acf9b5aeaccd",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -290,6 +290,7 @@ def __call__(\n \n         n_images_in_images = [0]\n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             images = make_nested_list_of_images(images)\n             n_images_in_images = [len(sample) for sample in images]\n "
        },
        {
            "sha": "7ab44704980007970d298da293dac01fd4df50de",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -21,7 +21,7 @@\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n+from ...image_utils import ImageInput, is_valid_image\n from ...processing_utils import (\n     ImagesKwargs,\n     MultiModalData,\n@@ -275,7 +275,6 @@ def __call__(\n                     )\n                     for prompt, image_list in zip(text, images)\n                 ]\n-                images = make_flat_list_of_images(images)\n             else:\n                 expanded_samples = []\n                 for sample in text:"
        },
        {
            "sha": "6022560ba01ad61ba9e4e0b7a6b2dcbb65dd13c7",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -32,7 +32,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_kwargs,\n@@ -397,7 +397,8 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n-        images = make_list_of_images(images)\n+        images = self.fetch_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images[0]):\n             raise ValueError("
        },
        {
            "sha": "42edbe24f1f527316521c93e9cf1718ad54ed91f",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -21,7 +21,7 @@\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image, load_image\n+from ...image_utils import ImageInput, is_valid_image\n from ...processing_utils import (\n     MultiModalData,\n     ProcessingKwargs,\n@@ -166,21 +166,6 @@ def __call__(\n         patch_size = self.patch_size * self.spatial_merge_size\n \n         if images is not None:\n-            if is_image_or_image_url(images):\n-                images = [images]\n-            elif isinstance(images, (list, tuple)) and is_image_or_image_url(images[0]):\n-                pass\n-            elif (\n-                isinstance(images, (list, tuple))\n-                and isinstance(images[0], (list, tuple))\n-                and is_image_or_image_url(images[0][0])\n-            ):\n-                images = [image for sublist in images for image in sublist]\n-            else:\n-                raise ValueError(\n-                    \"Invalid input images. Please provide a single image, a list of images, or a list of lists of images.\"\n-                )\n-            images = [load_image(im) if isinstance(im, str) else im for im in images]\n             image_inputs = self.image_processor(images, patch_size=patch_size, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}"
        },
        {
            "sha": "e7f2f3f5b66fdbd112d9bf27eb32200b076f0a4c",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -27,7 +27,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n class Qwen2_5_OmniVideosKwargs(VideosKwargs):\n@@ -154,7 +154,6 @@ def __call__(\n         seconds_per_chunk = output_kwargs[\"videos_kwargs\"].pop(\"seconds_per_chunk\")\n         position_id_per_seconds = output_kwargs[\"videos_kwargs\"].pop(\"position_id_per_seconds\")\n         use_audio_in_video = output_kwargs[\"videos_kwargs\"].pop(\"use_audio_in_video\")\n-        fps = output_kwargs[\"videos_kwargs\"].get(\"fps\", 2.0)\n \n         if audio is not None:\n             output_kwargs[\"audio_kwargs\"][\"padding\"] = \"max_length\"  # Support \"max_length\" padding only here\n@@ -179,14 +178,15 @@ def __call__(\n             image_grid_thw = iter([])\n \n         if videos is not None:\n-            videos = make_batched_videos(videos)\n             videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n-            fps = [fps] * len(videos)\n-            videos_inputs[\"video_second_per_grid\"] = [\n-                self.video_processor.temporal_patch_size / fps[i] for i in range(len(fps))\n-            ]\n-            video_grid_thw = iter(videos_inputs[\"video_grid_thw\"])\n-            video_second_per_grid = iter(videos_inputs[\"video_second_per_grid\"])\n+\n+            fps = output_kwargs[\"videos_kwargs\"].get(\"fps\", 2.0)\n+            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n+            second_per_grid_ts = [self.video_processor.temporal_patch_size / fps] * len(video_grid_thw)\n+            videos_inputs[\"video_second_per_grid\"] = second_per_grid_ts\n+\n+            video_grid_thw = iter(video_grid_thw)\n+            video_second_per_grid = iter(second_per_grid_ts)\n         else:\n             videos_inputs = {}\n             video_grid_thw = iter([])"
        },
        {
            "sha": "dbde67be1e69d1fa099608f39e41ee847235ac83",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -405,6 +405,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         if images is not None:\n+            images = self.fetch_images(images)\n             images = make_flat_list_of_images(images)\n \n         if images is not None and not valid_images(images):"
        },
        {
            "sha": "f73a65484219d16ecf89222e729532f7c1884582",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 22,
            "deletions": 48,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -134,59 +134,60 @@ def __init__(self, **kwargs: Unpack[Qwen2VLVideoProcessorInitKwargs]):\n \n     def sample_frames(\n         self,\n-        video: \"torch.Tensor\",\n-        frame_factor: int,\n-        min_frames: int,\n-        max_frames: int,\n-        metadata: Optional[Union[VideoMetadata, dict]] = None,\n+        metadata: VideoMetadata,\n+        temporal_patch_size: Optional[int] = None,\n+        min_frames: Optional[int] = None,\n+        max_frames: Optional[int] = None,\n         num_frames: Optional[int] = None,\n         fps: Optional[Union[int, float]] = None,\n+        **kwargs,\n     ):\n         \"\"\"\n         Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n         If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n         and `fps` are mutually exclusive.\n \n         Args:\n-            video (`torch.Tensor`):\n-                Video that need to be sampled.\n-            frame_factor (`int`):\n+            metadata (`VideoMetadata`):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            temporal_patch_size (`int`, *optional*):\n                 The temporal patch size of the vision encoder. Number of sampled frames will be rounded to be divisible by frame factor.\n-            min_frames (`int`):\n+            min_frames (`int`, *optional*):\n                 The minimum number of frames that can be sampled.\n-            max_frames (`int`):\n+            max_frames (`int`, *optional*):\n                 The maximum number of frames that can be sampled.\n-            metadata (`VideoMetadata`, *optional*):\n-                Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n             fps (`int` or `float`, *optional*):\n                 Target frames to sample per second. Defaults to `self.fps`.\n \n         Returns:\n-            torch.Tensor:\n-                Sampled video frames.\n+            np.ndarray:\n+                Indices to sample video frames.\n         \"\"\"\n         if fps is not None and num_frames is not None:\n             raise ValueError(\"`num_frames` and `fps` are mutually exclusive arguments, please use only one!\")\n \n         num_frames = num_frames if num_frames is not None else self.num_frames\n         fps = fps if fps is not None else self.fps\n-        total_num_frames = video.shape[0]\n+        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n+        min_frames = min_frames if min_frames is not None else self.min_frames\n+        max_frames = max_frames if max_frames is not None else self.max_frames\n+        total_num_frames = metadata.total_num_frames\n \n         # If num_frames is not given but fps is, calculate num_frames from fps\n         if num_frames is not None:\n-            num_frames = round(num_frames / frame_factor) * frame_factor\n+            num_frames = round(num_frames / temporal_patch_size) * temporal_patch_size\n         elif fps is not None:\n-            if metadata is None:\n+            if metadata is None or metadata.fps is None:\n                 raise ValueError(\n                     \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n                     \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n                 )\n-            max_frames = math.floor(min(max_frames, total_num_frames) / frame_factor) * frame_factor\n-            num_frames = total_num_frames / metadata[\"fps\"] * fps\n+            max_frames = math.floor(min(max_frames, total_num_frames) / temporal_patch_size) * temporal_patch_size\n+            num_frames = total_num_frames / metadata.fps * fps\n             num_frames = min(min(max(num_frames, min_frames), max_frames), total_num_frames)\n-            num_frames = math.floor(num_frames / frame_factor) * frame_factor\n+            num_frames = math.floor(num_frames / temporal_patch_size) * temporal_patch_size\n \n         if num_frames > total_num_frames:\n             raise ValueError(\n@@ -198,57 +199,30 @@ def sample_frames(\n             indices = torch.arange(0, total_num_frames, total_num_frames / num_frames).int()\n         else:\n             indices = torch.arange(0, total_num_frames).int()\n-        video = video[indices].contiguous()\n \n-        return video\n+        return indices\n \n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n-        video_metadata: Union[list[VideoMetadata], list[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        do_sample_frames: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         min_pixels: Optional[int] = None,\n         max_pixels: Optional[int] = None,\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n-        num_frames: Optional[int] = None,\n-        min_frames: Optional[int] = None,\n-        max_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         device: Optional[\"torch.Tensor\"] = None,\n         **kwargs,\n     ):\n-        if do_sample_frames:\n-            # Sample video frames\n-            videos = [\n-                self.sample_frames(\n-                    video,\n-                    frame_factor=temporal_patch_size,\n-                    min_frames=min_frames,\n-                    max_frames=max_frames,\n-                    metadata=metadata,\n-                    num_frames=num_frames,\n-                    fps=fps,\n-                )\n-                for video, metadata in zip(videos, video_metadata)\n-            ]\n-\n-        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n-        # moving the whole video incurs high GPU mem usage for long videos\n-        if device is not None:\n-            videos = [video.to(device) for video in videos]\n-\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "77b4b490e13605837a4f5dba68347d553b333eb8",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -36,6 +36,7 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n+    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -185,11 +186,19 @@ def _further_process_kwargs(\n         kwargs[\"pad_size\"] = pad_size\n         kwargs[\"mask_size\"] = mask_size\n         kwargs[\"mask_pad_size\"] = mask_pad_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"data_format\"] = data_format\n \n+        # torch resize uses interpolation instead of resample\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n         return kwargs\n \n     @auto_docstring"
        },
        {
            "sha": "4b65bec77b57c354e5febd1f24883c87ec63af91",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -427,11 +427,19 @@ def _further_process_kwargs(\n \n         kwargs[\"size\"] = size\n         kwargs[\"mask_size\"] = mask_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"data_format\"] = data_format\n \n+        # torch resize uses interpolation instead of resample\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n         return kwargs\n \n     @auto_docstring"
        },
        {
            "sha": "dc7ac715d40fcb2073f5c181a6b131782407965c",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -60,8 +60,6 @@\n     TensorType,\n     auto_docstring,\n     is_torch_available,\n-    is_torchvision_available,\n-    is_torchvision_v2_available,\n     logging,\n )\n from ..auto import AutoModel\n@@ -78,11 +76,6 @@\n     import torch\n     from torch.nn import functional as F\n \n-if is_torchvision_v2_available():\n-    pass\n-elif is_torchvision_available():\n-    pass\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -213,11 +206,19 @@ def _further_process_kwargs(\n \n         kwargs[\"size\"] = size\n         kwargs[\"mask_size\"] = mask_size\n-        kwargs[\"default_to_square\"] = default_to_square\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"data_format\"] = data_format\n \n+        # torch resize uses interpolation instead of resample\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n         return kwargs\n \n     def _apply_non_overlapping_constraints(self, pred_masks: torch.Tensor) -> torch.Tensor:"
        },
        {
            "sha": "152447bb5b873c17eb77787a2f43338579b50c44",
            "filename": "src/transformers/models/siglip/image_processing_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -181,6 +181,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "30b5f1b958af2dadd2c7dcc79619f418857086fc",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -267,6 +267,7 @@ def preprocess(\n         # Image processor does not support different output formats, because it returns patches.\n         data_format = ChannelDimension.LAST\n \n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "d92d9a3385eb7ca289348589d09228a36be272e9",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -684,6 +684,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n         do_pad = do_pad if do_pad is not None else self.do_pad\n \n+        images = self.fetch_images(images)\n         images_list = make_nested_list_of_images(images)\n \n         if not valid_images(images_list[0]):"
        },
        {
            "sha": "e9ae2099a43f74f6f55e8b50084d9aae786a3df9",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 47,
            "deletions": 48,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -120,6 +120,9 @@ class SmolVLMProcessorKwargs(ProcessingKwargs, total=False):\n         \"images_kwargs\": {\n             \"return_row_col_info\": True,\n         },\n+        \"videos_kwargs\": {\n+            \"return_metadata\": True,\n+        },\n     }\n \n \n@@ -174,23 +177,7 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template, **kwargs)\n \n-    def process_vision(self, text, images, output_kwargs):\n-        if text is not None:\n-            n_images_in_text = [sample.count(self.image_token) for sample in text]\n-\n-        n_images_in_images = [len(sublist) for sublist in images]\n-        image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-\n-        if text is None:\n-            return None, image_inputs\n-\n-        if n_images_in_images != n_images_in_text:\n-            raise ValueError(\n-                f\"The number of images in the text {n_images_in_text} and images {n_images_in_images} should be the same.\"\n-            )\n-        image_rows = image_inputs.pop(\"rows\", [[0] * len(text)])\n-        image_cols = image_inputs.pop(\"cols\", [[0] * len(text)])\n-\n+    def expand_text_with_image_tokens(self, text, image_rows, image_cols):\n         prompt_strings = []\n         for sample, sample_rows, sample_cols in zip(text, image_rows, image_cols):\n             # Replace the image token with fake tokens around the expanded image token sequence of length `image_seq_len`\n@@ -216,32 +203,25 @@ def process_vision(self, text, images, output_kwargs):\n                 sample += image_prompt_string + split_sample[i + 1]\n             prompt_strings.append(sample)\n \n-        return prompt_strings, image_inputs\n-\n-    def process_video(self, text, videos, output_kwargs):\n-        if text is not None:\n-            n_videos_in_text = [sample.count(self.video_token) for sample in text]\n-\n-        n_videos_in_videos = [len(sublist) for sublist in videos]\n-        video_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n+        return prompt_strings\n \n+    def expand_text_with_video_tokens(self, text, video_inputs):\n         num_frames = video_inputs[\"pixel_values\"].shape[1]\n-        batch_timestamps = iter(video_inputs.pop(\"timestamps\"))\n-        batch_durations = iter(video_inputs.pop(\"durations\"))\n-\n-        if text is None:\n-            return None, video_inputs\n-\n-        if n_videos_in_videos != n_videos_in_text:\n-            raise ValueError(\n-                f\"The number of videos in the text {n_videos_in_text} and videos {n_videos_in_videos} should be the same.\"\n-            )\n+        video_metadata = iter(video_inputs[\"video_metadata\"])\n \n         prompt_strings = []\n         for sample in text:\n             while self.video_token in sample:\n-                timestamps = next(batch_timestamps)\n-                duration = next(batch_durations)\n+                metadata = next(video_metadata)\n+                if metadata.fps is None:\n+                    logger.warning_once(\n+                        \"SmolVLM requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                        \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                        \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n+                    )\n+                    metadata.fps = 24  # Set the default fps to 24 for BC, otherwise `timestamps` can't be inferred\n+                timestamps = [(int(second // 60), int(second % 60)) for second in metadata.timestamps]\n+                duration = int(metadata.duration) if metadata.duration is not None else int(metadata.timestamps[-1])\n                 duration_td = timedelta(seconds=int(duration))\n                 image_prompt_strings = DEFAULT_VIDEO_INTRO.format(\n                     frame_count=num2words(num_frames), video_duration=str(duration_td)\n@@ -260,7 +240,7 @@ def process_video(self, text, videos, output_kwargs):\n                 image_prompt_strings += DEFAULT_MEDIA_OUTTRO\n                 sample = sample.replace(self.video_token, image_prompt_strings, 1)\n             prompt_strings.append(sample)\n-        return prompt_strings, video_inputs\n+        return prompt_strings\n \n     def __call__(\n         self,\n@@ -341,19 +321,38 @@ def __call__(\n         inputs = {}\n         # Images and videos are mutually exclusive, so process one which is present\n         if images is not None:\n+            images = self.image_processor.fetch_images(images)\n             images = make_nested_list_of_images(images)\n-            text, vision_inputs = self.process_vision(\n-                text,\n-                images,\n-                output_kwargs,\n-            )\n+            vision_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+            image_rows = vision_inputs.pop(\"rows\", [[0] * len(text)])\n+            image_cols = vision_inputs.pop(\"cols\", [[0] * len(text)])\n             inputs.update(vision_inputs)\n+\n+            if text is not None:\n+                n_images_in_text = [sample.count(self.image_token) for sample in text]\n+                n_images_in_images = [len(sublist) for sublist in images]\n+                if n_images_in_images != n_images_in_text:\n+                    raise ValueError(\n+                        f\"The number of images in the text {n_images_in_text} and images {n_images_in_images} should be the same.\"\n+                    )\n+                text = self.expand_text_with_image_tokens(text, image_rows=image_rows, image_cols=image_cols)\n+\n         elif videos is not None:\n-            text, vision_inputs = self.process_video(\n-                text,\n-                videos,\n-                output_kwargs,\n-            )\n+            vision_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n+            if text is not None:\n+                n_videos_in_text = [sample.count(self.video_token) for sample in text]\n+                n_videos_in_videos = [len(sublist) for sublist in videos]\n+                if n_videos_in_videos != n_videos_in_text:\n+                    raise ValueError(\n+                        f\"The number of videos in the text {n_videos_in_text} and videos {n_videos_in_videos} should be the same.\"\n+                    )\n+                text = self.expand_text_with_video_tokens(text, vision_inputs)\n+\n+            # If user has not requested video metadata, pop it. By default metadata\n+            # is always returned to expand video tokens correctly\n+            if \"return_metadata\" not in kwargs:\n+                vision_inputs.pop(\"video_metadata\")\n             inputs.update(vision_inputs)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)"
        },
        {
            "sha": "020f2d4c8e93144b12952e60f8ab7b1f73933204",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 14,
            "deletions": 51,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -246,11 +246,11 @@ def pad(\n \n     def sample_frames(\n         self,\n-        video: \"torch.Tensor\",\n-        metadata: Union[VideoMetadata, dict],\n+        metadata: VideoMetadata,\n         num_frames: Optional[int] = None,\n         fps: Optional[Union[int, float]] = None,\n         skip_secs: Optional[int] = 1,\n+        **kwargs,\n     ):\n         \"\"\"\n         Video sampling function which:\n@@ -260,8 +260,6 @@ def sample_frames(\n             - Uniformly samples the desired number of frames between the start and end indices.\n \n         Args:\n-            video (`torch.Tensor`):\n-                Video that need to be sampled.\n             metadata (`VideoMetadata`):\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n@@ -272,13 +270,18 @@ def sample_frames(\n                 Number of seconds to skip from the start and end if the video is long enough.\n \n         Returns:\n-            torch.Tensor:\n-                Sampled video frames.\n+            np.ndarray:\n+                Indices to sample video frames.\n         \"\"\"\n+        if metadata is None or getattr(metadata, \"fps\", None) is None:\n+            raise ValueError(\n+                \"Asked to sample frames per second but no video metadata was provided which is required when sampling in SmolVLM. \"\n+                \"Please pass in `VideoMetadata` object or set `do_sample_frames=False`\"\n+            )\n+\n         num_frames = num_frames if num_frames is not None else self.num_frames\n         fps = fps if fps is not None else self.fps\n-\n-        total_num_frames = video.shape[0]\n+        total_num_frames = metadata.total_num_frames\n \n         # Step 1) Estimate how many frames we'd sample at `target_fps`, fallback if target_fps <= 0\n         estimated_frames = int(round(fps * metadata[\"duration\"]))\n@@ -303,20 +306,12 @@ def sample_frames(\n \n         indices = np.linspace(start_idx, end_idx, desired_frames, dtype=int)\n         indices = np.unique(indices)\n-        video = video[indices].contiguous()\n \n-        timestamps = []\n-        for idx in indices:\n-            sec = idx / metadata[\"fps\"]\n-            mm = int(sec // 60)\n-            ss = int(sec % 60)\n-            timestamps.append([mm, ss])\n-        return video, timestamps, int(metadata[\"duration\"])\n+        return indices\n \n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n-        video_metadata: Union[list[VideoMetadata], list[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -325,44 +320,12 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_pad: bool,\n-        do_sample_frames: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        fps: Optional[Union[int, float]] = None,\n-        num_frames: Optional[int] = None,\n-        skip_secs: Optional[int] = 0,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        device: Optional[\"torch.Tensor\"] = None,\n         **kwargs,\n     ):\n-        # Group videos by size for batched resizing\n-        if do_sample_frames:\n-            if video_metadata[0] is None:\n-                raise ValueError(\n-                    \"Frame sampling is enabled but no video metadata was found. SmolVLM requires metadata to correctly sample frames. \"\n-                    \"Please pass in `VideoMetadata` object per each input video or set `do_sample_frames=False`\"\n-                )\n-            processed_videos = []\n-            timestamps_list, durations_list = [], []\n-            for video, metadata in zip(videos, video_metadata):\n-                video, timestamps, duration = self.sample_frames(video, metadata, num_frames, fps, skip_secs)\n-                timestamps_list.append(timestamps)\n-                durations_list.append(duration)\n-                processed_videos.append(video)\n-        else:\n-            # Assume 24 fps by default and prepare timestamps for the whole video when all frames are sampled\n-            processed_videos = videos\n-            timestamps_list = [\n-                [(int((idx / 24) // 60), int((idx / 24) % 60)) for idx in range(len(video))] for video in videos\n-            ]\n-            durations_list = [len(video) // 24 for video in videos]\n-\n-        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n-        # moving the whole video incurs high GPU mem usage for long videos\n-        if device is not None:\n-            videos = [video.to(device) for video in videos]\n-\n-        grouped_videos, grouped_videos_index = group_videos_by_shape(processed_videos)\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}\n         for shape, stacked_videos in grouped_videos.items():\n             if do_convert_rgb:\n@@ -400,7 +363,7 @@ def _preprocess(\n             pixel_attention_mask = reorder_videos(processed_padded_mask_grouped, grouped_videos_index)\n \n         processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n-        data = {\"pixel_values\": processed_videos, \"timestamps\": timestamps_list, \"durations\": durations_list}\n+        data = {\"pixel_values\": processed_videos}\n \n         if do_pad:\n             data[\"pixel_attention_mask\"] = ("
        },
        {
            "sha": "47b51869f600206cb1eabee9fcf50584b2db4c8a",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -257,6 +257,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         if images is not None:\n+            images = self.fetch_images(images)\n             images = make_list_of_images(images)\n \n         if images is not None and not valid_images(images):"
        },
        {
            "sha": "ec2e0dd57518bb0d858001a9f46467f48a6a748d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 53,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -31,14 +31,12 @@\n import typing_extensions\n from huggingface_hub.errors import EntryNotFoundError\n \n-from transformers.utils import is_torch_available\n-\n from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n-from .image_utils import ChannelDimension, is_vision_available, load_image\n+from .image_utils import ChannelDimension, is_vision_available\n from .utils.chat_template_utils import render_jinja_template\n-from .video_utils import VideoMetadata, load_video\n+from .video_utils import VideoMetadata\n \n \n if is_vision_available():\n@@ -66,6 +64,7 @@\n     download_url,\n     is_offline_mode,\n     is_remote_url,\n+    is_torch_available,\n     list_repo_templates,\n     logging,\n )\n@@ -250,7 +249,7 @@ class VideosKwargs(TypedDict, total=False):\n             Whether to center crop the video.\n         do_sample_frames (`bool`, *optional*):\n             Whether to sample frames from the video before processing or to process the whole video.\n-        video_metadata (`VideoMetadata`, *optional*):\n+        video_metadata (`Union[VideoMetadata, dict]`, *optional*):\n             Metadata of the video containing information about total duration, fps and total number of frames.\n         num_frames (`int`, *optional*):\n             Maximum number of frames to sample when `do_sample_frames=True`.\n@@ -262,6 +261,8 @@ class VideosKwargs(TypedDict, total=False):\n             The channel dimension format for the output video.\n         input_data_format (`ChannelDimension` or `str`, *optional*):\n             The channel dimension format for the input video.\n+        return_metadata (`ChannelDimension` or `str`, *optional*):\n+            Whether to return video metadata or not.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n@@ -285,6 +286,7 @@ class VideosKwargs(TypedDict, total=False):\n     video_metadata: Optional[Union[VideoMetadata, dict]]\n     fps: Optional[Union[int, float]]\n     num_frames: Optional[int]\n+    return_metadata: Optional[bool]\n \n \n class AudioKwargs(TypedDict, total=False):\n@@ -430,23 +432,11 @@ class ChatTemplateLoadKwargs(TypedDict, total=False):\n \n     num_frames (`int`, *optional*):\n         Number of frames to sample uniformly. If not passed, the whole video is loaded.\n-    video_load_backend (`str`, *optional*, defaults to `\"pyav\"`):\n-        The backend to use when loading the video which will be used only when there are videos in the conversation.\n-        Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\" because it is the only backend\n-        that supports all types of sources to load from.\n-    sample_indices_fn (`Callable`, *optional*):\n-            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n-            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n-            If not provided, simple uniformt sampling with fps is performed, otherwise `sample_indices_fn` has priority over other args.\n-            The function expects at input the all args along with all kwargs passed to `load_video` and should output valid\n-            indices at which the video should be sampled. For example:\n-\n-            def sample_indices_fn(num_frames, fps, metadata, **kwargs):\n-                # add you sampling logic here ...\n-                return np.linspace(start_idx, end_idx, num_frames, dtype=int)\n+    load_audio_from_video (`bool`, *optional*):\n+            Whether to use the audio track of input video. If `True` the audio track will be loaded and passed to the\n+            processor. This flag has no effect if the model doesn't support audio modality.\n     \"\"\"\n \n-    video_load_backend: Optional[str] = \"pyav\"\n     sampling_rate: Optional[int] = 16_000\n     load_audio_from_video: Optional[bool] = False\n \n@@ -1438,6 +1428,11 @@ def validate_init_kwargs(processor_config, valid_kwargs):\n         return unused_kwargs, valid_kwargs\n \n     @deprecate_kwarg(\"video_fps\", version=\"4.58\", new_name=\"fps\")\n+    @deprecate_kwarg(\n+        \"video_load_backend\",\n+        version=\"4.59\",\n+        additional_message=\". This function will use `torchcodec` by default, or `torchvision` if `torchcodec` is not installed.\",\n+    )\n     def apply_chat_template(\n         self,\n         conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n@@ -1525,6 +1520,9 @@ def apply_chat_template(\n                 if value is not None and not isinstance(value, dict):\n                     processed_kwargs[kwarg_type][key] = value\n \n+        # pop unused and deprecated kwarg\n+        kwargs.pop(\"video_load_backend\", None)\n+\n         # Pass unprocessed custom kwargs\n         processed_kwargs[\"template_kwargs\"].update(kwargs)\n \n@@ -1544,10 +1542,7 @@ def apply_chat_template(\n         if tokenize:\n             batch_images, batch_videos = [], []\n             batch_audios = []\n-            batch_video_metadata = []\n             for conversation in conversations:\n-                images, videos = [], []\n-                video_metadata = []\n                 for message in conversation:\n                     visuals = [content for content in message[\"content\"] if content[\"type\"] in [\"image\", \"video\"]]\n                     audio_fnames = [\n@@ -1569,9 +1564,6 @@ def apply_chat_template(\n                         if key in vision_info and vision_info[\"type\"] == \"video\"\n                     ]\n \n-                    for fname in image_fnames:\n-                        images.append(load_image(fname))\n-\n                     # Audio models do not accept nested list of audios (yet!) so we construct a flat input audio list\n                     if not mm_load_kwargs[\"load_audio_from_video\"]:\n                         for fname in audio_fnames:\n@@ -1580,32 +1572,12 @@ def apply_chat_template(\n                         for fname in video_fnames:\n                             batch_audios.append(load_audio(fname, sampling_rate=mm_load_kwargs[\"sampling_rate\"]))\n \n-                    for fname in video_fnames:\n-                        if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n-                            # Case a: Video is provided as a list of image file names\n-                            video = [np.array(load_image(image_fname)) for image_fname in fname]\n-                            video = np.stack(video)\n-                            metadata = None\n-                            logger.warning(\n-                                \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n-                                \"If your model requires metadata during processing, please load the whole video and let the processor sample frames instead.\"\n-                            )\n-                        else:\n-                            # Case b: Video is provided as a single file path or URL or decoded frames in a np.ndarray or torch.tensor\n-                            video, metadata = load_video(\n-                                fname,\n-                                backend=mm_load_kwargs[\"video_load_backend\"],\n-                            )\n-                        videos.append(video)\n-                        video_metadata.append(metadata)\n-\n-                # Currently all processors can accept nested list of batches, but not flat list of visuals\n-                # So we'll make a batched list of images and let the processor handle it\n-                if images:\n-                    batch_images.append(images)\n-                if videos:\n-                    batch_videos.append(videos)\n-                    batch_video_metadata.append(video_metadata)\n+                    # Currently all processors can accept nested list of batches, but not flat list of visuals\n+                    # So we'll make a batched list of images and let the processor handle it\n+                    if image_fnames:\n+                        batch_images.append(image_fnames)\n+                    if video_fnames:\n+                        batch_videos.append(video_fnames)\n \n         prompt, generation_indices = render_jinja_template(\n             conversations=conversations,\n@@ -1638,7 +1610,6 @@ def apply_chat_template(\n                 images=batch_images if batch_images else None,\n                 videos=batch_videos if batch_videos else None,\n                 audio=batch_audios if batch_audios else None,\n-                video_metadata=batch_video_metadata,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "17781ec8a3e9bec4cf5d55ca2701d44802c2bc35",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 89,
            "deletions": 61,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -17,7 +17,8 @@\n import os\n import warnings\n from copy import deepcopy\n-from typing import Any, Optional, Union\n+from functools import partial\n+from typing import Any, Callable, Optional, Union\n \n import numpy as np\n \n@@ -43,32 +44,29 @@\n     is_offline_mode,\n     is_remote_url,\n     is_torch_available,\n+    is_torchcodec_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n-    is_vision_available,\n     logging,\n )\n from .utils.import_utils import requires\n from .video_utils import (\n     VideoInput,\n     VideoMetadata,\n     group_videos_by_shape,\n+    is_valid_video,\n     load_video,\n+    make_batched_metadata,\n     make_batched_videos,\n     reorder_videos,\n     to_channel_dimension_format,\n )\n \n \n-if is_vision_available():\n-    from .image_utils import PILImageResampling\n-\n if is_torch_available():\n     import torch\n \n if is_torchvision_available():\n-    from .image_utils import pil_torch_interpolation_mapping\n-\n     if is_torchvision_v2_available():\n         from torchvision.transforms.v2 import functional as F\n     else:\n@@ -141,7 +139,10 @@\n             - `\"channels_last\"` or `ChannelDimension.LAST`: video in (height, width, num_channels) format.\n             - `\"none\"` or `ChannelDimension.NONE`: video in (height, width) format.\n         device (`torch.device`, *optional*):\n-            The device to process the videos on. If unset, the device is inferred from the input videos.\"\"\"\n+            The device to process the videos on. If unset, the device is inferred from the input videos.\n+        return_metadata (`bool`, *optional*):\n+            Whether to return video metadata or not.\n+        \"\"\"\n \n \n @add_start_docstrings(\n@@ -170,6 +171,7 @@ class BaseVideoProcessor(BaseImageProcessorFast):\n     fps = None\n     num_frames = None\n     video_metadata = None\n+    return_metadata = False\n     valid_kwargs = VideosKwargs\n     model_input_names = [\"pixel_values_videos\"]\n \n@@ -234,29 +236,27 @@ def convert_to_rgb(\n \n     def sample_frames(\n         self,\n-        video: \"torch.Tensor\",\n-        metadata: Optional[Union[VideoMetadata, dict]] = None,\n+        metadata: VideoMetadata,\n         num_frames: Optional[int] = None,\n         fps: Optional[Union[int, float]] = None,\n+        **kwargs,\n     ):\n         \"\"\"\n         Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n         If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n         and `fps` are mutually exclusive.\n \n         Args:\n-            video (`torch.Tensor`):\n-                Video that need to be sampled.\n-            metadata (`VideoMetadata`, *optional*):\n+            metadata (`VideoMetadata`):\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n             fps (`int` or `float`, *optional*):\n                 Target frames to sample per second. Defaults to `self.fps`.\n \n         Returns:\n-            torch.Tensor:\n-                Sampled video frames.\n+            np.ndarray:\n+                Indices to sample video frames.\n         \"\"\"\n         if fps is not None and num_frames is not None:\n             raise ValueError(\n@@ -265,16 +265,16 @@ def sample_frames(\n \n         num_frames = num_frames if num_frames is not None else self.num_frames\n         fps = fps if fps is not None else self.fps\n-        total_num_frames = video.shape[0]\n+        total_num_frames = metadata.total_num_frames\n \n         # If num_frames is not given but fps is, calculate num_frames from fps\n         if num_frames is None and fps is not None:\n-            if metadata is None:\n+            if metadata is None or metadata.fps is None:\n                 raise ValueError(\n                     \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n                     \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n                 )\n-            num_frames = int(total_num_frames / metadata[\"fps\"] * fps)\n+            num_frames = int(total_num_frames / metadata.fps * fps)\n \n         if num_frames > total_num_frames:\n             raise ValueError(\n@@ -285,25 +285,53 @@ def sample_frames(\n             indices = torch.arange(0, total_num_frames, total_num_frames / num_frames).int()\n         else:\n             indices = torch.arange(0, total_num_frames).int()\n+        return indices\n \n-        video = video[indices].contiguous()\n-        return video\n+    def _decode_and_sample_videos(\n+        self,\n+        videos: VideoInput,\n+        video_metadata: Union[VideoMetadata, dict],\n+        do_sample_frames: Optional[bool] = None,\n+        sample_indices_fn: Optional[Callable] = None,\n+    ) -> list[\"torch.Tensor\"]:\n+        \"\"\"\n+        Decode input videos and sample frames if needed.\n+        \"\"\"\n+        videos = make_batched_videos(videos)\n+        video_metadata = make_batched_metadata(videos, video_metadata=video_metadata)\n+\n+        # Only sample frames if an array video is passed, otherwise first decode -> then sample\n+        if is_valid_video(videos[0]) and do_sample_frames:\n+            sampled_videos = []\n+            for video, metadata in zip(videos, video_metadata):\n+                indices = sample_indices_fn(metadata=metadata)\n+                sampled_videos.append(video[indices])\n+            videos = sampled_videos\n+        elif not is_valid_video(videos[0]):\n+            if isinstance(videos[0], list):\n+                # Videos sometimes are passed as a list of image URLs, especially through templates\n+                videos = [\n+                    torch.stack([F.pil_to_tensor(image) for image in images], dim=0)\n+                    for images in self.fetch_images(videos)\n+                ]\n+                if do_sample_frames:\n+                    raise ValueError(\n+                        \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`.\"\n+                    )\n+            else:\n+                videos, video_metadata = self.fetch_videos(videos, sample_indices_fn=sample_indices_fn)\n+\n+        return videos, video_metadata\n \n     def _prepare_input_videos(\n         self,\n         videos: VideoInput,\n-        video_metadata: VideoMetadata = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        device: Optional[str] = None,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n         Prepare the input videos for processing.\n         \"\"\"\n-        videos = make_batched_videos(videos)\n-        if video_metadata is not None:\n-            batch_metadata = [metadata for batch_list in video_metadata for metadata in batch_list]\n-        else:\n-            batch_metadata = [None] * len(videos)\n-\n         processed_videos = []\n         for video in videos:\n             # `make_batched_videos` always returns a 4D array per video\n@@ -312,10 +340,15 @@ def _prepare_input_videos(\n                 # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n                 video = torch.from_numpy(video).contiguous()\n \n+            if device is not None:\n+                video = video.to(device)\n+\n             processed_videos.append(video)\n-        return processed_videos, batch_metadata\n+        return processed_videos\n \n-    @add_start_docstrings(BASE_VIDEO_PROCESSOR_DOCSTRING)\n+    @add_start_docstrings(\n+        BASE_VIDEO_PROCESSOR_DOCSTRING,\n+    )\n     def preprocess(\n         self,\n         videos: VideoInput,\n@@ -331,30 +364,34 @@ def preprocess(\n             kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n \n         input_data_format = kwargs.pop(\"input_data_format\")\n+        do_sample_frames = kwargs.pop(\"do_sample_frames\")\n+        device = kwargs.pop(\"device\")\n         video_metadata = kwargs.pop(\"video_metadata\")\n-        videos, video_metadata = self._prepare_input_videos(\n-            videos=videos, video_metadata=video_metadata, input_data_format=input_data_format\n+\n+        sample_indices_fn = partial(self.sample_frames, **kwargs) if do_sample_frames else None\n+        videos, video_metadata = self._decode_and_sample_videos(\n+            videos,\n+            video_metadata=video_metadata,\n+            do_sample_frames=do_sample_frames,\n+            sample_indices_fn=sample_indices_fn,\n         )\n+        videos = self._prepare_input_videos(videos=videos, input_data_format=input_data_format, device=device)\n \n         kwargs = self._further_process_kwargs(**kwargs)\n         self._validate_preprocess_kwargs(**kwargs)\n \n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n         # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n         kwargs.pop(\"data_format\")\n+        return_metadata = kwargs.pop(\"return_metadata\")\n \n-        return self._preprocess(videos=videos, video_metadata=video_metadata, **kwargs)\n+        preprocessed_videos = self._preprocess(videos=videos, **kwargs)\n+        if return_metadata:\n+            preprocessed_videos[\"video_metadata\"] = video_metadata\n+        return preprocessed_videos\n \n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n-        video_metadata: Union[list[VideoMetadata], list[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -368,24 +405,9 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        do_sample_frames: Optional[bool] = None,\n-        fps: Optional[Union[int, float]] = None,\n-        num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        device: Optional[\"torch.Tensor\"] = None,\n+        **kwargs,\n     ) -> BatchFeature:\n-        if do_sample_frames:\n-            # Sample video frames\n-            videos = [\n-                self.sample_frames(video, metadata=metadata, num_frames=num_frames, fps=fps)\n-                for video, metadata in zip(videos, video_metadata)\n-            ]\n-\n-        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n-        # moving the whole video incurs high GPU mem usage for long videos\n-        if device is not None:\n-            videos = [video.to(device) for video in videos]\n-\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}\n@@ -861,19 +883,25 @@ def register_for_auto_class(cls, auto_class=\"AutoVideoProcessor\"):\n \n         cls._auto_class = auto_class\n \n-    def fetch_videos(self, video_url_or_urls: Union[str, list[str]]):\n+    def fetch_videos(self, video_url_or_urls: Union[str, list[str], list[list[str]]], sample_indices_fn=None):\n         \"\"\"\n         Convert a single or a list of urls into the corresponding `np.array` objects.\n \n         If a single url is passed, the return value will be a single object. If a list is passed a list of objects is\n         returned.\n         \"\"\"\n+        backend = \"torchcodec\"\n+        if not is_torchcodec_available():\n+            warnings.warn(\n+                \"`torchcodec` is not installed and cannot be used to decode the video by default. \"\n+                \"Falling back to `torchvision`. Note that `torchvision` decoding is deprecated and will be removed in future versions. \"\n+            )\n+            backend = \"torchvision\"\n+\n         if isinstance(video_url_or_urls, list):\n-            return [self.fetch_videos(x) for x in video_url_or_urls]\n-        elif isinstance(video_url_or_urls, str):\n-            return load_video(video_url_or_urls)\n+            return list(zip(*[self.fetch_videos(x, sample_indices_fn=sample_indices_fn) for x in video_url_or_urls]))\n         else:\n-            raise TypeError(f\"only a single or a list of entries is supported but got type={type(video_url_or_urls)}\")\n+            return load_video(video_url_or_urls, backend=backend, sample_indices_fn=sample_indices_fn)\n \n \n BaseVideoProcessor.push_to_hub = copy_func(BaseVideoProcessor.push_to_hub)"
        },
        {
            "sha": "7576b05857df07fcd726f97c535501d92d1ed29e",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 145,
            "deletions": 57,
            "changes": 202,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -19,7 +19,7 @@\n from contextlib import redirect_stdout\n from dataclasses import dataclass\n from io import BytesIO\n-from typing import Callable, Optional, Union\n+from typing import Callable, NewType, Optional, Union\n from urllib.parse import urlparse\n \n import numpy as np\n@@ -56,6 +56,8 @@\n \n logger = logging.get_logger(__name__)\n \n+URL = NewType(\"URL\", str)\n+Path = NewType(\"Path\", str)\n \n VideoInput = Union[\n     list[\"PIL.Image.Image\"],\n@@ -66,19 +68,43 @@\n     list[list[\"PIL.Image.Image\"]],\n     list[list[\"np.ndarrray\"]],\n     list[list[\"torch.Tensor\"]],\n+    URL,\n+    list[URL],\n+    list[list[URL]],\n+    Path,\n+    list[Path],\n+    list[list[Path]],\n ]  # noqa\n \n \n @dataclass\n class VideoMetadata:\n     total_num_frames: int\n-    fps: float\n-    duration: float\n-    video_backend: str\n+    fps: float = None\n+    width: int = None\n+    height: int = None\n+    duration: float = None\n+    video_backend: str = None\n+    frames_indices: list[int] = None\n \n     def __getitem__(self, item):\n         return getattr(self, item)\n \n+    def __setitem__(self, key, value):\n+        return setattr(self, key, value)\n+\n+    @property\n+    def timestamps(self) -> float:\n+        \"Timestamps of the sampled frames in seconds.\"\n+        if self.fps is None:\n+            raise ValueError(\"Cannot infer video `timestamps` when `fps` is None.\")\n+        return [frame_idx / self.fps for frame_idx in self.frames_indices]\n+\n+    def update(self, dictionary):\n+        for key, value in dictionary.items():\n+            if hasattr(self, key):\n+                setattr(self, key, value)\n+\n \n def is_valid_video_frame(frame):\n     return isinstance(frame, PIL.Image.Image) or (\n@@ -130,7 +156,7 @@ def convert_pil_frames_to_video(videos: list[VideoInput]) -> list[Union[\"np.ndar\n             Video inputs to turn into a list of videos.\n     \"\"\"\n \n-    if not isinstance(videos[0], (list, tuple)):\n+    if not (isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0])):\n         return videos\n \n     video_converted = []\n@@ -141,7 +167,7 @@ def convert_pil_frames_to_video(videos: list[VideoInput]) -> list[Union[\"np.ndar\n     return video_converted\n \n \n-def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\"]]:\n+def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\", \"URL\", \"Path\"]]:\n     \"\"\"\n     Ensure that the input is a list of videos. If the input is a single video, it is converted to a list of length 1.\n     If the input is a batch of videos, it is converted to a list of 4D video arrays. Videos passed as list `PIL.Image`\n@@ -153,23 +179,64 @@ def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\"]]:\n         videos (`VideoInput`):\n             Video inputs to turn into a list of videos.\n     \"\"\"\n-    if not valid_videos:\n+    # Early exit for deeply nested list of image frame paths. We shouldn't flatten them\n+    try:\n+        if isinstance(videos[0][0][0], str):\n+            return [image_paths for sublist in videos for image_paths in sublist]\n+    except (IndexError, TypeError):\n+        pass\n+\n+    if isinstance(videos, str) or is_valid_video(videos):\n+        return convert_pil_frames_to_video([videos])\n+    # only one frame passed, thus we unsqueeze time dim\n+    elif is_valid_image(videos):\n+        return [np.array(videos)[None, ...]]\n+    elif not isinstance(videos, list):\n         raise ValueError(\n             f\"Invalid video input. Expected either a list of video frames or an input of 4 or 5 dimensions, but got\"\n             f\" type {type(videos)}.\"\n         )\n \n-    if is_batched_video(videos):\n-        pass\n-    elif is_valid_video(videos):\n-        videos = [videos]\n-    # only one frame passed, thus we unsqueeze time dim\n-    elif is_valid_image(videos):\n-        videos = [np.array(videos)[None, ...]]\n-    # nested batch so we need to unflatten\n-    elif isinstance(videos[0], (list, tuple)) and is_valid_video(videos[0][0]):\n-        videos = [video for sublist in videos for video in sublist]\n-    return convert_pil_frames_to_video(videos)\n+    # Recursively flatten any nested structure\n+    flat_videos_list = []\n+    for item in videos:\n+        if isinstance(item, str) or is_valid_video(item):\n+            flat_videos_list.append(item)\n+        elif isinstance(item, list):\n+            flat_videos_list.extend(make_batched_videos(item))\n+\n+    flat_videos_list = convert_pil_frames_to_video(flat_videos_list)\n+    return flat_videos_list\n+\n+\n+def make_batched_metadata(videos: VideoInput, video_metadata: Union[VideoMetadata, dict]):\n+    if video_metadata is None:\n+        # Create default metadata and fill attrbiutes we can infer from given video\n+        video_metadata = [\n+            {\n+                \"total_num_frames\": len(video),\n+                \"fps\": None,\n+                \"duration\": None,\n+                \"frames_indices\": list(range(len(video))),\n+                \"height\": get_video_size(video)[0] if is_valid_video(video) else None,\n+                \"width\": get_video_size(video)[1] if is_valid_video(video) else None,\n+            }\n+            for video in videos\n+        ]\n+\n+    if isinstance(video_metadata, list):\n+        # Flatten if nested list\n+        if isinstance(video_metadata[0], list):\n+            video_metadata = [\n+                VideoMetadata(**metadata) for metadata_list in video_metadata for metadata in metadata_list\n+            ]\n+        # Simply wrap in VideoMetadata if simple dict\n+        elif isinstance(video_metadata[0], dict):\n+            video_metadata = [VideoMetadata(**metadata) for metadata in video_metadata]\n+    else:\n+        # Create a batched list from single object\n+        video_metadata = [VideoMetadata(**video_metadata)]\n+    return video_metadata\n \n \n def get_video_size(video: np.ndarray, channel_dim: ChannelDimension = None) -> tuple[int, int]:\n@@ -186,7 +253,7 @@ def get_video_size(video: np.ndarray, channel_dim: ChannelDimension = None) -> t\n         A tuple of the video's height and width.\n     \"\"\"\n     if channel_dim is None:\n-        channel_dim = infer_channel_dimension_format(video)\n+        channel_dim = infer_channel_dimension_format(video, num_channels=(1, 3, 4))\n \n     if channel_dim == ChannelDimension.FIRST:\n         return video.shape[-2], video.shape[-1]\n@@ -253,7 +320,7 @@ def default_sample_indices_fn(metadata: VideoMetadata, num_frames=None, fps=None\n \n \n def read_video_opencv(\n-    video_path: str,\n+    video_path: Union[\"URL\", \"Path\"],\n     sample_indices_fn: Callable,\n     **kwargs,\n ):\n@@ -285,7 +352,12 @@ def sample_indices_fn(metadata, **kwargs):\n     video_fps = video.get(cv2.CAP_PROP_FPS)\n     duration = total_num_frames / video_fps if video_fps else 0\n     metadata = VideoMetadata(\n-        total_num_frames=int(total_num_frames), fps=float(video_fps), duration=float(duration), video_backend=\"opencv\"\n+        total_num_frames=int(total_num_frames),\n+        fps=float(video_fps),\n+        duration=float(duration),\n+        video_backend=\"opencv\",\n+        height=int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n+        width=int(video.get(cv2.CAP_PROP_FRAME_WIDTH)),\n     )\n     indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n@@ -310,8 +382,8 @@ def sample_indices_fn(metadata, **kwargs):\n \n \n def read_video_decord(\n-    video_path: str,\n-    sample_indices_fn: Optional[Callable] = None,\n+    video_path: Union[\"URL\", \"Path\"],\n+    sample_indices_fn: Callable,\n     **kwargs,\n ):\n     \"\"\"\n@@ -320,7 +392,7 @@ def read_video_decord(\n     Args:\n         video_path (`str`):\n             Path to the video file.\n-        sample_indices_fn (`Callable`, *optional*):\n+        sample_indices_fn (`Callable`):\n             A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n             by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n             If not provided, simple uniform sampling with fps is performed.\n@@ -342,18 +414,27 @@ def sample_indices_fn(metadata, **kwargs):\n     total_num_frames = len(vr)\n     duration = total_num_frames / video_fps if video_fps else 0\n     metadata = VideoMetadata(\n-        total_num_frames=int(total_num_frames), fps=float(video_fps), duration=float(duration), video_backend=\"decord\"\n+        total_num_frames=int(total_num_frames),\n+        fps=float(video_fps),\n+        duration=float(duration),\n+        video_backend=\"decord\",\n     )\n \n     indices = sample_indices_fn(metadata=metadata, **kwargs)\n-\n-    frames = vr.get_batch(indices).asnumpy()\n-    metadata.frames_indices = indices\n-    return frames, metadata\n+    video = vr.get_batch(indices).asnumpy()\n+\n+    metadata.update(\n+        {\n+            \"frames_indices\": indices,\n+            \"height\": video.shape[1],\n+            \"width\": video.shape[2],\n+        }\n+    )\n+    return video, metadata\n \n \n def read_video_pyav(\n-    video_path: str,\n+    video_path: Union[\"URL\", \"Path\"],\n     sample_indices_fn: Callable,\n     **kwargs,\n ):\n@@ -385,7 +466,12 @@ def sample_indices_fn(metadata, **kwargs):\n     video_fps = container.streams.video[0].average_rate  # should we better use `av_guess_frame_rate`?\n     duration = total_num_frames / video_fps if video_fps else 0\n     metadata = VideoMetadata(\n-        total_num_frames=int(total_num_frames), fps=float(video_fps), duration=float(duration), video_backend=\"pyav\"\n+        total_num_frames=int(total_num_frames),\n+        fps=float(video_fps),\n+        duration=float(duration),\n+        video_backend=\"pyav\",\n+        height=container.streams.video[0].height,\n+        width=container.streams.video[0].width,\n     )\n     indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n@@ -404,7 +490,7 @@ def sample_indices_fn(metadata, **kwargs):\n \n \n def read_video_torchvision(\n-    video_path: str,\n+    video_path: Union[\"URL\", \"Path\"],\n     sample_indices_fn: Callable,\n     **kwargs,\n ):\n@@ -423,8 +509,8 @@ def sample_indices_fn(metadata, **kwargs):\n                 return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        tuple[`np.array`, `VideoMetadata`]: A tuple containing:\n-            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+        tuple[`torch.Tensor`, `VideoMetadata`]: A tuple containing:\n+            - Torch tensor of frames in RGB (shape: [num_frames, height, width, 3]).\n             - `VideoMetadata` object.\n     \"\"\"\n     warnings.warn(\n@@ -436,7 +522,7 @@ def sample_indices_fn(metadata, **kwargs):\n         start_pts=0.0,\n         end_pts=None,\n         pts_unit=\"sec\",\n-        output_format=\"THWC\",\n+        output_format=\"TCHW\",\n     )\n     video_fps = info[\"video_fps\"]\n     total_num_frames = video.size(0)\n@@ -450,13 +536,19 @@ def sample_indices_fn(metadata, **kwargs):\n \n     indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n-    video = video[indices].contiguous().numpy()\n-    metadata.frames_indices = indices\n+    video = video[indices].contiguous()\n+    metadata.update(\n+        {\n+            \"frames_indices\": indices,\n+            \"height\": video.shape[1],\n+            \"width\": video.shape[2],\n+        }\n+    )\n     return video, metadata\n \n \n def read_video_torchcodec(\n-    video_path: str,\n+    video_path: Union[\"URL\", \"Path\"],\n     sample_indices_fn: Callable,\n     **kwargs,\n ):\n@@ -466,7 +558,7 @@ def read_video_torchcodec(\n     Args:\n         video_path (`str`):\n             Path to the video file.\n-        sample_indices_fn (`Callable`, *optional*):\n+        sample_indices_fn (`Callable`):\n             A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n             by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n             If not provided, simple uniform sampling with fps is performed.\n@@ -476,7 +568,7 @@ def sample_indices_fn(metadata, **kwargs):\n \n     Returns:\n         Tuple[`torch.Tensor`, `VideoMetadata`]: A tuple containing:\n-            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - Torch tensor of frames in RGB (shape: [num_frames, height, width, 3]).\n             - `VideoMetadata` object.\n     \"\"\"\n     # Lazy import torchcodec\n@@ -485,15 +577,19 @@ def sample_indices_fn(metadata, **kwargs):\n \n     decoder = VideoDecoder(\n         video_path,\n-        dimension_order=\"NHWC\",  # to be consistent with other decoders\n         # Interestingly `exact` mode takes less than approximate when we load the whole video\n         seek_mode=\"exact\",\n+        # Allow FFmpeg decide on the number of threads for efficiency\n+        num_ffmpeg_threads=0,\n+        device=kwargs.get(\"device\"),\n     )\n     metadata = VideoMetadata(\n         total_num_frames=decoder.metadata.num_frames,\n         fps=decoder.metadata.average_fps,\n         duration=decoder.metadata.duration_seconds,\n         video_backend=\"torchcodec\",\n+        height=decoder.metadata.height,\n+        width=decoder.metadata.width,\n     )\n     indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n@@ -512,7 +608,7 @@ def sample_indices_fn(metadata, **kwargs):\n \n \n def load_video(\n-    video: Union[str, \"VideoInput\"],\n+    video: VideoInput,\n     num_frames: Optional[int] = None,\n     fps: Optional[Union[int, float]] = None,\n     backend: str = \"pyav\",\n@@ -523,7 +619,7 @@ def load_video(\n     Loads `video` to a numpy array.\n \n     Args:\n-        video (`str` or `VideoInput`):\n+        video (`VideoInput`):\n             The video to convert to the numpy array format. Can be a link to video or local path.\n         num_frames (`int`, *optional*):\n             Number of frames to sample uniformly. If not passed, the whole video is loaded.\n@@ -563,13 +659,10 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n \n         sample_indices_fn = sample_indices_fn_func\n \n-    if is_valid_image(video) or (isinstance(video, (list, tuple)) and is_valid_image(video[0])):\n-        # Case 1: Video is provided as a 4D numpy array or torch tensor (frames, height, width, channels)\n-        if not is_valid_video(video):\n-            raise ValueError(\n-                f\"When passing video as decoded frames, video should be a 4D numpy array or torch tensor, but got {video.ndim} dimensions instead.\"\n-            )\n-        return video, None\n+    # Early exit if provided an array or `PIL` frames\n+    if not isinstance(video, str):\n+        metadata = [None] * len(video)\n+        return video, metadata\n \n     if urlparse(video).netloc in [\"www.youtube.com\", \"youtube.com\"]:\n         if not is_yt_dlp_available():\n@@ -593,13 +686,8 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n     # can also load with decord, but not cv2/torchvision\n     # both will fail in case of url links\n     video_is_url = video.startswith(\"http://\") or video.startswith(\"https://\")\n-    if video_is_url and backend in [\"opencv\", \"torchvision\"]:\n-        raise ValueError(\n-            \"If you are trying to load a video from URL, you can decode the video only with `pyav`, `decord` or `torchcodec` as backend\"\n-        )\n-\n-    if file_obj is None:\n-        return video\n+    if video_is_url and backend in [\"opencv\"]:\n+        raise ValueError(\"If you are trying to load a video from URL, you cannot use 'opencv' as backend\")\n \n     if (\n         (not is_decord_available() and backend == \"decord\")"
        },
        {
            "sha": "7025294a0f735f97a312b68d87ac511dbf8de928",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -29,7 +29,6 @@\n if is_vision_available():\n     from transformers import Glm4vProcessor\n \n-\n if is_torch_available():\n     import torch\n \n@@ -163,16 +162,6 @@ def _test_apply_chat_template(\n         for k in out_dict:\n             self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n \n-    @require_av\n-    @unittest.skip(\"GLM4V can't sample frames from image frames\")\n-    def test_apply_chat_template_video_1(self):\n-        pass\n-\n-    @require_av\n-    @unittest.skip(\"GLM4V can't sample frames from image frames\")\n-    def test_apply_chat_template_video_2(self):\n-        pass\n-\n     @require_av\n     def test_apply_chat_template_video_frame_sampling(self):\n         processor = self.get_processor()\n@@ -224,18 +213,17 @@ def test_apply_chat_template_video_frame_sampling(self):\n             video_fps=video_fps,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 40)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 20)\n \n         # Load without any arg should load the whole video\n         out_dict_with_video = processor.apply_chat_template(\n             messages,\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n-            do_sample_frames=False,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 600)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 40)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video\n@@ -256,6 +244,19 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 4)\n \n+        # When the inputs are frame URLs/paths we expect that those are already\n+        # sampled and will raise an error is asked to sample again.\n+        with self.assertRaisesRegex(\n+            ValueError, \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`\"\n+        ):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                do_sample_frames=True,\n+            )\n+\n     def test_model_input_names(self):\n         processor = self.get_processor()\n "
        },
        {
            "sha": "1dcd4bdecca65cd00bf5f6e2d6a3f4c2725e2ccc",
            "filename": "tests/models/glm4v/test_video_processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -96,7 +96,7 @@ def prepare_video_metadata(self, videos):\n             metadata = {\n                 \"fps\": 2,\n                 \"duration\": num_frames / 2,\n-                \"total_frames\": num_frames,\n+                \"total_num_frames\": num_frames,\n             }\n             video_metadata.append(metadata)\n         return video_metadata"
        },
        {
            "sha": "f9231d3b905a715b8b45bf081ed319296fa82c2f",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -519,11 +519,21 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 2904)\n \n+        # When the inputs are frame URLs/paths we expect that those are already\n+        # sampled and will raise an error is asked to sample again.\n+        with self.assertRaisesRegex(\n+            ValueError, \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`\"\n+        ):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                do_sample_frames=True,\n+            )\n+\n     @require_librosa\n     @require_av\n-    @unittest.skip(\n-        \"@raushan: librosa can'r decode this audio in CI runner, fix after adding moviepy or another decoder\"\n-    )\n     def test_chat_template_audio_from_video(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n@@ -570,7 +580,7 @@ def test_chat_template_audio_from_video(self):\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n-            return_tensors=\"np\",\n+            return_tensors=\"pt\",\n             load_audio_from_video=True,\n         )\n         self.assertTrue(self.audio_input_name in out_dict)"
        },
        {
            "sha": "8aaf869e329b1f71d442a80929d6a7782fcf9b95",
            "filename": "tests/models/qwen2_5_vl/test_processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -338,6 +338,19 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n \n+        # When the inputs are frame URLs/paths we expect that those are already\n+        # sampled and will raise an error is asked to sample again.\n+        with self.assertRaisesRegex(\n+            ValueError, \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`\"\n+        ):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                do_sample_frames=True,\n+            )\n+\n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor = self.get_processor()\n         self.skip_processor_without_typed_kwargs(processor)"
        },
        {
            "sha": "2d6e9ef5da17b86e5fd40ca4b40ec063b7702259",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -338,6 +338,19 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n \n+        # When the inputs are frame URLs/paths we expect that those are already\n+        # sampled and will raise an error is asked to sample again.\n+        with self.assertRaisesRegex(\n+            ValueError, \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`\"\n+        ):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                do_sample_frames=True,\n+            )\n+\n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor = self.get_processor()\n         self.skip_processor_without_typed_kwargs(processor)"
        },
        {
            "sha": "4ffb70fc40dfc23d0f5fedb81a17bf1b4f6d1825",
            "filename": "tests/models/qwen2_vl/test_video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -326,11 +326,6 @@ def test_call_sample_frames(self):\n             self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n             self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n \n-            # Sample with `fps` requires metadata to infer number of frames from total duration\n-            with self.assertRaises(ValueError):\n-                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3)[self.input_name]\n-                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", fps=3)[self.input_name]\n-\n             metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n             batched_metadata = metadata * len(video_inputs)\n             encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3, video_metadata=metadata)["
        },
        {
            "sha": "22e7c1d4f7bd17840a3e2e2cc341a2ebf366f3b7",
            "filename": "tests/models/smolvlm/test_video_processing_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fsmolvlm%2Ftest_video_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Fmodels%2Fsmolvlm%2Ftest_video_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_video_processing_smolvlm.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -136,15 +136,6 @@ def test_call_sample_frames(self):\n             metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n             batched_metadata = metadata * len(video_inputs)\n \n-            # Sample with `fps` requires metadata to infer number of frames from total duration\n-            with self.assertRaises(ValueError):\n-                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=6, fps=3)[\n-                    self.input_name\n-                ]\n-                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=6, fps=3)[\n-                    self.input_name\n-                ]\n-\n             encoded_videos = video_processing(\n                 video_inputs[0], return_tensors=\"pt\", num_frames=6, fps=3, video_metadata=metadata\n             )[self.input_name]\n@@ -154,14 +145,5 @@ def test_call_sample_frames(self):\n             self.assertEqual(encoded_videos.shape[1], 6)\n             self.assertEqual(encoded_videos_batched.shape[1], 6)\n \n-            # We should raise error when asked to sample more frames than there are in input video\n-            with self.assertRaises(ValueError):\n-                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=10, num_frames=20)[\n-                    self.input_name\n-                ]\n-                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", fps=10, num_frames=20)[\n-                    self.input_name\n-                ]\n-\n             # Assign back the actual num frames in tester\n             self.video_processor_tester.num_frames = prev_num_frames"
        },
        {
            "sha": "97506e6f120768c77a66617ec188237905b79790",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 4,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -51,7 +51,7 @@\n     ],\n     \"videos\": [\n         \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-        [\"https://www.ilankelman.org/stopsigns/australia.jpg\", \"https://www.ilankelman.org/stopsigns/australia.jpg\"],\n+        \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\",\n     ],\n     \"audio\": [\n         \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n@@ -977,7 +977,7 @@ def test_apply_chat_template_audio(self, batch_size: int, return_tensors: str):\n         )\n \n     @require_av\n-    @parameterized.expand([(1, \"pt\"), (2, \"pt\"), (3, \"pt\")])  # video processor supports only torchvision\n+    @parameterized.expand([(1, \"pt\"), (2, \"pt\")])  # video processor supports only torchvision\n     def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n         self._test_apply_chat_template(\n             \"video\", batch_size, return_tensors, \"videos_input_name\", \"video_processor\", MODALITY_INPUT_DATA[\"videos\"]\n@@ -1082,8 +1082,8 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 300)\n \n-        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n-        # because we assume they come from one video\n+        # Load video as a list of frames (i.e. images).\n+        # NOTE: each frame should have same size because we assume they come from one video\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n             \"url\": [\n@@ -1101,6 +1101,19 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)\n \n+        # When the inputs are frame URLs/paths we expect that those are already\n+        # sampled and will raise an error is asked to sample again.\n+        with self.assertRaisesRegex(\n+            ValueError, \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`\"\n+        ):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                do_sample_frames=True,\n+            )\n+\n     @require_librosa\n     @require_av\n     def test_chat_template_audio_from_video(self):"
        },
        {
            "sha": "d7f94f2c20f250639aa373a509c425ff05669754",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f690a2a1e09e8a8c7b04cc050ef24838c609060b/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=f690a2a1e09e8a8c7b04cc050ef24838c609060b",
            "patch": "@@ -34,6 +34,7 @@\n     torch_device,\n )\n from transformers.utils import is_torch_available, is_vision_available\n+from transformers.video_utils import VideoMetadata\n \n \n if is_torch_available():\n@@ -327,8 +328,8 @@ def test_call_sample_frames(self):\n \n             # Sample with `fps` requires metadata to infer number of frames from total duration\n             with self.assertRaises(ValueError):\n-                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3)[self.input_name]\n-                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", fps=3)[self.input_name]\n+                metadata = VideoMetadata(**{\"total_num_frames\": 8})\n+                video_processing.sample_frames(metadata=metadata, fps=3)\n \n             metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n             batched_metadata = metadata * len(video_inputs)"
        }
    ],
    "stats": {
        "total": 1358,
        "additions": 753,
        "deletions": 605
    }
}