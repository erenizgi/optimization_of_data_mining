{
    "author": "Cyrilvallez",
    "message": "Fix looping in torch guard decorator (#42260)\n\n* fix\n\n* add\n\n* fix\n\n* switch loop order for perfs\n\n* typo",
    "sha": "a5c903f877fda21e739027eed133e03162eb7712",
    "files": [
        {
            "sha": "afe9e6d35214ac3527e2329bbe932da6c1a4ec32",
            "filename": "src/transformers/initialization.py",
            "status": "modified",
            "additions": 28,
            "deletions": 11,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c903f877fda21e739027eed133e03162eb7712/src%2Ftransformers%2Finitialization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c903f877fda21e739027eed133e03162eb7712/src%2Ftransformers%2Finitialization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Finitialization.py?ref=a5c903f877fda21e739027eed133e03162eb7712",
            "patch": "@@ -162,6 +162,25 @@ def copy_(tensor: torch.Tensor, other: torch.Tensor) -> torch.Tensor:\n     return tensor\n \n \n+# Here, we need to check several modules imported, and hot patch all of them, as sometimes torch does\n+# something like `from torch.nn.init import xavier_uniform_` in their internals (e.g in torch.nn.modules.activations,\n+# where MultiHeadAttention lives), so the function name is binded at import time and just doing\n+# `setattr(torch.nn.init, name, globals()[name])` is thus not enough\n+# The following list should be enough for all torch versions we work with\n+TORCH_MODULES_TO_PATCH = (\n+    \"torch.nn.init\",\n+    \"torch.nn.modules.activation\",\n+    \"torch.nn.modules.transformer\",\n+    \"torch.nn.modules.linear\",\n+    \"torch.nn.modules.loss\",\n+    \"torch.nn.modules.batchnorm\",\n+    \"torch.nn.modules.conv\",\n+    \"torch.nn.modules.normalization\",\n+    \"torch.nn.modules.rnn\",\n+    \"torch.nn.modules.sparse\",\n+)\n+\n+\n @contextmanager\n def guard_torch_init_functions():\n     \"\"\"\n@@ -174,18 +193,16 @@ def guard_torch_init_functions():\n     originals = defaultdict(dict)\n     try:\n         # Replace all torch funcs by the ones in this file\n-        for name in TORCH_INIT_FUNCTIONS.keys():\n-            # Here, we need to check all modules imported, and hot patch all of them, as usually torch does\n-            # something like `from torch.nn.init import xavier_uniform_` in their internals (e.g in torch.nn.modules,\n-            # where MultiHeadAttention lives), so the function name is binded at import time and just doing\n-            # `setattr(torch.nn.init, name, gloabls()[name])` is thus not enough\n-            for module in sys.modules.copy().values():\n-                if module and hasattr(module, name):\n-                    originals[module][name] = getattr(module, name)\n-                    setattr(module, name, globals()[name])\n+        for module_name in TORCH_MODULES_TO_PATCH:\n+            if module_name in sys.modules:\n+                module = sys.modules[module_name]\n+                for func_name in TORCH_INIT_FUNCTIONS.keys():\n+                    if hasattr(module, func_name):\n+                        originals[module][func_name] = getattr(module, func_name)\n+                        setattr(module, func_name, globals()[func_name])\n         yield\n     finally:\n         # Set back the original functions on all modules\n         for module, functions in originals.items():\n-            for name, func in functions.items():\n-                setattr(module, name, func)\n+            for func_name, func in functions.items():\n+                setattr(module, func_name, func)"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 28,
        "deletions": 11
    }
}