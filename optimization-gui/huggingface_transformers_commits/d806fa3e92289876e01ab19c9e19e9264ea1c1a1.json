{
    "author": "anakin87",
    "message": "docs: fix outdated link to TF32 explanation (#32947)\n\nfix outdated link",
    "sha": "d806fa3e92289876e01ab19c9e19e9264ea1c1a1",
    "files": [
        {
            "sha": "51e0ee955dd336fdbe9fffef9d778c43fdd00a65",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d806fa3e92289876e01ab19c9e19e9264ea1c1a1/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d806fa3e92289876e01ab19c9e19e9264ea1c1a1/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=d806fa3e92289876e01ab19c9e19e9264ea1c1a1",
            "patch": "@@ -416,7 +416,7 @@ class TrainingArguments:\n         tf32 (`bool`, *optional*):\n             Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n             on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n-            the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation. This is an\n+            the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n             experimental API and it may change.\n         local_rank (`int`, *optional*, defaults to -1):\n             Rank of the process during distributed training."
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}