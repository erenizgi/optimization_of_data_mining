{
    "author": "Cyrilvallez",
    "message": "Improve modular documentation (#35737)\n\n* start a nice doc\r\n\r\n* keep improving the doc\r\n\r\n* Finalize doc\r\n\r\n* Update modular_transformers.md\r\n\r\n* apply suggestion",
    "sha": "8ac851b0b33ef7d3cb18157f9d84fee797ea7dab",
    "files": [
        {
            "sha": "dca1282bcf99d70a0fa4930b96e5d4fe451ac607",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 500,
            "deletions": 49,
            "changes": 549,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ac851b0b33ef7d3cb18157f9d84fee797ea7dab/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ac851b0b33ef7d3cb18157f9d84fee797ea7dab/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=8ac851b0b33ef7d3cb18157f9d84fee797ea7dab",
            "patch": "@@ -59,8 +59,8 @@ inheritance.\n For example:\n - If a configuration class inherits from another and adds/deletes an argument, the generated file will either directly \n   reference it (in case of addition) or completely remove it (in case of deletion).\n-- If a class inherits from another, for example: class GemmaModel(LlamaModel):, dependencies are automatically \n-  inferred. All submodules will be automatically inferred from the superclass.\n+- If a class inherits from another, for example: `class GemmaModel(LlamaModel):`, dependencies are automatically \n+  inferred. All submodules will be automatically added from the superclass.\n - If you define new functions in the `modular` and use them inside classes, the linter will automatically infer the \n \n You should be able to write everything (the tokenizer, the image processor, the model, the config) in this `modular` \n@@ -120,46 +120,362 @@ class RobertaForMaskedLM(BertForMaskedLM):\n     self.model = RobertaModel(config)\n ```\n \n-Note that if you do not use the dependency that you defined, you will have the following error:\n+## What it is not\n \n-```bash\n-ValueError: You defined `RobertaEmbeddings` in the modular_roberta.py, it should be used\n-                                    when you define `BertModel`, as it is one of it's direct dependencies. Make sure\n-                                    you use it in the `__init__` function.\n+It is not a replacement for the modeling code (yet?), and if your model is not based on anything else that ever existed, then you can add a `modeling` file as usual. Similarly, if you cannot easily inherit your `configuration` (or `tokenization` or `processing`) file from another model's similar file, you can add that filetype directly (even though defining it in the modular file would work, it would clutter it).\n+\n+\n+## Real world example breakdown\n+\n+As explained, modular allows you to use regular Python inheritance from any other model's code in the library, in order to define your own. For this reason, it will work better/be easier if you first browse the library a bit to find models close to yours, in order to inherit from them. For example, are you using a sliding window in the `Attention` class? Then start by checking models that are well known to use it, e.g. `Mistral`, or `Qwen2`! Are you using interleaved `RotaryEmbedding` modules? Check out `Cohere`, `Cohere2` and `Glm` models! Otherwise a very strong starting point is to check out `Llama`. And if you are doing a bit of all of that at once, then you can mix and match!\n+\n+Here are some common properties that your model might be using, and corresponding modeling files to check as an example:\n+- Mixture of expert: `SwitchTransformers` or `Mixtral`\n+- Interleaved (and/or partial) rotary embedding: `Glm`, `Phi`\n+- State space models: \n+    - Hybrid with attention: `Jamba` , `Bamba`, `Zamba`\n+    - Mamba2: `Mamba2` \n+- Recurrent hidden states: `Gemma2`\n+- Different sliding window attention/full attention patterns per layer: `Gemma2`, `Cohere2`\n+- Clipping of QKV: `Olmo`\n+- Normalization of QK: `Olmo2`, `Cohere`\n+- Fused QKV (not recommended): `Phi3`\n+\n+At Hugging Face, we feel that learning by example is usually (one of) the best way, so we will now go over a typical modular file, and the different features our linter provides (and its limitations)! ðŸ¤— Let's use a real world example with Olmo2 model, which I feel provides a very good illustration of the modular mechanisms. The original file can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo2/modular_olmo2.py). For simplicity, we will go over it class by class, and repeat the modular's definition of ech class. For reference, the modeling and configuration of Olmo (v1) on which we will inherit a lot can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo/modeling_olmo.py) and [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo/configuration_olmo.py) respectively. The final modeling of Olmo2 (generated by running our linter on the modular we will describe below) can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo2/modeling_olmo2.py)\n+\n+Let's break it down!\n+\n+\n+### Config class\n+\n+Here is the `Config` definition in modular:\n+\n+```py\n+from ..olmo.configuration_olmo import OlmoConfig\n+\n+class Olmo2Config(OlmoConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Olmo2Model`].\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=50304,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        use_cache=True,\n+        pad_token_id=1,\n+        bos_token_id=None,\n+        eos_token_id=50279,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        rms_norm_eps=1e-5,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            hidden_act=hidden_act,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            use_cache=use_cache,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            rope_theta=rope_theta,\n+            rope_scaling=rope_scaling,\n+            attention_bias=attention_bias,\n+            attention_dropout=attention_dropout,\n+            **kwargs,\n+        )\n+\n+        self.rms_norm_eps = rms_norm_eps\n+        del self.clip_qkv\n ```\n \n-Additionally, you may find a list of examples here:\n+Here, we correctly identified that the `Config` in Olmo2 is similar to Olmo's, up to a few details:\n+1. The default value of most arguments has changed\n+2. we have a new argument, `rms_norm_eps`\n+3. the argument `clip_qkv` is not used anymore\n \n-## What it is not\n+To solve points 1. and 2., simply overwriting the `__init__` function with the new default arguments and adding the new one is enough, as you would expect when you want to overwrite a method in Python! Of course you also need to assign the new attribute `rms_norm_eps` to `self` in the `__init__`'s body.  \n+For point 3., we use the special syntax `del self.clip_qkv`, which, has you can expect, removed the assignment of this attribute in the unravelled code (after the conversion with the linter).  \n+\n+Now, there is a subtility here: as you can see, we used `super().__init__(...)`. Usually, in Python, it is simply used to call the parent's `__init__`. In modular terms, however, it has a _slightly_ different meaning. When we find a call such as `super().my_function(...)` in the modular file, the linter will take the body of the `my_function` function in the parent, and unravel it where the call to `super().my_function(...)` occured. Then, the `del self.clip_qkv` statement will remove the reference to `self.clip_qkv` from the unravelled body. Thus `del self.xxx` can only work in pair with `super().my_function(...)`, and should always be placed after it (but you can add whatever you want _before_ calling `super()`, and it will be placed, as you can expect, before the parent's body).\n+\n+### Norm class\n+\n+Here is the `Norm` class:\n+\n+```py\n+from ..llama.modeling_llama import LlamaRMSNorm\n+\n+class Olmo2RMSNorm(LlamaRMSNorm):\n+    pass\n+```\n+\n+What to say here, it is pretty explicit isn't it? We do not modify anything from the `LlamaRMSNorm` definition. Thus the linter will unravel exactly the content of the parent (`LlamaRMSNorm`). Only change will be that every reference to \"llama\" on the docstrings, type hints, and comments (basically everywhere) will be changed to references to \"olmo2\" for consistency!\n+\n+### Attention class\n+\n+Here is the `Attention` class:\n+\n+```py\n+from ..llama.modeling_llama import eager_attention_forward\n+from ..olmo.modeling_olmo import OlmoAttention, apply_rotary_pos_emb\n+\n+\n+# Olmo2 attention is identical to OLMo attention except:\n+# - Norm is applied to attention queries and keys.\n+# - No qkv clipping.\n+class Olmo2Attention(OlmoAttention):\n+    def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx=layer_idx)\n+        self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states))\n+        key_states = self.k_norm(self.k_proj(hidden_states))\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+```\n+\n+Now, what's happening here? In the `__init__`, we call `super().__init__(...)`, thus copying the parent's definition, then add 2 new layers of the `Olmo2RMSNorm` we just added previously. Indeed, those were not present in the original `Olmo` (v1) model. So, now, we also have to overwrite the `forward` method to use these 2 new layers right? Indeed, if you check carefully, the definition of `forward` is identical to `Olmo`'s, but we added a pass with the norm layers just before projecting with `q_proj` and `k_proj`. However, to help us, we directly imported the functions `eager_attention_forward` from llama, and `apply_rotary_pos_emb` from olmo. The linter will then automatically add these imported functions in the final `modeling_olmo2.py` file, by copying their definitions from the source (imported) files. And it will even add the `rotate_half` and `repeat_kv` functions (which are used inside `apply_rotary_pos_embed` and `eager_attention_forward` respectively) by figuring out the dependency automatically. Neat, right?  \n+Note that we had to redefine this class, because we did not find any model defining the `Attention` layer with the added `RMSNorm` layer anywhere else in the library! Otherwise, we would have simply inherited from this model instead as we did for the `RMSNorm`!\n+\n+### The DecoderLayer class\n+\n+Here is the `DecoderLayer` class:\n+\n+```py\n+from ..olmo.modeling_olmo import OlmoDecoderLayer\n+\n+# The OLMo2 layers are identical to those of the OLMo model except:\n+# - RMSNorm is used instead of standard layer norm.\n+# - Norm is applied after attention/feedforward rather than before.\n+class Olmo2DecoderLayer(OlmoDecoderLayer):\n+    def __init__(self, config: Olmo2Config, layer_idx: int):\n+        super().__init__(config, layer_idx=layer_idx)\n+        self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.self_attn = Olmo2Attention(config=config, layer_idx=layer_idx)\n+        del self.input_layernorm\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+```\n+\n+At this point, you should start to pick up what is happening for this class. We switched the type of norm in the `__init__` by overwriting `self.post_attention_layernorm` after the call to `super().__init__(...)`, thus going from a `LayerNorm` in the parent class, to our `RMSNorm` in this class. Then we simply deleted the `self.input_layernorm` attribute, and replaced it by `self.post_feedforward_layernorm`, because the name was not making sense anymore as we apply it after in `Olmo2` instead of before in `Olmo`. For this reason, we also need to overwrite the `forward` method, to reflect the logic change.\n+\n+Note however that if we had only switched `self.post_attention_layernorm` and `self.input_layernorm` from `LayerNorm`s to `RMSNorm`s (without the name and logic change of `elf.input_layernorm`), we would not have had to redefine the `forward` method!\n+\n+### The Model class\n+\n+```py\n+from ..olmo.modeling_olmo import OlmoModel\n+\n+# The OLMo2 model is identical to the OLMo model, except RMSNorm is used instead of\n+# standard layer norm for the output norm.\n+class Olmo2Model(OlmoModel):\n+    def __init__(self, config: Olmo2Config):\n+        super().__init__(config)\n+        self.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.layers = nn.ModuleList(\n+            [Olmo2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+```\n \n-It is not a replacement for the modeling code (yet?), and if your model is not based on anything else that ever existed, then you can add a `modeling` file as usual.\n+Here, this is exactly what I was pointing out before: we simply change the _type_ of the `self.norm` attribute (going from `LayerNorn` in `Olmo` to `RMSNorm` in `Olmo2`). Since this change does not reflect the logic of the `forward` method (the name of the layer and where it is used is identical to the parent's), then we do not even need to overwrite it! It will be unravelled automatically! Note that we redefined `self.layers` for the sake of being explicit, but this is not even strictly required here as the definition is similar to what is found in `Olmo` (v1).\n \n+### Finally... The ForCausalLM class\n+\n+Finally, here is the definition of the `ForCausalLM`:\n+\n+```py\n+from ..olmo.modeling_olmo import OlmoForCausalLM\n+\n+class Olmo2ForCausalLM(OlmoForCausalLM):\n+    pass\n+```\n+\n+As for the `RMSNorm`, it is exactly similar to the parent's in logic, so we do not have anything to do, the linter will all figure it out by itself. Almost disappointing, no?\n+\n+\n+<a id=\"dependencies\"></a>\n+### But... What about the MLP, RotaryEmbedding and PreTrainedModel classes?\n+\n+Indeed, if you inspect the file [modeling_olmo2.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo2/modeling_olmo2.py) which is created by running the linter on `modular_olmo2.py`, you will notice that it also creates `Olmo2MLP`, `Olmo2RotaryEmbedding`, and `Olmo2PreTrainedModel` classes, that we did not define explicitly in `modular_olmo2.py`.  \n+\n+Well, it is one of the main feature of our modular linter. Similarly to how some functions were added automatically with the `Attention` class (without directly importing them), classes that are a dependency of one of the class inherited class and which are not explicitly defined in the modular file, will be added automatically as part of the dependeny tracing. For example, in `OlmoDecoderLayer`, there is an attribute defined as `self.mlp = OlmoMLP(config)`. Because we never explicitly redefined a class named `Olmo2MLP` in `modular_olmo2.py`, the linter automatically created a class `Olmo2MLP`, similar to `OlmoMLP`. This is exactly the same as if we had done:\n+\n+```py\n+from ..olmo.modeling_olmo import OlmoMLP\n+\n+class Olmo2MLP(OlmoMLP):\n+    pass\n+```\n+\n+but we did not even bother, because we _know_ this class is supposed to be exactly similar, and we never needed it anywhere else in the `modular_olmo2.py` file. In contrast, the class `Olmo2RMSNorm` was needed to (re)define the norms both in the `Attention` and `DecoderLayer` classes. The same logic is true for the `Olmo2PreTrainedModel` and `Olmo2RotaryEmbedding` classes.\n+\n+Note however that if not redefined, classes will be copied from the file in which an inherited module uses them first. So if you wanted e.g. `Olmo2MLP` to inherit from, say, `MistralMLP` instead of `OlmoMLP` (here it was `OlmoMLP` because it was first implicitly used in `Olmo2DecoderLayer`, which inherited from `OlmoDecoderLayer`), you would need to be explicit and do:\n+\n+```py\n+# switch to mistral definition\n+from ..mistral.modeling_mistral import MistralMLP\n+\n+class Olmo2MLP(MistralMLP):\n+    pass\n+```\n \n ## Advanced usage\n \n-### Removing attributes and functions\n-To remove attributes that are not used in your modular model, and that you don't want to see in the unravelled modeling: \n+Now that you should have a good grasp of how modular works, let's see some more advanced use cases and features you can use.\n \n-```python\n-class GemmaModel(LlamaModel):                 |           class GemmaModel(PreTrainedModel):\n-    def __init__(self, config):               |              def __init__(self, config):\n-        super().__init__(self, eos_token)     |                 super().__init__(config)\n-        del self.embed_tokens                 |                 self.padding_idx = config.pad_token_id\n-                                              |                 self.vocab_size = config.vocab_size\n-                                              |\n-                                              |                 self.layers = nn.ModuleList(\n-                                              |                     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-                                              |                 )\n-                                              |                 self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-                                              |                 self.rotary_emb = LlamaRotaryEmbedding(config=config)\n-                                              |                 self.gradient_checkpointing = False\n-                                              |                 \n-                                              |                 # Initialize weights and apply final processing\n-                                              |                 self.post_init()\n-```\n-If you check the original `LlamaModel`, it has a `embed_tokens` which was removed here (as you would expect!)\n-\n-Removing a function is pretty similar, you just need to write it with a `raise ValueError(\"\")` to mimick the behaviour you actually want when you remove a parent function in python.\n+### Removing attributes which are not just assignments\n+\n+As we have seen before, after using `super().__init__()`, we can use `del self.attribute` to remove a specific attribute which was defined in the parent. What if this attribute was used elsewhere though? Meaning it was not just \"defined to be stored\" as in the config for example. For example, consider the following case:\n+\n+```py\n+class DummyModel(nn.Module):\n+\n+  def __init__(self, config: DummyConfig):\n+    super().__init__()\n+    self.attribute = config.attribute\n+    if self.attribute:\n+      # do more stuff with `self.attribute` here\n+      ...\n+```\n+\n+Then inheriting from this `DummyModel` and doing\n+\n+```py\n+class MyNewDummyModel(DummyModel):\n+\n+  def __init__(self, config: MyNewDummyConfig):\n+    super().__init__(config)\n+    del self.attribute\n+```\n+\n+is not supported, because it will only suppress the assignment, i.e. the line `self.attribute = config.attribute` will disappear, but the `if` statement will stay and reference the attribute. We tried to make it work by suppressing every mentions of the attribute, however it it not a sound solution in the general case (it can lead to very surprising effects and remove other important parts) and is therefore not possible. \n+\n+But what if I still want to inherit from `DummyModel`? How to properly do it? How to use `super().__init__()` without copy/pasting the parent then? This brings us to the next point:\n+\n+### Avoiding super() special meaning\n+\n+Say you still want to inherit from `DummyModel` (because it is convenient for some other methods) but you do want to remove the `self.attribute`. How to properly override the `__init__` method, while calling `super()` but without unravelling the parent's code? Well, then be explicit about which class `super()`'s you are calling! If we want to call the `nn.Module`'s `super()` for example, we can do the following (unravelled code on the right):\n+\n+```py\n+class MyNewDummyModel(DummyModel, nn.Module):        |     class MyNewDummyModel(nn.Module):\n+                                                     |\n+  def __init__(self, config: MyNewDummyConfig):      |       def __init__(self, config: MyNewDummyConfig):\n+    nn.Module.__init__(config)                       |         super().__init__()\n+    self.foo = config.foo                            |         self.foo = config.foo\n+    ...                                              |         ...\n+```\n+\n+### Deleting unused methods\n+\n+Removing a class method is pretty similar to remove an attribute, you just need to overwrite it with a `raise AttributeError(\"\")` to mimick the behaviour you actually want when you remove a parent function in python. For example, the following will remove the methods in the unravelled code:\n \n ```python\n class GemmaTokenizer(LlamaTokenizer):\n@@ -174,37 +490,172 @@ class GemmaTokenizer(LlamaTokenizer):\n \n ### Define new functions\n \n-If you define a new function in the `modular` file to be used inside a class, say\n+Of course, if you define a new function in the `modular` file, and use it inside an inherited class, say\n \n ```python\n def my_new_function(*args, **kwargs):\n   # Do something here\n   pass\n \n-class GemmaModel(LlamaModel):\n+class DummyModel(LlamaModel):\n     def forward(*args, **kwargs):\n       # Call the function\n       example = my_new_function(*args, **kwargs)\n       # continue here\n ```\n \n-the `my_new_function` function (and, recursively, any other new functions called in its body) will be automatically copy-pasted \n-in the file where it is used.\n+the `my_new_function` function (and, recursively, any other functions called in its body) will be automatically added to the unravelled code even if it is not present in the parent's file (here Llama).\n \n-### Calling `super()`\n-We recently shipped a few features that allow you to go from:\n-```python\n-class GemmaTokenizer(LlamaTokenizer, PretrainedTokenizerFast):         |           class GemmaModel(nn.Module):\n-    def __init__(self, eos_token=\"</s>\"):                              |             def __init__(self):\n-        eos_token = AddedToken(eos_token)                              |                eos_token = AddedToken(eos_token)\n-        PretrainedTokenizerFast.__init__(self, eos_token)              |                super().__init__(eos_token)\n+### Decorators\n+\n+By default, if you inherit from a class and override a method which has 1 (or more) decorators in the parent's method, the decorators will be added as well in the unravelled code, _but only if you do not add any yourself_. Otherwise, it will of course use whatever decorator your redefined.\n+\n+That, is, imagine the following parent class\n+\n+```py\n+class DummyModel(nn.Module):\n+  ...\n+\n+  @decorator(...)\n+  def forward(...)\n+    # do stuff here\n ```\n-This is useful want you **don't** want to unravel the call to `super()`, and you want to differentiate which super init call you are doing!\n \n-### Special naming\n-We now also support special cases like\n-```python\n-class GemmaVisionModel(CLIPModel):                                 \n+Then, if you simply override the method it will produce (modular on the left, unravelled code on the right):\n+\n+```py\n+class NewModel(DummyModel):       |   class NewModel(nn.Module):\n+  ...                             |     ...\n+                                  |\n+  def forward(...):               |     @decorator(...)\n+    ...                           |     def forward(...):\n+                                  |       ...\n+```\n+\n+That is, it keeps the parent's decorators by default. However, if you do:\n+\n+```py\n+class NewModel(DummyModel):       |   class NewModel(nn.Module):\n+  ...                             |     ...\n+                                  |\n+  @my_new_decorator(...)          |     @my_new_decorator(...)\n+  def forward(...):               |     def forward(...):\n+    ...                           |       ...\n+```\n+\n+Then it keeps you own new decorator.\n+\n+### The super_kwargs special case\n+\n+In the above case about decorators, what if the `forward` method is really long, and I just want to switch the decorators? Do I really have to redefine it all and copy/paste the body just for the decorator? Fortunately, no. If you followed until this point, you now that you can use `super().forward(...)`, and it will unravel the parent's body automatically. But what if there are plenty of arguments in the function's signature, and we are very lazy? For that use-case, we introduced the special syntax `**super_kwargs` in the overriden method signature. It basically mean: \"unravel all the parent's signature arguments here\". For example, a common signature in the `ForCausalLM` model is the following (copied from llama's modeling):\n+\n+```py\n+class LlamaForCausalLM(nn.Module):\n+  ...\n+\n+  @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n+  @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+  def forward(\n+      self,\n+      input_ids: torch.LongTensor = None,\n+      attention_mask: Optional[torch.Tensor] = None,\n+      position_ids: Optional[torch.LongTensor] = None,\n+      past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+      inputs_embeds: Optional[torch.FloatTensor] = None,\n+      labels: Optional[torch.LongTensor] = None,\n+      use_cache: Optional[bool] = None,\n+      output_attentions: Optional[bool] = None,\n+      output_hidden_states: Optional[bool] = None,\n+      return_dict: Optional[bool] = None,\n+      cache_position: Optional[torch.LongTensor] = None,\n+      num_logits_to_keep: int = 0,\n+      **kwargs: Unpack[KwargsForCausalLM],\n+  ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ...\n+```\n+\n+As you can see, this is a rather long and complicated signature. But if you do the following (as usual, modular on the left, unravelled code by the linter on the right):\n+\n+```py\n+class NewModelForCausalLM(LlamaForCausalLM):    |    class LlamaForCausalLM(nn.Module):\n+  ...                                           |      ...\n+                                                |\n+  @my_new_decorator                             |     @my_new_decorator\n+  def forward(self, **super_kwargs):            |     def forward(\n+    super().forward(**super_kwargs)             |         self,\n+                                                |         input_ids: torch.LongTensor = None,\n+                                                |         attention_mask: Optional[torch.Tensor] = None,\n+                                                |         position_ids: Optional[torch.LongTensor] = None,\n+                                                |         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = |None,\n+                                                |         inputs_embeds: Optional[torch.FloatTensor] = None,\n+                                                |         labels: Optional[torch.LongTensor] = None,\n+                                                |         use_cache: Optional[bool] = None,\n+                                                |         output_attentions: Optional[bool] = None,\n+                                                |         output_hidden_states: Optional[bool] = None,\n+                                                |         return_dict: Optional[bool] = None,\n+                                                |         cache_position: Optional[torch.LongTensor] = None,\n+                                                |         num_logits_to_keep: int = 0,\n+                                                |         **kwargs: Unpack[KwargsForCausalLM],\n+                                                |     ) -> Union[Tuple, CausalLMOutputWithPast]:\n+                                                |       ...\n+```\n+\n+and the `**super_kwargs` syntax unravelled all the arguments, while the `super().forward()` syntax unravelled the whole body! As you can see, this is  great combo when you just want to switch the decorators, as it is very easy to use, and make it explicit that the only change you want to apply is the decorator.  \n+\n+However, we want to make it clear that the `**super_kwargs` syntax is not a replacement to being explicit when you redefine your methods: if you actually overwrite the method (i.e. you do not call `super().method()`), then we want you to explicitly write the signature as you would usually. This is only a short-cut when switching decorators, and a few other niche cases.\n+\n+### The DOCSTRING variables\n+\n+Usually, if whatever object is defned both in the modular file and the modeling file from which we inherit, then the definition of the modular takes precedence. However, this is not the case for assignments containing the pattern `DOCSTRING`. Indeed, we usually have variables defined as `MODEL_START_DOCSTRING` and `MODEL_INPUT_DOCSTRING` in the modeling files. These are just very big blocks of, well, docstrings... But they are (almost) always exactly the same up to the model name! And modular automatically rewrite the names everywhere! For this reason, assignments containing the pattern will _always_ use the definition found in the source file instead of the modular file. This is extremely handy if we need the variable reference somewhere (e.g. to redefine a decorator) but we do not want to clutter the modular file with 100 lines of docstrings which are always the same. It allows to do the following (taken from [modular_starcoder2.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/starcoder2/modular_starcoder2.py#L146))\n+\n+```py\n+STARCODER2_INPUTS_DOCSTRING = None  # will be automatically redefined\n+\n+class Starcoder2Model(MistralModel):\n+    ...\n+\n+    @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n+    def forward(...)\n+        ...\n+```\n+\n+and here, the linter will correctly take the same definition of the docstring as in `Mistral`, without having to clutter the modular file!\n+\n+## Limitations\n+\n+Now, let's go over some of the limitations of modular.\n+\n+### Special naming (essentially for multimodal models)\n+\n+Because our linter automatically renames everything when inheriting from a class (defining `class NewModelMLP(LlamaMLP)` will rename every mention of `Llama` to `NewModel`, and recursively for all dependencies grabbed), it has somewhat strict rules when it comes to naming. For consistency reasons, we require that you always use the same class name prefix when inheriting different classes from the same file. For example, doing:\n+\n+```py\n+class MyModelIncredibleMLP(LlamaMLP):\n+    ...\n+\n+class MyModelDecoderLayer(LlamaDecoderLayer):\n+    ...\n+```\n+\n+is not recommended, first because it breaks standards in the library and we do not like it, and second because the linter will not know how to rename potential high-order dependencies (should we use `MyModelIncredible`, or `MyModel`?).\n+\n+If there are no dependencies to grab implicitly however (see [this section](#dependencies) to understand implicit dependencies), local renaming (for a single class) will not be an issue and the linter will not complain. But make sure to explicitly redefine every other mentions of the class with the new name pattern! For example in the example above, all mentions of `LlamaMLP` in other modules inherited should be explicitly replaced by mentions to `MyModelIncredibleMLP`, otherwise the linter may add a new and unwanted `MyModelMLP` class!\n+\n+In any way, if there is an ambiguous case detected, the linter will raise a warning such as\n+\n+```\n+We detected multiple prefix names when inheriting from transformers.models.llama.modeling_llama: ('Emu3Text', 'Emu3'). We will only use the most used 'Emu3' prefix when grabbing args and dependencies. Make sure to subclass the intermediate classes with the prefix you want (if different from 'Emu3') or use a single prefix in all the modular (best).\n+```\n+\n+explaining what is happening, and which prefix is used by default for grabbing dependencies. As explained, if you see automatic dependencies appear with a prefix but you want another one, then explicitly rename these classes locally with a simple `pass` class, such as\n+\n+```py\n+class Emu3TextMLP(LlamaMLP):                                 \n     pass\n ```\n-where the name of your class `GemmaVision` is not the same as the modular `Gemma`. This is super useful for composite models.\n+\n+Such warnings and renaming patterns complications usually only arise when defining multimodel models, when you want to define e.g. the text part of your model from an existing model, but want to add the part `Text` to the class names to make it clear what they refer to in the multimodal setup.\n+\n+### Automatic docstrings issue (mostly for Configs)\n+\n+When inheriting a Config class and adding or deleting some attributes, it may be tempting to only redefine the new attributes in the docstring, and hoping that modular will do the rest. And similarly when deleting an argument, do nothing and hope that modular will remove itself from the docstring. However, due to current limitations of our linter, this is not yet supported. Thus, if you are in this case, you need to directly put the whole docstring (as it should appear in the end, with the correct arguments and default values) directly in the modular file under the class definition.\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 549,
        "additions": 500,
        "deletions": 49
    }
}