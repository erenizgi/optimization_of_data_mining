{
    "author": "eustlb",
    "message": "[test] Fix test_eager_matches_sdpa incorrectly skipped (#40852)\n\n* ouput_attentions in typed kwargs\n\n* correct typing in GenericForTokenClassification\n\n* improve",
    "sha": "8d8459132a7eb85e641b06cdbcc788c0cee714c1",
    "files": [
        {
            "sha": "dd2a3c76c254b66340d5cf253ec0885aec785be1",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d8459132a7eb85e641b06cdbcc788c0cee714c1/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d8459132a7eb85e641b06cdbcc788c0cee714c1/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=8d8459132a7eb85e641b06cdbcc788c0cee714c1",
            "patch": "@@ -262,7 +262,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> TokenClassifierOutput:\n         outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)(\n             input_ids,"
        },
        {
            "sha": "fac305d59ee050d0e9fccb08976963078bec2a45",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d8459132a7eb85e641b06cdbcc788c0cee714c1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d8459132a7eb85e641b06cdbcc788c0cee714c1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8d8459132a7eb85e641b06cdbcc788c0cee714c1",
            "patch": "@@ -230,6 +230,20 @@ def _test_eager_matches_sdpa_inference(\n \n     set_model_tester_for_less_flaky_test(self)\n \n+    def _can_output_attn(model):\n+        parameters = inspect.signature(model.forward).parameters\n+        if \"output_attentions\" in parameters:\n+            return True\n+\n+        kwargs_param = parameters.get(\"kwargs\")\n+        if kwargs_param is not None:\n+            try:\n+                annotation = kwargs_param.annotation.__args__\n+                return \"output_attentions\" in annotation[0].__annotations__\n+            except AttributeError:\n+                return False\n+        return False\n+\n     for model_class in self.all_model_classes:\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         set_config_for_less_flaky_test(config)\n@@ -263,7 +277,7 @@ def _test_eager_matches_sdpa_inference(\n         set_model_for_less_flaky_test(model_eager)\n         set_model_for_less_flaky_test(model_sdpa)\n \n-        can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+        can_output_attn = _can_output_attn(model_sdpa)\n         if not (self.has_attentions and can_output_attn) and output_attentions:\n             self.skipTest(reason=\"Model does not support output_attentions\")\n \n@@ -370,7 +384,7 @@ def _test_eager_matches_sdpa_inference(\n                 if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n                     processed_inputs[\"attention_mask\"] = dummy_attention_mask\n \n-                if self.has_attentions and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters:\n+                if self.has_attentions and _can_output_attn(model_sdpa):\n                     processed_inputs[\"output_attentions\"] = output_attentions\n             if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n                 dummy_mask = torch.ones((self.model_tester.num_masks,))"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 17,
        "deletions": 3
    }
}