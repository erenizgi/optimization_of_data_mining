{
    "author": "jackzhxng",
    "message": "Fix slow static cache export tests (#40261)",
    "sha": "2b59207a7238b04f02a3837e53006265b51199a8",
    "files": [
        {
            "sha": "abdf21d97fd55cd652b340dd334f0200b037e9fb",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -325,14 +325,14 @@ def export(\n                 \"input_ids\": input_ids,\n                 \"cache_position\": cache_position\n                 if cache_position is not None\n-                else torch.arange(input_ids.shape[-1], dtype=torch.long, model=model_device),\n+                else torch.arange(input_ids.shape[-1], dtype=torch.long, device=model_device),\n             }\n         else:  # inputs_embeds\n             input_kwargs = {\n                 \"inputs_embeds\": inputs_embeds,\n                 \"cache_position\": cache_position\n                 if cache_position is not None\n-                else torch.arange(inputs_embeds.shape[1], dtype=torch.long, model=model_device),\n+                else torch.arange(inputs_embeds.shape[1], dtype=torch.long, device=model_device),\n             }\n \n         exported_program = torch.export.export("
        },
        {
            "sha": "e8a6b73ae3022edcf265cbeb3765cb1122231efc",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -463,8 +463,8 @@ def test_export_static_cache(self):\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export(\n-            input_ids=prompt_token_ids,\n-            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n         )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens"
        },
        {
            "sha": "21618c7ff259aa74ca1536fb523d2a4b5b2b74d6",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -368,8 +368,8 @@ def test_export_static_cache(self):\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export(\n-            input_ids=prompt_token_ids,\n-            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n         )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens"
        },
        {
            "sha": "27322f68c50021486b94a7cabfb7dc521b8eeae5",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -354,8 +354,8 @@ def test_export_static_cache(self):\n \n             exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n             exported_program = exportable_module.export(\n-                input_ids=prompt_token_ids,\n-                cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+                input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+                cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n             )\n             ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n                 exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens"
        },
        {
            "sha": "850e96d5e31be7a5f578a1282036a782c096aa00",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -387,8 +387,8 @@ def test_export_static_cache(self):\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export(\n-            input_ids=prompt_token_ids,\n-            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n         )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens"
        },
        {
            "sha": "951fc39e514c6bc9d11a923b3b3236498f8452f6",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -415,8 +415,8 @@ def test_export_static_cache(self):\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export(\n-            input_ids=prompt_token_ids,\n-            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n         )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens"
        },
        {
            "sha": "be6515a60904dcc472a8dbf1bff938c35f25cb19",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -305,8 +305,8 @@ def test_export_static_cache(self):\n             \"2.7.0\"\n         )  # Due to https://github.com/pytorch/pytorch/issues/150994\n         exported_program = exportable_module.export(\n-            input_ids=prompt_token_ids,\n-            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n             strict=strict,\n         )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate("
        },
        {
            "sha": "b532d11afb2f7c6fac2f37632bfe7edd6260805d",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b59207a7238b04f02a3837e53006265b51199a8/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=2b59207a7238b04f02a3837e53006265b51199a8",
            "patch": "@@ -295,8 +295,8 @@ def test_export_static_cache(self):\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export(\n-            input_ids=prompt_token_ids,\n-            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n             strict=strict,\n         )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate("
        }
    ],
    "stats": {
        "total": 32,
        "additions": 16,
        "deletions": 16
    }
}