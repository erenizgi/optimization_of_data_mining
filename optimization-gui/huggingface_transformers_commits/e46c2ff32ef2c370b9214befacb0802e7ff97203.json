{
    "author": "remi-or",
    "message": "Add a safeguard around a flaky test in gemma2 (#41811)\n\n* Fix _compile flag in flex attn integration\n\n* Revert fix and add precaution around test",
    "sha": "e46c2ff32ef2c370b9214befacb0802e7ff97203",
    "files": [
        {
            "sha": "cb118b2267aad14d8f117a99588e8719c1873021",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e46c2ff32ef2c370b9214befacb0802e7ff97203/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e46c2ff32ef2c370b9214befacb0802e7ff97203/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=e46c2ff32ef2c370b9214befacb0802e7ff97203",
            "patch": "@@ -34,6 +34,7 @@\n     require_torch_accelerator,\n     require_torch_large_accelerator,\n     require_torch_large_gpu,\n+    run_test_using_subprocess,\n     slow,\n     torch_device,\n )\n@@ -136,6 +137,9 @@ def test_model_9b_pipeline_bf16(self):\n         self.assertEqual(output[0][0][\"generated_text\"], EXPECTED_TEXTS[0])\n         self.assertEqual(output[1][0][\"generated_text\"], EXPECTED_TEXTS[1])\n \n+    # TODO: run_test_using_subprocess was added because of an issue in torch 2.9, which is already fixed in nightly\n+    # We can remove this once we upgrade to torch 2.10\n+    @run_test_using_subprocess\n     @require_read_token\n     def test_model_2b_pipeline_bf16_flex_attention(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Gemma2 before this PR"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}