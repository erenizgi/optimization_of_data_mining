{
    "author": "jla524",
    "message": "Update llm_optims docs for `sdpa_kernel` (#35481)\n\nupdate: use sdpa_kernel",
    "sha": "44a26c871c89bba3cb959f91dba625e2b19e2979",
    "files": [
        {
            "sha": "37406ea0bef298f1fdbafe2a872d261a6dee2737",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/44a26c871c89bba3cb959f91dba625e2b19e2979/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/44a26c871c89bba3cb959f91dba625e2b19e2979/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=44a26c871c89bba3cb959f91dba625e2b19e2979",
            "patch": "@@ -156,9 +156,11 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n There are a few important things you must do to enable static kv-cache and `torch.compile` with the `StaticCache` method:\n 1. Initialize the [`StaticCache`] instance before using the model for inference. There you can configure parameters like the maximum batch size and sequence length.\n 2. Call `torch.compile` on the model to compile the forward pass with the static kv-cache.\n-3. Set `enable_math=True` in the [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) context manager to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more.\n+3. Use `SDPBackend.MATH` in the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more.\n \n ```py\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n+\n batch_size, seq_length = inputs[\"input_ids\"].shape\n with torch.no_grad():\n     past_key_values = StaticCache(\n@@ -179,7 +181,7 @@ with torch.no_grad():\n     decode_one_tokens = torch.compile(decode_one_tokens, mode=\"reduce-overhead\", fullgraph=True)\n     cache_position = torch.tensor([seq_length + 1], device=torch_device)\n     for _ in range(1, NUM_TOKENS_TO_GENERATE):\n-        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n+        with sdpa_kernel(SDPBackend.MATH):\n             next_token = decode_one_tokens(model, next_token.clone(), None, cache_position, past_key_values)\n             generated_ids[:, cache_position] = next_token.int()\n         cache_position += 1\n@@ -453,18 +455,19 @@ Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and\n > [!TIP]\n > SDPA supports FlashAttention-2 as long as you have the latest PyTorch version installed.\n \n-Use the [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) context manager to explicitly enable or disable any of the three attention algorithms. For example, set `enable_flash=True` to enable FlashAttention.\n+Use the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to explicitly enable or disable any of the four attention algorithms. For example, use `SDPBackend.FLASH_ATTENTION` to enable FlashAttention.\n \n ```py\n import torch\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n     torch_dtype=torch.bfloat16,\n )\n \n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n ```\n "
        }
    ],
    "stats": {
        "total": 11,
        "additions": 7,
        "deletions": 4
    }
}