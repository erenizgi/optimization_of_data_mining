{
    "author": "hellopahe",
    "message": "Add unified logits_to_keep support to LLMClass (#39472)\n\n* add supports for logits_to_keep for qwen25vl and glm4v\n\n* Update relevant modular files",
    "sha": "bda75b4011239d065de84aa3e744b67ebfa7b245",
    "files": [
        {
            "sha": "41f6e34ad2d1d019ba4a03b11200d6f258b969af",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=bda75b4011239d065de84aa3e744b67ebfa7b245",
            "patch": "@@ -1455,6 +1455,7 @@ def forward(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1523,7 +1524,10 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "789744e326427f482d4bb58ddca25976a3e62954",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=bda75b4011239d065de84aa3e744b67ebfa7b245",
            "patch": "@@ -1381,6 +1381,7 @@ def forward(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1449,7 +1450,10 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "13e9ecd5d15b7fefdd68f6021d2674eb52d87178",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=bda75b4011239d065de84aa3e744b67ebfa7b245",
            "patch": "@@ -1448,6 +1448,7 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1520,7 +1521,10 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "746c2f708f25e8644e21852621a579de5ac316ed",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bda75b4011239d065de84aa3e744b67ebfa7b245/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=bda75b4011239d065de84aa3e744b67ebfa7b245",
            "patch": "@@ -723,6 +723,7 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -795,7 +796,10 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 20,
        "deletions": 4
    }
}