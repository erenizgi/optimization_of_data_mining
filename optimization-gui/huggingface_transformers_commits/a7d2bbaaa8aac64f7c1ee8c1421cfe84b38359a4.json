{
    "author": "zshn25",
    "message": "Add EfficientNet Image PreProcessor (#37055)\n\n* added efficientnet image preprocessor but tests fail\n\n* ruff checks pass\n\n* ruff formatted\n\n* properly pass rescale_offset through the functions\n\n* - corrected indentation, ordering of methods\n- reshape test passes when casted to float64\n- equivalence test doesn't pass\n\n* all tests now pass\n- changes order of rescale, normalize acc to slow\n- rescale_offset defaults to False acc to slow\n- resample was causing difference in fast and slow. Changing test to bilinear resolves this difference\n\n* ruff reformat\n\n* F.InterpolationMode.NEAREST_EXACT gives TypeError: Object of type InterpolationMode is not JSON serializable\n\n* fixes offset not being applied when do_rescale and do_normalization are both true\n\n* - using nearest_exact sampling\n- added tests for rescale + normalize\n\n* resolving reviews\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
    "files": [
        {
            "sha": "17a96aeb5ad42c1a74e56052b928cbdea2fff234",
            "filename": "docs/source/en/model_doc/efficientnet.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md?ref=a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
            "patch": "@@ -43,6 +43,11 @@ The original code can be found [here](https://github.com/tensorflow/tpu/tree/mas\n [[autodoc]] EfficientNetImageProcessor\n     - preprocess\n \n+## EfficientNetImageProcessorFast\n+\n+[[autodoc]] EfficientNetImageProcessorFast\n+    - preprocess\n+\n ## EfficientNetModel\n \n [[autodoc]] EfficientNetModel"
        },
        {
            "sha": "b9241b057f1469bc3eebbbacdaf4d1ac633c21bb",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
            "patch": "@@ -65,7 +65,7 @@\n         from torchvision.transforms import InterpolationMode\n \n         pil_torch_interpolation_mapping = {\n-            PILImageResampling.NEAREST: InterpolationMode.NEAREST,\n+            PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT,\n             PILImageResampling.BOX: InterpolationMode.BOX,\n             PILImageResampling.BILINEAR: InterpolationMode.BILINEAR,\n             PILImageResampling.HAMMING: InterpolationMode.HAMMING,"
        },
        {
            "sha": "082a781996429a693f764a5b447b4fac60cab93e",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
            "patch": "@@ -56,7 +56,7 @@\n else:\n     IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n-            (\"align\", (\"EfficientNetImageProcessor\",)),\n+            (\"align\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"aria\", (\"AriaImageProcessor\",)),\n             (\"beit\", (\"BeitImageProcessor\",)),\n             (\"bit\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n@@ -83,7 +83,7 @@\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\",)),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n-            (\"efficientnet\", (\"EfficientNetImageProcessor\",)),\n+            (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"fuyu\", (\"FuyuImageProcessor\",)),"
        },
        {
            "sha": "24d58e81167ec8729d04fa52ce96ebc1737a5982",
            "filename": "src/transformers/models/efficientnet/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fmodels%2Fefficientnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fmodels%2Fefficientnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2F__init__.py?ref=a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_efficientnet import *\n     from .image_processing_efficientnet import *\n+    from .image_processing_efficientnet_fast import *\n     from .modeling_efficientnet import *\n else:\n     import sys"
        },
        {
            "sha": "fb639564014fa7eaa2c3114bce6a23e7e76ecf46",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "added",
            "additions": 226,
            "deletions": 0,
            "changes": 226,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
            "patch": "@@ -0,0 +1,226 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for EfficientNet.\"\"\"\n+\n+from functools import lru_cache\n+from typing import Optional, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class EfficientNetFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    rescale_offset: bool\n+    include_top: bool\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast EfficientNet image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class EfficientNetImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.NEAREST\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 346, \"width\": 346}\n+    crop_size = {\"height\": 289, \"width\": 289}\n+    do_resize = True\n+    do_center_crop = False\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    rescale_offset = False\n+    do_normalize = True\n+    include_top = True\n+    valid_kwargs = EfficientNetFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[EfficientNetFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def rescale(\n+        self,\n+        image: \"torch.Tensor\",\n+        scale: float,\n+        offset: Optional[bool] = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Rescale an image by a scale factor.\n+\n+        If `offset` is `True`, the image has its values rescaled by `scale` and then offset by 1. If `scale` is\n+        1/127.5, the image is rescaled between [-1, 1].\n+            image = image * scale - 1\n+\n+        If `offset` is `False`, and `scale` is 1/255, the image is rescaled between [0, 1].\n+            image = image * scale\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to rescale.\n+            scale (`float`):\n+                The scaling factor to rescale pixel values by.\n+            offset (`bool`, *optional*):\n+                Whether to scale the image in both negative and positive directions.\n+\n+        Returns:\n+            `torch.Tensor`: The rescaled image.\n+        \"\"\"\n+\n+        rescaled_image = image * scale\n+\n+        if offset:\n+            rescaled_image -= 1\n+\n+        return rescaled_image\n+\n+    @lru_cache(maxsize=10)\n+    def _fuse_mean_std_and_rescale_factor(\n+        self,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        rescale_offset: Optional[bool] = False,\n+    ) -> tuple:\n+        if do_rescale and do_normalize and not rescale_offset:\n+            # Fused rescale and normalize\n+            image_mean = torch.tensor(image_mean, device=device) * (1.0 / rescale_factor)\n+            image_std = torch.tensor(image_std, device=device) * (1.0 / rescale_factor)\n+            do_rescale = False\n+        return image_mean, image_std, do_rescale\n+\n+    def rescale_and_normalize(\n+        self,\n+        images: \"torch.Tensor\",\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, list[float]],\n+        image_std: Union[float, list[float]],\n+        rescale_offset: bool = False,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Rescale and normalize images.\n+        \"\"\"\n+        image_mean, image_std, do_rescale = self._fuse_mean_std_and_rescale_factor(\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            device=images.device,\n+            rescale_offset=rescale_offset,\n+        )\n+        # if/elif as we use fused rescale and normalize if both are set to True\n+        if do_rescale:\n+            images = self.rescale(images, rescale_factor, rescale_offset)\n+        if do_normalize:\n+            images = self.normalize(images.to(dtype=torch.float32), image_mean, image_std)\n+\n+        return images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        rescale_offset: bool,\n+        do_normalize: bool,\n+        include_top: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std, rescale_offset\n+            )\n+            if include_top:\n+                stacked_images = self.normalize(stacked_images, 0, image_std)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        rescale_offset (`bool`, *optional*, defaults to `self.rescale_offset`):\n+            Whether to rescale the image between [-max_range/2, scale_range/2] instead of [0, scale_range].\n+        include_top (`bool`, *optional*, defaults to `self.include_top`):\n+            Normalize the image again with the standard deviation only for image classification if set to True.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[EfficientNetFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+\n+__all__ = [\"EfficientNetImageProcessorFast\"]"
        },
        {
            "sha": "cb8fc8d92209c5246b93e3e6c941c5d4ab76d118",
            "filename": "tests/models/efficientnet/test_image_processing_efficientnet.py",
            "status": "modified",
            "additions": 87,
            "deletions": 19,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/tests%2Fmodels%2Fefficientnet%2Ftest_image_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4/tests%2Fmodels%2Fefficientnet%2Ftest_image_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_image_processing_efficientnet.py?ref=a7d2bbaaa8aac64f7c1ee8c1421cfe84b38359a4",
            "patch": "@@ -17,15 +17,26 @@\n \n import numpy as np\n \n+from transformers.image_utils import PILImageResampling\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import (\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_vision_available,\n+)\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_vision_available():\n     from transformers import EfficientNetImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import EfficientNetImageProcessorFast\n+\n \n class EfficientNetImageProcessorTester:\n     def __init__(\n@@ -41,6 +52,10 @@ def __init__(\n         do_normalize=True,\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n+        do_rescale=True,\n+        rescale_offset=True,\n+        rescale_factor=1 / 127.5,\n+        resample=PILImageResampling.BILINEAR,  # NEAREST is too different between PIL and torchvision\n     ):\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n@@ -54,6 +69,7 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n+        self.resample = resample\n \n     def prepare_image_processor_dict(self):\n         return {\n@@ -62,6 +78,7 @@ def prepare_image_processor_dict(self):\n             \"do_normalize\": self.do_normalize,\n             \"do_resize\": self.do_resize,\n             \"size\": self.size,\n+            \"resample\": self.resample,\n         }\n \n     def expected_output_image_shape(self, images):\n@@ -83,6 +100,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class EfficientNetImageProcessorTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = EfficientNetImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = EfficientNetImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -93,30 +111,80 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_rescale(self):\n         # EfficientNet optionally rescales between -1 and 1 instead of the usual 0 and 1\n         image = np.arange(0, 256, 1, dtype=np.uint8).reshape(1, 8, 32)\n \n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-\n-        rescaled_image = image_processor.rescale(image, scale=1 / 127.5)\n-        expected_image = (image * (1 / 127.5)).astype(np.float32) - 1\n-        self.assertTrue(np.allclose(rescaled_image, expected_image))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            if image_processing_class == EfficientNetImageProcessorFast:\n+                image = torch.from_numpy(image)\n+\n+                # Scale between [-1, 1] with rescale_factor 1/127.5 and rescale_offset=True\n+                rescaled_image = image_processor.rescale(image, scale=1 / 127.5, offset=True)\n+                expected_image = (image * (1 / 127.5)) - 1\n+                self.assertTrue(torch.allclose(rescaled_image, expected_image))\n+\n+                # Scale between [0, 1] with rescale_factor 1/255 and rescale_offset=True\n+                rescaled_image = image_processor.rescale(image, scale=1 / 255, offset=False)\n+                expected_image = image / 255.0\n+                self.assertTrue(torch.allclose(rescaled_image, expected_image))\n+\n+            else:\n+                rescaled_image = image_processor.rescale(image, scale=1 / 127.5, dtype=np.float64)\n+                expected_image = (image * (1 / 127.5)).astype(np.float64) - 1\n+                self.assertTrue(np.allclose(rescaled_image, expected_image))\n+\n+                rescaled_image = image_processor.rescale(image, scale=1 / 255, offset=False, dtype=np.float64)\n+                expected_image = (image / 255.0).astype(np.float64)\n+                self.assertTrue(np.allclose(rescaled_image, expected_image))\n+\n+    @require_vision\n+    @require_torch\n+    def test_rescale_normalize(self):\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        image = torch.arange(0, 256, 1, dtype=torch.uint8).reshape(1, 8, 32).repeat(3, 1, 1)\n+        image_mean_0 = (0.0, 0.0, 0.0)\n+        image_std_0 = (1.0, 1.0, 1.0)\n+        image_mean_1 = (0.5, 0.5, 0.5)\n+        image_std_1 = (0.5, 0.5, 0.5)\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # Rescale between [-1, 1] with rescale_factor=1/127.5 and rescale_offset=True. Then normalize\n+        rescaled_normalized = image_processor_fast.rescale_and_normalize(\n+            image, True, 1 / 127.5, True, image_mean_0, image_std_0, True\n+        )\n+        expected_image = (image * (1 / 127.5)) - 1\n+        expected_image = (expected_image - torch.tensor(image_mean_0).view(3, 1, 1)) / torch.tensor(image_std_0).view(\n+            3, 1, 1\n+        )\n+        self.assertTrue(torch.allclose(rescaled_normalized, expected_image, rtol=1e-3))\n \n-        rescaled_image = image_processor.rescale(image, scale=1 / 255, offset=False)\n-        expected_image = (image / 255.0).astype(np.float32)\n-        self.assertTrue(np.allclose(rescaled_image, expected_image))\n+        # Rescale between [0, 1] with rescale_factor=1/255 and rescale_offset=False. Then normalize\n+        rescaled_normalized = image_processor_fast.rescale_and_normalize(\n+            image, True, 1 / 255, True, image_mean_1, image_std_1, False\n+        )\n+        expected_image = image * (1 / 255.0)\n+        expected_image = (expected_image - torch.tensor(image_mean_1).view(3, 1, 1)) / torch.tensor(image_std_1).view(\n+            3, 1, 1\n+        )\n+        self.assertTrue(torch.allclose(rescaled_normalized, expected_image, rtol=1e-3))"
        }
    ],
    "stats": {
        "total": 344,
        "additions": 322,
        "deletions": 22
    }
}