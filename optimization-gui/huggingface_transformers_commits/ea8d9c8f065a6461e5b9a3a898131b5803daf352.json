{
    "author": "manueldeprada",
    "message": "ðŸš¨ Remove DoLa decoding strategy (#40082)\n\n* remove dola generation strategy\n\n* add fast test",
    "sha": "ea8d9c8f065a6461e5b9a3a898131b5803daf352",
    "files": [
        {
            "sha": "37c90ee43fa58fc671b0cea990a2d2ab19447515",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -248,66 +248,6 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n 'Hugging Face is an open-source company that provides a platform for building and deploying AI models.\\nHugging Face is an open-source company that provides a platform for building and deploying AI models. The platform allows developers to build and deploy AI models, as well as collaborate with other developers.\\nHugging Face was founded in 2019 by Thibault Wittemberg and ClÃ©ment Delangue. The company is based in Paris, France.\\nHugging Face has'\n ```\n \n-### DoLa\n-\n-[Decoding by Contrasting Layers (DoLa)](https://hf.co/papers/2309.03883) is a contrastive decoding strategy for improving factuality and reducing hallucination. This strategy works by contrasting the logit differences between the final and early layers. As a result, factual knowledge localized to particular layers are amplified. DoLa is not recommended for smaller models like GPT-2.\n-\n-Enable DoLa with the following parameters.\n-\n-- `dola_layers` are the candidate layers to be contrasted with the final layer. It can be a string (`low` or `high`) to contrast the lower or higher parts of a layer. `high` is recommended for short-answer tasks like TruthfulQA. `low` is recommended for long-answer reasoning tasks like GSM8K, StrategyQA, FACTOR, and VicunaQA.\n-\n-  When a model has tied word embeddings, layer 0 is skipped and it begins from layer 2.\n-\n-  It can also be a list of integers that represent the layer indices between 0 and the total number of layers. Layer 0 is the word embedding, 1 is the first transformer layer, and so on. Refer to the table below for the range of layer indices depending on the number of model layers.\n-\n-  | layers | low | high |\n-  |---|---|---|\n-  | > 40 | (0, 20, 2) | (N - 20, N, 2) |\n-  | <= 40 | range(0, N // 2, 2) | range(N // 2, N, 2) |\n-\n-- `repetition_penalty` reduces repetition and it is recommended to set it to 1.2.\n-\n-<hfoptions id=\"dola\">\n-<hfoption id=\"contrast higher layers\">\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n-\n-device = infer_device()\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)\n-inputs = tokenizer(\"What is the highest peak in the world??\", return_tensors=\"pt\").to(device)\n-\n-outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=\"high\", do_sample=False)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-\" Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak, lying almost 9.5 kilometers above the sea level and the tallest mountain from 19,036.91 ft. The mountain was\"\n-```\n-\n-</hfoption>\n-<hfoption id=\"contrast specific layers\">\n-\n-Contrast layers 18 and 20 with the final layer.\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n-\n-device = infer_device()\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)\n-inputs = tokenizer(\"What is the highest peak in the world?\", return_tensors=\"pt\").to(device)\n-\n-outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=[18,20], do_sample=False, repetition_penalty=1.2)\n-tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n-\" Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak above sea level and it rises to an incredible height of 29,028 feet above the ocean. Its summit is over a mile taller than Mt\"\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n ### Diverse beam search\n \n [Diverse beam search](https://hf.co/papers/1610.02424) is a variant of beam search that produces more diverse output candidates to choose from. This strategy measures the dissimilarity of sequences and a penalty is applied if sequences are too similar. To avoid high computation costs, the number of beams is divided into groups."
        },
        {
            "sha": "d1e1b441f67f73c404ce4fd07aa173ccf69af5f7",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -932,6 +932,8 @@ def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         if layer_idx < len(self.layers):\n             return self.layers[layer_idx].keys, self.layers[layer_idx].values\n+        # elif len(self.layers) == 0:\n+        #     return None, None\n         else:\n             raise KeyError(\n                 f\"Cache only has {len(self.layers)} layers, attempted to access layer with index {layer_idx}\""
        },
        {
            "sha": "82332b9e7809826d16de49b58bb95e2f19050c5b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 17,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -93,7 +93,6 @@ class GenerationConfig(PushToHubMixin):\n         - *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\n         - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\n         - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\n-        - *dola decoding* if `dola_layers` is passed to `.generate()`\n \n     To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n \n@@ -141,15 +140,6 @@ class GenerationConfig(PushToHubMixin):\n             [this paper](https://huggingface.co/papers/1610.02424) for more details.\n         penalty_alpha (`float`, *optional*):\n             The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n-        dola_layers (`str` or `list[int]`, *optional*):\n-            The layers to use for DoLa decoding. If `None`, DoLa decoding is not used. If a string, it must\n-            be one of \"low\" or \"high\", which means using the lower part or higher part of the model layers, respectively.\n-            \"low\" means the first half of the layers up to the first 20 layers, and \"high\" means the last half of the\n-            layers up to the last 20 layers.\n-            If a list of integers, it must contain the indices of the layers to use for candidate premature layers in DoLa.\n-            The 0-th layer is the word embedding layer of the model. Set to `'low'` to improve long-answer reasoning tasks,\n-            `'high'` to improve short-answer tasks. Check the [documentation](https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md)\n-            or [the paper](https://huggingface.co/papers/2309.03883) for more details.\n \n         > Parameters that control the cache\n \n@@ -542,6 +532,7 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n                 )\n \n         # DoLa generation may extend some generation modes\n+        # TODO joao, manuel: remove this in v4.62.0\n         if self.dola_layers is not None:\n             if generation_mode in (\"greedy_search\", \"sample\"):\n                 generation_mode = GenerationMode.DOLA_GENERATION\n@@ -658,13 +649,6 @@ def validate(self, strict=False):\n                 minor_issues[\"constraints\"] = single_beam_wrong_parameter_msg.format(\n                     flag_name=\"constraints\", flag_value=self.constraints\n                 )\n-            # DoLa generation needs num_beams == 1\n-            if self.dola_layers is not None and (self.repetition_penalty is None or self.repetition_penalty < 1.2):\n-                minor_issues[\"repetition_penalty\"] = (\n-                    \"`dola_layers` is set to trigger DoLa decoding, but `repetition_penalty` is set to a value of \"\n-                    f\"{self.repetition_penalty}, which could induce unwanted repetition. The recommended value for \"\n-                    \"DoLa decoding is `repetition_penalty>=1.2`.\",\n-                )\n \n         # 2.3. detect incorrect parameterization specific to advanced beam modes\n         else:"
        },
        {
            "sha": "3e387c0808d63d4bbe8735d2bcd8b46c68976527",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 301,
            "changes": 315,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -20,13 +20,11 @@\n from dataclasses import dataclass\n from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n \n-import numpy as np\n import torch\n import torch.distributed as dist\n from huggingface_hub import file_exists\n from packaging import version\n from torch import nn\n-from torch.nn import functional as F\n \n from ..cache_utils import (\n     Cache,\n@@ -2494,26 +2492,25 @@ def generate(\n                 streamer=streamer,\n                 **model_kwargs,\n             )\n+        # TODO joao, manuel: remove this in v4.62.0\n         elif generation_mode == GenerationMode.DOLA_GENERATION:\n+            logger.warning_once(\n+                \"DoLa generation was moved to a `custom_generate` repo: https://hf.co/transformers-community/dola. \"\n+                \"To prevent loss of backward compatibility, add `custom_generate='transformers-community/dola'` \"\n+                \"to your `generate` call before v4.62.0.\"\n+            )\n             if not trust_remote_code:\n-                logger.warning_once(\n-                    \"DoLa Decoding is scheduled to be moved to a `custom_generate` repository in v4.55.0. \"\n-                    \"To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\"\n-                )\n-            if self._is_stateful:\n-                # DoLa decoding was not designed for stateful models, and would require some changes\n                 raise ValueError(\n-                    f\"dola decoding is not supported with stateful models, such as {self.__class__.__name__}\"\n+                    \"DoLa generation requires `trust_remote_code=True` in your `generate` call, since \"\n+                    \"it loads https://hf.co/transformers-community/dola.\"\n                 )\n-            result = self._dola_decoding(\n-                input_ids,\n-                dola_layers=generation_config.dola_layers,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n+            return GenerationMixin.generate(\n+                self,\n+                inputs,\n+                custom_generate=\"transformers-community/dola\",\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n-                **model_kwargs,\n+                trust_remote_code=trust_remote_code,\n+                **kwargs,\n             )\n \n         elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n@@ -2768,218 +2765,6 @@ def heal_tokens(\n \n         return input_ids\n \n-    def _dola_decoding(\n-        self,\n-        input_ids: torch.LongTensor,\n-        dola_layers: Union[str, list[int]],\n-        logits_processor: LogitsProcessorList,\n-        stopping_criteria: StoppingCriteriaList,\n-        generation_config: GenerationConfig,\n-        synced_gpus: bool,\n-        streamer: \"BaseStreamer\",\n-        **model_kwargs,\n-    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n-        r\"\"\"\n-        Generates sequences of token ids for models with a language modeling head using **dola decoding** and can be\n-        used for decoder-only text models.\n-        The method is based on the paper \"DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language\n-        Models\" (https://huggingface.co/papers/2309.03883) in ICLR 2024.\n-\n-        Parameters:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                The sequence used as a prompt for the generation.\n-            dola_layers (`Union[str, list[int]]`):\n-                The candidate layers used in contrasting layers of DoLa. It can be either 1) 'low' or 'high', which\n-                means the lower part or higher part of the model layers, respectively, or 2) a list of layer indices\n-                to be used for candidate layers. The 0-th layer is the word embedding layer of the model.\n-            logits_processor (`LogitsProcessorList`):\n-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n-                used to modify the prediction scores of the language modeling head applied at each generation step.\n-            stopping_criteria (`StoppingCriteriaList`, *optional*):\n-                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n-                used to tell if the generation loop should stop.\n-            generation_config ([`~generation.GenerationConfig`]):\n-                The generation configuration to be used as parametrization of the decoding method.\n-            synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n-                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n-            streamer (`BaseStreamer`, *optional*):\n-                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n-                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n-            model_kwargs:\n-                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n-                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n-\n-        Return:\n-            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`]\n-            or `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n-            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n-            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n-            `model.config.is_encoder_decoder=True`.\n-        \"\"\"\n-\n-        if self.config.is_encoder_decoder:\n-            raise ValueError(\"DoLa decoding is only available for decoder-only models.\")\n-        # init values\n-\n-        pad_token_id = generation_config._pad_token_tensor\n-        output_attentions = generation_config.output_attentions\n-        output_hidden_states = generation_config.output_hidden_states\n-        output_scores = generation_config.output_scores\n-        output_logits = generation_config.output_logits\n-        return_dict_in_generate = generation_config.return_dict_in_generate\n-        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n-        do_sample = generation_config.do_sample\n-\n-        # init attention / hidden states / scores tuples\n-        scores = () if (return_dict_in_generate and output_scores) else None\n-        raw_logits = () if (return_dict_in_generate and output_logits) else None\n-        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n-        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n-        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n-\n-        # keep track of which sequences are already finished\n-        batch_size, cur_length = input_ids.shape[:2]\n-        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(cur_length, input_ids.device, model_kwargs)\n-\n-        this_peer_finished = False\n-\n-        # prepare layers for DoLa decoding\n-        final_layer = self.config.get_text_config().num_hidden_layers\n-        # if the model has tied word embeddings, we skip the word embeddings (0-th) layer and start from the 2nd layer,\n-        # as the early exit from word embeddings will become identity function\n-        # if the model is really shallow (<=2 layers), we use the 1st layer if it's not the final layer and the 0-th\n-        # layer otherwise. Notice that DoLa does not help shallow models much.\n-        if not self.config.tie_word_embeddings:\n-            start_layer = 0\n-        elif final_layer > 2:\n-            start_layer = 2\n-        elif final_layer == 2:\n-            start_layer = 1\n-        else:\n-            start_layer = 0\n-\n-        # For `N`-layer models with `N <= 40` layers, the layers of `range(0, N // 2, 2)` and `range(N // 2, N, 2)`\n-        # are used for `'low'` and `'high'` layers, respectively.\n-        # For models with `N > 40` layers, the layers of `range(0, 20, 2)` and `range(N - 20, N, 2)` are used for\n-        # `'low'` and `'high'` layers, respectively.\n-        if isinstance(dola_layers, str) and dola_layers == \"low\":\n-            if start_layer == final_layer // 2:\n-                candidate_premature_layers = [start_layer]\n-            else:\n-                candidate_premature_layers = (\n-                    list(range(start_layer, final_layer // 2, 2))\n-                    if final_layer <= 40\n-                    else list(range(start_layer, 20, 2))\n-                )\n-        elif isinstance(dola_layers, str) and dola_layers == \"high\":\n-            candidate_premature_layers = (\n-                list(range(final_layer // 2, final_layer, 2))\n-                if final_layer <= 40\n-                else list(range(final_layer - 20, final_layer, 2))\n-            )\n-        # Set the `dola_layers` to a list of integers for layer indices to contrast manually specified layers.\n-        elif isinstance(dola_layers, list):\n-            candidate_premature_layers = [i for i in dola_layers if i < final_layer]\n-        else:\n-            raise ValueError(\"dola_layers must be either 'low', 'high' or a list of integers.\")\n-\n-        lm_head = self.get_output_embeddings()\n-        if lm_head is None:\n-            raise ValueError(\"DoLa is not supported for models that don't have output embeddings.\")\n-\n-        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            # prepare model inputs\n-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n-\n-            # forward pass to get next token\n-            outputs = self(\n-                **model_inputs,\n-                return_dict=True,\n-                output_attentions=output_attentions,\n-                output_hidden_states=True,\n-            )\n-\n-            # .float() is needed to retain precision for later logits manipulations\n-            final_layer_next_token_logits = outputs.logits[:, -1, :].detach().to(copy=True, dtype=torch.float32)\n-            final_logits = outputs.logits[:, -1, :].float()\n-            candidate_premature_logits = {}\n-            for candidate_premature_layer in candidate_premature_layers:\n-                candidate_premature_logits[candidate_premature_layer] = lm_head(\n-                    outputs.hidden_states[candidate_premature_layer][:, -1, :]\n-                ).to(final_logits.device)\n-\n-            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n-            if synced_gpus and this_peer_finished:\n-                continue\n-\n-            next_token_logits = _dola_select_contrast(\n-                candidate_premature_layers, candidate_premature_logits, final_logits\n-            )\n-            next_token_logits = next_token_logits.to(input_ids.device)\n-            # pre-process distribution\n-            next_token_scores = logits_processor(input_ids, next_token_logits)\n-\n-            # Store scores, attentions and hidden_states when required\n-            if return_dict_in_generate:\n-                if output_scores:\n-                    scores += (next_token_scores,)\n-                if output_logits:\n-                    raw_logits += (final_layer_next_token_logits,)\n-                if output_attentions:\n-                    decoder_attentions += (\n-                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n-                    )\n-                    if self.config.is_encoder_decoder:\n-                        cross_attentions += (outputs.cross_attentions,)\n-\n-                if output_hidden_states:\n-                    decoder_hidden_states += (\n-                        (outputs.decoder_hidden_states,)\n-                        if self.config.is_encoder_decoder\n-                        else (outputs.hidden_states,)\n-                    )\n-\n-            if do_sample:  # sample\n-                probs = nn.functional.softmax(next_token_scores, dim=-1)\n-                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n-            else:  # argmax\n-                next_tokens = torch.argmax(next_token_scores, dim=-1)\n-\n-            # finished sentences should have their next token be a padding token\n-            if has_eos_stopping_criteria:\n-                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n-\n-            # update generated ids, model inputs, and length for next step\n-            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n-            if streamer is not None:\n-                streamer.put(next_tokens.cpu())\n-\n-            # stop when each sentence is finished\n-            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n-            this_peer_finished = unfinished_sequences.max() == 0\n-\n-        if streamer is not None:\n-            streamer.end()\n-\n-        if return_dict_in_generate:\n-            return GenerateDecoderOnlyOutput(\n-                sequences=input_ids,\n-                scores=scores,\n-                logits=raw_logits,\n-                attentions=decoder_attentions,\n-                hidden_states=decoder_hidden_states,\n-                past_key_values=model_kwargs.get(\"past_key_values\"),\n-            )\n-        else:\n-            return input_ids\n-\n     @torch.no_grad()\n     def _contrastive_search(\n         self,\n@@ -5166,75 +4951,3 @@ def _concat(data):\n \n     # Return a new object of the inferred class with the concatenated attributes\n     return model_output_cls(**concatenated_data)\n-\n-\n-def _relative_top_filter(\n-    scores: torch.FloatTensor,\n-    baseline_scores: torch.FloatTensor,\n-    relative_top: float = 0.1,\n-    filter_value: float = -float(\"Inf\"),\n-    base_filter_value=-1e-3,\n-    min_tokens_to_keep: int = 1,\n-) -> torch.FloatTensor:\n-    \"\"\"\n-    Reference: https://github.com/XiangLi1999/ContrastiveDecoding/blob/170e9142e92159c1237d731e240f5eb14aabf428/transformers/src/transformers/generation_logits_process.py#L235\n-    Apply filtering to only keep tokens with a probability above a certain threshold. The threshold is defined as `relative_top` * max probability in the distribution.\n-    \"\"\"\n-    scores_normalized = scores.log_softmax(dim=-1)\n-    baseline_scores_normalized = baseline_scores.log_softmax(dim=-1)\n-    sorted_logits, sorted_indices = torch.sort(scores_normalized, descending=True)\n-    min_thresh = sorted_logits[..., min_tokens_to_keep - 1]\n-    probs_max = torch.max(scores_normalized, dim=-1).values\n-    probs_thresh = probs_max + np.log(relative_top)\n-    probs_thresh = torch.min(min_thresh, probs_thresh)\n-    probs_thresh = probs_thresh.unsqueeze(-1)\n-    baseline_scores_normalized[scores_normalized < probs_thresh] = base_filter_value\n-    scores_normalized[scores_normalized < probs_thresh] = filter_value\n-    return scores_normalized, baseline_scores_normalized\n-\n-\n-def _dola_select_contrast(\n-    candidate_premature_layers: list[int],\n-    candidate_premature_logits: dict[int, torch.FloatTensor],\n-    final_logits: torch.FloatTensor,\n-) -> torch.FloatTensor:\n-    if len(candidate_premature_layers) == 1:\n-        base_logits = candidate_premature_logits[candidate_premature_layers[0]]\n-        final_logits, base_logits = _relative_top_filter(final_logits, base_logits)\n-        logits = final_logits - base_logits\n-        return logits\n-\n-    # 1. Stacking all premature_layers into a new dimension\n-    stacked_premature_layers = torch.stack([candidate_premature_logits[i] for i in candidate_premature_layers], dim=0)\n-\n-    # 2. Calculate the softmax values for mature_layer and all premature_layers\n-    # shape: (batch_size, vocab_size)\n-    softmax_mature_layer = F.softmax(final_logits, dim=-1)\n-    # shape: (num_premature_layers, batch_size, vocab_size)\n-    softmax_premature_layers = F.softmax(stacked_premature_layers, dim=-1)\n-\n-    # 3. Calculate the average distribution\n-    # shape: (num_premature_layers, batch_size, vocab_size)\n-    avg_dist = 0.5 * (softmax_mature_layer[None, :, :] + softmax_premature_layers)\n-\n-    # 4. Calculate log-softmax for the KL divergence\n-    # shape: (batch_size, vocab_size)\n-    log_softmax_mature_layer = F.log_softmax(final_logits, dim=-1)\n-    # shape: (num_premature_layers, batch_size, vocab_size)\n-    log_softmax_premature_layers = F.log_softmax(stacked_premature_layers, dim=-1)\n-\n-    # 5. Calculate the KL divergences and then the JS divergences\n-    # shape: (num_premature_layers, batch_size)\n-    kl1 = F.kl_div(log_softmax_mature_layer[None, :, :], avg_dist, reduction=\"none\").mean(-1)\n-    # shape: (num_premature_layers, batch_size)\n-    kl2 = F.kl_div(log_softmax_premature_layers, avg_dist, reduction=\"none\").mean(-1)\n-    js_divs = 0.5 * (kl1 + kl2)  # shape: (num_premature_layers, batch_size)\n-\n-    # 6. Reduce the batchmean\n-    js_divs = js_divs.mean(-1)  # shape: (num_premature_layers,)\n-    premature_layer = candidate_premature_layers[int(js_divs.argmax().item())]\n-\n-    base_logits = candidate_premature_logits[premature_layer]\n-    final_logits, base_logits = _relative_top_filter(final_logits, base_logits)\n-    logits = final_logits - base_logits\n-    return logits"
        },
        {
            "sha": "92d770ff10d2f3cce86c17b2d65c16b813fdd5ce",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -1233,60 +1233,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             for output in (output_greedy, output_prompt_lookup):\n                 self._check_generate_outputs(output, model.config, use_cache=True)\n \n-    @pytest.mark.generate\n-    def test_dola_decoding_sample(self):\n-        # TODO (joao): investigate skips, try to reduce incompatibilities\n-        for model_class in self.all_generative_model_classes:\n-            if model_class._is_stateful:\n-                self.skipTest(reason=\"Stateful models don't support DoLa decoding\")\n-\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"reformer\"]):\n-                self.skipTest(\"Skip Reformer as the lm_head input size is 2 * hidden size, adopted from Rev Nets.\")\n-\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"marian\", \"mbart\", \"pegasus\"]):\n-                self.skipTest(\"DoLa is not supported for models that don't return layerwise hidden states\")\n-\n-            if any(model_name == model_class.__name__ for model_name in [\"LlavaNextVideoForConditionalGeneration\"]):\n-                self.skipTest(f\"DoLa is failing for {model_class.__name__}\")\n-\n-            # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            # force eager attention to support output attentions\n-            if self.has_attentions:\n-                config._attn_implementation = \"eager\"\n-\n-            # Encoder-decoder models are not supported\n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n-                self.skipTest(\"DoLa is not supported for encoder-decoder models\")\n-            config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n-\n-            if model.get_output_embeddings() is None:\n-                self.skipTest(\"DoLa is not supported for models that don't have output embeddings\")\n-\n-            logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n-\n-            # Sets dola generation arguments such that:\n-            # a) no EOS is generated, to ensure generation doesn't break early\n-            # b) there are at least two forward passes in the main model, to ensure the input preparation of\n-            #    the main model is correct\n-            generation_kwargs = {\n-                \"eos_token_id\": -1,  # see a)\n-                \"max_new_tokens\": 4,  # see b)\n-                \"num_beams\": 1,\n-                \"do_sample\": True,\n-                \"output_scores\": True,\n-                \"output_logits\": True,\n-                \"output_hidden_states\": True,\n-                \"output_attentions\": self.has_attentions,\n-                \"return_dict_in_generate\": True,\n-                \"use_cache\": getattr(config, \"use_cache\", False),  # Some models don't support the cache\n-                \"dola_layers\": \"low\",\n-            }\n-            output_dola = model.generate(**generation_kwargs, **logits_processor_kwargs, **inputs_dict)\n-            self._check_generate_outputs(output_dola, model.config, use_cache=getattr(config, \"use_cache\", False))\n-\n     @pytest.mark.generate\n     def test_assisted_decoding_sample(self):\n         # In this test we don't check assisted vs non-assisted output -- seeded assisted decoding with sample will not\n@@ -5166,6 +5112,40 @@ def test_generate_custom_cache_position(self):\n                     )\n                 )\n \n+    @pytest.mark.generate\n+    def test_dola_hub_runs(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\",\n+            device_map=torch_device,\n+            attn_implementation=\"eager\",\n+        ).eval()\n+        model_inputs = {\n+            \"input_ids\": torch.tensor([[1, 22557, 28725, 1526, 28808]], device=torch_device),\n+            \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1]], device=torch_device),\n+        }\n+        # Sets dola generation arguments such that:\n+        # a) no EOS is generated, to ensure generation doesn't break early\n+        # b) there are at least two forward passes in the main model, to ensure the input preparation of\n+        #    the main model is correct\n+        generation_kwargs = {\n+            \"eos_token_id\": -1,  # see a)\n+            \"max_new_tokens\": 4,  # see b)\n+            \"num_beams\": 1,\n+            \"do_sample\": True,\n+            \"output_scores\": True,\n+            \"output_logits\": True,\n+            \"output_hidden_states\": True,\n+            \"output_attentions\": True,\n+            \"return_dict_in_generate\": True,\n+            \"use_cache\": True,\n+            \"dola_layers\": \"low\",\n+            \"trust_remote_code\": True,\n+            \"custom_generate\": \"transformers-community/dola\",\n+        }\n+        torch.manual_seed(0)\n+        output_dola = model.generate(**generation_kwargs, **model_inputs)\n+        self.assertEqual(output_dola.sequences.shape, (1, 9))\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        },
        {
            "sha": "a392b3949d8c20e8f1436162a0b3b2c42011d01b",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -228,10 +228,6 @@ def test_feed_forward_chunking(self):\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(reason=\"Unstable test\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n     @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n     def test_generate_with_static_cache(self):\n         pass"
        },
        {
            "sha": "fa29ec4df6e79174a33ee0d674509fa08ec5ecef",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -247,11 +247,6 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n     def test_assisted_decoding_sample(self):\n         pass\n \n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"CSM does not support Dola decoding.\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n     @pytest.mark.generate\n     @unittest.skip(reason=\"CSM does not support beam search.\")\n     def test_beam_sample_generate(self):"
        },
        {
            "sha": "9b41e027770fe331ce657233b6d6538639859a61",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -260,10 +260,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n     def test_assisted_decoding_sample(self):\n         pass\n \n-    @unittest.skip(\"DeepseekV3 is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n     @unittest.skip(\"DeepseekV3 doesn't support contrastive generation\")\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n         pass"
        },
        {
            "sha": "5f51649619fe87e0f912a929423ce4b80b4d36bc",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -241,7 +241,6 @@ def skip_non_greedy_generate(self):\n             \"test_constrained_beam\",\n             \"test_contrastive\",\n             \"test_assisted\",\n-            \"test_dola\",\n             \"test_prompt_lookup\",\n             \"test_model_parallel_beam_search\",\n             \"test_generate_without_input_ids\","
        },
        {
            "sha": "284cd4c19909d46b8c715c529ec2b1cd9a12cae4",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -486,6 +486,7 @@ def test_export_static_cache(self):\n \n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     def test_model_2b_bf16_dola(self):\n         model_id = \"google/gemma-2b\"\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n@@ -509,7 +510,12 @@ def test_model_2b_bf16_dola(self):\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(\n-            **inputs, max_new_tokens=20, do_sample=False, dola_layers=\"low\", repetition_penalty=1.2\n+            **inputs,\n+            max_new_tokens=20,\n+            do_sample=False,\n+            dola_layers=\"low\",\n+            repetition_penalty=1.2,\n+            trust_remote_code=True,\n         )\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(output_text, EXPECTED_TEXTS)"
        },
        {
            "sha": "af0918596fed9325a9341c1f534f58549aedcbe9",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -420,13 +420,6 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n     def test_contrastive_generate_low_memory(self):\n         pass\n \n-    @pytest.mark.generate\n-    @unittest.skip(\n-        \"Gemma3n has a special shape for hidden states (due to per-layer projs) which is not compatible with dola decoding\"\n-    )\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n     @pytest.mark.generate\n     @unittest.skip(\"Gemma3n does not support QuantizedCache as it performs cache manipulation in the forward pass\")\n     def test_generate_with_quant_cache(self):"
        },
        {
            "sha": "71b7b845f6f7d1bd7da980cdfde66515fc1fe47c",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -493,10 +493,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"GIT has pixel values as additional input\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n \n @require_torch\n @require_vision"
        },
        {
            "sha": "7a27474b1b492f34f5d75a032747dd62ad100a63",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -349,7 +349,6 @@ def test_model_3b_logits(self):\n \n     @slow\n     def test_model_3b_generation(self):\n-        # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n         # fmt: off\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {"
        },
        {
            "sha": "2186985408b48c353b8a41fc87972afe40b79180",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -352,7 +352,6 @@ def test_model_3b_logits(self):\n \n     @slow\n     def test_model_3b_generation(self):\n-        # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n         # fmt: off\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {"
        },
        {
            "sha": "03458d53a37b87fbdd5a40245614bd1a2c27e241",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -227,6 +227,7 @@ def test_model_7b_logits(self):\n             )\n         )\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     # TODO: check why we have the following strange situation.\n     # without running in subprocess, this test causes subsequent tests failing with `RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0!`\n     @run_test_using_subprocess"
        },
        {
            "sha": "4a62f2bb1a9b1b738a0522b1b255e2b8d57f97ac",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -172,6 +172,7 @@ def test_model_7b_generation(self):\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     @slow\n     def test_model_7b_dola_generation(self):\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n@@ -185,7 +186,12 @@ def test_model_7b_dola_generation(self):\n \n         # greedy generation outputs\n         generated_ids = model.generate(\n-            input_ids, max_new_tokens=20, temperature=0, dola_layers=\"low\", repetition_penalty=1.2\n+            input_ids,\n+            max_new_tokens=20,\n+            temperature=0,\n+            dola_layers=\"low\",\n+            repetition_penalty=1.2,\n+            trust_remote_code=True,\n         )\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)"
        },
        {
            "sha": "8932a9a27e7505f2e47c6bc71129fddbab9b0c33",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -434,10 +434,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_constrained_beam_search_generate_dict_output(self):\n         pass\n \n-    @unittest.skip(\"Cannot do dola generation, has custom `generate()`\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n     @unittest.skip(\"Cannot generate from inputs embeds\")\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass"
        },
        {
            "sha": "df8d2d6e508a9ae4c7c986c5ecac912bab2e3496",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea8d9c8f065a6461e5b9a3a898131b5803daf352/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=ea8d9c8f065a6461e5b9a3a898131b5803daf352",
            "patch": "@@ -133,11 +133,6 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n     def test_constrained_beam_search_generate_dict_output(self):\n         pass\n \n-    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n-    @pytest.mark.generate\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n     @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n     @pytest.mark.generate\n     def test_generate_without_input_ids(self):"
        }
    ],
    "stats": {
        "total": 536,
        "additions": 66,
        "deletions": 470
    }
}