{
    "author": "co63oc",
    "message": "Fix typos in strings and comments (#37910)",
    "sha": "5b573bebb9793c393da035afd985c7078a460498",
    "files": [
        {
            "sha": "cc07ffb2ef2777f4d27e52bd1dd08b7eb93a5867",
            "filename": "examples/legacy/multiple_choice/utils_multiple_choice.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -539,7 +539,7 @@ def convert_examples_to_features(\n             if \"num_truncated_tokens\" in inputs and inputs[\"num_truncated_tokens\"] > 0:\n                 logger.info(\n                     \"Attention! you are cropping tokens (swag task is ok). \"\n-                    \"If you are training ARC and RACE and you are poping question + options, \"\n+                    \"If you are training ARC and RACE and you are popping question + options, \"\n                     \"you need to try to use a bigger max seq length!\"\n                 )\n "
        },
        {
            "sha": "39ba14a12afade7c97bb39ed0c8a399ca6853bf1",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -745,7 +745,7 @@ def main():\n         args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n         do_lower_case=args.do_lower_case,\n         cache_dir=args.cache_dir if args.cache_dir else None,\n-        use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n+        use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handling\n     )\n     model = AutoModelForQuestionAnswering.from_pretrained(\n         args.model_name_or_path,\n@@ -795,7 +795,7 @@ def main():\n         # Load a trained model and vocabulary that you have fine-tuned\n         model = AutoModelForQuestionAnswering.from_pretrained(args.output_dir)  # , force_download=True)\n \n-        # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n+        # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handling\n         # So we use use_fast=False here for now until Fast-tokenizer-compatible-examples are out\n         tokenizer = AutoTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case, use_fast=False)\n         model.to(args.device)"
        },
        {
            "sha": "d3730d1bc0ba27a7c07df13cc6258b0216f27448",
            "filename": "examples/legacy/question-answering/run_squad_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -122,7 +122,7 @@ def main():\n     tokenizer = AutoTokenizer.from_pretrained(\n         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n         cache_dir=model_args.cache_dir,\n-        use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n+        use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handling\n     )\n     model = AutoModelForQuestionAnswering.from_pretrained(\n         model_args.model_name_or_path,"
        },
        {
            "sha": "1af855be139288b253e45bdab9531d76f5df410f",
            "filename": "examples/legacy/run_transfo_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Frun_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Frun_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_transfo_xl.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -71,7 +71,7 @@ def main():\n     # You can also build the corpus yourself using TransfoXLCorpus methods\n     # The pre-processing involve computing word frequencies to prepare the Adaptive input and SoftMax\n     # and tokenizing the dataset\n-    # The pre-processed corpus is a convertion (using the conversion script )\n+    # The pre-processed corpus is a conversion (using the conversion script )\n     corpus = TransfoXLCorpus.from_pretrained(args.model_name)\n \n     va_iter = corpus.get_iterator(\"valid\", args.batch_size, args.tgt_len, device=device, ext_len=args.ext_len)"
        },
        {
            "sha": "034537d1cff05624890d2ee6367e5f1a037fdce7",
            "filename": "examples/legacy/seq2seq/pack_dataset.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -40,7 +40,7 @@ def is_too_big(strang):\n     for src, tgt in tqdm(sorted_examples[1:]):\n         cand_src = new_src + \" \" + src\n         cand_tgt = new_tgt + \" \" + tgt\n-        if is_too_big(cand_src) or is_too_big(cand_tgt):  # cant fit, finalize example\n+        if is_too_big(cand_src) or is_too_big(cand_tgt):  # can't fit, finalize example\n             finished_src.append(new_src)\n             finished_tgt.append(new_tgt)\n             new_src, new_tgt = src, tgt"
        },
        {
            "sha": "8c1a66c7f6512e6a6918b5a7c887d7b1c7f91492",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -804,7 +804,7 @@ def compute_metrics(pred):\n     if \"common_voice\" in data_args.dataset_name:\n         kwargs[\"language\"] = config_name\n \n-    # make sure that adapter weights are saved seperately\n+    # make sure that adapter weights are saved separately\n     adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(data_args.target_language)\n     adapter_file = os.path.join(training_args.output_dir, adapter_file)\n     logger.info(f\"Saving adapter weights under {adapter_file}...\")"
        },
        {
            "sha": "c0a3839afbb2cc2f72709244473e4d6f3ff70bc6",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -516,7 +516,7 @@ def convert_and_export_with_cache(\n                     \"Dynamic shapes spec will be ignored by convert_and_export_with_cache for torch < 2.6.0.\"\n                 )\n             if strict is not None:\n-                logging.warning(\"The strict flag will be ingored by convert_and_export_with_cache for torch < 2.6.0.\")\n+                logging.warning(\"The strict flag will be ignored by convert_and_export_with_cache for torch < 2.6.0.\")\n             # We have to keep this path for BC.\n             #\n             # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal"
        },
        {
            "sha": "010c044924c1e02b088b3964948e165e51030d17",
            "filename": "src/transformers/loss/loss_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_d_fine.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -152,8 +152,8 @@ def bbox2distance(points, bbox, max_num_bins, reg_scale, up, eps=0.1):\n         points (Tensor): (n, 4) [x, y, w, h], where (x, y) is the center.\n         bbox (Tensor): (n, 4) bounding boxes in \"xyxy\" format.\n         max_num_bins (float): Maximum bin value.\n-        reg_scale (float): Controling curvarture of W(n).\n-        up (Tensor): Controling upper bounds of W(n).\n+        reg_scale (float): Controlling curvarture of W(n).\n+        up (Tensor): Controlling upper bounds of W(n).\n         eps (float): Small value to ensure target < max_num_bins.\n \n     Returns:"
        },
        {
            "sha": "7acd58c96ab24f298e7560f5734ae10ce6566e4d",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -28,7 +28,7 @@\n \n \n # TODO: Attribute map assignment logic should be fixed in modular\n-# as well as super() call parsing becuase otherwise we cannot re-write args after initialization\n+# as well as super() call parsing because otherwise we cannot re-write args after initialization\n class DFineConfig(PretrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`DFineModel`]. It is used to instantiate a D-FINE"
        },
        {
            "sha": "f4fd0992e404f5f03826a1f9871cd77393c7dc5e",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -47,7 +47,7 @@\n \n \n # TODO: Attribute map assignment logic should be fixed in modular\n-# as well as super() call parsing becuase otherwise we cannot re-write args after initialization\n+# as well as super() call parsing because otherwise we cannot re-write args after initialization\n class DFineConfig(PretrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`DFineModel`]. It is used to instantiate a D-FINE"
        },
        {
            "sha": "80a276b5bd7e6c6eb1c84ccb9a06dcc50f722053",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -1615,7 +1615,7 @@ def forward(\n                 Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n                 much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n                 that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-                size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+                size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n                 In the order (`x1`, `y1`, `x2`, `y2`):\n \n                 - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "50b5f2aecc85d6bc5481cc12b682940f613c69b4",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -551,7 +551,7 @@ def forward(\n                 Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n                 much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n                 that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-                size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+                size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n                 In the order (`x1`, `y1`, `x2`, `y2`):\n \n                 - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "433db70df2bdb43216b0707a164ae5ba3851b8f0",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -586,10 +586,10 @@ def test_different_timm_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)\n@@ -618,10 +618,10 @@ def test_hf_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)"
        },
        {
            "sha": "8c9d007f46c7831cf1ab6a0fa31687629ba40ece",
            "filename": "tests/models/fsmt/test_modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -423,7 +423,7 @@ def test_prepare_fsmt_decoder_inputs(self):\n \n \n def _assert_tensors_equal(a, b, atol=1e-12, prefix=\"\"):\n-    \"\"\"If tensors not close, or a and b arent both tensors, raise a nice Assertion error.\"\"\"\n+    \"\"\"If tensors not close, or a and b aren't both tensors, raise a nice Assertion error.\"\"\"\n     if a is None and b is None:\n         return True\n     try:"
        },
        {
            "sha": "d08a5ee6a7f6c73ca723d78745fc8342f1f99c1e",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -149,7 +149,7 @@ def test_sdpa_equivalence(self):\n \n     @unittest.skip(\n         reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n+        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n     )\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "b4b9d99c8ebeb6460f169da48668ca36a462c18c",
            "filename": "tests/models/qwen2_5_omni/test_processor_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -567,7 +567,7 @@ def test_chat_template_audio_from_video(self):\n             signature.parameters.get(\"videos\") is not None\n             and signature.parameters[\"videos\"].annotation == inspect._empty\n         ):\n-            self.skipTest(f\"{self.processor_class} does not suport video inputs\")\n+            self.skipTest(f\"{self.processor_class} does not support video inputs\")\n \n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")"
        },
        {
            "sha": "fa5a2a78bc3e80b3625e8cb62a5632dfd61fcfb8",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b573bebb9793c393da035afd985c7078a460498/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=5b573bebb9793c393da035afd985c7078a460498",
            "patch": "@@ -244,13 +244,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -682,13 +682,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 25,
        "deletions": 25
    }
}