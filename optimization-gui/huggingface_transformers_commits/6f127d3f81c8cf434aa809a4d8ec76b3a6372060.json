{
    "author": "ydshieh",
    "message": "Skip `torchscript` tests if a cache object is in model's outputs (#35596)\n\n* fix 1\r\n\r\n* fix 1\r\n\r\n* comment\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "6f127d3f81c8cf434aa809a4d8ec76b3a6372060",
    "files": [
        {
            "sha": "bb2b17f8e5288ee50cd7fec5215bc2888b14a9a1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 24,
            "deletions": 7,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f127d3f81c8cf434aa809a4d8ec76b3a6372060/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f127d3f81c8cf434aa809a4d8ec76b3a6372060/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6f127d3f81c8cf434aa809a4d8ec76b3a6372060",
            "patch": "@@ -122,7 +122,7 @@\n     from torch import nn\n \n     from transformers import MODEL_MAPPING, AdaptiveEmbedding\n-    from transformers.cache_utils import DynamicCache\n+    from transformers.cache_utils import Cache, DynamicCache\n     from transformers.modeling_utils import load_state_dict, no_init_weights\n     from transformers.pytorch_utils import id_tensor_storage\n \n@@ -1109,22 +1109,33 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n                         attention_mask = inputs[\"attention_mask\"]\n                         decoder_input_ids = inputs[\"decoder_input_ids\"]\n                         decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n-                        model(main_input, attention_mask, decoder_input_ids, decoder_attention_mask)\n+                        outputs = model(main_input, attention_mask, decoder_input_ids, decoder_attention_mask)\n+                        # `torchscript` doesn't work with outputs containing `Cache` object. However, #35235 makes\n+                        # several models to use `Cache` by default instead of the legacy cache (tuple), and\n+                        # their `torchscript` tests are failing. We won't support them anyway, but we still want to keep\n+                        # the tests for encoder models like `BERT`. So we skip the checks if the model's output contains\n+                        # a `Cache` object.\n+                        if any(isinstance(x, Cache) for x in outputs):\n+                            continue\n                         traced_model = torch.jit.trace(\n                             model, (main_input, attention_mask, decoder_input_ids, decoder_attention_mask)\n                         )\n                     elif \"bbox\" in inputs and \"image\" in inputs:  # LayoutLMv2 requires additional inputs\n                         input_ids = inputs[\"input_ids\"]\n                         bbox = inputs[\"bbox\"]\n                         image = inputs[\"image\"].tensor\n-                        model(input_ids, bbox, image)\n+                        outputs = model(input_ids, bbox, image)\n+                        if any(isinstance(x, Cache) for x in outputs):\n+                            continue\n                         traced_model = torch.jit.trace(\n                             model, (input_ids, bbox, image), check_trace=False\n                         )  # when traced model is checked, an error is produced due to name mangling\n                     elif \"bbox\" in inputs:  # Bros requires additional inputs (bbox)\n                         input_ids = inputs[\"input_ids\"]\n                         bbox = inputs[\"bbox\"]\n-                        model(input_ids, bbox)\n+                        outputs = model(input_ids, bbox)\n+                        if any(isinstance(x, Cache) for x in outputs):\n+                            continue\n                         traced_model = torch.jit.trace(\n                             model, (input_ids, bbox), check_trace=False\n                         )  # when traced model is checked, an error is produced due to name mangling\n@@ -1134,7 +1145,9 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n                         pixel_values = inputs[\"pixel_values\"]\n                         prompt_pixel_values = inputs[\"prompt_pixel_values\"]\n                         prompt_masks = inputs[\"prompt_masks\"]\n-                        model(pixel_values, prompt_pixel_values, prompt_masks)\n+                        outputs = model(pixel_values, prompt_pixel_values, prompt_masks)\n+                        if any(isinstance(x, Cache) for x in outputs):\n+                            continue\n                         traced_model = torch.jit.trace(\n                             model, (pixel_values, prompt_pixel_values, prompt_masks), check_trace=False\n                         )  # when traced model is checked, an error is produced due to name mangling\n@@ -1149,11 +1162,15 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n                             else:\n                                 self.skipTest(reason=\"testing SDPA without attention_mask is not supported\")\n \n-                            model(main_input, attention_mask=inputs[\"attention_mask\"])\n+                            outputs = model(main_input, attention_mask=inputs[\"attention_mask\"])\n+                            if any(isinstance(x, Cache) for x in outputs):\n+                                continue\n                             # example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.\n                             traced_model = torch.jit.trace(model, example_kwarg_inputs=trace_input)\n                         else:\n-                            model(main_input)\n+                            outputs = model(main_input)\n+                            if any(isinstance(x, Cache) for x in outputs):\n+                                continue\n                             traced_model = torch.jit.trace(model, (main_input,))\n                 except RuntimeError:\n                     self.fail(\"Couldn't trace module.\")"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 24,
        "deletions": 7
    }
}