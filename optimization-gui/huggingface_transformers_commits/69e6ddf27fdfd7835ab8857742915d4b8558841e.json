{
    "author": "Rocketknight1",
    "message": "Delete hubconf.py (#37455)\n\n* Delete hubconf.py\n\n* Trigger tests",
    "sha": "69e6ddf27fdfd7835ab8857742915d4b8558841e",
    "files": [
        {
            "sha": "412cb27f6380dfb3c2c6119bde21a64fa97c02dc",
            "filename": "hubconf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 162,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/hubconf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/hubconf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/hubconf.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -1,162 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import os\n-import sys\n-\n-\n-SRC_DIR = os.path.join(os.path.dirname(__file__), \"src\")\n-sys.path.append(SRC_DIR)\n-\n-\n-from transformers import (\n-    AutoConfig,\n-    AutoModel,\n-    AutoModelForCausalLM,\n-    AutoModelForMaskedLM,\n-    AutoModelForQuestionAnswering,\n-    AutoModelForSequenceClassification,\n-    AutoTokenizer,\n-    add_start_docstrings,\n-)\n-\n-\n-dependencies = [\"torch\", \"numpy\", \"tokenizers\", \"filelock\", \"requests\", \"tqdm\", \"regex\", \"sentencepiece\", \"sacremoses\", \"importlib_metadata\", \"huggingface_hub\"]\n-\n-\n-@add_start_docstrings(AutoConfig.__doc__)\n-def config(*args, **kwargs):\n-    r\"\"\"\n-                # Using torch.hub !\n-                import torch\n-\n-                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased')  # Download configuration from huggingface.co and cache.\n-                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n-                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/my_configuration.json')\n-                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False)\n-                assert config.output_attentions == True\n-                config, unused_kwargs = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False, return_unused_kwargs=True)\n-                assert config.output_attentions == True\n-                assert unused_kwargs == {'foo': False}\n-\n-            \"\"\"\n-\n-    return AutoConfig.from_pretrained(*args, **kwargs)\n-\n-\n-@add_start_docstrings(AutoTokenizer.__doc__)\n-def tokenizer(*args, **kwargs):\n-    r\"\"\"\n-        # Using torch.hub !\n-        import torch\n-\n-        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', 'google-bert/bert-base-uncased')    # Download vocabulary from huggingface.co and cache.\n-        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', './test/bert_saved_model/')  # E.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`\n-\n-    \"\"\"\n-\n-    return AutoTokenizer.from_pretrained(*args, **kwargs)\n-\n-\n-@add_start_docstrings(AutoModel.__doc__)\n-def model(*args, **kwargs):\n-    r\"\"\"\n-            # Using torch.hub !\n-            import torch\n-\n-            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n-            model = torch.hub.load('huggingface/transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n-            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n-            assert model.config.output_attentions == True\n-            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n-            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n-            model = torch.hub.load('huggingface/transformers', 'model', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n-\n-        \"\"\"\n-\n-    return AutoModel.from_pretrained(*args, **kwargs)\n-\n-\n-@add_start_docstrings(AutoModelForCausalLM.__doc__)\n-def modelForCausalLM(*args, **kwargs):\n-    r\"\"\"\n-        # Using torch.hub !\n-        import torch\n-\n-        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2')    # Download model and configuration from huggingface.co and cache.\n-        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n-        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2', output_attentions=True)  # Update configuration during loading\n-        assert model.config.output_attentions == True\n-        # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n-        config = AutoConfig.from_pretrained('./tf_model/gpt_tf_model_config.json')\n-        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './tf_model/gpt_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n-\n-    \"\"\"\n-    return AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n-\n-\n-@add_start_docstrings(AutoModelForMaskedLM.__doc__)\n-def modelForMaskedLM(*args, **kwargs):\n-    r\"\"\"\n-            # Using torch.hub !\n-            import torch\n-\n-            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n-            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n-            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n-            assert model.config.output_attentions == True\n-            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n-            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n-            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n-\n-        \"\"\"\n-\n-    return AutoModelForMaskedLM.from_pretrained(*args, **kwargs)\n-\n-\n-@add_start_docstrings(AutoModelForSequenceClassification.__doc__)\n-def modelForSequenceClassification(*args, **kwargs):\n-    r\"\"\"\n-            # Using torch.hub !\n-            import torch\n-\n-            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n-            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n-            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n-            assert model.config.output_attentions == True\n-            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n-            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n-            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n-\n-        \"\"\"\n-\n-    return AutoModelForSequenceClassification.from_pretrained(*args, **kwargs)\n-\n-\n-@add_start_docstrings(AutoModelForQuestionAnswering.__doc__)\n-def modelForQuestionAnswering(*args, **kwargs):\n-    r\"\"\"\n-        # Using torch.hub !\n-        import torch\n-\n-        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n-        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n-        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n-        assert model.config.output_attentions == True\n-        # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n-        config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n-        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n-\n-    \"\"\"\n-    return AutoModelForQuestionAnswering.from_pretrained(*args, **kwargs)"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 0,
        "deletions": 162
    }
}