{
    "author": "sbucaille",
    "message": "[pipeline] Add Keypoint Matching pipeline (#39970)\n\n* feat: keypoint-matcher pipeline\n\n* docs: added keypoint-matcher pipeline in docs\n\n* fix: added missing statements for repo consistency\n\n* docs: updated SuperGlue, LightGlue and EfficientLoFTR docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* test: fixed run_pipeline_test\n\n* update pipeline typing and docs\n\n* update tests\n\n* update docs snippets\n\n* Fix import error\n\n* fix: pipeline init\n\n* pt framework\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "78f32c3917596b4558d4738e61ff0c39ec0d16ac",
    "files": [
        {
            "sha": "0e4cf55995bf7c4bd7a8d1cffede34c3c7a03f86",
            "filename": "docs/source/en/main_classes/pipelines.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -363,6 +363,12 @@ Pipelines available for computer vision tasks include the following.\n     - __call__\n     - all\n \n+### KeypointMatchingPipeline\n+\n+[[autodoc]] KeypointMatchingPipeline\n+    - __call__\n+    - all\n+\n ### ObjectDetectionPipeline\n \n [[autodoc]] ObjectDetectionPipeline"
        },
        {
            "sha": "2af5db3f74d6a5806352f4c7a1621d9102b04132",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -28,9 +28,24 @@ rendered properly in your Markdown viewer.\n >\n > Click on the EfficientLoFTR models in the right sidebar for more examples of how to apply EfficientLoFTR to different computer vision tasks.\n \n-The example below demonstrates how to match keypoints between two images with the [`AutoModel`] class.\n+The example below demonstrates how to match keypoints between two images with [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+\n+keypoint_matcher = pipeline(task=\"keypoint-matching\", model=\"zju-community/efficientloftr\")\n+\n+url_0 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n+url_1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n+\n+results = keypoint_matcher([url_0, url_1], threshold=0.9)\n+print(results[0])\n+# {'keypoint_image_0': {'x': ..., 'y': ...}, 'keypoint_image_1': {'x': ..., 'y': ...}, 'score': ...}\n+```\n+<hfoption id=\"AutoModel\">\n <hfoption id=\"AutoModel\">\n \n ```py"
        },
        {
            "sha": "e0ecce5cf3e95d7076764f73df7f5e74159c76a6",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -30,9 +30,23 @@ You can find all the original LightGlue checkpoints under the [ETH-CVG](https://\n >\n > Click on the LightGlue models in the right sidebar for more examples of how to apply LightGlue to different computer vision tasks.\n \n-The example below demonstrates how to match keypoints between two images with the [`AutoModel`] class.\n+The example below demonstrates how to match keypoints between two images with [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+\n+keypoint_matcher = pipeline(task=\"keypoint-matching\", model=\"ETH-CVG/lightglue_superpoint\")\n+\n+url_0 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n+url_1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n+\n+results = keypoint_matcher([url_0, url_1], threshold=0.9)\n+print(results[0])\n+# {'keypoint_image_0': {'x': ..., 'y': ...}, 'keypoint_image_1': {'x': ..., 'y': ...}, 'score': ...}\n+```\n <hfoption id=\"AutoModel\">\n \n ```py"
        },
        {
            "sha": "b3f43663cc5579f3238ea84ce1b0bc22b8966970",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -30,9 +30,25 @@ You can find all the original SuperGlue checkpoints under the [Magic Leap Commun\n >\n > Click on the SuperGlue models in the right sidebar for more examples of how to apply SuperGlue to different computer vision tasks.\n \n-The example below demonstrates how to match keypoints between two images with the [`AutoModel`] class.\n+The example below demonstrates how to match keypoints between two images with [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+\n+keypoint_matcher = pipeline(task=\"keypoint-matching\", model=\"magic-leap-community/superglue_outdoor\")\n+\n+url_0 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n+url_1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n+\n+results = keypoint_matcher([url_0, url_1], threshold=0.9)\n+print(results[0])\n+# {'keypoint_image_0': {'x': ..., 'y': ...}, 'keypoint_image_1': {'x': ..., 'y': ...}, 'score': ...}\n+```\n+\n+</hfoption>\n <hfoption id=\"AutoModel\">\n \n ```py"
        },
        {
            "sha": "3349a1698eb8aa641f569b587571557a3e9af546",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -163,6 +163,7 @@\n         \"ImageToImagePipeline\",\n         \"ImageToTextPipeline\",\n         \"JsonPipelineDataFormat\",\n+        \"KeypointMatchingPipeline\",\n         \"MaskGenerationPipeline\",\n         \"NerPipeline\",\n         \"ObjectDetectionPipeline\",\n@@ -826,6 +827,7 @@\n     from .pipelines import ImageToImagePipeline as ImageToImagePipeline\n     from .pipelines import ImageToTextPipeline as ImageToTextPipeline\n     from .pipelines import JsonPipelineDataFormat as JsonPipelineDataFormat\n+    from .pipelines import KeypointMatchingPipeline as KeypointMatchingPipeline\n     from .pipelines import MaskGenerationPipeline as MaskGenerationPipeline\n     from .pipelines import NerPipeline as NerPipeline\n     from .pipelines import ObjectDetectionPipeline as ObjectDetectionPipeline"
        },
        {
            "sha": "f18c6d5fe5506fd7b19872f8d2e75ab101d13de8",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -70,6 +70,7 @@\n from .image_text_to_text import ImageTextToTextPipeline\n from .image_to_image import ImageToImagePipeline\n from .image_to_text import ImageToTextPipeline\n+from .keypoint_matching import KeypointMatchingPipeline\n from .mask_generation import MaskGenerationPipeline\n from .object_detection import ObjectDetectionPipeline\n from .question_answering import QuestionAnsweringArgumentHandler, QuestionAnsweringPipeline\n@@ -121,6 +122,7 @@\n         AutoModelForImageClassification,\n         AutoModelForImageSegmentation,\n         AutoModelForImageTextToText,\n+        AutoModelForKeypointMatching,\n         AutoModelForMaskedLM,\n         AutoModelForMaskGeneration,\n         AutoModelForObjectDetection,\n@@ -439,6 +441,13 @@\n         \"default\": {\"model\": {\"pt\": (\"caidas/swin2SR-classical-sr-x2-64\", \"cee1c92\")}},\n         \"type\": \"image\",\n     },\n+    \"keypoint-matching\": {\n+        \"impl\": KeypointMatchingPipeline,\n+        \"tf\": (),\n+        \"pt\": (AutoModelForKeypointMatching,) if is_torch_available() else (),\n+        \"default\": {\"model\": {\"pt\": (\"magic-leap-community/superglue_outdoor\", \"f4041f8\")}},\n+        \"type\": \"image\",\n+    },\n }\n \n PIPELINE_REGISTRY = PipelineRegistry(supported_tasks=SUPPORTED_TASKS, task_aliases=TASK_ALIASES)\n@@ -499,6 +508,7 @@ def check_task(task: str) -> tuple[str, dict, Any]:\n             - `\"image-segmentation\"`\n             - `\"image-to-text\"`\n             - `\"image-to-image\"`\n+            - `\"keypoint-matching\"`\n             - `\"object-detection\"`\n             - `\"question-answering\"`\n             - `\"summarization\"`\n@@ -581,6 +591,8 @@ def pipeline(task: Literal[\"image-to-image\"], model: Optional[Union[str, \"PreTra\n @overload\n def pipeline(task: Literal[\"image-to-text\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageToTextPipeline: ...\n @overload\n+def pipeline(task: Literal[\"keypoint-matching\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> KeypointMatchingPipeline: ...\n+@overload\n def pipeline(task: Literal[\"mask-generation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> MaskGenerationPipeline: ...\n @overload\n def pipeline(task: Literal[\"object-detection\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ObjectDetectionPipeline: ...\n@@ -675,6 +687,7 @@ def pipeline(\n             - `\"image-text-to-text\"`: will return a [`ImageTextToTextPipeline`].\n             - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n             - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n+            - `\"keypoint-matching\"`: will return a [`KeypointMatchingPipeline`].\n             - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n             - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n             - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`]."
        },
        {
            "sha": "cb7f9d2e5eb90590f6bba775a734fa5c7d4ef472",
            "filename": "src/transformers/pipelines/keypoint_matching.py",
            "status": "added",
            "additions": 169,
            "deletions": 0,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/src%2Ftransformers%2Fpipelines%2Fkeypoint_matching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/src%2Ftransformers%2Fpipelines%2Fkeypoint_matching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fkeypoint_matching.py?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -0,0 +1,169 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Any, Sequence, TypedDict, Union\n+\n+from typing_extensions import TypeAlias, overload\n+\n+from ..image_utils import is_pil_image\n+from ..utils import is_vision_available, requires_backends\n+from .base import Pipeline\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from ..image_utils import load_image\n+\n+\n+ImagePair: TypeAlias = Sequence[Union[\"Image.Image\", str]]\n+\n+Keypoint = TypedDict(\"Keypoint\", {\"x\": float, \"y\": float})\n+Match = TypedDict(\"Match\", {\"keypoint_image_0\": Keypoint, \"keypoint_image_1\": Keypoint, \"score\": float})\n+\n+\n+def validate_image_pairs(images: Any) -> Sequence[Sequence[ImagePair]]:\n+    error_message = (\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of images.\",\n+        \" - A list of pairs of images.\",\n+    )\n+\n+    def _is_valid_image(image):\n+        \"\"\"images is a PIL Image or a string.\"\"\"\n+        return is_pil_image(image) or isinstance(image, str)\n+\n+    if isinstance(images, Sequence):\n+        if len(images) == 2 and all((_is_valid_image(image)) for image in images):\n+            return [images]\n+        if all(\n+            isinstance(image_pair, Sequence)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            return images\n+    raise ValueError(error_message)\n+\n+\n+class KeypointMatchingPipeline(Pipeline):\n+    \"\"\"\n+    Keypoint matching pipeline using any `AutoModelForKeypointMatching`. This pipeline matches keypoints between two images.\n+    \"\"\"\n+\n+    _load_processor = False\n+    _load_image_processor = True\n+    _load_feature_extractor = False\n+    _load_tokenizer = False\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        requires_backends(self, \"vision\")\n+        if self.framework != \"pt\":\n+            raise ValueError(\"Keypoint matching pipeline only supports PyTorch (framework='pt').\")\n+\n+    def _sanitize_parameters(self, threshold=None, timeout=None):\n+        preprocess_params = {}\n+        if timeout is not None:\n+            preprocess_params[\"timeout\"] = timeout\n+        postprocess_params = {}\n+        if threshold is not None:\n+            postprocess_params[\"threshold\"] = threshold\n+        return preprocess_params, {}, postprocess_params\n+\n+    @overload\n+    def __call__(self, inputs: ImagePair, threshold: float = 0.0, **kwargs: Any) -> list[Match]: ...\n+\n+    @overload\n+    def __call__(self, inputs: list[ImagePair], threshold: float = 0.0, **kwargs: Any) -> list[list[Match]]: ...\n+\n+    def __call__(\n+        self,\n+        inputs: Union[list[ImagePair], ImagePair],\n+        threshold: float = 0.0,\n+        **kwargs: Any,\n+    ) -> Union[list[Match], list[list[Match]]]:\n+        \"\"\"\n+        Find matches between keypoints in two images.\n+\n+        Args:\n+            inputs (`str`, `list[str]`, `PIL.Image` or `list[PIL.Image]`):\n+                The pipeline handles three types of images:\n+\n+                - A string containing a http link pointing to an image\n+                - A string containing a local path to an image\n+                - An image loaded in PIL directly\n+\n+                The pipeline accepts either a single pair of images or a batch of image pairs, which must then be passed as a string.\n+                Images in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\n+                images.\n+\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                The threshold to use for keypoint matching. Keypoints matched with a lower matching score will be filtered out.\n+                A value of 0 means that all matched keypoints will be returned.\n+\n+            kwargs:\n+                `timeout (`float`, *optional*, defaults to None)`\n+                    The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n+                    the call may block forever.\n+\n+        Return:\n+            Union[list[Match], list[list[Match]]]:\n+                A list of matches or a list if a single image pair is provided, or of lists of matches if a batch\n+                of image pairs is provided. Each match is a dictionary containing the following keys:\n+\n+                - **keypoint_image_0** (`Keypoint`): The keypoint in the first image (x, y coordinates).\n+                - **keypoint_image_1** (`Keypoint`): The keypoint in the second image (x, y coordinates).\n+                - **score** (`float`): The matching score between the two keypoints.\n+        \"\"\"\n+        if inputs is None:\n+            raise ValueError(\"Cannot call the keypoint-matching pipeline without an inputs argument!\")\n+        formatted_inputs = validate_image_pairs(inputs)\n+        outputs = super().__call__(formatted_inputs, threshold=threshold, **kwargs)\n+        if len(formatted_inputs) == 1:\n+            return outputs[0]\n+        return outputs\n+\n+    def preprocess(self, images, timeout=None):\n+        images = [load_image(image, timeout=timeout) for image in images]\n+        model_inputs = self.image_processor(images=images, return_tensors=self.framework)\n+        model_inputs = model_inputs.to(self.torch_dtype)\n+        target_sizes = [image.size for image in images]\n+        preprocess_outputs = {\"model_inputs\": model_inputs, \"target_sizes\": target_sizes}\n+        return preprocess_outputs\n+\n+    def _forward(self, preprocess_outputs):\n+        model_inputs = preprocess_outputs[\"model_inputs\"]\n+        model_outputs = self.model(**model_inputs)\n+        forward_outputs = {\"model_outputs\": model_outputs, \"target_sizes\": [preprocess_outputs[\"target_sizes\"]]}\n+        return forward_outputs\n+\n+    def postprocess(self, forward_outputs, threshold=0.0) -> list[Match]:\n+        model_outputs = forward_outputs[\"model_outputs\"]\n+        target_sizes = forward_outputs[\"target_sizes\"]\n+        postprocess_outputs = self.image_processor.post_process_keypoint_matching(\n+            model_outputs, target_sizes=target_sizes, threshold=threshold\n+        )\n+        postprocess_outputs = postprocess_outputs[0]\n+        pair_result = []\n+        for kp_0, kp_1, score in zip(\n+            postprocess_outputs[\"keypoints0\"],\n+            postprocess_outputs[\"keypoints1\"],\n+            postprocess_outputs[\"matching_scores\"],\n+        ):\n+            kp_0 = Keypoint(x=kp_0[0].item(), y=kp_0[1].item())\n+            kp_1 = Keypoint(x=kp_1[0].item(), y=kp_1[1].item())\n+            pair_result.append(Match(keypoint_image_0=kp_0, keypoint_image_1=kp_1, score=score.item()))\n+        pair_result = sorted(pair_result, key=lambda x: x[\"score\"], reverse=True)\n+        return pair_result"
        },
        {
            "sha": "e2ed1a2480d7bd8c73a796f426f1eedf4dbbbd4f",
            "filename": "tests/pipelines/test_pipelines_keypoint_matching.py",
            "status": "added",
            "additions": 193,
            "deletions": 0,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/tests%2Fpipelines%2Ftest_pipelines_keypoint_matching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/tests%2Fpipelines%2Ftest_pipelines_keypoint_matching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_keypoint_matching.py?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -0,0 +1,193 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import unittest\n+\n+import datasets\n+\n+from transformers.models.auto.modeling_auto import MODEL_FOR_KEYPOINT_MATCHING_MAPPING\n+from transformers.pipelines import KeypointMatchingPipeline, pipeline\n+from transformers.testing_utils import (\n+    is_pipeline_test,\n+    is_vision_available,\n+    require_torch,\n+    require_vision,\n+)\n+\n+from .test_pipelines_common import ANY\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+@is_pipeline_test\n+@require_torch\n+@require_vision\n+class KeypointMatchingPipelineTests(unittest.TestCase):\n+    model_mapping = MODEL_FOR_KEYPOINT_MATCHING_MAPPING\n+    _dataset = None\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            cls._dataset = datasets.load_dataset(\"hf-internal-testing/image-matching-dataset\", split=\"train\")\n+\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        image_matcher = KeypointMatchingPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n+        examples = [\n+            Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),\n+            \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+        ]\n+        return image_matcher, examples\n+\n+    def run_pipeline_test(self, image_matcher, examples):\n+        self._load_dataset()\n+        outputs = image_matcher(\n+            [\n+                Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),\n+                \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+            ]\n+        )\n+\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"keypoint_image_0\": {\"x\": ANY(float), \"y\": ANY(float)},\n+                    \"keypoint_image_1\": {\"x\": ANY(float), \"y\": ANY(float)},\n+                    \"score\": ANY(float),\n+                }\n+            ]\n+            * 2,  # 2 matches per image pair\n+        )\n+\n+        # Accepts URL + PIL.Image + lists\n+        outputs = image_matcher(\n+            [\n+                [\n+                    Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),\n+                    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+                ],\n+                [self._dataset[0][\"image\"], self._dataset[1][\"image\"]],\n+                [self._dataset[1][\"image\"], self._dataset[2][\"image\"]],\n+                [self._dataset[2][\"image\"], self._dataset[0][\"image\"]],\n+            ]\n+        )\n+        self.assertEqual(\n+            outputs,\n+            [\n+                [\n+                    {\n+                        \"keypoint_image_0\": {\"x\": ANY(float), \"y\": ANY(float)},\n+                        \"keypoint_image_1\": {\"x\": ANY(float), \"y\": ANY(float)},\n+                        \"score\": ANY(float),\n+                    }\n+                ]\n+                * 2  # 2 matches per image pair\n+            ]\n+            * 4,  # 4 image pairs\n+        )\n+\n+    @require_torch\n+    def test_single_image(self):\n+        self._load_dataset()\n+        small_model = \"magic-leap-community/superglue_outdoor\"\n+        image_matcher = pipeline(\"keypoint-matching\", model=small_model)\n+\n+        with self.assertRaises(ValueError):\n+            image_matcher(\n+                self._dataset[0][\"image\"],\n+                threshold=0.0,\n+            )\n+        with self.assertRaises(ValueError):\n+            image_matcher(\n+                [self._dataset[0][\"image\"]],\n+                threshold=0.0,\n+            )\n+\n+    @require_torch\n+    def test_single_pair(self):\n+        self._load_dataset()\n+        small_model = \"magic-leap-community/superglue_outdoor\"\n+        image_matcher = pipeline(\"keypoint-matching\", model=small_model)\n+\n+        image_0: Image.Image = self._dataset[0][\"image\"]\n+        image_1: Image.Image = self._dataset[1][\"image\"]\n+        outputs = image_matcher((image_0, image_1), threshold=0.0)\n+\n+        output = outputs[0]  # first match from image pair\n+        self.assertAlmostEqual(output[\"keypoint_image_0\"][\"x\"], 698, places=1)\n+        self.assertAlmostEqual(output[\"keypoint_image_0\"][\"y\"], 469, places=1)\n+        self.assertAlmostEqual(output[\"keypoint_image_1\"][\"x\"], 434, places=1)\n+        self.assertAlmostEqual(output[\"keypoint_image_1\"][\"y\"], 440, places=1)\n+        self.assertAlmostEqual(output[\"score\"], 0.9905, places=3)\n+\n+    @require_torch\n+    def test_multiple_pairs(self):\n+        self._load_dataset()\n+        small_model = \"magic-leap-community/superglue_outdoor\"\n+        image_matcher = pipeline(\"keypoint-matching\", model=small_model)\n+\n+        image_0: Image.Image = self._dataset[0][\"image\"]\n+        image_1: Image.Image = self._dataset[1][\"image\"]\n+        image_2: Image.Image = self._dataset[2][\"image\"]\n+\n+        outputs = image_matcher(\n+            [\n+                (image_0, image_1),\n+                (image_1, image_2),\n+                (image_2, image_0),\n+            ],\n+            threshold=1e-4,\n+        )\n+\n+        # Test first pair (image_0, image_1)\n+        output_0 = outputs[0][0]  # First match from first pair\n+        self.assertAlmostEqual(output_0[\"keypoint_image_0\"][\"x\"], 698, places=1)\n+        self.assertAlmostEqual(output_0[\"keypoint_image_0\"][\"y\"], 469, places=1)\n+        self.assertAlmostEqual(output_0[\"keypoint_image_1\"][\"x\"], 434, places=1)\n+        self.assertAlmostEqual(output_0[\"keypoint_image_1\"][\"y\"], 440, places=1)\n+        self.assertAlmostEqual(output_0[\"score\"], 0.9905, places=3)\n+\n+        # Test second pair (image_1, image_2)\n+        output_1 = outputs[1][0]  # First match from second pair\n+        self.assertAlmostEqual(output_1[\"keypoint_image_0\"][\"x\"], 272, places=1)\n+        self.assertAlmostEqual(output_1[\"keypoint_image_0\"][\"y\"], 310, places=1)\n+        self.assertAlmostEqual(output_1[\"keypoint_image_1\"][\"x\"], 228, places=1)\n+        self.assertAlmostEqual(output_1[\"keypoint_image_1\"][\"y\"], 568, places=1)\n+        self.assertAlmostEqual(output_1[\"score\"], 0.9890, places=3)\n+\n+        # Test third pair (image_2, image_0)\n+        output_2 = outputs[2][0]  # First match from third pair\n+        self.assertAlmostEqual(output_2[\"keypoint_image_0\"][\"x\"], 385, places=1)\n+        self.assertAlmostEqual(output_2[\"keypoint_image_0\"][\"y\"], 677, places=1)\n+        self.assertAlmostEqual(output_2[\"keypoint_image_1\"][\"x\"], 689, places=1)\n+        self.assertAlmostEqual(output_2[\"keypoint_image_1\"][\"y\"], 351, places=1)\n+        self.assertAlmostEqual(output_2[\"score\"], 0.9900, places=3)"
        },
        {
            "sha": "e188bac152f35b7a25f479f15266cad0a30417d0",
            "filename": "utils/update_metadata.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f32c3917596b4558d4738e61ff0c39ec0d16ac/utils%2Fupdate_metadata.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f32c3917596b4558d4738e61ff0c39ec0d16ac/utils%2Fupdate_metadata.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fupdate_metadata.py?ref=78f32c3917596b4558d4738e61ff0c39ec0d16ac",
            "patch": "@@ -120,6 +120,7 @@\n     (\"mask-generation\", \"MODEL_FOR_MASK_GENERATION_MAPPING_NAMES\", \"AutoModelForMaskGeneration\"),\n     (\"text-to-audio\", \"MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES\", \"AutoModelForTextToSpectrogram\"),\n     (\"text-to-audio\", \"MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES\", \"AutoModelForTextToWaveform\"),\n+    (\"keypoint-matching\", \"MODEL_FOR_KEYPOINT_MATCHING_MAPPING_NAMES\", \"AutoModelForKeypointMatching\"),\n ]\n \n "
        }
    ],
    "stats": {
        "total": 435,
        "additions": 432,
        "deletions": 3
    }
}