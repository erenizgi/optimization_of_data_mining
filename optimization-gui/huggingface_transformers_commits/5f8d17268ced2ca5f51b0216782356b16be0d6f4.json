{
    "author": "monk1337",
    "message": "Update modeling_llama4.py (#37841)\n\n* Update modeling_llama4.py\n\n* Update modeling_llama4.py\n\n* do not pass device\n\n---------\n\nCo-authored-by: raushan <raushan@huggingface.co>",
    "sha": "5f8d17268ced2ca5f51b0216782356b16be0d6f4",
    "files": [
        {
            "sha": "e021bfa7266ecc773c3f418893744ce447c9bcbe",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d17268ced2ca5f51b0216782356b16be0d6f4/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d17268ced2ca5f51b0216782356b16be0d6f4/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=5f8d17268ced2ca5f51b0216782356b16be0d6f4",
            "patch": "@@ -873,7 +873,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -906,16 +905,18 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.to(device).reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(device)\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    cache_position.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}