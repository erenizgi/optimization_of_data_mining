{
    "author": "MagnusS0",
    "message": "Support for SDPA for SAM models (#34110)\n\n* feat: add support for sdpa and gradient checkpointing\n\n* fix: ruff format\n\n* fix: config sdpa\n\n* fix: sdpa layer naming convention\n\n* fix: update test_eager_matches_sdpa_inference to handle vision_hidden_states\n\n* test: skip incompatible tests and fix loading issue with sdpa\n\n- Updated tests to skip cases flash and dynamic compile.\n- Minor adjustment to ensure correct loading of model with sdpa for dispatch test.\n\n* style: apply Ruff formatting\n\n* ruff fix again after rebase\n\n* [run-slow] sam\n\n* [run-slow] sam\n\n* refactor: Address review comments and improve sub-config handling in SAM model tests\n\n- Added attributes for sub_configs as per PR #34410.\n- Enabled tests for configs, ensuring the composite model (SAM) has several sub-configs in the main config.\n- Added class attribute _is_composite=True to the tester class\n- test_sdpa_can_dispatch_composite_models added\n\n* [run-slow] sam\n\n* style: ruff\n\n* [run-slow] sam\n\n* style: ruff again ...\n\n* [run-slow] sam",
    "sha": "6eb00dd2f0283f46d21ce9466d8d4e21dfd02550",
    "files": [
        {
            "sha": "22a237615d1280e41bbb7b5e47a75cee6bde7f60",
            "filename": "src/transformers/models/sam/configuration_sam.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py?ref=6eb00dd2f0283f46d21ce9466d8d4e21dfd02550",
            "patch": "@@ -46,6 +46,8 @@ class SamPromptEncoderConfig(PretrainedConfig):\n             The non-linear activation function in the encoder and pooler.\n     \"\"\"\n \n+    base_config_key = \"prompt_encoder_config\"\n+\n     def __init__(\n         self,\n         hidden_size=256,\n@@ -102,6 +104,8 @@ class SamMaskDecoderConfig(PretrainedConfig):\n \n     \"\"\"\n \n+    base_config_key = \"mask_decoder_config\"\n+\n     def __init__(\n         self,\n         hidden_size=256,\n@@ -181,6 +185,8 @@ class SamVisionConfig(PretrainedConfig):\n             hidden_size`.\n     \"\"\"\n \n+    base_config_key = \"vision_config\"\n+\n     def __init__(\n         self,\n         hidden_size=768,\n@@ -278,6 +284,11 @@ class SamConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"sam\"\n+    sub_configs = {\n+        \"prompt_encoder_config\": SamPromptEncoderConfig,\n+        \"mask_decoder_config\": SamMaskDecoderConfig,\n+        \"vision_config\": SamVisionConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b935bc9e421e01db073ce637b6a68bce16ab81c3",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 160,
            "deletions": 7,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=6eb00dd2f0283f46d21ce9466d8d4e21dfd02550",
            "patch": "@@ -246,6 +246,47 @@ def forward(self, query: Tensor, key: Tensor, value: Tensor, attention_similarit\n         return out\n \n \n+class SamSdpaAttention(SamAttention):\n+    \"\"\"\n+    SAM's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n+    values. Using SDPA instead of the default attention.\n+    \"\"\"\n+\n+    def __init__(self, config, downsample_rate=None):\n+        super().__init__(config, downsample_rate)\n+\n+    def forward(self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Tensor = None) -> Tensor:\n+        # Input projections\n+        query = self.q_proj(query)\n+        key = self.k_proj(key)\n+        value = self.v_proj(value)\n+\n+        point_batch_size = query.shape[1]\n+        # Separate into heads\n+        query = self._separate_heads(query, self.num_attention_heads)\n+        key = self._separate_heads(key, self.num_attention_heads)\n+        value = self._separate_heads(value, self.num_attention_heads)\n+\n+        # Scaled dot product attention\n+        attn_mask = None\n+        if attention_similarity is not None:\n+            attn_mask = attention_similarity.unsqueeze(1).expand(-1, self.num_attention_heads, -1, -1)\n+\n+        out = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)\n+\n+        # Get output\n+        out = self._recombine_heads(out, point_batch_size)\n+        out = self.out_proj(out)\n+\n+        return out\n+\n+\n+SAM_ATTENTION_CLASSES = {\n+    \"eager\": SamAttention,\n+    \"sdpa\": SamSdpaAttention,\n+}\n+\n+\n class SamTwoWayAttentionBlock(nn.Module):\n     def __init__(self, config, attention_downsample_rate: int = 2, skip_first_layer_pe: bool = False):\n         \"\"\"\n@@ -266,18 +307,21 @@ def __init__(self, config, attention_downsample_rate: int = 2, skip_first_layer_\n         self.hidden_size = config.hidden_size\n         self.layer_norm_eps = config.layer_norm_eps\n \n-        self.self_attn = SamAttention(config, downsample_rate=1)\n+        self.self_attn = SAM_ATTENTION_CLASSES[config._attn_implementation](config, downsample_rate=1)\n         self.layer_norm1 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n-        self.cross_attn_token_to_image = SamAttention(config, downsample_rate=attention_downsample_rate)\n+        self.cross_attn_token_to_image = SAM_ATTENTION_CLASSES[config._attn_implementation](\n+            config, downsample_rate=attention_downsample_rate\n+        )\n         self.layer_norm2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n         self.mlp = SamMLPBlock(config)\n         self.layer_norm3 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n         self.layer_norm4 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n-        self.cross_attn_image_to_token = SamAttention(config, downsample_rate=attention_downsample_rate)\n-\n+        self.cross_attn_image_to_token = SAM_ATTENTION_CLASSES[config._attn_implementation](\n+            config, downsample_rate=attention_downsample_rate\n+        )\n         self.skip_first_layer_pe = skip_first_layer_pe\n \n     def forward(\n@@ -344,7 +388,7 @@ def __init__(self, config: SamMaskDecoderConfig):\n         for i in range(self.num_hidden_layers):\n             self.layers.append(SamTwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0)))\n \n-        self.final_attn_token_to_image = SamAttention(config)\n+        self.final_attn_token_to_image = SAM_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.layer_norm_final_attn = nn.LayerNorm(config.hidden_size)\n \n     def forward(\n@@ -431,7 +475,7 @@ def forward(self, hidden_states):\n class SamMaskDecoder(nn.Module):\n     def __init__(self, config: SamMaskDecoderConfig):\n         super().__init__()\n-\n+        self.config = config\n         self.hidden_size = config.hidden_size\n \n         self.num_multimask_outputs = config.num_multimask_outputs\n@@ -856,11 +900,118 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n         return outputs\n \n \n+class SamVisionSdpaAttention(SamVisionAttention):\n+    \"\"\"\n+    Multi-head Attention block with relative position embeddings.\n+    Using SDPA instead of the default attention.\n+    \"\"\"\n+\n+    def __init__(self, config, window_size):\n+        super().__init__(config, window_size)\n+\n+    def add_decomposed_rel_pos(\n+        self,\n+        query: torch.Tensor,\n+        rel_pos_h: torch.Tensor,\n+        rel_pos_w: torch.Tensor,\n+        q_size: Tuple[int, int],\n+        k_size: Tuple[int, int],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n+        https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n+        This method is reimplemented to follow the implementation in:\n+        https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py   # noqa B950\n+        This implementation is more memory efficient when using SDPA in the forward method.\n+        Args:\n+            q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n+            rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n+            rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n+            q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n+            k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n+\n+        Returns:\n+            attn (Tensor): attention map with added relative positional embeddings.\n+        \"\"\"\n+        query_height, query_width = q_size\n+        key_height, key_width = k_size\n+        relative_position_height = self.get_rel_pos(query_height, key_height, rel_pos_h)\n+        relative_position_width = self.get_rel_pos(query_width, key_width, rel_pos_w)\n+\n+        batch_size, _, dim = query.shape\n+        reshaped_query = query.reshape(batch_size, query_height, query_width, dim)\n+        rel_h = torch.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n+        rel_w = torch.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n+        rel_h = rel_h.unsqueeze(-1)\n+        rel_w = rel_w.unsqueeze(-2)\n+        rel_h = rel_h.reshape(batch_size, query_height * query_width, key_height, 1)\n+        rel_w = rel_w.reshape(batch_size, query_height * query_width, 1, key_width)\n+\n+        return rel_h, rel_w\n+\n+    def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n+        batch_size, height, width, _ = hidden_states.shape\n+        # qkv with shape (3, B, nHead, H * W, C)\n+        qkv = (\n+            self.qkv(hidden_states)\n+            .reshape(batch_size, height * width, 3, self.num_attention_heads, -1)\n+            .permute(2, 0, 3, 1, 4)\n+        )\n+        # q, k, v with shape (B * nHead, H * W, C)\n+        query, key, value = qkv.reshape(3, batch_size * self.num_attention_heads, height * width, -1).unbind(0)\n+\n+        rel_h, rel_w = None, None\n+        if self.use_rel_pos:\n+            rel_h, rel_w = self.add_decomposed_rel_pos(\n+                query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            )\n+\n+        query = query.view(batch_size, self.num_attention_heads, height * width, -1)\n+        key = key.view(batch_size, self.num_attention_heads, height * width, -1)\n+        value = value.view(batch_size, self.num_attention_heads, height * width, -1)\n+\n+        if self.use_rel_pos:\n+            rel_h = rel_h.view(batch_size, self.num_attention_heads, rel_h.size(1), rel_h.size(2), rel_h.size(3))\n+            rel_w = rel_w.view(batch_size, self.num_attention_heads, rel_w.size(1), rel_w.size(2), rel_w.size(3))\n+            attn_bias = (rel_h + rel_w).view(\n+                batch_size, self.num_attention_heads, rel_h.size(2), rel_h.size(3) * rel_w.size(4)\n+            )\n+            attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=attn_bias)\n+        else:\n+            attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value)\n+\n+        attn_output = (\n+            attn_output.view(batch_size, self.num_attention_heads, height, width, -1)\n+            .permute(0, 2, 3, 1, 4)\n+            .reshape(batch_size, height, width, -1)\n+        )\n+\n+        attn_output = self.proj(attn_output)\n+\n+        if output_attentions:\n+            # For output_attentions, calculate the attention weights\n+            attn_weights = (query @ key.transpose(-2, -1)) * self.scale\n+            if attn_bias is not None:\n+                attn_weights = attn_weights + attn_bias\n+            attn_weights = F.softmax(attn_weights, dim=-1)\n+            outputs = (attn_output, attn_weights)\n+        else:\n+            outputs = (attn_output, None)\n+\n+        return outputs\n+\n+\n+SAM_VISION_ATTENTION_CLASSES = {\n+    \"eager\": SamVisionAttention,\n+    \"sdpa\": SamVisionSdpaAttention,\n+}\n+\n+\n class SamVisionLayer(nn.Module):\n     def __init__(self, config, window_size):\n         super().__init__()\n         self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.attn = SamVisionAttention(config, window_size)\n+        self.attn = SAM_VISION_ATTENTION_CLASSES[config._attn_implementation](config, window_size)\n         self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.mlp = SamMLPBlock(config)\n         self.window_size = window_size\n@@ -1071,6 +1222,8 @@ class SamPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"sam\"\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"SamVisionAttention\"]\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "351016716a0cf135de3b0e1b8a1a088b228815e6",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 69,
            "deletions": 14,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=6eb00dd2f0283f46d21ce9466d8d4e21dfd02550",
            "patch": "@@ -14,12 +14,13 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch SAM model.\"\"\"\n \n+import tempfile\n import unittest\n \n import requests\n \n from transformers import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig, pipeline\n-from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n+from transformers.testing_utils import cleanup, require_torch, require_torch_sdpa, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -295,6 +296,7 @@ class SamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torchscript = False\n+    _is_composite = True\n \n     # TODO: Fix me @Arthur: `run_batch_test` in `tests/test_pipeline_mixin.py` not working\n     def is_pipeline_test_to_skip(\n@@ -311,22 +313,13 @@ def is_pipeline_test_to_skip(\n \n     def setUp(self):\n         self.model_tester = SamModelTester(self)\n-        self.vision_config_tester = ConfigTester(self, config_class=SamVisionConfig, has_text_modality=False)\n-        self.prompt_encoder_config_tester = ConfigTester(\n-            self,\n-            config_class=SamPromptEncoderConfig,\n-            has_text_modality=False,\n-            num_attention_heads=12,\n-            num_hidden_layers=2,\n-        )\n-        self.mask_decoder_config_tester = ConfigTester(\n-            self, config_class=SamMaskDecoderConfig, has_text_modality=False\n+        common_properties = [\"initializer_range\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=SamConfig, has_text_modality=False, common_properties=common_properties\n         )\n \n     def test_config(self):\n-        self.vision_config_tester.run_common_tests()\n-        self.prompt_encoder_config_tester.run_common_tests()\n-        self.mask_decoder_config_tester.run_common_tests()\n+        self.config_tester.run_common_tests()\n \n     @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n     def test_inputs_embeds(self):\n@@ -450,6 +443,68 @@ def test_model_from_pretrained(self):\n         model = SamModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+                # Root model determines SDPA support\n+                attn_impl = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+                # Check config propagation to submodels that support it\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_sdpa.vision_encoder.config._attn_implementation == attn_impl)\n+                self.assertTrue(model_sdpa.mask_decoder.config._attn_implementation == attn_impl)\n+\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_encoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.mask_decoder.config._attn_implementation == \"eager\")\n+\n+                # Verify SDPA/eager layer presence\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+\n+                if not has_sdpa and attn_impl == \"sdpa\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n \n def prepare_image():\n     img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\""
        },
        {
            "sha": "3aaf18c945451f2deeeaaf492cb389fe3d4bf5c5",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6eb00dd2f0283f46d21ce9466d8d4e21dfd02550/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6eb00dd2f0283f46d21ce9466d8d4e21dfd02550",
            "patch": "@@ -4202,16 +4202,20 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                                             outputs_eager = model_eager(**prepared_inputs)\n                                             outputs_sdpa = model_sdpa(**prepared_inputs)\n \n-                                    logits_eager = (\n-                                        outputs_eager.hidden_states[-1]\n-                                        if not is_encoder_decoder\n-                                        else outputs_eager.decoder_hidden_states[-1]\n-                                    )\n-                                    logits_sdpa = (\n-                                        outputs_sdpa.hidden_states[-1]\n-                                        if not is_encoder_decoder\n-                                        else outputs_sdpa.decoder_hidden_states[-1]\n-                                    )\n+                                    if hasattr(outputs_eager, \"vision_hidden_states\"):\n+                                        logits_eager = outputs_eager.vision_hidden_states[-1]\n+                                        logits_sdpa = outputs_sdpa.vision_hidden_states[-1]\n+                                    else:\n+                                        logits_eager = (\n+                                            outputs_eager.hidden_states[-1]\n+                                            if not is_encoder_decoder\n+                                            else outputs_eager.decoder_hidden_states[-1]\n+                                        )\n+                                        logits_sdpa = (\n+                                            outputs_sdpa.hidden_states[-1]\n+                                            if not is_encoder_decoder\n+                                            else outputs_sdpa.decoder_hidden_states[-1]\n+                                        )\n \n                                     if torch_device in [\"cpu\", \"cuda\"]:\n                                         atol = atols[torch_device, enable_kernels, torch_dtype]\n@@ -4287,6 +4291,8 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 )\n             if config.model_type in [\"idefics\", \"idefics2\", \"idefics3\"]:\n                 self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n+            if config.model_type in [\"sam\"]:\n+                self.skipTest(reason=\"SAM requires an attention_mask input for relative positional embeddings\")\n             model = model_class(config)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:"
        }
    ],
    "stats": {
        "total": 287,
        "additions": 256,
        "deletions": 31
    }
}