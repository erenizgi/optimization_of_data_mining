{
    "author": "guangy10",
    "message": "Allow override inputs to export recipe (#37508)\n\nAdd option to specify dynamic shapes during export\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "a57274466f7f72efaa2662d1738cdaf28ae8071f",
    "files": [
        {
            "sha": "88dfe4640cac8e518324c5b5408cb04d5b0f44cf",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 32,
            "deletions": 12,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/a57274466f7f72efaa2662d1738cdaf28ae8071f/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a57274466f7f72efaa2662d1738cdaf28ae8071f/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=a57274466f7f72efaa2662d1738cdaf28ae8071f",
            "patch": "@@ -10,6 +10,7 @@\n # an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n # specific language governing permissions and limitations under the License.\n \n+import logging\n from typing import Optional\n \n import torch\n@@ -50,14 +51,22 @@ def __init__(\n         \"\"\"\n         super().__init__()\n \n-        if model.config.cache_implementation == \"static\":\n+        if not hasattr(model.config, \"use_cache\") or model.config.use_cache is False:\n+            raise ValueError(\"The model must have caching enabled to be performant.\")\n+\n+        if not hasattr(model.config, \"cache_implementation\"):\n+            # If `cache_implementation` is not specified explicitly in the config, `DynamicCache` will\n+            # be used by default, so export will use `StaticCache` by default.\n+            logging.info(\"Using `StaticCache` for export as `cache_implementation` is not specified in the config.\")\n             self.model = TorchExportableModuleWithStaticCache(model)\n-        elif model.config.cache_implementation == \"hybrid\":\n-            self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n         else:\n-            raise ValueError(\n-                f\"Unsupported cache implementation in this export recipe: '{model.config.cache_implementation}'\"\n-            )\n+            if model.config.cache_implementation == \"hybrid\":\n+                self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n+            else:\n+                raise ValueError(\n+                    f\"Unsupported cache implementation: {model.config.cache_implementation}. \"\n+                    \"Please use `hybrid` or `static`.\"\n+                )\n \n     def forward(\n         self,\n@@ -462,15 +471,19 @@ def convert_and_export_with_cache(\n     model: PreTrainedModel,\n     example_input_ids: Optional[torch.Tensor] = None,\n     example_cache_position: Optional[torch.Tensor] = None,\n+    dynamic_shapes: Optional[dict] = None,\n+    strict: Optional[bool] = None,\n ):\n     \"\"\"\n     Convert a `PreTrainedModel` into an exportable module and export it using `torch.export`,\n     ensuring the exported model is compatible with `ExecuTorch`.\n \n     Args:\n         model (`PreTrainedModel`): The pretrained model to be exported.\n-        example_input_ids (`torch.Tensor`): Example input token id used by `torch.export`.\n-        example_cache_position (`torch.Tensor`): Example current cache position used by `torch.export`.\n+        example_input_ids (`Optional[torch.Tensor]`): Example input token id used by `torch.export`.\n+        example_cache_position (`Optional[torch.Tensor]`): Example current cache position used by `torch.export`.\n+        dynamic_shapes(`Optional[dict]`): Dynamic shapes used by `torch.export`.\n+        strict(`Optional[bool]`): Flag to instruct `torch.export` to use `torchdynamo`.\n \n     Returns:\n         Exported program (`torch.export.ExportedProgram`): The exported program generated via `torch.export`.\n@@ -489,14 +502,21 @@ def convert_and_export_with_cache(\n             example_cache_position if example_cache_position is not None else torch.tensor([0], dtype=torch.long)\n         )\n \n-        if is_torch_greater_or_equal(\"2.5.0\"):\n+        if is_torch_greater_or_equal(\"2.6.0\"):\n             exported_program = torch.export.export(\n                 TorchExportableModuleWithStaticCache(model),\n-                args=(example_input_ids,),\n-                kwargs={\"cache_position\": example_cache_position},\n-                strict=True,\n+                args=(example_input_ids, example_cache_position),\n+                kwargs={},\n+                dynamic_shapes=dynamic_shapes,\n+                strict=strict if strict is not None else True,\n             )\n         else:\n+            if dynamic_shapes is not None:\n+                logging.warning(\n+                    \"Dynamic shapes spec will be ignored by convert_and_export_with_cache for torch < 2.6.0.\"\n+                )\n+            if strict is not None:\n+                logging.warning(\"The strict flag will be ingored by convert_and_export_with_cache for torch < 2.6.0.\")\n             # We have to keep this path for BC.\n             #\n             # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal"
        },
        {
            "sha": "3fbc29935552fdf4ebfc0b6b8d3168bb5b8f37a0",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 58,
            "deletions": 4,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/a57274466f7f72efaa2662d1738cdaf28ae8071f/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a57274466f7f72efaa2662d1738cdaf28ae8071f/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=a57274466f7f72efaa2662d1738cdaf28ae8071f",
            "patch": "@@ -25,7 +25,6 @@\n     is_torch_available,\n     require_gptq,\n     require_non_xpu,\n-    require_read_token,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n@@ -693,8 +692,6 @@ def test_dynamic_cache_exportability(self):\n         for v1, v2 in zip(res.past_key_values.value_cache, res_eager.past_key_values.value_cache):\n             self.assertTrue(torch.allclose(v1, v2))\n \n-    @slow\n-    @require_read_token\n     def test_static_cache_exportability(self):\n         \"\"\"\n         Tests that static cache works with `torch.export()`\n@@ -709,8 +706,9 @@ def test_static_cache_exportability(self):\n         attn_implementation = \"sdpa\"  # Export and ExecuTorch only works for SdpaAttention\n         batch_size = 1\n         max_cache_len = 1234\n+        model_id = \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n         model = AutoModelForCausalLM.from_pretrained(\n-            \"google/gemma-2b\",\n+            model_id,\n             device_map=device,\n             torch_dtype=dtype,\n             attn_implementation=attn_implementation,\n@@ -748,3 +746,59 @@ def test_static_cache_exportability(self):\n                 n_static_value_caches = n_static_value_caches + 1\n         self.assertEqual(n_static_key_caches, model.config.num_hidden_layers)\n         self.assertEqual(n_static_value_caches, model.config.num_hidden_layers)\n+\n+        # Export with dynamic shapes using Dim.AUTO\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        input_ids = tokenizer(\"Here's everything I know\", return_tensors=\"pt\").input_ids\n+        dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.AUTO}, \"cache_position\": None}\n+        exported_program = convert_and_export_with_cache(\n+            model,\n+            example_input_ids=input_ids,\n+            dynamic_shapes=dynamic_shapes,\n+            strict=False,\n+        )\n+\n+    def test_hybrid_cache_exportability(self):\n+        \"\"\"\n+        Tests that static cache works with `torch.export()`\n+        \"\"\"\n+        if not is_torch_greater_or_equal(\"2.6\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")\n+\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        set_seed(0)\n+        model_id = \"hf-internal-testing/tiny-random-Gemma3ForCausalLM\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id)\n+        model.eval()\n+        self.assertEqual(model.config.use_cache, True)\n+        self.assertEqual(model.config.cache_implementation, \"hybrid\")\n+\n+        # Export + HybridCache\n+        model.eval()\n+        max_batch_size = 1\n+        max_cache_len = 23\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model, max_batch_size, max_cache_len)\n+        exported_program = exportable_module.export()\n+        n_g_key_caches = n_g_value_caches = 0\n+        for buffer_name, buffer in exported_program.named_buffers():\n+            if buffer_name.startswith(\"key_cache\"):\n+                self.assertTrue(buffer.shape[0] == max_batch_size)\n+                self.assertTrue(buffer.shape[2] == max_cache_len)\n+                n_g_key_caches = n_g_key_caches + 1\n+            if buffer_name.startswith(\"value_cache\"):\n+                self.assertTrue(buffer.shape[0] == max_batch_size)\n+                self.assertTrue(buffer.shape[2] == max_cache_len)\n+                n_g_value_caches = n_g_value_caches + 1\n+        self.assertEqual(n_g_key_caches, model.config.num_hidden_layers)\n+        self.assertEqual(n_g_value_caches, model.config.num_hidden_layers)\n+\n+        # Export with dynamic shapes using Dim.AUTO\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        input_ids = tokenizer(\"Here's everything I know\", return_tensors=\"pt\").input_ids\n+        dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.AUTO}, \"cache_position\": None}\n+        exported_program = exportable_module.export(\n+            input_ids=input_ids,\n+            dynamic_shapes=dynamic_shapes,\n+            strict=False,\n+        )"
        }
    ],
    "stats": {
        "total": 106,
        "additions": 90,
        "deletions": 16
    }
}