{
    "author": "gante",
    "message": "[Generation] remove leftover code from end-to-end compilation (#36685)",
    "sha": "0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
    "files": [
        {
            "sha": "654edb1f9517c644f7bf3176ddfa296b9bdce0ef",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 66,
            "deletions": 110,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -47,7 +47,6 @@\n     is_accelerate_available,\n     is_hqq_available,\n     is_optimum_quanto_available,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from .beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n@@ -393,7 +392,6 @@ def prepare_inputs_for_generation(\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n         # Excpetion 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n         # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n@@ -402,7 +400,7 @@ def prepare_inputs_for_generation(\n                 inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n             elif (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n@@ -1339,7 +1337,7 @@ def _validate_model_class(self):\n         # TODO(joao): remove this function in v4.50, i.e. when we remove the inheritance of `GenerationMixin` from\n         # `PreTrainedModel`. With that inheritance removed, all model classes inheriting from `GenerationMixin` can\n         # safely call `GenerationMixin.generate`\n-        if not is_torchdynamo_compiling() and not self.can_generate():\n+        if not self.can_generate():\n             terminations_with_generation_support = [\n                 \"ForCausalLM\",\n                 \"ForConditionalGeneration\",\n@@ -1440,11 +1438,6 @@ def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n \n     def _validate_generated_length(self, generation_config, input_ids_length, has_default_max_length):\n         \"\"\"Performs validation related to the resulting generated length\"\"\"\n-\n-        # Can't throw warnings/exceptions during compilation\n-        if is_torchdynamo_compiling():\n-            return\n-\n         # 1. Max length warnings related to poor parameterization\n         if has_default_max_length and generation_config.max_new_tokens is None and generation_config.max_length == 20:\n             # 20 is the default max_length of the generation config\n@@ -1550,11 +1543,10 @@ def _prepare_generation_config(\n         Prepares the base generation config, then applies any generation configuration options from kwargs. This\n         function handles retrocompatibility with respect to configuration files.\n         \"\"\"\n-        # TODO joao: when we can detect `fullgraph=True` in `torch.compile` (https://github.com/pytorch/pytorch/pull/120400)\n-        # replace `is_torchdynamo_compiling` by the corresponding check. As it is, we are being too restrictive with\n-        # the parameterization in `fullgraph=False` so as to enable `fullgraph=True`.\n+        # parameterization priority:\n+        # kwargs > non-global default values in `generation_config` > `model.generation_config` > GenerationConfig()\n+        # TODO (joao): per-model generation config classes.\n \n-        # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n         using_model_generation_config = False\n         if generation_config is None:\n             # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\n@@ -1563,10 +1555,8 @@ def _prepare_generation_config(\n             # 2) the generation config must have seen no modification since its creation (the hash is the same);\n             # 3) there are non-default generation parameters in the model config.\n             # 4) the user must have set new generation parameters in the model config.\n-            # NOTE: `torch.compile` can't compile `hash`, this legacy support is disabled with compilation.\n             if (\n-                not is_torchdynamo_compiling()\n-                and self.generation_config._from_model_config  # 1)\n+                self.generation_config._from_model_config  # 1)\n                 and self.generation_config._original_object_hash == hash(self.generation_config)  # 2)\n                 and len(self.config._get_non_default_generation_parameters()) > 0  # 3)\n             ):\n@@ -1584,35 +1574,29 @@ def _prepare_generation_config(\n             generation_config = self.generation_config\n             using_model_generation_config = True\n \n-        # `torch.compile` can't compile `copy.deepcopy`, arguments in `kwargs` that are part of `generation_config`\n-        # will mutate the object with `.update`. As such, passing these arguments through `kwargs` is disabled -- an\n-        # exception will be raised in `_validate_model_kwargs`\n-        if not is_torchdynamo_compiling():\n-            generation_config = copy.deepcopy(generation_config)\n-\n-            # If `generation_config` is provided, let's fallback ALL default values to the model's generation config\n-            # TODO (joao): per-model generation config classes.\n-            if not using_model_generation_config:\n-                modified_values = {}\n-                default_generation_config = GenerationConfig()\n-                for key, default_value in default_generation_config.__dict__.items():\n-                    if key.startswith(\"_\"):  # metadata\n-                        continue\n-                    custom_gen_config_value = getattr(generation_config, key)\n-                    model_gen_config_value = getattr(self.generation_config, key)\n-                    if custom_gen_config_value == default_value and model_gen_config_value != default_value:\n-                        modified_values[key] = model_gen_config_value\n-                        setattr(generation_config, key, model_gen_config_value)\n-                if len(modified_values) > 0:\n-                    logger.warning_once(\n-                        f\"`generation_config` default values have been modified to match model-specific defaults: \"\n-                        f\"{modified_values}. If this is not desired, please set these values explicitly.\"\n-                    )\n+        generation_config = copy.deepcopy(generation_config)\n+\n+        # If `generation_config` is provided, let's fallback ALL default values to the model's generation config\n+        if not using_model_generation_config:\n+            modified_values = {}\n+            default_generation_config = GenerationConfig()\n+            for key, default_value in default_generation_config.__dict__.items():\n+                if key.startswith(\"_\"):  # metadata\n+                    continue\n+                custom_gen_config_value = getattr(generation_config, key)\n+                model_gen_config_value = getattr(self.generation_config, key)\n+                if custom_gen_config_value == default_value and model_gen_config_value != default_value:\n+                    modified_values[key] = model_gen_config_value\n+                    setattr(generation_config, key, model_gen_config_value)\n+            if len(modified_values) > 0:\n+                logger.warning_once(\n+                    f\"`generation_config` default values have been modified to match model-specific defaults: \"\n+                    f\"{modified_values}. If this is not desired, please set these values explicitly.\"\n+                )\n+\n+        # Finally, apply any passed kwargs\n+        model_kwargs = generation_config.update(**kwargs)\n \n-            # Finally, apply any passed kwargs\n-            model_kwargs = generation_config.update(**kwargs)\n-        else:\n-            model_kwargs = kwargs\n         return generation_config, model_kwargs\n \n     def _get_initial_cache_position(self, input_ids, model_kwargs):\n@@ -1636,10 +1620,7 @@ def _get_initial_cache_position(self, input_ids, model_kwargs):\n             elif hasattr(cache, \"get_seq_length\") and cache.get_seq_length() is not None:\n                 past_length = cache.get_seq_length()\n \n-            # TODO(joao): this is not torch.compile-friendly, find a work-around. If the cache is not empty,\n-            # end-to-end compilation will yield bad results because `cache_position` will be incorrect.\n-            if not is_torchdynamo_compiling():\n-                cache_position = cache_position[past_length:]\n+            cache_position = cache_position[past_length:]\n \n         model_kwargs[\"cache_position\"] = cache_position\n         return model_kwargs\n@@ -1716,13 +1697,7 @@ def _get_cache(\n             if hasattr(self.config, \"_pre_quantization_dtype\"):\n                 cache_dtype = self.config._pre_quantization_dtype\n             else:\n-                if not is_torchdynamo_compiling():\n-                    cache_dtype = self.dtype\n-                else:\n-                    # NOTE: self.dtype is not compatible with torch.compile, as it calls `self.parameters()`.\n-                    # Workaround: trust the lm_head, whose attribute name is somewhat consistent across generative\n-                    # models. May cause trobles with non-text modalities.\n-                    cache_dtype = self.get_output_embeddings().weight.dtype\n+                cache_dtype = self.dtype\n \n             layer_device_map = self._get_layer_device_map_for_cache_init()\n             cache_kwargs = {\n@@ -1924,12 +1899,11 @@ def _tensor_or_none(token, device=None):\n \n         # Set pad token if unset (and there are conditions to do so)\n         if pad_token_tensor is None and eos_token_tensor is not None:\n-            if not is_torchdynamo_compiling():\n-                if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n-                    logger.warning(\n-                        \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n-                        \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n-                    )\n+            if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n+                logger.warning(\n+                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n+                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n+                )\n             pad_token_tensor = eos_token_tensor[0]\n             logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\n \n@@ -1938,24 +1912,23 @@ def _tensor_or_none(token, device=None):\n             raise ValueError(\n                 \"`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\"\n             )\n-        if not is_torchdynamo_compiling():  # Checks that depend on tensor-dependent control flow\n-            if (\n-                eos_token_tensor is not None\n-                and isin_mps_friendly(elements=eos_token_tensor, test_elements=pad_token_tensor).any()\n-            ):\n-                if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n-                    logger.warning_once(\n-                        \"The attention mask is not set and cannot be inferred from input because pad token is same as \"\n-                        \"eos token. As a consequence, you may observe unexpected behavior. Please pass your input's \"\n-                        \"`attention_mask` to obtain reliable results.\"\n-                    )\n-            if eos_token_tensor is not None and (\n-                torch.is_floating_point(eos_token_tensor) or (eos_token_tensor < 0).any()\n-            ):\n-                logger.warning(\n-                    f\"`eos_token_id` should consist of positive integers, but is {eos_token_tensor}. Your generation \"\n-                    \"will not stop until the maximum length is reached. Depending on other flags, it may even crash.\"\n+        if (\n+            eos_token_tensor is not None\n+            and isin_mps_friendly(elements=eos_token_tensor, test_elements=pad_token_tensor).any()\n+        ):\n+            if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n+                logger.warning_once(\n+                    \"The attention mask is not set and cannot be inferred from input because pad token is same as \"\n+                    \"eos token. As a consequence, you may observe unexpected behavior. Please pass your input's \"\n+                    \"`attention_mask` to obtain reliable results.\"\n                 )\n+        if eos_token_tensor is not None and (\n+            torch.is_floating_point(eos_token_tensor) or (eos_token_tensor < 0).any()\n+        ):\n+            logger.warning(\n+                f\"`eos_token_id` should consist of positive integers, but is {eos_token_tensor}. Your generation \"\n+                \"will not stop until the maximum length is reached. Depending on other flags, it may even crash.\"\n+            )\n \n         # Update generation config with the updated special tokens tensors\n         # NOTE: this must be written into a different attribute name than the one holding the original special tokens\n@@ -2095,7 +2068,7 @@ def generate(\n         self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n \n         # decoder-only models must use left-padding for batched generation.\n-        if not self.config.is_encoder_decoder and not is_torchdynamo_compiling():\n+        if not self.config.is_encoder_decoder:\n             # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n             # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n             if (\n@@ -2192,7 +2165,7 @@ def generate(\n                 \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n             )\n \n-        if not is_torchdynamo_compiling() and self.device.type != input_ids.device.type:\n+        if self.device.type != input_ids.device.type:\n             warnings.warn(\n                 \"You are calling .generate() with the `input_ids` being on a device type different\"\n                 f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n@@ -2442,43 +2415,29 @@ def typeerror():\n         # Convert to legacy cache format if requested\n         if (\n             generation_config.return_legacy_cache is True\n-            and not is_torchdynamo_compiling()\n             and hasattr(result, \"past_key_values\")\n             and getattr(result.past_key_values, \"to_legacy_cache\") is not None\n         ):\n             result.past_key_values = result.past_key_values.to_legacy_cache()\n         return result\n \n-    def _has_unfinished_sequences(\n-        self,\n-        this_peer_finished: bool,\n-        synced_gpus: bool,\n-        device: torch.device,\n-        cur_len: Optional[int] = None,\n-        max_length: Optional[int] = None,\n-    ) -> bool:\n+    def _has_unfinished_sequences(self, this_peer_finished: bool, synced_gpus: bool, device: torch.device) -> bool:\n         \"\"\"\n         Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\n         fed through `this_peer_finished`. ZeRO stage 3-friendly.\n         \"\"\"\n-        # torch.compile does not support data-dependent control flow. This is a workaround to allow torch.compile,\n-        # although we lose the ability to stop when all sequences return an EOS token (and other stopping criteria)\n-        # TODO (joao): remove this when torch's support for control flow is not experimental (https://pytorch.org/docs/stable/generated/torch.cond.html)\n-        if is_torchdynamo_compiling():\n-            return cur_len < max_length\n-        else:\n-            if synced_gpus:\n-                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n-                # The following logic allows an early break if all peers finished generating their sequence\n-                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(device)\n-                # send 0.0 if we finished, 1.0 otherwise\n-                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n-                # did all peers finish? the reduced sum will be 0.0 then\n-                if this_peer_finished_flag.item() == 0.0:\n-                    return False\n-            elif this_peer_finished:\n+        if synced_gpus:\n+            # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n+            # The following logic allows an early break if all peers finished generating their sequence\n+            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(device)\n+            # send 0.0 if we finished, 1.0 otherwise\n+            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n+            # did all peers finish? the reduced sum will be 0.0 then\n+            if this_peer_finished_flag.item() == 0.0:\n                 return False\n-            return True\n+        elif this_peer_finished:\n+            return False\n+        return True\n \n     def heal_tokens(\n         self, input_ids: torch.LongTensor, tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None\n@@ -3240,7 +3199,6 @@ def _sample(\n         output_scores = generation_config.output_scores\n         output_logits = generation_config.output_logits\n         return_dict_in_generate = generation_config.return_dict_in_generate\n-        max_length = generation_config.max_length\n         has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n         do_sample = generation_config.do_sample\n \n@@ -3277,9 +3235,7 @@ def _sample(\n                 model_forward = self.get_compiled_call(generation_config.compile_config)\n \n         is_prefill = True\n-        while self._has_unfinished_sequences(\n-            this_peer_finished, synced_gpus, device=input_ids.device, cur_len=cur_len, max_length=max_length\n-        ):\n+        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n             # prepare model inputs\n             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n "
        },
        {
            "sha": "e9a2c5f81a9c42f8df6b495cc1cc37f71c68fa3c",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -43,7 +43,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1590,7 +1589,7 @@ def prepare_inputs_for_generation(\n         if not empty_past_kv:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "f7fb333142857f4d00795901290d49463ae4a69c",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -51,7 +51,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1274,7 +1273,7 @@ def prepare_inputs_for_generation(\n         if not empty_past_kv:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "da9528cbc4ef3967eb7ebe3e699cc2b05f70848e",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_bloom import BloomConfig\n@@ -919,7 +918,7 @@ def prepare_inputs_for_generation(\n                 inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n             elif (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "3ed8fb39c2202e3c3db6854e90500ad8b84b42b2",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -36,7 +36,6 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -931,7 +930,7 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "154330b1c99aa5cac68a40ba8bed10c795f1b94e",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -29,7 +29,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n-    is_torchdynamo_compiling,\n     logging,\n )\n from ..cohere.modeling_cohere import (\n@@ -603,7 +602,7 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "abd0d44fe7913df8be5ab9af65cc620c81f3384b",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -42,7 +42,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1571,7 +1570,7 @@ def prepare_inputs_for_generation(\n         if not empty_past_kv:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "fb0ff7d04561ab133739819542ea98f81a34287e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -33,7 +33,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -2194,7 +2193,7 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "6a6151b6788aaf605c9d39f3fd286c1aa301c068",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -2507,7 +2507,7 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "6c8b4c520b7cc70e9014208140d677295aa99a3b",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -45,7 +45,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1325,7 +1324,7 @@ def prepare_inputs_for_generation(\n             #              (we can't check exception 3 while compiling)\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        },
        {
            "sha": "dc83935ce3705c5b75109148f90114a10c6467b8",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fe0bae0a833febfb7209fe133837b4a97bbe3fb/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=0fe0bae0a833febfb7209fe133837b4a97bbe3fb",
            "patch": "@@ -40,7 +40,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1762,7 +1761,7 @@ def prepare_inputs_for_generation(\n             #              (we can't check exception 3 while compiling)\n             if (\n                 inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n             ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)"
        }
    ],
    "stats": {
        "total": 205,
        "additions": 76,
        "deletions": 129
    }
}