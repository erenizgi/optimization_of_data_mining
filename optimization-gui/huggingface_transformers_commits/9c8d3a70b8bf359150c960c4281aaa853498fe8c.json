{
    "author": "eustlb",
    "message": "Pipeline: fix unnecessary warnings (#35753)\n\n* return attention mask\n\n* use correct model input name\n\n* fix\n\n* make",
    "sha": "9c8d3a70b8bf359150c960c4281aaa853498fe8c",
    "files": [
        {
            "sha": "e8b4af94c72c580034e1bfe57670f95f6858e948",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c8d3a70b8bf359150c960c4281aaa853498fe8c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c8d3a70b8bf359150c960c4281aaa853498fe8c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=9c8d3a70b8bf359150c960c4281aaa853498fe8c",
            "patch": "@@ -64,7 +64,12 @@ def chunk_iter(inputs, feature_extractor, chunk_len, stride_left, stride_right,\n     for chunk_start_idx in range(0, inputs_len, step):\n         chunk_end_idx = chunk_start_idx + chunk_len\n         chunk = inputs[chunk_start_idx:chunk_end_idx]\n-        processed = feature_extractor(chunk, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")\n+        processed = feature_extractor(\n+            chunk,\n+            sampling_rate=feature_extractor.sampling_rate,\n+            return_tensors=\"pt\",\n+            return_attention_mask=True,\n+        )\n         if dtype is not None:\n             processed = processed.to(dtype=dtype)\n         _stride_left = 0 if chunk_start_idx == 0 else stride_left\n@@ -507,11 +512,14 @@ def _forward(self, model_inputs, return_timestamps=False, **generate_kwargs):\n             if \"generation_config\" not in generate_kwargs:\n                 generate_kwargs[\"generation_config\"] = self.generation_config\n \n-            tokens = self.model.generate(\n-                inputs=inputs,\n-                attention_mask=attention_mask,\n+            main_input_name = self.model.main_input_name if hasattr(self.model, \"main_input_name\") else \"inputs\"\n+            generate_kwargs = {\n+                main_input_name: inputs,\n+                \"attention_mask\": attention_mask,\n                 **generate_kwargs,\n-            )\n+            }\n+            tokens = self.model.generate(**generate_kwargs)\n+\n             # whisper longform generation stores timestamps in \"segments\"\n             if return_timestamps == \"word\" and self.type == \"seq2seq_whisper\":\n                 if \"segments\" not in tokens:"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 13,
        "deletions": 5
    }
}