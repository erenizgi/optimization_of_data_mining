{
    "author": "majiayu000",
    "message": "fix(tvp): add missing type_vocab_size parameter to TvpConfig (#42928)\n\nFixes #42925\n\nThe `type_vocab_size` parameter was missing from TvpConfig, causing an\nAttributeError when instantiating TvpModel. This adds the parameter\nwith a default value of 2, which matches all TVP models on Hugging Face\nHub.\n\nCo-authored-by: Claude Opus 4.5 <noreply@anthropic.com>",
    "sha": "b62e5b3e4060c07b0e595f670135d85bb51673fe",
    "files": [
        {
            "sha": "7458cf4df91a12b382487c71be2f8b0e7a8f7a96",
            "filename": "src/transformers/models/tvp/configuration_tvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b62e5b3e4060c07b0e595f670135d85bb51673fe/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b62e5b3e4060c07b0e595f670135d85bb51673fe/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py?ref=b62e5b3e4060c07b0e595f670135d85bb51673fe",
            "patch": "@@ -68,6 +68,8 @@ class TvpConfig(PreTrainedConfig):\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the Tvp text model. Defines the number of different tokens that can be represented by\n             the `inputs_ids` passed when calling [`TvpModel`].\n+        type_vocab_size (`int`, *optional*, defaults to 2):\n+            The vocabulary size of the `token_type_ids` passed when calling [`TvpModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers.\n         intermediate_size (`int`, *optional*, defaults to 3072):\n@@ -114,6 +116,7 @@ def __init__(\n         max_img_size=448,\n         num_frames=48,\n         vocab_size=30522,\n+        type_vocab_size=2,\n         hidden_size=768,\n         intermediate_size=3072,\n         num_hidden_layers=12,\n@@ -157,6 +160,7 @@ def __init__(\n         self.max_img_size = max_img_size\n         self.num_frames = num_frames\n         self.vocab_size = vocab_size\n+        self.type_vocab_size = type_vocab_size\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}