{
    "author": "imkero",
    "message": "Fix Qwen2.5-Omni get_chunked_index chunking functionality (#37631)\n\n* fix: qwen2.5 omni modular get_rope_index\n\n* test: add test for qwen2.5 omni rope index (video with audio input)\n\n* style\n\n* expected_position_ids readability\n\n* fix: use spatial_merge_size = 1 in unit test",
    "sha": "5f791281c363a58dd6b30b9217333679d1802a73",
    "files": [
        {
            "sha": "bda93a796e906f98ee3b02a8ed3e34e7f8c427b1",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f791281c363a58dd6b30b9217333679d1802a73/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f791281c363a58dd6b30b9217333679d1802a73/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=5f791281c363a58dd6b30b9217333679d1802a73",
            "patch": "@@ -244,7 +244,8 @@ def get_chunked_index(\n         - the second chunk contains values >= 1000 and < 2000, and so on.\n \n         Parameters:\n-            token_indices (`List[int]`): A monotonically increasing list of token index values.\n+            token_indices (`torch.Tensor` of shape `(seq_len, )`): A monotonically increasing list of\n+                                token index values.\n             t_ntoken_per_chunk (`int`): Number of tokens per chunk (used as the chunk size threshold).\n             remove_index (`int`) An index id to subtract from `token_indices` before chunking\n \n@@ -257,12 +258,12 @@ def _iter():\n             i, start_idx = 0, 0  # skip bos token\n             current_chunk = 1\n             while i < len(token_indices):  # skip eos token\n-                if token_indices[0][i] - remove_index >= current_chunk * tokens_per_chunk:\n+                if token_indices[i] - remove_index >= current_chunk * tokens_per_chunk:\n                     yield (start_idx, i)\n                     start_idx = i\n                     current_chunk += 1\n                 i += 1\n-            yield (start_idx, token_indices.shape[1])\n+            yield (start_idx, len(token_indices))\n \n         return list(_iter())\n \n@@ -499,8 +500,8 @@ def get_rope_index(\n                         )\n \n                         t_ntoken_per_chunk = int(position_id_per_seconds * seconds_per_chunk)\n-                        video_chunk_indexes = self.get_chunked_index(video_llm_pos_ids, t_ntoken_per_chunk, st_idx)\n-                        audio_chunk_indexes = self.get_chunked_index(audio_llm_pos_ids, t_ntoken_per_chunk, st_idx)\n+                        video_chunk_indexes = self.get_chunked_index(video_llm_pos_ids[0], t_ntoken_per_chunk, st_idx)\n+                        audio_chunk_indexes = self.get_chunked_index(audio_llm_pos_ids[0], t_ntoken_per_chunk, st_idx)\n                         sub_len = 0\n                         for j in range(max(len(video_chunk_indexes), len(audio_chunk_indexes))):\n                             video_chunk_index = video_chunk_indexes[j] if j < len(video_chunk_indexes) else None"
        },
        {
            "sha": "61ef59d9a169d6cf88b733c224b55d96d43f0aed",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f791281c363a58dd6b30b9217333679d1802a73/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f791281c363a58dd6b30b9217333679d1802a73/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=5f791281c363a58dd6b30b9217333679d1802a73",
            "patch": "@@ -1145,7 +1145,8 @@ def get_chunked_index(\n         - the second chunk contains values >= 1000 and < 2000, and so on.\n \n         Parameters:\n-            token_indices (`List[int]`): A monotonically increasing list of token index values.\n+            token_indices (`torch.Tensor` of shape `(seq_len, )`): A monotonically increasing list of\n+                                token index values.\n             t_ntoken_per_chunk (`int`): Number of tokens per chunk (used as the chunk size threshold).\n             remove_index (`int`) An index id to subtract from `token_indices` before chunking\n \n@@ -1158,12 +1159,12 @@ def _iter():\n             i, start_idx = 0, 0  # skip bos token\n             current_chunk = 1\n             while i < len(token_indices):  # skip eos token\n-                if token_indices[0][i] - remove_index >= current_chunk * tokens_per_chunk:\n+                if token_indices[i] - remove_index >= current_chunk * tokens_per_chunk:\n                     yield (start_idx, i)\n                     start_idx = i\n                     current_chunk += 1\n                 i += 1\n-            yield (start_idx, token_indices.shape[1])\n+            yield (start_idx, len(token_indices))\n \n         return list(_iter())\n \n@@ -1400,8 +1401,8 @@ def get_rope_index(\n                         )\n \n                         t_ntoken_per_chunk = int(position_id_per_seconds * seconds_per_chunk)\n-                        video_chunk_indexes = self.get_chunked_index(video_llm_pos_ids, t_ntoken_per_chunk, st_idx)\n-                        audio_chunk_indexes = self.get_chunked_index(audio_llm_pos_ids, t_ntoken_per_chunk, st_idx)\n+                        video_chunk_indexes = self.get_chunked_index(video_llm_pos_ids[0], t_ntoken_per_chunk, st_idx)\n+                        audio_chunk_indexes = self.get_chunked_index(audio_llm_pos_ids[0], t_ntoken_per_chunk, st_idx)\n                         sub_len = 0\n                         for j in range(max(len(video_chunk_indexes), len(audio_chunk_indexes))):\n                             video_chunk_index = video_chunk_indexes[j] if j < len(video_chunk_indexes) else None"
        },
        {
            "sha": "64b444b716f8ce48ea75d305e41b09ca535caff1",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f791281c363a58dd6b30b9217333679d1802a73/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f791281c363a58dd6b30b9217333679d1802a73/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=5f791281c363a58dd6b30b9217333679d1802a73",
            "patch": "@@ -289,7 +289,7 @@ def get_chunked_index(self, token_indices: np.ndarray, tokens_per_chunk: int) ->\n         - the second chunk contains values >= 1000 and < 2000, and so on.\n \n         Parameters:\n-            token_indices (`List[int]`): A monotonically increasing list of token index values.\n+            token_indices (`np.ndarray`): A monotonically increasing list of token index values.\n             t_ntoken_per_chunk (`int`): Number of tokens per chunk (used as the chunk size threshold).\n \n         Returns:"
        },
        {
            "sha": "116425f349c0a11f477cde1500104f794124a38f",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f791281c363a58dd6b30b9217333679d1802a73/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f791281c363a58dd6b30b9217333679d1802a73/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=5f791281c363a58dd6b30b9217333679d1802a73",
            "patch": "@@ -381,6 +381,143 @@ def test_generate_with_static_cache(self):\n     def test_custom_4d_attention_mask(self):\n         pass\n \n+    def test_get_rope_index_video_with_audio(self):\n+        image_grid_thw = torch.empty((0, 3), dtype=torch.long)\n+\n+        # 3 * 2 * 2 = 12 video tokens\n+        video_grid_thw = torch.tensor([[3, 2, 2]], dtype=torch.long)\n+\n+        # num_audio_tokens = ((audio_seqlen - 1) // 2 + 1 - 2) // 2 + 1\n+        # i.e.: 300 audio_seqlen -> 75 audio tokens\n+        audio_seqlens = torch.tensor([300], dtype=torch.long)\n+\n+        second_per_grids = torch.tensor([1.0], dtype=torch.float)\n+\n+        use_audio_in_video = True\n+\n+        # fmt: off\n+        expected_position_ids = torch.tensor([\n+            [[\n+                 0,  1, # text\n+                 2,  2, # vision_bos + audio_bos\n+\n+                # video chunk\n+                  3,  3,  3,  3,\n+                 28, 28, 28, 28,\n+\n+                # audio chunk\n+                 3,  4,  5,  6,  7,  8, 9, 10, 11, 12, 13, 14, 15, 16,\n+                17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n+                31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n+                45, 46, 47, 48, 49, 50, 51, 52,\n+\n+                # video chunk\n+                53, 53, 53, 53,\n+\n+                # audio chunk\n+                53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n+                67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n+\n+                78, 78, # audio_eos + vision_eos\n+                79, 80, # text\n+            ]],\n+            [[\n+                 0,  1, # text\n+                 2,  2, # vision_bos + audio_bos\n+\n+                # video chunk\n+                 3,  3,  4,  4,\n+                 3,  3,  4,  4,\n+\n+                # audio chunk\n+                 3,  4,  5,  6,  7,  8, 9, 10, 11, 12, 13, 14, 15, 16,\n+                17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n+                31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n+                45, 46, 47, 48, 49, 50, 51, 52,\n+\n+                # video chunk\n+                 3,  3,  4,  4,\n+\n+                # audio chunk\n+                53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n+                67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n+\n+                78, 78, # audio_eos + vision_eos\n+                79, 80, # text\n+            ]],\n+            [[\n+                 0,  1, # text\n+                 2,  2, # vision_bos + audio_bos\n+\n+                # video chunk\n+                 3,  4,  3,  4,\n+                 3,  4,  3,  4,\n+\n+                # audio chunk\n+                 3,  4,  5,  6,  7,  8, 9, 10, 11, 12, 13, 14, 15, 16,\n+                17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n+                31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n+                45, 46, 47, 48, 49, 50, 51, 52,\n+\n+                # video chunk\n+                3,  4,  3,  4,\n+\n+                # audio chunk\n+                53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n+                67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n+\n+                78, 78, # audio_eos + vision_eos\n+                79, 80, # text\n+            ]],\n+        ], dtype=torch.long)\n+        # fmt: on\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            input_ids = torch.tensor(\n+                [\n+                    [\n+                        100,\n+                        101,\n+                    ]\n+                    + [\n+                        config.vision_start_token_id,\n+                        config.audio_start_token_id,\n+                    ]\n+                    # 1st chunk: 8 video tokens, 50 audio tokens\n+                    + [config.video_token_id] * 2 * 2 * 2\n+                    + [config.audio_token_id] * 50\n+                    +\n+                    # 2nd chunk: 4 video tokens, 25 audio tokens\n+                    [config.video_token_id] * 1 * 2 * 2\n+                    + [config.audio_token_id] * 25\n+                    + [\n+                        config.audio_end_token_id,\n+                        config.vision_end_token_id,\n+                    ]\n+                    + [\n+                        102,\n+                        103,\n+                    ]\n+                ],\n+                dtype=torch.long,\n+            )\n+\n+            model = model_class(config)\n+\n+            position_ids, mrope_position_deltas = model.get_rope_index(\n+                input_ids=input_ids,\n+                image_grid_thw=image_grid_thw,\n+                video_grid_thw=video_grid_thw,\n+                attention_mask=None,\n+                use_audio_in_video=use_audio_in_video,\n+                audio_seqlens=audio_seqlens,\n+                second_per_grids=second_per_grids,\n+            )\n+\n+            self.assertTrue(torch.equal(position_ids, expected_position_ids))\n+\n \n @require_torch\n class Qwen2_5OmniModelIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 161,
        "additions": 150,
        "deletions": 11
    }
}