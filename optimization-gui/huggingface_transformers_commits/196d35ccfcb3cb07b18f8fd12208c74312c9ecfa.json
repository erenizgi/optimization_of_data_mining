{
    "author": "matthewdouglas",
    "message": "Add AdEMAMix optimizer (#33682)\n\n* Add AdEMAMix optimizer\r\n\r\n* Fix test\r\n\r\n* Update tests/trainer/test_trainer.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "196d35ccfcb3cb07b18f8fd12208c74312c9ecfa",
    "files": [
        {
            "sha": "216d5cd4296008b7fbbac08742b2cbb335060779",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/196d35ccfcb3cb07b18f8fd12208c74312c9ecfa/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/196d35ccfcb3cb07b18f8fd12208c74312c9ecfa/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=196d35ccfcb3cb07b18f8fd12208c74312c9ecfa",
            "patch": "@@ -1237,6 +1237,10 @@ def get_optimizer_cls_and_kwargs(\n             OptimizerNames.ADAMW_8BIT,\n             OptimizerNames.PAGED_ADAMW,\n             OptimizerNames.PAGED_ADAMW_8BIT,\n+            OptimizerNames.ADEMAMIX,\n+            OptimizerNames.ADEMAMIX_8BIT,\n+            OptimizerNames.PAGED_ADEMAMIX,\n+            OptimizerNames.PAGED_ADEMAMIX_8BIT,\n             OptimizerNames.LION,\n             OptimizerNames.LION_8BIT,\n             OptimizerNames.PAGED_LION,\n@@ -1266,6 +1270,33 @@ def get_optimizer_cls_and_kwargs(\n                     # Above we pass all `adam_kwargs` to the optimizer, here\n                     # we only pass `optim_args` which can be passed by the user.\n                     additional_optim_kwargs = optim_args\n+                elif \"ademamix\" in args.optim:\n+                    if is_bitsandbytes_available() and version.parse(\n+                        importlib.metadata.version(\"bitsandbytes\")\n+                    ) < version.parse(\"0.44.0\"):\n+                        raise ValueError(\n+                            \"The AdEMAMix optimizer is not supported by your current version of `bitsandbytes`. \"\n+                            \"Please install `bitsandbytes` >= 0.44.0.\"\n+                        )\n+\n+                    from bitsandbytes.optim import AdEMAMix\n+\n+                    optimizer_cls = AdEMAMix\n+                    additional_optim_kwargs = {\n+                        \"betas\": (\n+                            float(optim_args.get(\"beta1\", args.adam_beta1)),\n+                            float(optim_args.get(\"beta2\", args.adam_beta2)),\n+                            float(optim_args.get(\"beta3\", 0.9999)),\n+                        ),\n+                        \"alpha\": float(optim_args.get(\"alpha\", 5.0)),\n+                        \"eps\": float(optim_args.get(\"eps\", args.adam_epsilon)),\n+                    }\n+\n+                    if \"t_alpha\" in optim_args:\n+                        additional_optim_kwargs[\"t_alpha\"] = int(optim_args[\"t_alpha\"])\n+\n+                    if \"t_beta3\" in optim_args:\n+                        additional_optim_kwargs[\"t_beta3\"] = int(optim_args[\"t_beta3\"])\n \n                 bnb_kwargs = {\"optim_bits\": optim_bits}\n                 if \"rmsprop\" not in args.optim:"
        },
        {
            "sha": "596917928350bfac4996990f17af5c6921fbf756",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/196d35ccfcb3cb07b18f8fd12208c74312c9ecfa/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/196d35ccfcb3cb07b18f8fd12208c74312c9ecfa/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=196d35ccfcb3cb07b18f8fd12208c74312c9ecfa",
            "patch": "@@ -155,14 +155,18 @@ class OptimizerNames(ExplicitEnum):\n     ADAFACTOR = \"adafactor\"\n     ADAMW_ANYPRECISION = \"adamw_anyprecision\"\n     ADAMW_TORCH_4BIT = \"adamw_torch_4bit\"\n+    ADEMAMIX = \"ademamix\"\n     SGD = \"sgd\"\n     ADAGRAD = \"adagrad\"\n     ADAMW_BNB = \"adamw_bnb_8bit\"\n     ADAMW_8BIT = \"adamw_8bit\"  # just an alias for adamw_bnb_8bit\n+    ADEMAMIX_8BIT = \"ademamix_8bit\"\n     LION_8BIT = \"lion_8bit\"\n     LION = \"lion_32bit\"\n     PAGED_ADAMW = \"paged_adamw_32bit\"\n     PAGED_ADAMW_8BIT = \"paged_adamw_8bit\"\n+    PAGED_ADEMAMIX = \"paged_ademamix_32bit\"\n+    PAGED_ADEMAMIX_8BIT = \"paged_ademamix_8bit\"\n     PAGED_LION = \"paged_lion_32bit\"\n     PAGED_LION_8BIT = \"paged_lion_8bit\"\n     RMSPROP = \"rmsprop\"\n@@ -618,7 +622,7 @@ class TrainingArguments:\n             \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n             for a full list of optimizers.\n         optim_args (`str`, *optional*):\n-            Optional arguments that are supplied to AnyPrecisionAdamW.\n+            Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n         group_by_length (`bool`, *optional*, defaults to `False`):\n             Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n             padding applied and be more efficient). Only useful if applying dynamic padding."
        },
        {
            "sha": "0035ff7de8ba9776e977f10b587bc5dc7011bf08",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 165,
            "deletions": 0,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/196d35ccfcb3cb07b18f8fd12208c74312c9ecfa/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/196d35ccfcb3cb07b18f8fd12208c74312c9ecfa/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=196d35ccfcb3cb07b18f8fd12208c74312c9ecfa",
            "patch": "@@ -15,6 +15,7 @@\n \n import dataclasses\n import gc\n+import importlib\n import json\n import math\n import os\n@@ -32,6 +33,7 @@\n \n import numpy as np\n from huggingface_hub import HfFolder, ModelCard, create_branch, delete_repo, list_repo_commits, list_repo_files\n+from packaging import version\n from parameterized import parameterized\n from requests.exceptions import HTTPError\n \n@@ -1091,6 +1093,40 @@ def test_rmsprop_bnb(self):\n             # Check that it trains without errors\n             trainer.train()\n \n+    @require_bitsandbytes\n+    def test_ademamix_bnb(self):\n+        config = GPT2Config(vocab_size=100, n_positions=128, n_embd=32, n_layer=3, n_head=4)\n+        tiny_gpt2 = GPT2LMHeadModel(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            # Trainer without inf/nan filter\n+            args = TrainingArguments(\n+                tmpdir, learning_rate=1e-9, logging_steps=5, logging_nan_inf_filter=False, optim=\"ademamix\"\n+            )\n+            trainer = Trainer(tiny_gpt2, args, train_dataset=train_dataset)\n+\n+            # Check that it trains without errors\n+            trainer.train()\n+\n+    @require_bitsandbytes\n+    def test_ademamix_bnb_8bit(self):\n+        config = GPT2Config(vocab_size=100, n_positions=128, n_embd=32, n_layer=3, n_head=4)\n+        tiny_gpt2 = GPT2LMHeadModel(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            # Trainer without inf/nan filter\n+            args = TrainingArguments(\n+                tmpdir, learning_rate=1e-9, logging_steps=5, logging_nan_inf_filter=False, optim=\"ademamix_8bit\"\n+            )\n+            trainer = Trainer(tiny_gpt2, args, train_dataset=train_dataset)\n+\n+            # Check that it trains without errors\n+            trainer.train()\n+\n     @require_bitsandbytes\n     def test_rmsprop_bnb_8bit(self):\n         config = GPT2Config(vocab_size=100, n_positions=128, n_embd=32, n_layer=3, n_head=4)\n@@ -4187,6 +4223,13 @@ def hp_name(trial):\n         \"lr\": TrainingArguments.learning_rate,\n     }\n \n+    default_ademamix_kwargs = {\n+        \"betas\": (TrainingArguments.adam_beta1, TrainingArguments.adam_beta2, 0.9999),\n+        \"alpha\": 5.0,\n+        \"eps\": TrainingArguments.adam_epsilon,\n+        \"lr\": TrainingArguments.learning_rate,\n+    }\n+\n     default_anyprecision_kwargs = {\n         \"use_kahan_summation\": False,\n         \"momentum_dtype\": torch.float32,\n@@ -4291,6 +4334,36 @@ def hp_name(trial):\n             )\n         )\n \n+        if version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\"0.44.0\"):\n+            optim_test_params.append(\n+                (\n+                    TrainingArguments(optim=OptimizerNames.ADEMAMIX, output_dir=\"None\"),\n+                    bnb.optim.AdEMAMix,\n+                    default_ademamix_kwargs,\n+                )\n+            )\n+            optim_test_params.append(\n+                (\n+                    TrainingArguments(optim=OptimizerNames.ADEMAMIX_8BIT, output_dir=\"None\"),\n+                    bnb.optim.AdEMAMix,\n+                    default_ademamix_kwargs,\n+                )\n+            )\n+            optim_test_params.append(\n+                (\n+                    TrainingArguments(optim=OptimizerNames.PAGED_ADEMAMIX_8BIT, output_dir=\"None\"),\n+                    bnb.optim.AdEMAMix,\n+                    default_ademamix_kwargs,\n+                )\n+            )\n+            optim_test_params.append(\n+                (\n+                    TrainingArguments(optim=OptimizerNames.PAGED_ADEMAMIX, output_dir=\"None\"),\n+                    bnb.optim.AdEMAMix,\n+                    default_ademamix_kwargs,\n+                )\n+            )\n+\n     if is_torchdistx_available():\n         import torchdistx\n \n@@ -4420,6 +4493,62 @@ def test_bnb_paged_adam8bit(self):\n                 default_adam_kwargs,\n             )\n \n+    def test_bnb_ademamix(self):\n+        mock = Mock()\n+        modules = {\n+            \"bitsandbytes\": mock,\n+            \"bitsandbytes.optim\": mock.optim,\n+            \"bitsandbytes.optim.AdEMAMix\": mock.optim.AdEMAMix,\n+        }\n+        with patch.dict(\"sys.modules\", modules):\n+            self.check_optim_and_kwargs(\n+                TrainingArguments(optim=OptimizerNames.ADEMAMIX, output_dir=\"None\"),\n+                mock.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n+            )\n+\n+    def test_bnb_ademamix8bit(self):\n+        mock = Mock()\n+        modules = {\n+            \"bitsandbytes\": mock,\n+            \"bitsandbytes.optim\": mock.optim,\n+            \"bitsandbytes.optim.AdEMAMix\": mock.optim.AdEMAMix,\n+        }\n+        with patch.dict(\"sys.modules\", modules):\n+            self.check_optim_and_kwargs(\n+                TrainingArguments(optim=OptimizerNames.ADEMAMIX_8BIT, output_dir=\"None\"),\n+                mock.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n+            )\n+\n+    def test_bnb_paged_ademamix(self):\n+        mock = Mock()\n+        modules = {\n+            \"bitsandbytes\": mock,\n+            \"bitsandbytes.optim\": mock.optim,\n+            \"bitsandbytes.optim.AdEMAMix\": mock.optim.AdEMAMix,\n+        }\n+        with patch.dict(\"sys.modules\", modules):\n+            self.check_optim_and_kwargs(\n+                TrainingArguments(optim=OptimizerNames.PAGED_ADEMAMIX, output_dir=\"None\"),\n+                mock.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n+            )\n+\n+    def test_bnb_paged_ademamix8bit(self):\n+        mock = Mock()\n+        modules = {\n+            \"bitsandbytes\": mock,\n+            \"bitsandbytes.optim\": mock.optim,\n+            \"bitsandbytes.optim.AdEMAMix\": mock.optim.AdEMAMix,\n+        }\n+        with patch.dict(\"sys.modules\", modules):\n+            self.check_optim_and_kwargs(\n+                TrainingArguments(optim=OptimizerNames.PAGED_ADEMAMIX_8BIT, output_dir=\"None\"),\n+                mock.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n+            )\n+\n     def test_bnb_lion(self):\n         mock = Mock()\n         modules = {\n@@ -4503,6 +4632,42 @@ def test_bnb_paged_adam8bit_no_bnb(self):\n             with self.assertRaises(ValueError):\n                 Trainer.get_optimizer_cls_and_kwargs(args)\n \n+    def test_bnb_ademamix_no_bnb(self):\n+        args = TrainingArguments(optim=OptimizerNames.ADEMAMIX, output_dir=\"None\")\n+\n+        # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n+        # bnb will fail even if `bitsandbytes` is installed.\n+        with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n+            with self.assertRaises(ValueError):\n+                Trainer.get_optimizer_cls_and_kwargs(args)\n+\n+    def test_bnb_ademamix8bit_no_bnb(self):\n+        args = TrainingArguments(optim=OptimizerNames.ADEMAMIX_8BIT, output_dir=\"None\")\n+\n+        # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n+        # bnb will fail even if `bitsandbytes` is installed.\n+        with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n+            with self.assertRaises(ValueError):\n+                Trainer.get_optimizer_cls_and_kwargs(args)\n+\n+    def test_bnb_paged_ademamix_no_bnb(self):\n+        args = TrainingArguments(optim=OptimizerNames.PAGED_ADEMAMIX, output_dir=\"None\")\n+\n+        # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n+        # bnb will fail even if `bitsandbytes` is installed.\n+        with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n+            with self.assertRaises(ValueError):\n+                Trainer.get_optimizer_cls_and_kwargs(args)\n+\n+    def test_bnb_paged_ademamix8bit_no_bnb(self):\n+        args = TrainingArguments(optim=OptimizerNames.PAGED_ADEMAMIX_8BIT, output_dir=\"None\")\n+\n+        # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n+        # bnb will fail even if `bitsandbytes` is installed.\n+        with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n+            with self.assertRaises(ValueError):\n+                Trainer.get_optimizer_cls_and_kwargs(args)\n+\n     def test_bnb_paged_lion_no_bnb(self):\n         args = TrainingArguments(optim=OptimizerNames.PAGED_LION, output_dir=\"None\")\n "
        }
    ],
    "stats": {
        "total": 202,
        "additions": 201,
        "deletions": 1
    }
}