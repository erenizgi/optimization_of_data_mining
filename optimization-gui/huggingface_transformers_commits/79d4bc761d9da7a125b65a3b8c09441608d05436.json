{
    "author": "zucchini-nlp",
    "message": "[causal mask] fix preparation with multi-gpu (#37612)\n\n* fix multi-gpu\n\n* forgot non-copied models\n\n* fixup",
    "sha": "79d4bc761d9da7a125b65a3b8c09441608d05436",
    "files": [
        {
            "sha": "c0f6da8d76813b53ea6c354804378f819d59c38e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -557,10 +557,8 @@ def prepare_inputs_for_generation(\n         if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n             if model_inputs[\"inputs_embeds\"] is not None:\n                 batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n             else:\n                 batch_size, sequence_length = model_inputs[input_ids_key].shape\n-                device = model_inputs[input_ids_key].device\n \n             # Create the causal mask with fixed shape in advance, to reduce recompilations. If the function to create\n             # the 4D causal mask exists, it should be present in the base model (XXXModel class) or in its decoder.\n@@ -586,7 +584,6 @@ def prepare_inputs_for_generation(\n                     sequence_length=sequence_length,\n                     target_length=past_key_values.get_max_cache_shape(),\n                     dtype=self.dtype,\n-                    device=device,\n                     cache_position=cache_position,\n                     batch_size=batch_size,\n                     config=self.config,"
        },
        {
            "sha": "8bab20bcee142a497be58f6dd29a49a7a899035d",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1003,7 +1003,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1020,7 +1020,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1045,7 +1044,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1065,8 +1063,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1078,11 +1074,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "074eea3aa971b4a747396f454be38f08ffc136ea",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1313,7 +1313,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         target_length = (\n             attention_mask.shape[-1]\n@@ -1327,7 +1327,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1352,7 +1351,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1372,8 +1370,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1385,11 +1381,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "dd0d0e62c6214230f83fc65aa8e12d0d29347070",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1081,7 +1081,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         target_length = (\n             attention_mask.shape[-1]\n@@ -1095,7 +1095,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1120,7 +1119,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1140,8 +1138,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1153,11 +1149,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "a7293bf541545883cf77b0e09d05c9384519f258",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -773,7 +773,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -790,7 +790,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -816,7 +815,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -836,8 +834,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -849,11 +845,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "9a7d43bdb5e0d9eefc3a103659b123b0b00a3e67",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1406,7 +1406,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1423,7 +1423,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1449,7 +1448,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1469,8 +1467,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1482,11 +1478,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "0444c278d759805decd6b43720e11deb4ff0ceb5",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -619,7 +619,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -636,7 +636,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -662,7 +661,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -682,8 +680,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -695,11 +691,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "610144d43c9b0dbc739c1c7e7d2968ec0d3ff02f",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -652,7 +652,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -669,7 +669,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -694,7 +693,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -714,8 +712,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -727,11 +723,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "ab6055eaa11111d3202178e5c2eb3fbb9ca6512d",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -686,7 +686,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -706,8 +705,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -719,11 +716,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "565a1b62390b2f49dedac4fc79b0261bac0c6c2f",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1144,7 +1144,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1161,7 +1161,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1187,7 +1186,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1207,8 +1205,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1220,11 +1216,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "043f1fbcbf021a47131d34d70e21a4d2457c14e0",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -797,7 +797,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -814,7 +814,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -839,7 +838,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -859,8 +857,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -872,11 +868,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "10bed9ccaa5d60d504d5f7941d58f5644257c5ad",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -900,7 +900,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -917,7 +917,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -942,7 +941,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -962,8 +960,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -975,11 +971,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "f5a626a21fac9b0f6d5849c966bb43294411dde4",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1486,7 +1486,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1503,7 +1503,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1528,7 +1527,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1548,8 +1546,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1561,11 +1557,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "b3f5b12f626e7093417292af556382e76c0876b4",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1076,7 +1076,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1096,8 +1095,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1109,11 +1106,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "0d011a5f916c544a308bb5bf664ae4213095094d",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -620,7 +620,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -637,7 +637,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -662,7 +661,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -682,8 +680,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -695,11 +691,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "f779a74fae078bfddf38c32bc61f240345572a7b",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -711,7 +711,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -731,8 +730,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -744,11 +741,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "79da2149d9e9914d03a49035a554a314947db2aa",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -796,7 +796,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -816,8 +815,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -829,11 +826,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "1d72c48232a12aa595d12c71e43ebdc82176dc3f",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -635,7 +635,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -652,7 +652,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -677,7 +676,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -697,8 +695,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -710,11 +706,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "aee3d975727b5aa0bea405a2dfb31ea1d28b104c",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -643,7 +643,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -660,7 +660,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -685,7 +684,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -705,8 +703,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -718,11 +714,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "3b38c7fd034773457645a6e4bf24b0f736b63130",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -820,7 +820,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -837,7 +837,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -863,7 +862,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -883,8 +881,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -896,11 +892,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "ded5ad00d45a60dbd110ca28989a9cd0046d3ed2",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -634,7 +634,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -651,7 +651,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -676,7 +675,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -696,8 +694,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -709,11 +705,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "cd4d904b126ee88001e16e2a6381c85492ac6544",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -671,7 +671,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -688,7 +688,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -714,7 +713,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -734,8 +732,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -747,11 +743,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "a6e06f80ed852643ada1cdc5ff023ca38c15be8c",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -921,7 +921,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -938,7 +938,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -964,7 +963,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -984,8 +982,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -997,11 +993,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "9446013cf4476ecb748ef7eb993c4da23713b947",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -635,7 +635,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -652,7 +652,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -677,7 +676,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -697,8 +695,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -710,11 +706,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "d417535db785d0c77cedf27dc977c2f3107c4d04",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1120,7 +1120,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1137,7 +1137,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1163,7 +1162,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1183,8 +1181,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1196,11 +1192,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "59d679fb3c20e0b6206abf90524ed50c1d48f266",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1065,7 +1065,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1082,7 +1082,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1107,7 +1106,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1127,8 +1125,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1140,11 +1136,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "90d0c7011bd8c4fcde912a8dbd3a2b1ea8cdde16",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -620,7 +620,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -637,7 +637,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -662,7 +661,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -682,8 +680,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -695,11 +691,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "9b7d2603004bc723dee1a8c82ce8eb297fb7c708",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1416,7 +1416,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1433,7 +1433,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1459,7 +1458,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1479,8 +1477,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1492,11 +1488,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "cabebb90ef38421bc91ccc6bb9a66aa12e403e3a",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1123,7 +1123,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1140,7 +1140,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1166,7 +1165,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1186,8 +1184,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1199,11 +1195,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "86a4613b155c5d107fa5dc560e668a823da43087",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -625,7 +625,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -642,7 +642,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -667,7 +666,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -687,8 +685,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -700,11 +696,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "d1611a2fe0931d626e004a48c8fd3c05c85ca4b9",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -785,7 +785,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=max(full_cache_length, attention_chunk_size),\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1846,7 +1845,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1866,8 +1864,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1879,11 +1875,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "b4434eae50a38b76d3ee45df86ba909afd017348",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1631,7 +1631,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1648,7 +1648,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1674,7 +1673,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1694,8 +1692,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1707,11 +1703,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "b8f24040ddb1a586550d7eeda2b35eba76972f8b",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1086,7 +1086,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1106,7 +1106,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1133,7 +1132,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: MimiConfig,\n@@ -1152,8 +1150,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1169,14 +1165,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "357598523238a30eaaefb761f5eefd6470f86ba2",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -609,7 +609,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -629,7 +629,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -655,7 +654,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: MistralConfig,\n@@ -674,8 +672,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -691,14 +687,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "0d525407152682652244d6ee69708b9dc2f5f365",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -171,7 +171,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -191,7 +191,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -217,7 +216,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: MistralConfig,\n@@ -236,8 +234,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -253,14 +249,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "e8012a8d9efc6a4b8778a707f3eb4d1455bf868a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -751,7 +751,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -771,7 +771,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -797,7 +796,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: MixtralConfig,\n@@ -816,8 +814,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -833,14 +829,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "85a2ccd5ecf3a374d080367a180f2655e266543d",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1092,7 +1092,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1109,7 +1109,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1135,7 +1134,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1155,8 +1153,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1168,11 +1164,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "3d9b96ecdb4d13bb5a3fdd4c82b5cfe4c413493d",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -968,7 +968,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -985,7 +985,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1010,7 +1009,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1030,8 +1028,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1043,11 +1039,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "6fa1c0c5e4ef99df0ca8a06282166bbdd7c00f7a",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 16,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1312,7 +1312,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1332,7 +1332,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1359,7 +1358,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: MoshiDepthConfig,\n@@ -1378,8 +1376,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1395,14 +1391,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n@@ -1630,7 +1628,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1650,7 +1648,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1677,7 +1674,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: MoshiConfig,\n@@ -1696,8 +1692,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1713,14 +1707,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "d2d615dbfd8485759618da37acb4c023ece07645",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1222,7 +1222,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1239,7 +1239,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1265,7 +1264,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1285,8 +1283,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1298,11 +1294,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "6d1b9609e8fd67e4af3fd24398d45c9f116c27fa",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -882,7 +882,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -899,7 +899,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -925,7 +924,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -945,8 +943,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -958,11 +954,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "5ffcd27a2369c6cde83bbd88bb9dedb4097e898f",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -595,7 +595,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -612,7 +612,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -637,7 +636,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -657,8 +655,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -670,11 +666,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "e2c246fa1add42d9be9d26e2e70704b9a6fe7cb2",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -601,7 +601,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -618,7 +618,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -643,7 +642,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -663,8 +661,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -676,11 +672,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "61bec50b67a078659facc8d4fb91c91fd6d3c10f",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -672,7 +672,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -689,7 +689,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -715,7 +714,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -735,8 +733,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -748,11 +744,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "39b63c4c406f6c7b90907f640db85cb904434e23",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -49,7 +49,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     sequence_length: int,\n     target_length: int,\n     dtype: torch.dtype,\n-    device: torch.device,\n     min_dtype: float,\n     cache_position: torch.Tensor,\n     batch_size: int,\n@@ -70,8 +69,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n         dtype (`torch.dtype`):\n             The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to place the 4D attention mask on.\n         min_dtype (`float`):\n             The minimum value representable with the dtype `dtype`.\n         cache_position (`torch.Tensor`):\n@@ -85,7 +82,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n         causal_mask = attention_mask\n     else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        causal_mask = torch.full(\n+            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+        )\n         # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n         if sequence_length != 1:\n             if is_training:"
        },
        {
            "sha": "8789c205bed8d18d73aecce22453dd732ffd4203",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -682,7 +682,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -699,7 +699,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -725,7 +724,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -745,8 +743,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -758,11 +754,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "aa746de1ff2eb6a60d6fb093ed38b5ee08b98daa",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -606,7 +606,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -623,7 +623,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -648,7 +647,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -668,8 +666,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -681,11 +677,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "03422502e115903b7a7a524bb187e869373c3e8e",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -664,7 +664,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -684,7 +684,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -710,7 +709,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Phi3Config,\n@@ -729,8 +727,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -746,14 +742,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "72558e3f7c8900b26a297e8d6687725e96a5cf11",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1949,7 +1949,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1969,7 +1969,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1995,7 +1994,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Phi4MultimodalConfig,\n@@ -2014,8 +2012,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -2031,14 +2027,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "388505cf33e9780c252929dc076962c2a41dbd22",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1221,7 +1221,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1241,7 +1241,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1268,7 +1267,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: PhimoeConfig,\n@@ -1287,8 +1285,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1304,14 +1300,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "63250a47f60d7938136e7088409b8bb5c942f40a",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1618,7 +1618,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1635,7 +1635,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1661,7 +1660,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1681,8 +1679,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1694,11 +1690,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "6080a710c05bcce2b0aef04493dad1fadf69b3e6",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1031,7 +1031,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1048,7 +1048,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1074,7 +1073,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1094,8 +1092,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1107,11 +1103,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "ea70a06ee3c0436c34cebaf88810df295b99c854",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -622,7 +622,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -642,7 +642,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -668,7 +667,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen2Config,\n@@ -687,8 +685,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -704,14 +700,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "e79f641de3b0a30b6a2ffef21a194ddcbe4eacec",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 12,
            "deletions": 16,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -2088,7 +2088,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -2108,7 +2108,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -2134,7 +2133,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen2_5OmniConfig,\n@@ -2153,8 +2151,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -2170,14 +2166,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n@@ -2806,7 +2804,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -2826,7 +2824,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -2852,7 +2849,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen2_5OmniConfig,\n@@ -2871,8 +2867,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -2888,14 +2882,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "6c19b9fdcfb068e60c6d7c6bb2d68be37d3facb5",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1266,7 +1266,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1286,7 +1286,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1312,7 +1311,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen2_5_VLConfig,\n@@ -1331,8 +1329,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1348,14 +1344,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "00cb7896fd1a91d85df0acb760f119117f5ec2b4",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1082,7 +1082,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1102,7 +1102,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1129,7 +1128,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen2MoeConfig,\n@@ -1148,8 +1146,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1165,14 +1161,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "ce008fdaf9fa027052a70b59e851c50233f0e046",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1218,7 +1218,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -1238,7 +1238,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -1265,7 +1264,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen2VLConfig,\n@@ -1284,8 +1282,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1301,14 +1297,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n@@ -1692,9 +1690,6 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n         if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n             # calculate RoPE index once per generation in the pre-fill stage only"
        },
        {
            "sha": "d44eb4fb9ae3e34c296237ca5f3f48af136272a5",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -649,7 +649,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -669,7 +669,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -695,7 +694,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen3Config,\n@@ -714,8 +712,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -731,14 +727,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "9a476e07689929ffdafd28a759c9d032bd6f7b44",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -765,7 +765,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -785,7 +785,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -811,7 +810,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Qwen3MoeConfig,\n@@ -830,8 +828,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -847,14 +843,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "c09e4962a0d6411c3f8b5246b27a77121dbf9403",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -936,7 +936,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -953,7 +953,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -979,7 +978,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -999,8 +997,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1012,11 +1008,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "b56a4fcf68bc433294e858733ebcaf7acae47abd",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -613,7 +613,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         # SlidingWindowCache or StaticCache\n@@ -633,7 +633,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n             config=self.config,\n@@ -659,7 +658,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         config: Starcoder2Config,\n@@ -678,8 +676,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -695,14 +691,16 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             if config.get_text_config().sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - config.get_text_config().sliding_window\n                     )\n                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)"
        },
        {
            "sha": "396c448cc740be394aabc5e7609232809851ad10",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1165,7 +1165,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1182,7 +1182,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1208,7 +1207,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1228,8 +1226,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1241,11 +1237,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "8f6b5de8081d11d788157539af7db69ce9d08db2",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1236,7 +1236,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1253,7 +1253,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1279,7 +1278,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1299,8 +1297,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1312,11 +1308,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "665eedc81068dc21bf2da77fde9c6f690bc3e8d5",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1568,7 +1568,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1585,7 +1585,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1611,7 +1610,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1631,8 +1629,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1644,11 +1640,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "18fb5cc5f04809f4934e0f733e09a18e0cf92f10",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -879,7 +879,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -896,7 +896,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -922,7 +921,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -942,8 +940,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -955,11 +951,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "8c5432c5c6585974d3c0d9a706a8778239b01d5e",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -1406,7 +1406,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1423,7 +1423,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1449,7 +1448,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1469,8 +1467,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1482,11 +1478,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "03601a6d72a452b231d31b7f85ccb207976126c4",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/79d4bc761d9da7a125b65a3b8c09441608d05436/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79d4bc761d9da7a125b65a3b8c09441608d05436/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=79d4bc761d9da7a125b65a3b8c09441608d05436",
            "patch": "@@ -4018,6 +4018,34 @@ def test_generate_with_static_cache_multi_gpu(self):\n         value_cache_1 = results.past_key_values.value_cache[1]\n         self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n \n+    @pytest.mark.generate\n+    @require_torch_multi_gpu\n+    def test_generate_multi_gpu_causal_mask(self):\n+        \"\"\"\n+        Tests that cache position device doesn't clash with causal mask device when we are using multi-gpus.\n+        In real life happens only when multimodal encoder size is big, so `embed_tokens` gets allocated to the next device.\n+        The error will be triggered whenever a bacthed input is used, so that `causal_mask` is actually prepared instead of\n+        being `None`.\n+        \"\"\"\n+        # need to split manually as auto doesn't work well with unbalanced model\n+        device_map = {\n+            \"visual\": 0,\n+            \"model.embed_tokens\": 1,\n+            \"model.layers.0\": 1,\n+            \"model.layers.1\": 1,\n+            \"model.rotary_emb\": 1,\n+            \"model.norm.weight\": 1,\n+            \"lm_head\": 1,\n+        }\n+        model = AutoModelForImageTextToText.from_pretrained(\n+            \"hf-internal-testing/tiny-random-Qwen2VLForConditionalGeneration\", device_map=device_map\n+        )\n+        processor = AutoProcessor.from_pretrained(\"hf-internal-testing/tiny-random-Qwen2VLForConditionalGeneration\")\n+\n+        text = [\"Hello world\", \"Today I went to the supermarket to buy\"]\n+        inputs = processor(text=text, padding=True, return_tensors=\"pt\").to(torch_device)\n+        _ = model.generate(**inputs, max_new_tokens=20)\n+\n     @pytest.mark.generate\n     @require_torch_multi_gpu\n     def test_init_static_cache_multi_gpu(self):"
        }
    ],
    "stats": {
        "total": 759,
        "additions": 278,
        "deletions": 481
    }
}