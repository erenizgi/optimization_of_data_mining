{
    "author": "Cyrilvallez",
    "message": "Fixes for Arcee model (#39001)\n\n* fix modular\n\n* Update modular_arcee.py\n\n* fix",
    "sha": "1636a7bcb942370bb4098c8e67e4c3d3fd6a1740",
    "files": [
        {
            "sha": "909783c5d82e3993057507db7c4d908871793c18",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1636a7bcb942370bb4098c8e67e4c3d3fd6a1740/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1636a7bcb942370bb4098c8e67e4c3d3fd6a1740/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=1636a7bcb942370bb4098c8e67e4c3d3fd6a1740",
            "patch": "@@ -128,7 +128,6 @@ class ArceeConfig(PretrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }"
        },
        {
            "sha": "dc8b7880c41882c1fcf9888282cc1e06b37ce3e7",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 29,
            "deletions": 57,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/1636a7bcb942370bb4098c8e67e4c3d3fd6a1740/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1636a7bcb942370bb4098c8e67e4c3d3fd6a1740/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=1636a7bcb942370bb4098c8e67e4c3d3fd6a1740",
            "patch": "@@ -51,8 +51,6 @@\n \n \n class ArceeMLP(nn.Module):\n-    \"\"\"Arcee MLP with configurable activation function (typically relu2)\"\"\"\n-\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n@@ -87,40 +85,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-@auto_docstring\n-class ArceePreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = ArceeConfig\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"ArceeDecoderLayer\"]\n-    _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_sdpa = True\n-    _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n-    _supports_static_cache = True\n-    _supports_attention_backend = True\n-\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, ArceeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n-\n class ArceeRotaryEmbedding(nn.Module):\n     def __init__(self, config: ArceeConfig, device=None):\n         super().__init__()\n@@ -351,14 +315,36 @@ def forward(\n \n \n @auto_docstring\n-class ArceeModel(ArceePreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`ArceeDecoderLayer`]\n+class ArceePreTrainedModel(PreTrainedModel):\n+    config_class = ArceeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"ArceeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, ArceeRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n-    Args:\n-        config: ArceeConfig\n-    \"\"\"\n \n+@auto_docstring\n+class ArceeModel(ArceePreTrainedModel):\n     def __init__(self, config: ArceeConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -485,10 +471,8 @@ def forward(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-@auto_docstring\n+@auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForCausalLM(ArceePreTrainedModel, GenerationMixin):\n-    \"\"\"Arcee Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\"\"\"\n-\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n@@ -598,10 +582,6 @@ def forward(\n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForSequenceClassification(ArceePreTrainedModel):\n-    \"\"\"\n-    The Arcee Model transformer with a sequence classification head on top (linear layer).\n-    \"\"\"\n-\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n@@ -689,10 +669,6 @@ def forward(\n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForQuestionAnswering(ArceePreTrainedModel):\n-    \"\"\"\n-    The Arcee Model transformer with a span classification head on top for extractive question-answering tasks.\n-    \"\"\"\n-\n     base_model_prefix = \"transformer\"\n \n     def __init__(self, config):\n@@ -756,10 +732,6 @@ def forward(\n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForTokenClassification(ArceePreTrainedModel):\n-    \"\"\"\n-    The Arcee Model transformer with a token classification head on top.\n-    \"\"\"\n-\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels"
        },
        {
            "sha": "7be3b8031ad82556e56a1fbc2bdb7807f703a426",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 3,
            "deletions": 41,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/1636a7bcb942370bb4098c8e67e4c3d3fd6a1740/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1636a7bcb942370bb4098c8e67e4c3d3fd6a1740/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=1636a7bcb942370bb4098c8e67e4c3d3fd6a1740",
            "patch": "@@ -22,8 +22,6 @@\n     LlamaForQuestionAnswering,\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n-    LlamaModel,\n-    LlamaPreTrainedModel,\n )\n from ..nemotron.modeling_nemotron import NemotronMLP\n \n@@ -135,7 +133,6 @@ class ArceeConfig(LlamaConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n@@ -194,61 +191,26 @@ def __init__(\n \n \n class ArceeMLP(NemotronMLP):\n-    \"\"\"Arcee MLP with configurable activation function (typically relu2)\"\"\"\n-\n-    pass\n-\n-\n-class ArceePreTrainedModel(LlamaPreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    pass\n-\n-\n-class ArceeModel(LlamaModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`ArceeDecoderLayer`]\n-\n-    Args:\n-        config: ArceeConfig\n-    \"\"\"\n-\n     pass\n \n \n+@auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForCausalLM(LlamaForCausalLM):\n-    \"\"\"Arcee Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\"\"\"\n-\n     pass\n \n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForSequenceClassification(LlamaForSequenceClassification):\n-    \"\"\"\n-    The Arcee Model transformer with a sequence classification head on top (linear layer).\n-    \"\"\"\n-\n     pass\n \n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForQuestionAnswering(LlamaForQuestionAnswering):\n-    \"\"\"\n-    The Arcee Model transformer with a span classification head on top for extractive question-answering tasks.\n-    \"\"\"\n-\n     pass\n \n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForTokenClassification(LlamaForTokenClassification):\n-    \"\"\"\n-    The Arcee Model transformer with a token classification head on top.\n-    \"\"\"\n-\n     pass\n \n \n@@ -258,6 +220,6 @@ class ArceeForTokenClassification(LlamaForTokenClassification):\n     \"ArceeForQuestionAnswering\",\n     \"ArceeForSequenceClassification\",\n     \"ArceeForTokenClassification\",\n-    \"ArceeModel\",\n-    \"ArceePreTrainedModel\",\n+    \"ArceeModel\",  # noqa: F822\n+    \"ArceePreTrainedModel\",  # noqa: F822\n ]"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 32,
        "deletions": 99
    }
}