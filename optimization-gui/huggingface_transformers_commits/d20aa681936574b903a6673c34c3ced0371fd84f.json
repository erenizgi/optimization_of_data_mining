{
    "author": "iambogeumkim",
    "message": "ğŸŒ [i18n-KO] Translated `gpu_selection.md` to Korean (#36757)\n\n* Add _toctree.yml\n\n* feat: serving.md draft\n\n* Add _toctree.yml\n\n* feat: gpu_selection.md nmt draft\n\n* fix: TOC edit\n\n* Update docs/source/ko/serving.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/ko/gpu_selection.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/ko/serving.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update _toctree.yml\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "d20aa681936574b903a6673c34c3ced0371fd84f",
    "files": [
        {
            "sha": "64b39378f86933bd81e3d8ca21338932054b1ffe",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d20aa681936574b903a6673c34c3ced0371fd84f/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d20aa681936574b903a6673c34c3ced0371fd84f/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=d20aa681936574b903a6673c34c3ced0371fd84f",
            "patch": "@@ -97,6 +97,8 @@\n     sections:\n     - local: generation_strategies\n       title: í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ ì‚¬ìš©ì ì •ì˜\n+    - local: serving\n+      title: ëª¨ë¸ ì„œë¹™í•˜ê¸°ê¸°\n     title: ìƒì„±\n   - isExpanded: false\n     sections:\n@@ -123,6 +125,8 @@\n     title: Amazon SageMakerì—ì„œ í•™ìŠµ ì‹¤í–‰í•˜ê¸°\n   - local: serialization\n     title: ONNXë¡œ ë‚´ë³´ë‚´ê¸°\n+  - local: gpu_selection\n+    title: GPU ì„ íƒí•˜ê¸°\n   - local: tflite\n     title: TFLiteë¡œ ë‚´ë³´ë‚´ê¸°\n   - local: torchscript"
        },
        {
            "sha": "5d4c2108077b2f31346481bff1eafb23f6dfbf56",
            "filename": "docs/source/ko/gpu_selection.md",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/d20aa681936574b903a6673c34c3ced0371fd84f/docs%2Fsource%2Fko%2Fgpu_selection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d20aa681936574b903a6673c34c3ced0371fd84f/docs%2Fsource%2Fko%2Fgpu_selection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fgpu_selection.md?ref=d20aa681936574b903a6673c34c3ced0371fd84f",
            "patch": "@@ -0,0 +1,96 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# GPU ì„ íƒí•˜ê¸° [[gpu-selection]]\n+\n+ë¶„ì‚° í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©í•  GPUì˜ ê°œìˆ˜ì™€ ìˆœì„œë¥¼ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì„œë¡œ ë‹¤ë¥¸ ì—°ì‚° ì„±ëŠ¥ì„ ê°€ì§„ GPUê°€ ìˆì„ ë•Œ ë” ë¹ ë¥¸ GPUë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì¤‘ ì¼ë¶€ë§Œ ì„ íƒí•˜ì—¬ í™œìš©í•˜ê³ ì í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤. ì´ ì„ íƒ ê³¼ì •ì€ [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)ê³¼ [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)ì—ì„œ ëª¨ë‘ ì‘ë™í•©ë‹ˆë‹¤. Accelerateë‚˜ [DeepSpeed í†µí•©](./main_classes/deepspeed)ì€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n+\n+ì´ ê°€ì´ë“œëŠ” ì‚¬ìš©í•  GPUì˜ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ê³¼ ì‚¬ìš© ìˆœì„œë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n+\n+## GPU ê°œìˆ˜ ì§€ì • [[number-of-gpus]]\n+\n+ì˜ˆë¥¼ ë“¤ì–´, GPUê°€ 4ê°œ ìˆê³  ê·¸ì¤‘ ì²˜ìŒ 2ê°œë§Œ ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš°, ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n+\n+<hfoptions id=\"select-gpu\">\n+<hfoption id=\"torchrun\">\n+\n+ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ ì •í•˜ê¸° ìœ„í•´ `--nproc_per_node` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+```bash\n+torchrun --nproc_per_node=2  trainer-program.py ...\n+```\n+\n+</hfoption>\n+<hfoption id=\"Accelerate\">\n+\n+ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ ì •í•˜ê¸° ìœ„í•´ `--num_processes` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+```bash\n+accelerate launch --num_processes 2 trainer-program.py ...\n+```\n+\n+</hfoption>\n+<hfoption id=\"DeepSpeed\">\n+\n+ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ ì •í•˜ê¸° ìœ„í•´ `--num_gpus` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+```bash\n+deepspeed --num_gpus 2 trainer-program.py ...\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+### GPU ìˆœì„œ [[order-of-gpus]]\n+\n+ì‚¬ìš©í•  GPUì™€ ê·¸ ìˆœì„œë¥¼ ì§€ì •í•˜ë ¤ë©´ `CUDA_VISIBLE_DEVICES` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”. ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ `~/bashrc` ë˜ëŠ” ë‹¤ë¥¸ ì‹œì‘ ì„¤ì • íŒŒì¼ì—ì„œ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. `CUDA_VISIBLE_DEVICES`ëŠ” ì‚¬ìš©í•  GPUë¥¼ ë§¤í•‘í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, GPUê°€ 4ê°œ (0, 1, 2, 3) ìˆê³  ê·¸ì¤‘ì—ì„œ 0ë²ˆê³¼ 2ë²ˆ GPUë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ê²½ìš°, ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```bash\n+CUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n+```\n+\n+ì˜¤ì§ ë‘ ê°œì˜ ë¬¼ë¦¬ì  GPU(0, 2)ë§Œ PyTorchì—ì„œ \"ë³´ì´ëŠ”\" ìƒíƒœê°€ ë˜ë©°, ê°ê° `cuda:0`ê³¼ `cuda:1`ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤. ë˜í•œ, GPU ì‚¬ìš© ìˆœì„œë¥¼ ë°˜ëŒ€ë¡œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, GPU 0ì´ `cuda:1`, GPU 2ê°€ `cuda:0`ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤.\"\n+\n+```bash\n+CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n+```\n+\n+`CUDA_VISIBLE_DEVICES` í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ GPUê°€ ì—†ëŠ” í™˜ê²½ì„ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```bash\n+CUDA_VISIBLE_DEVICES= python trainer-program.py ...\n+```\n+\n+> [!WARNING]\n+> ë‹¤ë¥¸ í™˜ê²½ ë³€ìˆ˜ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, CUDA_VISIBLE_DEVICESë¥¼ ì»¤ë§¨ë“œ ë¼ì¸ì— ì¶”ê°€í•˜ëŠ” ëŒ€ì‹  exportí•˜ì—¬ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë°©ì‹ì€ í™˜ê²½ ë³€ìˆ˜ê°€ ì–´ë–»ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ë¥¼ ìŠì–´ë²„ë¦´ ê²½ìš°, ì˜ëª»ëœ GPUë¥¼ ì‚¬ìš©í•  ìœ„í—˜ì´ ìˆê¸° ë•Œë¬¸ì— ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŠ¹ì • í•™ìŠµ ì‹¤í–‰ì— ëŒ€í•´ ë™ì¼í•œ ì»¤ë§¨ë“œ ë¼ì¸ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.\n+\n+`CUDA_DEVICE_ORDER`ëŠ” GPUì˜ ìˆœì„œë¥¼ ì œì–´í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëŒ€ì²´ í™˜ê²½ ë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ GPU ìˆœì„œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+1. NVIDIA ë° AMD GPUì˜ PCIe ë²„ìŠ¤ IDëŠ” ê°ê° [nvidia-smi](https://developer.nvidia.com/nvidia-system-management-interface)ì™€ [rocm-smi](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html)ì˜ ìˆœì„œì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.\n+\n+```bash\n+export CUDA_DEVICE_ORDER=PCI_BUS_ID\n+```\n+\n+2. GPU ì—°ì‚° ëŠ¥ë ¥\n+\n+```bash\n+export CUDA_DEVICE_ORDER=FASTEST_FIRST\n+```\n+\n+The `CUDA_DEVICE_ORDER` is especially useful if your training setup consists of an older and newer GPU, where the older GPU appears first, but you cannot physically swap the cards to make the newer GPU appear first. In this case, set `CUDA_DEVICE_ORDER=FASTEST_FIRST` to always use the newer and faster GPU first (`nvidia-smi` or `rocm-smi` still reports the GPUs in their PCIe order). Or you could also set `export CUDA_VISIBLE_DEVICES=1,0`.\n+\n+`CUDA_DEVICE_ORDER`ëŠ” êµ¬í˜• GPUì™€ ì‹ í˜• GPUê°€ í˜¼í•©ëœ í™˜ê²½ì—ì„œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, êµ¬í˜• GPUê°€ ë¨¼ì € í‘œì‹œë˜ì§€ë§Œ ë¬¼ë¦¬ì ìœ¼ë¡œ êµì²´í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, `CUDA_DEVICE_ORDER=FASTEST_FIRST`ë¥¼ ì„¤ì •í•˜ë©´ í•­ìƒ ì‹ í˜• ë° ë” ë¹ ë¥¸ GPUë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©(nvidia-smi ë˜ëŠ” rocm-smiëŠ” PCIe ìˆœì„œëŒ€ë¡œ GPUë¥¼ í‘œì‹œí•¨)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜ëŠ”, `export CUDA_VISIBLE_DEVICES=1,0`ì„ ì„¤ì •í•˜ì—¬ GPU ì‚¬ìš© ìˆœì„œë¥¼ ì§ì ‘ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\\ No newline at end of file"
        },
        {
            "sha": "ae9bc730f1952d4fe35e3f7c1f2b819d569e9942",
            "filename": "docs/source/ko/serving.md",
            "status": "added",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/d20aa681936574b903a6673c34c3ced0371fd84f/docs%2Fsource%2Fko%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d20aa681936574b903a6673c34c3ced0371fd84f/docs%2Fsource%2Fko%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fserving.md?ref=d20aa681936574b903a6673c34c3ced0371fd84f",
            "patch": "@@ -0,0 +1,64 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ëª¨ë¸ ì„œë¹™ [[Serving]]\n+\n+Text Generation Inference (TGI) ë° vLLMê³¼ ê°™ì€ íŠ¹ìˆ˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ Transformer ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” vLLMì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, Transformersì—ëŠ” í¬í•¨ë˜ì§€ ì•Šì€ ê³ ìœ í•œ ìµœì í™” ê¸°ëŠ¥ì„ ë‹¤ì–‘í•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤.\n+\n+## TGI [[TGI]]\n+\n+[ë„¤ì´í‹°ë¸Œë¡œ êµ¬í˜„ëœ ëª¨ë¸](https://huggingface.co/docs/text-generation-inference/supported_models)ì´ ì•„ë‹ˆë”ë¼ë„ TGIë¡œ Transformers êµ¬í˜„ ëª¨ë¸ì„ ì„œë¹™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. TGIì—ì„œ ì œê³µí•˜ëŠ” ì¼ë¶€ ê³ ì„±ëŠ¥ ê¸°ëŠ¥ì€ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ ì—°ì† ë°°ì¹­ì´ë‚˜ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ê°™ì€ ê¸°ëŠ¥ë“¤ì€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> ë” ìì„¸í•œ ë‚´ìš©ì€ [ë…¼-ì½”ì–´ ëª¨ë¸ ì„œë¹™](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models) ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+TGI ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ë°©ì‹ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ Transformer êµ¬í˜„ ëª¨ë¸ì„ ì„œë¹™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```docker\n+docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id gpt2\n+```\n+\n+ì»¤ìŠ¤í…€ Transformers ëª¨ë¸ì„ ì„œë¹™í•˜ë ¤ë©´ `--trust-remote_code`ë¥¼ ëª…ë ¹ì–´ì— ì¶”ê°€í•˜ì„¸ìš”.\n+\n+```docker\n+docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id <CUSTOM_MODEL_ID> --trust-remote-code\n+```\n+\n+## vLLM [[vLLM]]\n+\n+[vLLM](https://docs.vllm.ai/en/latest/index.html)ì€ íŠ¹ì • ëª¨ë¸ì´ vLLMì—ì„œ [ë„¤ì´í‹°ë¸Œë¡œ êµ¬í˜„ëœ ëª¨ë¸](https://docs.vllm.ai/en/latest/models/supported_models.html#list-of-text-only-language-models)ì´ ì•„ë‹ ê²½ìš°, Transformers êµ¬í˜„ ëª¨ë¸ì„ ì„œë¹™í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. \n+\n+Transformers êµ¬í˜„ì—ì„œëŠ” ì–‘ìí™”, LoRA ì–´ëŒ‘í„°, ë¶„ì‚° ì¶”ë¡  ë° ì„œë¹™ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì´ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+> [!TIP]\n+> [Transformers fallback](https://docs.vllm.ai/en/latest/models/supported_models.html#transformers-fallback) ì„¹ì…˜ì—ì„œ ë” ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ê¸°ë³¸ì ìœ¼ë¡œ vLLMì€ ë„¤ì´í‹°ë¸Œ êµ¬í˜„ì„ ì„œë¹™í•  ìˆ˜ ìˆì§€ë§Œ, í•´ë‹¹ êµ¬í˜„ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ Transformers êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ `--model-impl transformers` ì˜µì…˜ì„ ì„¤ì •í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ Transformers ëª¨ë¸ êµ¬í˜„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```shell\n+vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n+    --task generate \\\n+    --model-impl transformers \\\n+```\n+\n+`trust-remote-code` íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•´ ì›ê²© ì½”ë“œ ëª¨ë¸ ë¡œë“œë¥¼ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```shell\n+vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n+    --task generate \\\n+    --model-impl transformers \\\n+    --trust-remote-code \\\n+```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 164,
        "additions": 164,
        "deletions": 0
    }
}