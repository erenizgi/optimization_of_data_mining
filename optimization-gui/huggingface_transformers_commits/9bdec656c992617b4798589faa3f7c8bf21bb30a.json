{
    "author": "zucchini-nlp",
    "message": "Let's break Qwen-VL ðŸš¨   (#42420)\n\n* Lets braeaak!\n\n* pop attributes so we don't have 2 different values for the same key. Reported by TRL in the past\n\n* oh no, tying is messing up with slow tests....\n\n* fix copies\n\n* migration guide",
    "sha": "9bdec656c992617b4798589faa3f7c8bf21bb30a",
    "files": [
        {
            "sha": "9c73c84f6414ae1293d34d72e1df5b64d0498ed7",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bdec656c992617b4798589faa3f7c8bf21bb30a/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bdec656c992617b4798589faa3f7c8bf21bb30a/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=9bdec656c992617b4798589faa3f7c8bf21bb30a",
            "patch": "@@ -128,7 +128,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n - Methods to init a nested config such as `from_xxx_config` are deleted. Configs can be init from the `__init__` method in the same way. See [#41314](https://github.com/huggingface/transformers/pull/41314).\n - It is no longer possible to load a config class from a URL file. Configs must be loaded from either a local path or a repo on the Hub. See [#42383](https://github.com/huggingface/transformers/pull/42383).\n - All parameters for configuring model's rotary embedding are now stored under `mode.rope_parameters`, including the `rope_theta` and `rope_type`. Model's `config.rope_parameters` is a simple dictionaty in most cases, and can also be a nested dict in special cases (i.e. Gemma3 and ModernBert) with different rope parameterization for each layer type. See [#39847](https://github.com/huggingface/transformers/pull/39847)\n-\n+- Qwen-VL family configuration is in a nested format and trying to access keys directly will throw an error (e.g. `config.vocab_size`). Users are expected to access keys from their respective sub-configs (`config.text_config.vocab_size`).\n \n ## Processing\n "
        },
        {
            "sha": "b0ab2188488c5bebfc53e908f20552705dddc620",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 27,
            "deletions": 33,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bdec656c992617b4798589faa3f7c8bf21bb30a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bdec656c992617b4798589faa3f7c8bf21bb30a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=9bdec656c992617b4798589faa3f7c8bf21bb30a",
            "patch": "@@ -24,6 +24,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import inspect\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n@@ -127,6 +128,12 @@ class Qwen2_5_VLTextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n+        bos_token_id (`int`, *optional*, defaults to 151643):\n+            The id of the _beginning-of-stream_ token.\n+        eos_token_id (`int`, *optional*, defaults to 151645):\n+            The id of the _end-of-stream_ token.\n+        pad_token_id (`int`, *optional*):\n+            The id of the _padding_ token.\n \n     ```python\n     >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig\n@@ -180,6 +187,9 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         attention_dropout: Optional[float] = 0.0,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        bos_token_id: Optional[int] = 151643,\n+        eos_token_id: Optional[int] = 151645,\n+        pad_token_id: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -222,7 +232,14 @@ def __init__(\n         if self.rope_parameters[\"rope_type\"] == \"mrope\":\n             self.rope_parameters[\"rope_type\"] = \"default\"\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            **kwargs,\n+        )\n \n \n class Qwen2_5_VLConfig(PreTrainedConfig):\n@@ -277,11 +294,6 @@ def __init__(\n         vision_end_token_id=151653,\n         **kwargs,\n     ):\n-        # We need to init super() here so that it does not reset values\n-        # that are in text config to the BaseClass defaults. The Base\n-        # config has many text related defaults and not all defaults are same as for `Qwen2_5_VLTextConfig`\n-        super().__init__(**kwargs)\n-\n         if isinstance(vision_config, dict):\n             self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n         elif vision_config is None:\n@@ -290,39 +302,21 @@ def __init__(\n         if isinstance(text_config, dict):\n             self.text_config = self.sub_configs[\"text_config\"](**text_config)\n         elif text_config is None:\n-            # For BC use all kwargs to init `TextConfig`\n-            self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n+            # Hub configs are saved as flat dicts so we pop some of kwargs to init `TextConfig`\n+            text_params = inspect.signature(self.sub_configs[\"text_config\"].__init__).parameters.keys()\n+            text_params = list(text_params) + [\"rope_scaling\", \"rope_theta\"]\n+            text_config = {key: kwargs.pop(key) for key in text_params if key in kwargs}\n+            text_config[\"dtype\"] = kwargs.get(\"torch_dtype\", kwargs.get(\"dtype\"))  # don't pop the dtype\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n \n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n         self.vision_start_token_id = vision_start_token_id\n         self.vision_end_token_id = vision_end_token_id\n \n-        # Attention implementation to use. It sets it recursively on sub-configs so we call it again in the end\n-        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n-\n-    def __setattr__(self, key, value):\n-        if (\n-            (text_config := super().__getattribute__(\"__dict__\").get(\"text_config\")) is not None\n-            and key not in [\"_name_or_path\", \"model_type\", \"dtype\", \"_attn_implementation_internal\"]\n-            and key in text_config.__dict__\n-        ):\n-            setattr(text_config, key, value)\n-        else:\n-            super().__setattr__(key, value)\n-\n-    def __getattribute__(self, key):\n-        if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n-            \"_name_or_path\",\n-            \"model_type\",\n-            \"dtype\",\n-            \"_attn_implementation_internal\",\n-        ]:\n-            text_config = super().__getattribute__(\"text_config\")\n-            if key in text_config.__dict__:\n-                return getattr(text_config, key)\n-\n-        return super().__getattribute__(key)\n+        # FIXME: arthur/cyril - tying has to be used from the text config\n+        kwargs[\"tie_word_embeddings\"] = self.text_config.tie_word_embeddings\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Qwen2_5_VLConfig\", \"Qwen2_5_VLTextConfig\"]"
        },
        {
            "sha": "e63caaf75271568beae9a55647ac936f89e37ccd",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 27,
            "deletions": 33,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bdec656c992617b4798589faa3f7c8bf21bb30a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bdec656c992617b4798589faa3f7c8bf21bb30a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=9bdec656c992617b4798589faa3f7c8bf21bb30a",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \"\"\"Qwen2VL model configuration\"\"\"\n \n+import inspect\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n@@ -115,6 +116,12 @@ class Qwen2VLTextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n+        bos_token_id (`int`, *optional*, defaults to 151643):\n+            The id of the _beginning-of-stream_ token.\n+        eos_token_id (`int`, *optional*, defaults to 151645):\n+            The id of the _end-of-stream_ token.\n+        pad_token_id (`int`, *optional*):\n+            The id of the _padding_ token.\n \n     ```python\n     >>> from transformers import Qwen2VLTextModel, Qwen2VLConfig\n@@ -168,6 +175,9 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         attention_dropout: Optional[float] = 0.0,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        bos_token_id: Optional[int] = 151643,\n+        eos_token_id: Optional[int] = 151645,\n+        pad_token_id: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -210,7 +220,14 @@ def __init__(\n         if self.rope_parameters[\"rope_type\"] == \"mrope\":\n             self.rope_parameters[\"rope_type\"] = \"default\"\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            **kwargs,\n+        )\n \n \n class Qwen2VLConfig(PreTrainedConfig):\n@@ -265,11 +282,6 @@ def __init__(\n         vision_end_token_id=151653,\n         **kwargs,\n     ):\n-        # We need to init super() here so that it does not reset values\n-        # that are in text config to the BaseClass defaults. The Base\n-        # config has many text related defaults and not all defaults are same as for `Qwen2VLTextConfig`\n-        super().__init__(**kwargs)\n-\n         if isinstance(vision_config, dict):\n             self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n         elif vision_config is None:\n@@ -278,39 +290,21 @@ def __init__(\n         if isinstance(text_config, dict):\n             self.text_config = self.sub_configs[\"text_config\"](**text_config)\n         elif text_config is None:\n-            # For BC use all kwargs to init `TextConfig`\n-            self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n+            # Hub configs are saved as flat dicts so we pop some of kwargs to init `TextConfig`\n+            text_params = inspect.signature(self.sub_configs[\"text_config\"].__init__).parameters.keys()\n+            text_params = list(text_params) + [\"rope_scaling\", \"rope_theta\"]\n+            text_config = {key: kwargs.pop(key) for key in text_params if key in kwargs}\n+            text_config[\"dtype\"] = kwargs.get(\"torch_dtype\", kwargs.get(\"dtype\"))  # don't pop the dtype\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n \n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n         self.vision_start_token_id = vision_start_token_id\n         self.vision_end_token_id = vision_end_token_id\n \n-        # Attention implementation to use. It sets it recursively on sub-configs so we call it again in the end\n-        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n-\n-    def __setattr__(self, key, value):\n-        if (\n-            (text_config := super().__getattribute__(\"__dict__\").get(\"text_config\")) is not None\n-            and key not in [\"_name_or_path\", \"model_type\", \"dtype\", \"_attn_implementation_internal\"]\n-            and key in text_config.__dict__\n-        ):\n-            setattr(text_config, key, value)\n-        else:\n-            super().__setattr__(key, value)\n-\n-    def __getattribute__(self, key):\n-        if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n-            \"_name_or_path\",\n-            \"model_type\",\n-            \"dtype\",\n-            \"_attn_implementation_internal\",\n-        ]:\n-            text_config = super().__getattribute__(\"text_config\")\n-            if key in text_config.__dict__:\n-                return getattr(text_config, key)\n-\n-        return super().__getattribute__(key)\n+        # FIXME: arthur/cyril - tying has to be used from the text config\n+        kwargs[\"tie_word_embeddings\"] = self.text_config.tie_word_embeddings\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Qwen2VLConfig\", \"Qwen2VLTextConfig\"]"
        },
        {
            "sha": "8f4459e369338e57f218c30e8abaeb48cbce1fe5",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bdec656c992617b4798589faa3f7c8bf21bb30a/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bdec656c992617b4798589faa3f7c8bf21bb30a/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=9bdec656c992617b4798589faa3f7c8bf21bb30a",
            "patch": "@@ -209,44 +209,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    def test_text_config(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_config_dict = config.to_dict()\n-        base_config = Qwen2_5_VLConfig(**base_config_dict)\n-\n-        # Trying to get or set text related attributes happens via text config\n-        vocab_size = base_config.vocab_size\n-        text_vocab_size = base_config.text_config.vocab_size\n-        self.assertEqual(vocab_size, text_vocab_size)\n-\n-        base_config.vocab_size = 55\n-        self.assertEqual(base_config.vocab_size, 55)\n-        self.assertEqual(base_config.text_config.vocab_size, 55)\n-\n-        # We can still initialize config from old-format json, i.e. flat structure\n-        text_config_dict = base_config_dict.pop(\"text_config\")\n-        flat_config_dict = {**text_config_dict, **base_config_dict}\n-        config_from_flat_dict = Qwen2_5_VLConfig(**flat_config_dict)\n-        config_from_flat_dict.vocab_size = 78\n-        self.assertEqual(config_from_flat_dict.vocab_size, 78)\n-        self.assertEqual(config_from_flat_dict.text_config.vocab_size, 78)\n-\n-        # Vision config attributes are NOT force-set via vision config\n-        base_config.patch_size = 8\n-        self.assertEqual(base_config.patch_size, 8)\n-        self.assertNotEqual(base_config.vision_config.patch_size, 8)\n-\n-        # Test for making sure config save and load preserves correct model type\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        self.assertEqual(config.model_type, \"qwen2_5_vl\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            config.save_pretrained(tmp_dir)\n-\n-            loaded_config = Qwen2_5_VLConfig.from_pretrained(tmp_dir)\n-            self.assertEqual(loaded_config.model_type, \"qwen2_5_vl\")\n-\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        },
        {
            "sha": "cdbe65bba5cfb4fc0228f8588b26ad49b8fc5043",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bdec656c992617b4798589faa3f7c8bf21bb30a/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bdec656c992617b4798589faa3f7c8bf21bb30a/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=9bdec656c992617b4798589faa3f7c8bf21bb30a",
            "patch": "@@ -192,44 +192,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    def test_text_config(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_config_dict = config.to_dict()\n-        base_config = Qwen2VLConfig(**base_config_dict)\n-\n-        # Trying to get or set text related attributes happens via text config\n-        vocab_size = base_config.vocab_size\n-        text_vocab_size = base_config.text_config.vocab_size\n-        self.assertEqual(vocab_size, text_vocab_size)\n-\n-        base_config.vocab_size = 55\n-        self.assertEqual(base_config.vocab_size, 55)\n-        self.assertEqual(base_config.text_config.vocab_size, 55)\n-\n-        # We can still initialize config from old-format json, i.e. flat structure\n-        text_config_dict = base_config_dict.pop(\"text_config\")\n-        flat_config_dict = {**text_config_dict, **base_config_dict}\n-        config_from_flat_dict = Qwen2VLConfig(**flat_config_dict)\n-        config_from_flat_dict.vocab_size = 78\n-        self.assertEqual(config_from_flat_dict.vocab_size, 78)\n-        self.assertEqual(config_from_flat_dict.text_config.vocab_size, 78)\n-\n-        # Vision config attributes are NOT force-set via vision config\n-        base_config.patch_size = 8\n-        self.assertEqual(base_config.patch_size, 8)\n-        self.assertNotEqual(base_config.vision_config.patch_size, 8)\n-\n-        # Test for making sure config save and load preserves correct model type\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        self.assertEqual(config.model_type, \"qwen2_vl\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            config.save_pretrained(tmp_dir)\n-\n-            loaded_config = Qwen2VLConfig.from_pretrained(tmp_dir)\n-            self.assertEqual(loaded_config.model_type, \"qwen2_vl\")\n-\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 55,
        "deletions": 143
    }
}