{
    "author": "molbap",
    "message": ":red_circle: :red_circle:  fix `query_pre_attn_scalar` different of `num_heads` in default gemma2 config (#34540)\n\n* fix query_pre_attn_scalar different of num_heads in default config\r\n\r\n* propagate modular changes\r\n\r\n* fix copies\r\n\r\n* fix modular copies\r\n\r\n* fix copies?\r\n\r\n* correct copies fix",
    "sha": "86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94",
    "files": [
        {
            "sha": "98d9064dc1a294f2d57c445471166282e6db08a5",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94",
            "patch": "@@ -36,15 +36,15 @@ class Gemma2Config(PretrainedConfig):\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma2 model. Defines the number of different tokens that can be represented by the\n             `inputs_ids` passed when calling [`Gemma2Model`]\n-        hidden_size (`int`, *optional*, defaults to 3072):\n+        hidden_size (`int`, *optional*, defaults to 2304):\n             Dimension of the hidden representations.\n-        intermediate_size (`int`, *optional*, defaults to 24576):\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n             Dimension of the MLP representations.\n-        num_hidden_layers (`int`, *optional*, defaults to 28):\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n             Number of hidden layers in the Transformer decoder.\n-        num_attention_heads (`int`, *optional*, defaults to 16):\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n             Number of attention heads for each attention layer in the Transformer decoder.\n-        num_key_value_heads (`int`, *optional*, defaults to 16):\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n@@ -80,7 +80,7 @@ class Gemma2Config(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        query_pre_attn_scalar (`float`, *optional*, defaults to 224): scaling factor used on the attention scores\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256): scaling factor used on the attention scores\n         sliding_window (`int`, *optional*, defaults to 4096): in Gemma2, every other layer uses sliding window attention. This is the\n             size of the sliding window.\n         final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n@@ -103,11 +103,11 @@ class Gemma2Config(PretrainedConfig):\n     def __init__(\n         self,\n         vocab_size=256000,\n-        hidden_size=3072,\n-        intermediate_size=24576,\n-        num_hidden_layers=28,\n-        num_attention_heads=16,\n-        num_key_value_heads=16,\n+        hidden_size=2304,\n+        intermediate_size=9216,\n+        num_hidden_layers=26,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n         head_dim=256,\n         hidden_activation=\"gelu_pytorch_tanh\",\n         max_position_embeddings=8192,\n@@ -121,7 +121,7 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n-        query_pre_attn_scalar=224,\n+        query_pre_attn_scalar=256,\n         sliding_window=4096,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,"
        },
        {
            "sha": "dacaca1c7ef4a98acc2091fd9a4e459ca1c9ad2b",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=86701f2b6ff2085a3cd3ad1d30bc2ff2b10fbd94",
            "patch": "@@ -67,15 +67,15 @@ class Gemma2Config(PretrainedConfig):\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma2 model. Defines the number of different tokens that can be represented by the\n             `inputs_ids` passed when calling [`Gemma2Model`]\n-        hidden_size (`int`, *optional*, defaults to 3072):\n+        hidden_size (`int`, *optional*, defaults to 2304):\n             Dimension of the hidden representations.\n-        intermediate_size (`int`, *optional*, defaults to 24576):\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n             Dimension of the MLP representations.\n-        num_hidden_layers (`int`, *optional*, defaults to 28):\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n             Number of hidden layers in the Transformer decoder.\n-        num_attention_heads (`int`, *optional*, defaults to 16):\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n             Number of attention heads for each attention layer in the Transformer decoder.\n-        num_key_value_heads (`int`, *optional*, defaults to 16):\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n@@ -111,7 +111,7 @@ class Gemma2Config(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        query_pre_attn_scalar (`float`, *optional*, defaults to 224): scaling factor used on the attention scores\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256): scaling factor used on the attention scores\n         sliding_window (`int`, *optional*, defaults to 4096): in Gemma2, every other layer uses sliding window attention. This is the\n             size of the sliding window.\n         final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n@@ -134,11 +134,11 @@ class Gemma2Config(PretrainedConfig):\n     def __init__(\n         self,\n         vocab_size=256000,\n-        hidden_size=3072,\n-        intermediate_size=24576,\n-        num_hidden_layers=28,\n-        num_attention_heads=16,\n-        num_key_value_heads=16,\n+        hidden_size=2304,\n+        intermediate_size=9216,\n+        num_hidden_layers=26,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n         head_dim=256,\n         hidden_activation=\"gelu_pytorch_tanh\",\n         max_position_embeddings=8192,\n@@ -152,7 +152,7 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n-        query_pre_attn_scalar=224,\n+        query_pre_attn_scalar=256,\n         sliding_window=4096,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 24,
        "deletions": 24
    }
}