{
    "author": "saqlain2204",
    "message": "[Tests] Reduced model size for albert-test model (#38480)\n\n* Reduced model size for albert-test model\n\n* Run checks\n\n* Removed test_save_load\n\n* Removed test skipping functions",
    "sha": "e0545ef0b8026bf53678e89270fecab5351cb50c",
    "files": [
        {
            "sha": "f0440fb349d6cb09ac2780efbcd3b63426df6306",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0545ef0b8026bf53678e89270fecab5351cb50c/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0545ef0b8026bf53678e89270fecab5351cb50c/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=e0545ef0b8026bf53678e89270fecab5351cb50c",
            "patch": "@@ -51,19 +51,19 @@ def __init__(\n         use_input_mask=True,\n         use_token_type_ids=True,\n         use_labels=True,\n-        vocab_size=99,\n-        embedding_size=16,\n-        hidden_size=36,\n+        vocab_size=32,\n+        embedding_size=8,\n+        hidden_size=12,\n         num_hidden_layers=2,\n         # this needs to be the same as `num_hidden_layers`!\n         num_hidden_groups=2,\n-        num_attention_heads=6,\n-        intermediate_size=37,\n+        num_attention_heads=4,\n+        intermediate_size=16,\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n+        max_position_embeddings=8,\n+        type_vocab_size=2,\n         type_sequence_label_size=2,\n         initializer_range=0.02,\n         num_labels=3,\n@@ -121,6 +121,7 @@ def prepare_config_and_inputs(self):\n     def get_config(self):\n         return AlbertConfig(\n             vocab_size=self.vocab_size,\n+            embedding_size=self.embedding_size,\n             hidden_size=self.hidden_size,\n             num_hidden_layers=self.num_hidden_layers,\n             num_attention_heads=self.num_attention_heads,\n@@ -132,6 +133,7 @@ def get_config(self):\n             type_vocab_size=self.type_vocab_size,\n             initializer_range=self.initializer_range,\n             num_hidden_groups=self.num_hidden_groups,\n+            inner_group_num=1,\n         )\n \n     def create_and_check_model("
        }
    ],
    "stats": {
        "total": 16,
        "additions": 9,
        "deletions": 7
    }
}