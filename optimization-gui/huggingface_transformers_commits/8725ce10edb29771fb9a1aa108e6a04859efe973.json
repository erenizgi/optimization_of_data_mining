{
    "author": "fjosw",
    "message": "[Fix] Deepseek V3 expert bias routing (#41647)\n\n* [Fix] Deepseek V3 expert bias routing\n\n* [Fix] fix-copies\n\n* [Fix] Run make style",
    "sha": "8725ce10edb29771fb9a1aa108e6a04859efe973",
    "files": [
        {
            "sha": "957295bc4ca07929db4a37af5a6d3416bc45ec55",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=8725ce10edb29771fb9a1aa108e6a04859efe973",
            "patch": "@@ -176,9 +176,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -188,7 +190,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
        },
        {
            "sha": "b523c3b21493b486f15278d2ce4ecfd6e1a40419",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=8725ce10edb29771fb9a1aa108e6a04859efe973",
            "patch": "@@ -132,9 +132,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -144,7 +146,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
        },
        {
            "sha": "194e8df51766878e3b39e6ea9d5be7aa7ec8fb03",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=8725ce10edb29771fb9a1aa108e6a04859efe973",
            "patch": "@@ -319,9 +319,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -331,7 +333,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
        },
        {
            "sha": "3ba8a70d4793216926451ae7f2baa84bcfaa8c99",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8725ce10edb29771fb9a1aa108e6a04859efe973/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=8725ce10edb29771fb9a1aa108e6a04859efe973",
            "patch": "@@ -376,9 +376,11 @@ def __init__(self, config: Glm4vMoeTextConfig):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -388,7 +390,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 20,
        "deletions": 12
    }
}