{
    "author": "Joshua-Chin",
    "message": "Add `tokenizer_kwargs`  argument to the text generation pipeline (#40364)\n\n* Add `tokenizer_kwargs`  arg to text generation pipeline.\n\n* chore: re-run CI\n\n* Rename `tokenizer_kwargs` to `tokenizer_encode_kwargs` for text generation pipeline\n\n* Fix `tokenizer_encode_kwargs` doc string.\n\n* Fix note related to `tokenizer _kwargs` in text generation pipeline\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "d8f2edcc4692637637374c4495882c039e3967ba",
    "files": [
        {
            "sha": "4f7dce96da17c60f7735b9644657f0f29ea55819",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f2edcc4692637637374c4495882c039e3967ba/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f2edcc4692637637374c4495882c039e3967ba/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=d8f2edcc4692637637374c4495882c039e3967ba",
            "patch": "@@ -158,6 +158,7 @@ def _sanitize_parameters(\n         max_length=None,\n         continue_final_message=None,\n         skip_special_tokens=None,\n+        tokenizer_encode_kwargs=None,\n         **generate_kwargs,\n     ):\n         # preprocess kwargs\n@@ -194,6 +195,10 @@ def _sanitize_parameters(\n \n         if continue_final_message is not None:\n             preprocess_params[\"continue_final_message\"] = continue_final_message\n+\n+        if tokenizer_encode_kwargs is not None:\n+            preprocess_params[\"tokenizer_encode_kwargs\"] = tokenizer_encode_kwargs\n+\n         preprocess_params.update(generate_kwargs)\n \n         # forward kwargs\n@@ -288,6 +293,9 @@ def __call__(self, text_inputs, **kwargs):\n                 - `None` : default strategy where nothing in particular happens\n                 - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n                   truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n+            tokenizer_encode_kwargs (`dict`, *optional*):\n+                Additional keyword arguments to pass along to the encoding step of the tokenizer. If the text input is\n+                a chat, it is passed to `apply_chat_template`. Otherwise, it is passed to `__call__`.\n             generate_kwargs (`dict`, *optional*):\n                 Additional keyword arguments to pass along to the generate method of the model (see the generate method\n                 corresponding to your framework [here](./text_generation)).\n@@ -333,16 +341,18 @@ def preprocess(\n         padding=None,\n         max_length=None,\n         continue_final_message=None,\n+        tokenizer_encode_kwargs=None,\n         **generate_kwargs,\n     ):\n         # Only set non-None tokenizer kwargs, so as to rely on the tokenizer's defaults\n         tokenizer_kwargs = {\n             \"add_special_tokens\": add_special_tokens,\n             \"truncation\": truncation,\n             \"padding\": padding,\n-            \"max_length\": max_length,  # TODO: name clash -- this is broken, `max_length` is also a `generate` arg\n+            \"max_length\": max_length,  # NOTE: `max_length` is also a `generate` arg. Use `tokenizer_encode_kwargs` to avoid a name clash\n         }\n         tokenizer_kwargs = {key: value for key, value in tokenizer_kwargs.items() if value is not None}\n+        tokenizer_kwargs.update(tokenizer_encode_kwargs or {})\n \n         if isinstance(prompt_text, Chat):\n             tokenizer_kwargs.pop(\"add_special_tokens\", None)  # ignore add_special_tokens on chats"
        },
        {
            "sha": "a408266036c39c5968c9386d5214d6800471b288",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f2edcc4692637637374c4495882c039e3967ba/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f2edcc4692637637374c4495882c039e3967ba/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=d8f2edcc4692637637374c4495882c039e3967ba",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import unittest\n+from unittest.mock import patch\n \n from transformers import (\n     MODEL_FOR_CAUSAL_LM_MAPPING,\n@@ -555,3 +556,20 @@ def test_pipeline_skip_special_tokens(self):\n         # forcing special tokens to be included in the output\n         output = generator(chat, max_new_tokens=1000, do_sample=False, skip_special_tokens=False)\n         self.assertIn(\"<end_of_turn>\", str(output[0][\"generated_text\"]))\n+\n+    @require_torch\n+    def test_forward_tokenizer_kwargs(self):\n+        chat = [\n+            {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+            {\"role\": \"user\", \"content\": \"This is a test\"},\n+        ]\n+        model = \"hf-internal-testing/tiny-gpt2-with-chatml-template\"\n+        text_generator = pipeline(\"text-generation\", model, max_new_tokens=5)\n+        tokenizer = text_generator.tokenizer\n+\n+        with patch.object(tokenizer, \"apply_chat_template\", wraps=tokenizer.apply_chat_template) as mock:\n+            text_generator(chat, tokenizer_encode_kwargs={\"enable_thinking\": True})\n+            self.assertGreater(mock.call_count, 0)\n+            kw_call_args = mock.call_args[1]\n+            self.assertIn(\"enable_thinking\", kw_call_args)\n+            self.assertEqual(kw_call_args[\"enable_thinking\"], True)"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 29,
        "deletions": 1
    }
}