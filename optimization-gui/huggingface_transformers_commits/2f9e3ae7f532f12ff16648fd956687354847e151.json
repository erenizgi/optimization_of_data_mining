{
    "author": "yao-matrix",
    "message": "make lfm2_moe integration test pass on XPU (#41796)\n\n* make lfm2_moe integration test pass on XPU\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* xx\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* Update test_modeling_lfm2_moe.py\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "2f9e3ae7f532f12ff16648fd956687354847e151",
    "files": [
        {
            "sha": "9a106e9172ce0925c8d848c6853b845ea2fe81f4",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 40,
            "deletions": 32,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f9e3ae7f532f12ff16648fd956687354847e151/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f9e3ae7f532f12ff16648fd956687354847e151/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=2f9e3ae7f532f12ff16648fd956687354847e151",
            "patch": "@@ -18,7 +18,9 @@\n \n from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n@@ -170,36 +172,30 @@ def test_model_1a8b_logits(self):\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n+        # fmt: off\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor(\n-            [\n-                [\n-                    -1.3855,\n-                    -0.5123,\n-                    -1.3143,\n-                    -1.2144,\n-                    -1.0791,\n-                    -1.2117,\n-                    -1.4704,\n-                    -0.7648,\n-                    -0.6175,\n-                    -1.2402,\n-                    -1.1459,\n-                    -1.0083,\n-                    -1.0247,\n-                    -0.8830,\n-                    -1.5643,\n-                    -1.7266,\n-                    -1.6254,\n-                ]\n-            ]\n+        EXPECTED_MEANS = Expectations(\n+            {\n+                (\"cuda\", None): torch.tensor([[-1.3855, -0.5123, -1.3143, -1.2144, -1.0791, -1.2117, -1.4704, -0.7648, -0.6175, -1.2402, -1.1459, -1.0083, -1.0247, -0.8830, -1.5643, -1.7266, -1.6254,]]),\n+                (\"xpu\", None): torch.tensor([[-1.3863, -0.4653, -1.3246, -1.3199, -1.0940, -1.2254, -1.4716, -0.8852, -0.5920, -1.2182, -1.1782, -1.0268, -1.0114, -0.8816, -1.5774, -1.7408, -1.6147,]]),\n+            }\n         )\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # fmt: on\n+        EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n+        out_mean = out.mean(-1)\n+        torch.testing.assert_close(out_mean, EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # fmt: off\n         # Expected portion of the logits\n-        EXPECTED_SLICE = torch.tensor(\n-            [-1.2656, 2.4844, 5.5000, -1.3359, -1.3203, -1.3438, 1.9375, 5.8438, -0.6523, -1.2891]\n+        EXPECTED_SLICES = Expectations(\n+            {\n+                (\"cuda\", None): torch.tensor([-1.2656, 2.4844, 5.5000, -1.3359, -1.3203, -1.3438, 1.9375, 5.8438, -0.6523, -1.2891]),\n+                (\"xpu\", None): torch.tensor([-1.2656, 2.4531, 5.4375, -1.3438, -1.3203, -1.3516, 1.9297, 5.7812, -0.6719, -1.3203]),\n+            }\n         )\n-        torch.testing.assert_close(out[0, 0, :10], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+        # fmt: on\n+        EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n+        out_slice = out[0, 0, :10]\n+        torch.testing.assert_close(out_slice, EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_model_1a8b_generation(self):\n@@ -217,13 +213,25 @@ def test_model_1a8b_generation(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_model_1a8b_batched_chat_generation(self):\n         prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n-        EXPECTED_TEXT_COMPLETIONS = [\n-            \"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n-            \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n-            \"The Meji Restoration in Japan ended or the Meiji Restoration (1868–1912) marked a pivotal\",\n-        ]\n+        # fmt: off\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"cuda\", None): [\"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n+                                 \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n+                                 \"The Meji Restoration in Japan ended or the Meiji Restoration (1868–1912) marked a pivotal\",\n+                                ],\n+                (\"xpu\", None): ['Who are you? (AI) designed to assist?  \\nI am an AI assistant developed to',\n+                                'Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor',\n+                                'The Meji Restoration in Japan ended**  \\n* **Key Event:** The overthrow of the Tokugawa'\n+                               ],\n+            }\n+        )\n+        # fmt: on\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n+\n         set_seed(1789)\n         tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n         model = self.get_model()\n@@ -233,4 +241,4 @@ def test_model_1a8b_batched_chat_generation(self):\n         with torch.no_grad():\n             generated_ids = model.generate(**batched_input_ids, max_new_tokens=15, do_sample=False)\n         text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-        self.assertEqual(EXPECTED_TEXT_COMPLETIONS, text)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)"
        }
    ],
    "stats": {
        "total": 72,
        "additions": 40,
        "deletions": 32
    }
}