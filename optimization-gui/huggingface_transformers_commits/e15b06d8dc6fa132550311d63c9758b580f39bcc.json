{
    "author": "qubvel",
    "message": "[typing] better return typehints for `from_pretrained` (#39184)\n\n* config\n\n* processor\n\n* feature-extractor\n\n* jukebox\n\n* fixup\n\n* update other methods in config\n\n* remove \"PretrainedConfig\" annotations",
    "sha": "e15b06d8dc6fa132550311d63c9758b580f39bcc",
    "files": [
        {
            "sha": "b5fe34e14142c8eea543948e9b1d1a2d406ff19b",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -18,7 +18,7 @@\n import json\n import os\n import warnings\n-from typing import Any, Optional, Union\n+from typing import Any, Optional, TypeVar, Union\n \n from packaging import version\n \n@@ -42,6 +42,10 @@\n logger = logging.get_logger(__name__)\n \n \n+# type hinting: specifying the type of config class that inherits from PretrainedConfig\n+SpecificPretrainedConfigType = TypeVar(\"SpecificPretrainedConfigType\", bound=\"PretrainedConfig\")\n+\n+\n class PretrainedConfig(PushToHubMixin):\n     # no-format\n     r\"\"\"\n@@ -191,7 +195,7 @@ class PretrainedConfig(PushToHubMixin):\n \n     model_type: str = \"\"\n     base_config_key: str = \"\"\n-    sub_configs: dict[str, \"PretrainedConfig\"] = {}\n+    sub_configs: dict[str, type[\"PretrainedConfig\"]] = {}\n     has_no_defaults_at_init: bool = False\n     attribute_map: dict[str, str] = {}\n     base_model_tp_plan: Optional[dict[str, Any]] = None\n@@ -474,15 +478,15 @@ def _set_token_in_kwargs(kwargs, token=None):\n \n     @classmethod\n     def from_pretrained(\n-        cls,\n+        cls: type[SpecificPretrainedConfigType],\n         pretrained_model_name_or_path: Union[str, os.PathLike],\n         cache_dir: Optional[Union[str, os.PathLike]] = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n         **kwargs,\n-    ) -> \"PretrainedConfig\":\n+    ) -> SpecificPretrainedConfigType:\n         r\"\"\"\n         Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n \n@@ -717,7 +721,9 @@ def _get_config_dict(\n         return config_dict, kwargs\n \n     @classmethod\n-    def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n+    def from_dict(\n+        cls: type[SpecificPretrainedConfigType], config_dict: dict[str, Any], **kwargs\n+    ) -> SpecificPretrainedConfigType:\n         \"\"\"\n         Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n \n@@ -778,7 +784,9 @@ def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n             return config\n \n     @classmethod\n-    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> \"PretrainedConfig\":\n+    def from_json_file(\n+        cls: type[SpecificPretrainedConfigType], json_file: Union[str, os.PathLike]\n+    ) -> SpecificPretrainedConfigType:\n         \"\"\"\n         Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n "
        },
        {
            "sha": "c539288cbed81de7dd5513d6777f97b99fe594ff",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -20,7 +20,7 @@\n import os\n import warnings\n from collections import UserDict\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n \n import numpy as np\n \n@@ -55,6 +55,9 @@\n \n PreTrainedFeatureExtractor = Union[\"SequenceFeatureExtractor\"]  # noqa: F821\n \n+# type hinting: specifying the type of feature extractor class that inherits from FeatureExtractionMixin\n+SpecificFeatureExtractorType = TypeVar(\"SpecificFeatureExtractorType\", bound=\"FeatureExtractionMixin\")\n+\n \n class BatchFeature(UserDict):\n     r\"\"\"\n@@ -270,15 +273,15 @@ def _set_processor_class(self, processor_class: str):\n \n     @classmethod\n     def from_pretrained(\n-        cls,\n+        cls: type[SpecificFeatureExtractorType],\n         pretrained_model_name_or_path: Union[str, os.PathLike],\n         cache_dir: Optional[Union[str, os.PathLike]] = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n         **kwargs,\n-    ):\n+    ) -> SpecificFeatureExtractorType:\n         r\"\"\"\n         Instantiate a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a feature extractor, *e.g.* a\n         derived class of [`SequenceFeatureExtractor`]."
        },
        {
            "sha": "fc0a249850c4990ead9dd60235553c98a9a9a32c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -2616,7 +2616,7 @@ def _check_and_enable_flash_attn_3(\n         return config\n \n     @classmethod\n-    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> PretrainedConfig:\n+    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n         \"\"\"\n         Checks the availability of SDPA for a given model.\n "
        },
        {
            "sha": "e5d9957cbd4009a49096be686e3936d1b427e102",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -131,7 +131,7 @@ def __init__(\n     @classmethod\n     def from_pretrained(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], config_type: str = \"text_config\", **kwargs\n-    ) -> \"PretrainedConfig\":\n+    ):\n         cls._set_token_in_kwargs(kwargs)\n \n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)"
        },
        {
            "sha": "067d61fe2e028aefb1e96e936029c90b9431954c",
            "filename": "src/transformers/models/deprecated/jukebox/configuration_jukebox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -345,9 +345,7 @@ def __init__(\n         self.zero_out = zero_out\n \n     @classmethod\n-    def from_pretrained(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], level=0, **kwargs\n-    ) -> \"PretrainedConfig\":\n+    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], level=0, **kwargs):\n         cls._set_token_in_kwargs(kwargs)\n \n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n@@ -470,7 +468,7 @@ def __init__(\n         self.zero_out = zero_out\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n+    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs):\n         cls._set_token_in_kwargs(kwargs)\n \n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)"
        },
        {
            "sha": "a3606e0b11c7d8246c8b5ebc2684e360ccfb04cf",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch Falcon model.\"\"\"\n \n import math\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -47,9 +47,6 @@\n from .configuration_falcon import FalconConfig\n \n \n-if TYPE_CHECKING:\n-    from ...configuration_utils import PretrainedConfig\n-\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -688,7 +685,7 @@ def _init_weights(self, module: nn.Module):\n \n     # Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa\n     @classmethod\n-    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> \"PretrainedConfig\":\n+    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n         _is_bettertransformer = getattr(cls, \"use_bettertransformer\", False)\n         if _is_bettertransformer:\n             return config"
        },
        {
            "sha": "51897b40681afecffc9041a0db65ad1900bc2148",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -1074,7 +1074,7 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+    def get_text_config(self, decoder=False):\n         \"\"\"\n         Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n         itself. On specific composite models, it is under a set of valid names."
        },
        {
            "sha": "b3d4ae90e886002b5585cb3b9e97e63dfccd505c",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -1114,7 +1114,7 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+    def get_text_config(self, decoder=False):\n         \"\"\"\n         Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n         itself. On specific composite models, it is under a set of valid names."
        },
        {
            "sha": "ad4213d3fb025c0c9d2f4efae7a61520747e00dd",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -324,7 +324,7 @@ def __setattr__(self, key, value):\n             setattr(self.decoder, key, value)\n         super().__setattr__(key, value)\n \n-    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+    def get_text_config(self, decoder=False):\n         # Always return self, regardless of the decoder option.\n         del decoder\n         return self"
        },
        {
            "sha": "970816fc38f929007a82f53bc3ff66a8c671d9c4",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -213,7 +213,7 @@ def __setattr__(self, key, value):\n             setattr(self.decoder, key, value)\n         super().__setattr__(key, value)\n \n-    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+    def get_text_config(self, decoder=False):\n         # Always return self, regardless of the decoder option.\n         del decoder\n         return self"
        },
        {
            "sha": "25add6dbe7525857c73fef93363596ff33967268",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -24,7 +24,7 @@\n import warnings\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any, Optional, TypedDict, Union\n+from typing import Any, Optional, TypedDict, TypeVar, Union\n \n import numpy as np\n import typing_extensions\n@@ -75,6 +75,9 @@\n \n logger = logging.get_logger(__name__)\n \n+# type hinting: specifying the type of processor class that inherits from ProcessorMixin\n+SpecificProcessorType = TypeVar(\"SpecificProcessorType\", bound=\"ProcessorMixin\")\n+\n # Dynamically import the Transformers module to grab the attribute classes of the processor from their names.\n transformers_module = direct_transformers_import(Path(__file__).parent)\n \n@@ -1246,15 +1249,15 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n \n     @classmethod\n     def from_pretrained(\n-        cls,\n+        cls: type[SpecificProcessorType],\n         pretrained_model_name_or_path: Union[str, os.PathLike],\n         cache_dir: Optional[Union[str, os.PathLike]] = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n         **kwargs,\n-    ):\n+    ) -> SpecificProcessorType:\n         r\"\"\"\n         Instantiate a processor associated with a pretrained model.\n "
        }
    ],
    "stats": {
        "total": 63,
        "additions": 36,
        "deletions": 27
    }
}