{
    "author": "younesbelkada",
    "message": "Fix: Fix `FalconMamba` training issues due to incompatible kernels (#33195)\n\n* fix FM training kernels\r\n\r\n* fix copies\r\n\r\n* fix copies\r\n\r\n* propagate to slow path\r\n\r\n* make it BC\r\n\r\n* add comment\r\n\r\n* fix test",
    "sha": "47b096412da9cbeb9351806e9f0eb70a693b2859",
    "files": [
        {
            "sha": "da88e3394f653369a7443245c67dcbe57f2ed23e",
            "filename": "src/transformers/kernels/falcon_mamba/__init__.py",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2F__init__.py?ref=47b096412da9cbeb9351806e9f0eb70a693b2859",
            "patch": "@@ -0,0 +1,15 @@\n+# coding=utf-8\n+# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from .selective_scan_with_ln_interface import mamba_inner_fn"
        },
        {
            "sha": "4a74986a81a13f9428eab353de5b61a4d101972d",
            "filename": "src/transformers/kernels/falcon_mamba/selective_scan_with_ln_interface.py",
            "status": "added",
            "additions": 525,
            "deletions": 0,
            "changes": 525,
            "blob_url": "https://github.com/huggingface/transformers/blob/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py?ref=47b096412da9cbeb9351806e9f0eb70a693b2859",
            "patch": "@@ -0,0 +1,525 @@\n+# coding=utf-8\n+# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# Original code from: https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py\n+\n+import torch\n+import torch.nn.functional as F\n+from einops import rearrange, repeat\n+from torch.cuda.amp import custom_bwd, custom_fwd\n+\n+\n+try:\n+    import causal_conv1d_cuda\n+except ImportError:\n+    causal_conv1d_cuda = None\n+\n+import mamba_ssm\n+import selective_scan_cuda\n+\n+\n+# For BC for old mamba-ssm versions: https://github.com/huggingface/transformers/pull/33195#discussion_r1736401127\n+if hasattr(mamba_ssm.ops.triton, \"layernorm\"):\n+    from mamba_ssm.ops.triton.layernorm import _layer_norm_fwd\n+else:\n+    from mamba_ssm.ops.triton.layer_norm import _layer_norm_fwd\n+\n+\n+class SelectiveScanFn(torch.autograd.Function):\n+    @staticmethod\n+    def forward(\n+        ctx, u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False, return_last_state=False\n+    ):\n+        if u.stride(-1) != 1:\n+            u = u.contiguous()\n+        if delta.stride(-1) != 1:\n+            delta = delta.contiguous()\n+        if D is not None:\n+            D = D.contiguous()\n+        if B.stride(-1) != 1:\n+            B = B.contiguous()\n+        if C.stride(-1) != 1:\n+            C = C.contiguous()\n+        if z is not None and z.stride(-1) != 1:\n+            z = z.contiguous()\n+        if B.dim() == 3:\n+            B = rearrange(B, \"b dstate l -> b 1 dstate l\")\n+            ctx.squeeze_B = True\n+        if C.dim() == 3:\n+            C = rearrange(C, \"b dstate l -> b 1 dstate l\")\n+            ctx.squeeze_C = True\n+        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus)\n+        ctx.delta_softplus = delta_softplus\n+        ctx.has_z = z is not None\n+        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)\n+        if not ctx.has_z:\n+            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)\n+            return out if not return_last_state else (out, last_state)\n+        else:\n+            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)\n+            out_z = rest[0]\n+            return out_z if not return_last_state else (out_z, last_state)\n+\n+    @staticmethod\n+    def backward(ctx, dout, *args):\n+        if not ctx.has_z:\n+            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors\n+            z = None\n+            out = None\n+        else:\n+            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors\n+        if dout.stride(-1) != 1:\n+            dout = dout.contiguous()\n+        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n+        # backward of selective_scan_cuda with the backward of chunk).\n+        # Here we just pass in None and dz will be allocated in the C++ code.\n+        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(\n+            u,\n+            delta,\n+            A,\n+            B,\n+            C,\n+            D,\n+            z,\n+            delta_bias,\n+            dout,\n+            x,\n+            out,\n+            None,\n+            ctx.delta_softplus,\n+            False,  # option to recompute out_z, not used here\n+        )\n+        dz = rest[0] if ctx.has_z else None\n+        dB = dB.squeeze(1) if getattr(ctx, \"squeeze_B\", False) else dB\n+        dC = dC.squeeze(1) if getattr(ctx, \"squeeze_C\", False) else dC\n+        return (\n+            du,\n+            ddelta,\n+            dA,\n+            dB,\n+            dC,\n+            dD if D is not None else None,\n+            dz,\n+            ddelta_bias if delta_bias is not None else None,\n+            None,\n+            None,\n+        )\n+\n+\n+def rms_norm_forward(\n+    x,\n+    weight,\n+    bias,\n+    eps=1e-6,\n+    is_rms_norm=True,\n+):\n+    # x (b l) d\n+    if x.stride(-1) != 1:\n+        x = x.contiguous()\n+    weight = weight.contiguous()\n+    if bias is not None:\n+        bias = bias.contiguous()\n+    y = _layer_norm_fwd(x, weight, bias, eps, None, residual_dtype=None, is_rms_norm=is_rms_norm)[0]\n+    # y (b l) d\n+    return y\n+\n+\n+def selective_scan_fn(\n+    u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False, return_last_state=False\n+):\n+    \"\"\"if return_last_state is True, returns (out, last_state)\n+    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is\n+    not considered in the backward pass.\n+    \"\"\"\n+    return SelectiveScanFn.apply(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)\n+\n+\n+def selective_scan_ref(\n+    u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False, return_last_state=False\n+):\n+    \"\"\"\n+    u: r(B D L)\n+    delta: r(B D L)\n+    A: c(D N) or r(D N)\n+    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n+    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n+    D: r(D)\n+    z: r(B D L)\n+    delta_bias: r(D), fp32\n+\n+    out: r(B D L)\n+    last_state (optional): r(B D dstate) or c(B D dstate)\n+    \"\"\"\n+    dtype_in = u.dtype\n+    u = u.float()\n+    delta = delta.float()\n+    if delta_bias is not None:\n+        delta = delta + delta_bias[..., None].float()\n+    if delta_softplus:\n+        delta = F.softplus(delta)\n+    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n+    is_variable_B = B.dim() >= 3\n+    is_variable_C = C.dim() >= 3\n+    if A.is_complex():\n+        if is_variable_B:\n+            B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n+        if is_variable_C:\n+            C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n+    else:\n+        B = B.float()\n+        C = C.float()\n+    x = A.new_zeros((batch, dim, dstate))\n+    ys = []\n+    deltaA = torch.exp(torch.einsum(\"bdl,dn->bdln\", delta, A))\n+    if not is_variable_B:\n+        deltaB_u = torch.einsum(\"bdl,dn,bdl->bdln\", delta, B, u)\n+    else:\n+        if B.dim() == 3:\n+            deltaB_u = torch.einsum(\"bdl,bnl,bdl->bdln\", delta, B, u)\n+        else:\n+            B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n+            deltaB_u = torch.einsum(\"bdl,bdnl,bdl->bdln\", delta, B, u)\n+    if is_variable_C and C.dim() == 4:\n+        C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n+    last_state = None\n+    for i in range(u.shape[2]):\n+        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n+        if not is_variable_C:\n+            y = torch.einsum(\"bdn,dn->bd\", x, C)\n+        else:\n+            if C.dim() == 3:\n+                y = torch.einsum(\"bdn,bn->bd\", x, C[:, :, i])\n+            else:\n+                y = torch.einsum(\"bdn,bdn->bd\", x, C[:, :, :, i])\n+        if i == u.shape[2] - 1:\n+            last_state = x\n+        if y.is_complex():\n+            y = y.real * 2\n+        ys.append(y)\n+    y = torch.stack(ys, dim=2)  # (batch dim L)\n+    out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n+    if z is not None:\n+        out = out * F.silu(z)\n+    out = out.to(dtype=dtype_in)\n+    return out if not return_last_state else (out, last_state)\n+\n+\n+class MambaInnerFn(torch.autograd.Function):\n+    @staticmethod\n+    @custom_fwd\n+    def forward(\n+        ctx,\n+        xz,\n+        conv1d_weight,\n+        conv1d_bias,\n+        x_proj_weight,\n+        delta_proj_weight,\n+        out_proj_weight,\n+        out_proj_bias,\n+        A,\n+        B=None,\n+        C=None,\n+        D=None,\n+        delta_bias=None,\n+        B_proj_bias=None,\n+        C_proj_bias=None,\n+        delta_softplus=True,\n+        checkpoint_lvl=1,\n+        b_rms_weight=None,\n+        c_rms_weight=None,\n+        dt_rms_weight=None,\n+        b_c_dt_rms_eps=1e-6,\n+    ):\n+        \"\"\"\n+        xz: (batch, dim, seqlen)\n+        \"\"\"\n+        assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n+        assert checkpoint_lvl in [0, 1]\n+        L = xz.shape[-1]\n+        delta_rank = delta_proj_weight.shape[1]\n+        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n+        if torch.is_autocast_enabled():\n+            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n+            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n+            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n+            out_proj_bias = (\n+                out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype()) if out_proj_bias is not None else None\n+            )\n+        if xz.stride(-1) != 1:\n+            xz = xz.contiguous()\n+        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n+        x, z = xz.chunk(2, dim=1)\n+        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n+        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, None, None, None, True)\n+        # We're being very careful here about the layout, to avoid extra transposes.\n+        # We want delta to have d as the slowest moving dimension\n+        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n+        x_dbl = F.linear(rearrange(conv1d_out, \"b d l -> (b l) d\"), x_proj_weight)  # (bl d)\n+        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l=L)\n+        ctx.is_variable_B = B is None\n+        ctx.is_variable_C = C is None\n+        ctx.B_proj_bias_is_None = B_proj_bias is None\n+        ctx.C_proj_bias_is_None = C_proj_bias is None\n+        if B is None:  # variable B\n+            B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl dstate)\n+            if B_proj_bias is not None:\n+                B = B + B_proj_bias.to(dtype=B.dtype)\n+            if not A.is_complex():\n+                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n+                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n+            else:\n+                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n+        else:\n+            if B.stride(-1) != 1:\n+                B = B.contiguous()\n+        if C is None:  # variable C\n+            C = x_dbl[:, -d_state:]  # (bl dstate)\n+            if C_proj_bias is not None:\n+                C = C + C_proj_bias.to(dtype=C.dtype)\n+            if not A.is_complex():\n+                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n+                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n+            else:\n+                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n+        else:\n+            if C.stride(-1) != 1:\n+                C = C.contiguous()\n+        if D is not None:\n+            D = D.contiguous()\n+\n+        if b_rms_weight is not None:\n+            B = rearrange(B, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n+            B = rms_norm_forward(B, b_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n+            B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n+        if c_rms_weight is not None:\n+            C = rearrange(C, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n+            C = rms_norm_forward(C, c_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n+            C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n+        if dt_rms_weight is not None:\n+            delta = rearrange(delta, \"b d l -> (b l) d\", l=L).contiguous()\n+            delta = rms_norm_forward(delta, dt_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n+            delta = rearrange(delta, \"(b l) d -> b d l\", l=L).contiguous()\n+\n+        out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n+            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n+        )\n+        ctx.delta_softplus = delta_softplus\n+        ctx.out_proj_bias_is_None = out_proj_bias is None\n+        ctx.checkpoint_lvl = checkpoint_lvl\n+        ctx.b_rms_weight = b_rms_weight\n+        ctx.c_rms_weight = c_rms_weight\n+        ctx.dt_rms_weight = dt_rms_weight\n+        ctx.b_c_dt_rms_eps = b_c_dt_rms_eps\n+        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n+            conv1d_out, delta = None, None\n+        ctx.save_for_backward(\n+            xz,\n+            conv1d_weight,\n+            conv1d_bias,\n+            x_dbl,\n+            x_proj_weight,\n+            delta_proj_weight,\n+            out_proj_weight,\n+            conv1d_out,\n+            delta,\n+            A,\n+            B,\n+            C,\n+            D,\n+            delta_bias,\n+            scan_intermediates,\n+            b_rms_weight,\n+            c_rms_weight,\n+            dt_rms_weight,\n+            out,\n+        )\n+        return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n+\n+    @staticmethod\n+    @custom_bwd\n+    def backward(ctx, dout):\n+        # dout: (batch, seqlen, dim)\n+        assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n+        (\n+            xz,\n+            conv1d_weight,\n+            conv1d_bias,\n+            x_dbl,\n+            x_proj_weight,\n+            delta_proj_weight,\n+            out_proj_weight,\n+            conv1d_out,\n+            delta,\n+            A,\n+            B,\n+            C,\n+            D,\n+            delta_bias,\n+            scan_intermediates,\n+            b_rms_weight,\n+            c_rms_weight,\n+            dt_rms_weight,\n+            out,\n+        ) = ctx.saved_tensors\n+        L = xz.shape[-1]\n+        delta_rank = delta_proj_weight.shape[1]\n+        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n+        x, z = xz.chunk(2, dim=1)\n+        if dout.stride(-1) != 1:\n+            dout = dout.contiguous()\n+        if ctx.checkpoint_lvl == 1:\n+            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, None, None, None, True)\n+            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l=L)\n+            if dt_rms_weight is not None:\n+                delta = rearrange(delta, \"b d l -> (b l) d\", l=L).contiguous()\n+                delta = rms_norm_forward(delta, ctx.dt_rms_weight, None, ctx.b_c_dt_rms_eps)\n+                delta = rearrange(delta, \"(b l) d -> b d l\", l=L).contiguous()\n+            if b_rms_weight is not None:\n+                # Recompute & RMSNorm B\n+                B = rearrange(B, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n+                B = rms_norm_forward(B, ctx.b_rms_weight, None, ctx.b_c_dt_rms_eps)\n+                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n+            if c_rms_weight is not None:\n+                # Recompute & RMSNorm C\n+                C = rearrange(C, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n+                C = rms_norm_forward(C, ctx.c_rms_weight, None, ctx.b_c_dt_rms_eps)\n+                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n+\n+        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n+        # backward of selective_scan_cuda with the backward of chunk).\n+        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n+        dx, dz = dxz.chunk(2, dim=1)\n+        dout = rearrange(dout, \"b l e -> e (b l)\")\n+        dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n+        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n+            conv1d_out,\n+            delta,\n+            A,\n+            B,\n+            C,\n+            D,\n+            z,\n+            delta_bias,\n+            dout_y,\n+            scan_intermediates,\n+            out,\n+            dz,\n+            ctx.delta_softplus,\n+            True,  # option to recompute out_z\n+        )\n+        dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n+        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n+        dD = dD if D is not None else None\n+        dx_dbl = torch.empty_like(x_dbl)\n+        dB_proj_bias = None\n+        if ctx.is_variable_B:\n+            if not A.is_complex():\n+                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n+            else:\n+                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n+            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n+            dx_dbl[:, delta_rank : delta_rank + d_state] = dB  # (bl d)\n+            dB = None\n+        dC_proj_bias = None\n+        if ctx.is_variable_C:\n+            if not A.is_complex():\n+                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n+            else:\n+                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n+            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n+            dx_dbl[:, -d_state:] = dC  # (bl d)\n+            dC = None\n+        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n+        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n+        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n+        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n+        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n+        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n+        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n+        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n+        # backward of conv1d with the backward of chunk).\n+        dx, dconv1d_weight, dconv1d_bias, *_ = causal_conv1d_cuda.causal_conv1d_bwd(\n+            x, conv1d_weight, conv1d_bias, dconv1d_out, None, None, None, dx, False, True\n+        )\n+        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n+        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n+        return (\n+            dxz,\n+            dconv1d_weight,\n+            dconv1d_bias,\n+            dx_proj_weight,\n+            ddelta_proj_weight,\n+            dout_proj_weight,\n+            dout_proj_bias,\n+            dA,\n+            dB,\n+            dC,\n+            dD,\n+            ddelta_bias if delta_bias is not None else None,\n+            # 6-None are delta_softplus, checkpoint_lvl, b_rms_weight, c_rms_weight, dt_rms_weight, b_c_dt_rms_eps\n+            dB_proj_bias,\n+            dC_proj_bias,\n+            None,\n+            None,\n+            None,\n+            None,\n+            None,\n+            None,\n+        )\n+\n+\n+def mamba_inner_fn(\n+    xz,\n+    conv1d_weight,\n+    conv1d_bias,\n+    x_proj_weight,\n+    delta_proj_weight,\n+    out_proj_weight,\n+    out_proj_bias,\n+    A,\n+    B=None,\n+    C=None,\n+    D=None,\n+    delta_bias=None,\n+    B_proj_bias=None,\n+    C_proj_bias=None,\n+    delta_softplus=True,\n+    checkpoint_lvl=1,\n+    b_rms_weight=None,\n+    c_rms_weight=None,\n+    dt_rms_weight=None,\n+    b_c_dt_rms_eps=1e-6,\n+):\n+    return MambaInnerFn.apply(\n+        xz,\n+        conv1d_weight,\n+        conv1d_bias,\n+        x_proj_weight,\n+        delta_proj_weight,\n+        out_proj_weight,\n+        out_proj_bias,\n+        A,\n+        B,\n+        C,\n+        D,\n+        delta_bias,\n+        B_proj_bias,\n+        C_proj_bias,\n+        delta_softplus,\n+        checkpoint_lvl,\n+        b_rms_weight,\n+        c_rms_weight,\n+        dt_rms_weight,\n+        b_c_dt_rms_eps,\n+    )"
        },
        {
            "sha": "cabba738a479e1678bbfccb74d4da22b603c9ccf",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=47b096412da9cbeb9351806e9f0eb70a693b2859",
            "patch": "@@ -23,7 +23,6 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mamba.configuration_mamba.MambaConfig with mamba->falcon_mamba,Mamba->FalconMamba,MAMBA->FALCON_MAMBA,state-spaces/falcon_mamba-2.8b->tiiuae/falcon-mamba-7b,use_falcon_mambapy->use_mambapy\n class FalconMambaConfig(PretrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`FalconMambaModel`]. It is used to instantiate a FALCON_MAMBA\n@@ -82,8 +81,8 @@ class FalconMambaConfig(PretrainedConfig):\n             Whether or not the cache should be used.\n         use_mambapy (`bool`, *optional*, defaults to `False`):\n             Determines the fallback strategy during training if the CUDA-based official implementation of FalconMamba is not avaiable. If `True`, the falcon_mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n-\n-\n+        mixer_rms_eps (`float`, *optional*, defaults to 1e-06):\n+            The RMS norm epsilon value that is used in the Mixer RMS norm for B, C and dt states.\n     Example:\n \n     ```python\n@@ -127,6 +126,7 @@ def __init__(\n         rescale_prenorm_residual=False,\n         use_cache=True,\n         use_mambapy=False,\n+        mixer_rms_eps=1e-6,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -154,5 +154,6 @@ def __init__(\n         self.residual_in_fp32 = residual_in_fp32\n         self.use_cache = use_cache\n         self.use_mambapy = use_mambapy\n+        self.mixer_rms_eps = mixer_rms_eps\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)"
        },
        {
            "sha": "f682f75f222ef6915f8560a94e0caa47490998b9",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 23,
            "deletions": 8,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47b096412da9cbeb9351806e9f0eb70a693b2859/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=47b096412da9cbeb9351806e9f0eb70a693b2859",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2024 state-spaces/falcon_mamba org and HuggingFace Inc. team.\n+# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -45,8 +45,10 @@\n     pscan = None\n \n if is_mamba_ssm_available():\n-    from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n+    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n     from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n+\n+    from ...kernels.falcon_mamba import mamba_inner_fn\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n@@ -131,6 +133,15 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n \n+        # Triton expects to pass RMS weights even if they are non learnable, thus we need to create these weights here\n+        self.register_buffer(\n+            \"b_c_rms\", torch.nn.Parameter(torch.ones(self.ssm_state_size), requires_grad=False), persistent=False\n+        )\n+        self.register_buffer(\n+            \"dt_rms\", torch.nn.Parameter(torch.ones(self.intermediate_size), requires_grad=False), persistent=False\n+        )\n+        self.rms_eps = config.mixer_rms_eps\n+\n         if not is_fast_path_available:\n             if self.use_mambapy:\n                 if is_mambapy_available():\n@@ -175,6 +186,10 @@ def cuda_kernels_forward(\n                 self.D.float(),\n                 delta_bias=self.dt_proj.bias.float(),\n                 delta_softplus=True,\n+                b_rms_weight=self.b_c_rms,\n+                c_rms_weight=self.b_c_rms,\n+                dt_rms_weight=self.dt_rms,\n+                b_c_dt_rms_eps=self.rms_eps,\n             )\n \n         else:\n@@ -214,9 +229,9 @@ def cuda_kernels_forward(\n                 ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n             )\n \n-            B = rms_forward(B)\n-            C = rms_forward(C)\n-            time_step = rms_forward(time_step)\n+            B = rms_forward(B, variance_epsilon=self.rms_eps)\n+            C = rms_forward(C, variance_epsilon=self.rms_eps)\n+            time_step = rms_forward(time_step, variance_epsilon=self.rms_eps)\n \n             # In case the model has been quantized, we need a hack to properly call the `nn.Linear` module\n             # at the price of a small overhead.\n@@ -315,9 +330,9 @@ def slow_forward(\n             ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n         )\n \n-        B = rms_forward(B)\n-        C = rms_forward(C)\n-        time_step = rms_forward(time_step)\n+        B = rms_forward(B, variance_epsilon=self.rms_eps)\n+        C = rms_forward(C, variance_epsilon=self.rms_eps)\n+        time_step = rms_forward(time_step, variance_epsilon=self.rms_eps)\n \n         discrete_time_step = self.dt_proj(time_step)  # [batch, seq_len, intermediate_size]\n         discrete_time_step = nn.functional.softplus(discrete_time_step).transpose("
        },
        {
            "sha": "b94f235a1a61e36334fe4898703938a49b7d3470",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/47b096412da9cbeb9351806e9f0eb70a693b2859/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47b096412da9cbeb9351806e9f0eb70a693b2859/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=47b096412da9cbeb9351806e9f0eb70a693b2859",
            "patch": "@@ -524,3 +524,32 @@ def test_batched_generation(self):\n         out = tok.batch_decode(out, skip_special_tokens=True)\n \n         self.assertListEqual(out, EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_training_kernel(self):\n+        model_id = \"tiiuae/falcon-mamba-7b\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n+\n+        text = \"Hello today\"\n+\n+        inputs = tokenizer(text, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            logits = torch.argmax(model(**inputs).logits, dim=-1)\n+\n+        out_no_training = tokenizer.batch_decode(logits)\n+\n+        model.train()\n+        lm_logits = model(**inputs).logits\n+        next_token = torch.argmax(lm_logits, dim=-1)\n+\n+        out_training = tokenizer.batch_decode(next_token)\n+\n+        # Just verify backward works\n+        loss = (1 - lm_logits).mean()\n+        loss.backward()\n+\n+        self.assertEqual(out_training, out_no_training)"
        },
        {
            "sha": "a9a841f3bbffb6c3953a92d368e7fe285cff85c2",
            "filename": "utils/check_inits.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/47b096412da9cbeb9351806e9f0eb70a693b2859/utils%2Fcheck_inits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47b096412da9cbeb9351806e9f0eb70a693b2859/utils%2Fcheck_inits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_inits.py?ref=47b096412da9cbeb9351806e9f0eb70a693b2859",
            "patch": "@@ -332,6 +332,7 @@ def get_transformers_submodules() -> List[str]:\n     \"modeling_attn_mask_utils\",\n     \"safetensors_conversion\",\n     \"modeling_gguf_pytorch_utils\",\n+    \"kernels.falcon_mamba\",\n ]\n \n "
        }
    ],
    "stats": {
        "total": 608,
        "additions": 597,
        "deletions": 11
    }
}