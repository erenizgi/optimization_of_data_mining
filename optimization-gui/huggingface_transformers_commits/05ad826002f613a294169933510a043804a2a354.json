{
    "author": "itazap",
    "message": "remove unhandled parameter (#38145)",
    "sha": "05ad826002f613a294169933510a043804a2a354",
    "files": [
        {
            "sha": "236ab21d2d2a467a36a83e683f9a4fbf024496e3",
            "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05ad826002f613a294169933510a043804a2a354/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05ad826002f613a294169933510a043804a2a354/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py?ref=05ad826002f613a294169933510a043804a2a354",
            "patch": "@@ -231,7 +231,6 @@ def test_batch_tokenization(self):\n                     batch = tokenizer(\n                         text=text,\n                         max_length=3,\n-                        max_target_length=10,\n                         return_tensors=\"pt\",\n                     )\n                 except NotImplementedError:\n@@ -241,7 +240,7 @@ def test_batch_tokenization(self):\n                 batch = tokenizer(text, max_length=3, return_tensors=\"pt\")\n                 self.assertEqual(batch.input_ids.shape[1], 3)\n \n-                batch_encoder_only = tokenizer(text=text, max_length=3, max_target_length=10, return_tensors=\"pt\")\n+                batch_encoder_only = tokenizer(text=text, max_length=3, return_tensors=\"pt\")\n                 self.assertEqual(batch_encoder_only.input_ids.shape[1], 3)\n                 self.assertEqual(batch_encoder_only.attention_mask.shape[1], 3)\n                 self.assertNotIn(\"decoder_input_ids\", batch_encoder_only)"
        },
        {
            "sha": "91a5cebaed599e33ce10ccd6214ac08f7ceee776",
            "filename": "tests/models/gemma/test_tokenization_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05ad826002f613a294169933510a043804a2a354/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05ad826002f613a294169933510a043804a2a354/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py?ref=05ad826002f613a294169933510a043804a2a354",
            "patch": "@@ -79,7 +79,6 @@ def test_batch_tokenization(self):\n                     batch = tokenizer(\n                         text=text,\n                         max_length=3,\n-                        max_target_length=10,\n                         return_tensors=\"pt\",\n                     )\n                 except NotImplementedError:\n@@ -89,7 +88,7 @@ def test_batch_tokenization(self):\n                 batch = tokenizer(text, max_length=3, return_tensors=\"pt\")\n                 self.assertEqual(batch.input_ids.shape[1], 3)\n \n-                batch_encoder_only = tokenizer(text=text, max_length=3, max_target_length=10, return_tensors=\"pt\")\n+                batch_encoder_only = tokenizer(text=text, max_length=3, return_tensors=\"pt\")\n                 self.assertEqual(batch_encoder_only.input_ids.shape[1], 3)\n                 self.assertEqual(batch_encoder_only.attention_mask.shape[1], 3)\n                 self.assertNotIn(\"decoder_input_ids\", batch_encoder_only)"
        },
        {
            "sha": "aa2cf1610365171d57032fa8e4be9ea3898f0084",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05ad826002f613a294169933510a043804a2a354/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05ad826002f613a294169933510a043804a2a354/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=05ad826002f613a294169933510a043804a2a354",
            "patch": "@@ -229,7 +229,6 @@ def test_batch_tokenization(self):\n                     batch = tokenizer(\n                         text=text,\n                         max_length=3,\n-                        max_target_length=10,\n                         return_tensors=\"pt\",\n                     )\n                 except NotImplementedError:\n@@ -239,7 +238,7 @@ def test_batch_tokenization(self):\n                 batch = tokenizer(text, max_length=3, return_tensors=\"pt\")\n                 self.assertEqual(batch.input_ids.shape[1], 3)\n \n-                batch_encoder_only = tokenizer(text=text, max_length=3, max_target_length=10, return_tensors=\"pt\")\n+                batch_encoder_only = tokenizer(text=text, max_length=3, return_tensors=\"pt\")\n                 self.assertEqual(batch_encoder_only.input_ids.shape[1], 3)\n                 self.assertEqual(batch_encoder_only.attention_mask.shape[1], 3)\n                 self.assertNotIn(\"decoder_input_ids\", batch_encoder_only)"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 3,
        "deletions": 6
    }
}