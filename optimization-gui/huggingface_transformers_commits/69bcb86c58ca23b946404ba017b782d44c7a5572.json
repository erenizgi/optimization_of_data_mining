{
    "author": "guangy10",
    "message": "Export for Phi4-mini (#36780)\n\n* Export for Phi4-mini\n\n* Update tests/models/phi3/test_modeling_phi3.py\n\n---------\n\nCo-authored-by: Guang Yang <guangyang@fb.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "69bcb86c58ca23b946404ba017b782d44c7a5572",
    "files": [
        {
            "sha": "2edf52db3aaf94b0b70acf7d74f217d695472a64",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/69bcb86c58ca23b946404ba017b782d44c7a5572/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69bcb86c58ca23b946404ba017b782d44c7a5572/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=69bcb86c58ca23b946404ba017b782d44c7a5572",
            "patch": "@@ -21,6 +21,7 @@\n from parameterized import parameterized\n \n from transformers import Phi3Config, StaticCache, is_torch_available, set_seed\n+from transformers.models.auto.configuration_auto import AutoConfig\n from transformers.testing_utils import (\n     require_torch,\n     slow,\n@@ -707,3 +708,72 @@ def test_phi3_mini_4k_sliding_window(self):\n         ]\n \n         self.assertListEqual(output_text, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_export_static_cache(self):\n+        from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n+\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        model_id = \"microsoft/Phi-4-mini-instruct\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"</s>\", padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user. A 45-year-old patient with a 10-year history of type 2 diabetes mellitus, who is currently on metformin and a SGLT2 inhibitor, presents with a 2-year history\"\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load config\n+        config = AutoConfig.from_pretrained(model_id)\n+        # NOTE: To make the model exportable we need to set the rope scaling to default to avoid hitting\n+        # the data-dependent control flow in _longrope_frequency_update. Alternatively, we can rewrite\n+        # that function to avoid the data-dependent control flow.\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            config.rope_scaling[\"type\"] = \"default\"\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id,\n+            config=config,\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompt = [\n+            \"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.\"\n+        ]\n+        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 70,
        "deletions": 0
    }
}