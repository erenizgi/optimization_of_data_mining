{
    "author": "Cyrilvallez",
    "message": "Fix tied weight loading with TP and loading sub state_dicts (#37758)\n\nUpdate modeling_utils.py",
    "sha": "0af0a5f9698f501240464613d52677f206cdc291",
    "files": [
        {
            "sha": "168d421c99475b43ab365dbec06bf4b6c54fa7da",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0af0a5f9698f501240464613d52677f206cdc291/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0af0a5f9698f501240464613d52677f206cdc291/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0af0a5f9698f501240464613d52677f206cdc291",
            "patch": "@@ -4978,7 +4978,10 @@ def _load_pretrained_model(\n                     name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n                 }\n                 for name, param in parameters_to_initialize.items():\n-                    # First move data to correct\n+                    # If it is still on meta here, it means that it's a tied weight that will be tied later anyway -> skip it\n+                    if param.device.type == \"meta\":\n+                        continue\n+                    # Shard the param\n                     to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_regex)\n                     shard_and_distribute_module(\n                         model,"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 4,
        "deletions": 1
    }
}