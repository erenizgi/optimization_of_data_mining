{
    "author": "patrickvonplaten",
    "message": "[Ministral 3] Add ministral 3 (#42498)\n\n* Up\n\n* WIP\n\n* WIP\n\n* WIP\n\n* Apply suggestions from code review\n\nCo-authored-by: Julien Denize <40604584+juliendenize@users.noreply.github.com>\n\n* Update src/transformers/models/ministral3/configuration_ministral3.py\n\nCo-authored-by: Julien Denize <40604584+juliendenize@users.noreply.github.com>\n\n* fix most tests\n\n* update docsting\n\n* fixup\n\n* typo in the ocnfig\n\n* make the last 3 tests pass\n\n* fix auto\n\n* nits\n\n* WIP\n\n* WIP\n\n* WIP\n\n* per tensor\n\n* WIP\n\n* WIP\n\n* WIP\n\n* style\n\n* fixup\n\n* WIP\n\n* WIP\n\n* WIP\n\n* hack for now\n\n* add todo\n\n* fixup\n\n* WIP\n\n---------\n\nCo-authored-by: Julien Denize <40604584+juliendenize@users.noreply.github.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\nCo-authored-by: medmekk <mekk.cyber@gmail.com>",
    "sha": "bf3f0ae70d0e902efab4b8517fce88f6697636ce",
    "files": [
        {
            "sha": "bccb67835be8aee62a559d43d1e084d6fa376881",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -598,6 +598,8 @@\n         title: MiniMax\n       - local: model_doc/ministral\n         title: Ministral\n+      - local: model_doc/ministral3\n+        title: Ministral3\n       - local: model_doc/mistral\n         title: Mistral\n       - local: model_doc/mixtral"
        },
        {
            "sha": "93eee195f19163b36ac099a0c38b769836e37896",
            "filename": "docs/source/en/model_doc/ministral3.md",
            "status": "added",
            "additions": 114,
            "deletions": 0,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,114 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# Ministral3\n+\n+## Overview\n+\n+A balanced model in the Ministral 3 family, Ministral 3 8B is a powerful, efficient tiny language model with vision capabilities.\n+\n+This model is the instruct post-trained version, fine-tuned for instruction tasks, making it ideal for chat and instruction based use cases.\n+\n+The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.\n+\n+Key features:\n+- Vision: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n+- Multilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n+- System Prompt: Maintains strong adherence and support for system prompts.\n+- Agentic: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n+- Edge-Optimized: Delivers best-in-class performance at a small scale, deployable anywhere.\n+- Apache 2.0 License: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\n+- Large Context Window: Supports a 256k context window.\n+\n+## Usage examples\n+\n+```py\n+import torch\n+from transformers import Mistral3ForConditionalGeneration, MistralCommonBackend\n+\n+\n+model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n+\n+tokenizer = MistralCommonBackend.from_pretrained(model_id)\n+model = Mistral3ForConditionalGeneration.from_pretrained(\n+    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n+)\n+\n+image_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"text\",\n+                \"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n+            },\n+            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n+        ],\n+    },\n+]\n+\n+tokenized = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n+\n+tokenized[\"input_ids\"] = tokenized[\"input_ids\"].to(device=\"cuda\")\n+tokenized[\"pixel_values\"] = tokenized[\"pixel_values\"].to(dtype=torch.bfloat16, device=\"cuda\")\n+image_sizes = [tokenized[\"pixel_values\"].shape[-2:]]\n+\n+output = model.generate(\n+    **tokenized,\n+    image_sizes=image_sizes,\n+    max_new_tokens=512,\n+)[0]\n+\n+decoded_output = tokenizer.decode(output[len(tokenized[\"input_ids\"][0]):])\n+print(decoded_output)\n+```\n+\n+\n+## Ministral3Config\n+\n+[[autodoc]] Ministral3Config\n+\n+## Ministral3PreTrainedModel\n+\n+[[autodoc]] Ministral3PreTrainedModel\n+    - forward\n+\n+## Ministral3Model\n+\n+[[autodoc]] Ministral3Model\n+    - forward\n+\n+## Ministral3ForCausalLM\n+\n+[[autodoc]] Ministral3ForCausalLM\n+\n+## Ministral3ForSequenceClassification\n+\n+[[autodoc]] Ministral3ForSequenceClassification\n+\n+## Ministral3ForTokenClassification\n+\n+[[autodoc]] Ministral3ForTokenClassification\n+\n+## Ministral3ForQuestionAnswering\n+\n+[[autodoc]] Ministral3ForQuestionAnswering"
        },
        {
            "sha": "f07edc4f4fa185796e818aa1969613ecbcff2b51",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 40,
            "deletions": 19,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -185,14 +185,15 @@ def w8a8_block_fp8_matmul_triton(\n     block_n, block_k = block_size[0], block_size[1]\n \n     assert A.shape[-1] == B.shape[-1]\n+\n     assert A.shape[:-1] == As.shape[:-1] and A.is_contiguous()\n     assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]\n     M = A.numel() // A.shape[-1]\n \n     assert B.ndim == 2 and B.is_contiguous() and Bs.ndim == 2\n     N, K = B.shape\n-    assert triton.cdiv(N, block_n) == Bs.shape[0]\n-    assert triton.cdiv(K, block_k) == Bs.shape[1]\n+    assert triton.cdiv(N, block_n) == Bs.shape[0], f\"{N}, {block_n}, {Bs.shape}\"\n+    assert triton.cdiv(K, block_k) == Bs.shape[1], f\"{K}, {block_k}, {Bs.shape}\"\n \n     C_shape = A.shape[:-1] + (N,)\n     C = A.new_empty(C_shape, dtype=output_dtype)\n@@ -322,21 +323,29 @@ def __init__(\n         self.in_features = in_features\n         self.out_features = out_features\n \n+        if block_size is not None:\n+            self.block_size = block_size\n+        else:\n+            self.block_size = (out_features, in_features)\n+\n         self.weight = torch.nn.Parameter(torch.empty(out_features, in_features, dtype=FP8Linear.dtype, device=device))\n \n         if self.weight.element_size() == 1:\n-            scale_out_features = (out_features + block_size[0] - 1) // block_size[0]\n-            scale_in_features = (in_features + block_size[1] - 1) // block_size[1]\n-            self.weight_scale_inv = nn.Parameter(\n-                torch.empty(scale_out_features, scale_in_features, dtype=torch.float32, device=device)\n-            )\n+            scale_out_features = (out_features + self.block_size[0] - 1) // self.block_size[0]\n+            scale_in_features = (in_features + self.block_size[1] - 1) // self.block_size[1]\n+            if scale_out_features * scale_in_features == 1:\n+                self.weight_scale_inv = nn.Parameter(torch.tensor(1.0, dtype=torch.float32, device=device))\n+            else:\n+                self.weight_scale_inv = nn.Parameter(\n+                    torch.empty(scale_out_features, scale_in_features, dtype=torch.float32, device=device)\n+                )\n         else:\n             self.register_parameter(\"weight_scale_inv\", None)\n-\n-        self.block_size = block_size\n-\n         self.activation_scheme = activation_scheme\n \n+        if self.activation_scheme == \"static\":\n+            self.activation_scale = nn.Parameter(torch.tensor(1.0, dtype=torch.float32, device=device))\n+\n         if bias:\n             self.bias = nn.Parameter(torch.empty(self.out_features))\n         else:\n@@ -356,15 +365,27 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n             device_type = torch.accelerator.current_accelerator().type if is_torch_accelerator_available() else \"cuda\"\n             torch_accelerator_module = getattr(torch, device_type, torch.cuda)\n             with torch_accelerator_module.device(input.device):\n-                qinput, scale = act_quant(input, self.block_size[1])\n-                output = w8a8_block_fp8_matmul_triton(\n-                    qinput,\n-                    weight,\n-                    scale,\n-                    scale_inv,\n-                    self.block_size,\n-                    output_dtype=input.dtype,\n-                )\n+                if self.activation_scheme == \"dynamic\":\n+                    qinput, scale = act_quant(input, self.block_size[1])\n+                elif self.activation_scheme == \"static\":\n+                    scale = self.activation_scale\n+                    qinput = (input / scale).to(torch.float8_e4m3fn)\n+                else:\n+                    raise NotImplementedError(\"Not supported\")\n+                # TODO: fix this later to use the triton kernel\n+                if self.activation_scheme == \"static\":\n+                    output = F.linear(qinput.to(torch.bfloat16), weight.to(torch.bfloat16), None) * scale_inv * scale\n+                    output = output.to(input.dtype)\n+                else:\n+                    output = w8a8_block_fp8_matmul_triton(\n+                        qinput,\n+                        weight,\n+                        scale,\n+                        scale_inv,\n+                        self.block_size,\n+                        output_dtype=input.dtype,\n+                    )\n+\n             # Blocks the CPU until all accelerator operations on the specified device are complete. It is used to ensure that the results of the\n             # preceding operations are ready before proceeding\n             torch_accelerator_module.synchronize()"
        },
        {
            "sha": "f10bf73f73a59aa2b3db043720b6dcc058d1d5e4",
            "filename": "src/transformers/integrations/mistral.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmistral.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -84,19 +84,23 @@ def convert_tekken_tokenizer(tokenizer_file: str):\n \n     # Extract vocab and special tokens\n     vocab = mistral_tokenizer.instruct_tokenizer.tokenizer._tekken_token2id_nospecial\n-    all_special = [\n-        token.get(\"token_str\", str(token))\n-        if isinstance(token, dict)\n-        else (token.value if hasattr(token, \"value\") else str(token))\n-        for token in mistral_tokenizer.instruct_tokenizer.tokenizer._all_special_tokens\n-    ]\n-    specials_tokens = {token: all_special.index(token) for token in all_special}\n+    sorted_tokens = sorted(mistral_tokenizer.instruct_tokenizer.tokenizer._all_special_tokens, key=lambda x: x[\"rank\"])\n+    all_special = [token[\"token_str\"] for token in sorted_tokens]\n+\n+    specials_tokens = {token: idx for idx, token in enumerate(all_special)}\n+\n     specials_tokens.update(vocab)\n     vocab = specials_tokens\n \n+    # TODO(juliendenize): expose this in mistral-common to avoid accessing private attributes\n+    # and improve maintainability\n+    pattern = mistral_tokenizer.instruct_tokenizer.tokenizer._model._pat_str\n+\n     # Convert\n     tokenizer = PreTrainedTokenizerFast(\n-        tokenizer_object=MistralConverter(vocab=vocab, additional_special_tokens=all_special).converted()\n+        tokenizer_object=MistralConverter(\n+            vocab=vocab, additional_special_tokens=all_special, pattern=pattern\n+        ).converted()\n     )\n \n     # Post-process"
        },
        {
            "sha": "87fd2699775a486212d9c33a99747c0d17d0b957",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -222,6 +222,7 @@\n     from .mimi import *\n     from .minimax import *\n     from .ministral import *\n+    from .ministral3 import *\n     from .mistral import *\n     from .mistral3 import *\n     from .mixtral import *"
        },
        {
            "sha": "5ce3d1c4dde31b0a55ffbfede115c4c6c1658769",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -258,6 +258,7 @@\n         (\"mimi\", \"MimiConfig\"),\n         (\"minimax\", \"MiniMaxConfig\"),\n         (\"ministral\", \"MinistralConfig\"),\n+        (\"ministral3\", \"Ministral3Config\"),\n         (\"mistral\", \"MistralConfig\"),\n         (\"mistral3\", \"Mistral3Config\"),\n         (\"mixtral\", \"MixtralConfig\"),\n@@ -703,6 +704,7 @@\n         (\"mimi\", \"Mimi\"),\n         (\"minimax\", \"MiniMax\"),\n         (\"ministral\", \"Ministral\"),\n+        (\"ministral3\", \"Ministral3\"),\n         (\"mistral\", \"Mistral\"),\n         (\"mistral3\", \"Mistral3\"),\n         (\"mixtral\", \"Mixtral\"),"
        },
        {
            "sha": "29ecde2c0f7e17e176d2f66cc8193b7c2d470495",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -258,6 +258,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mimi\", \"MimiModel\"),\n         (\"minimax\", \"MiniMaxModel\"),\n         (\"ministral\", \"MinistralModel\"),\n+        (\"ministral3\", \"Ministral3Model\"),\n         (\"mistral\", \"MistralModel\"),\n         (\"mistral3\", \"Mistral3Model\"),\n         (\"mixtral\", \"MixtralModel\"),\n@@ -700,6 +701,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"megatron-bert\", \"MegatronBertForCausalLM\"),\n         (\"minimax\", \"MiniMaxForCausalLM\"),\n         (\"ministral\", \"MinistralForCausalLM\"),\n+        (\"ministral3\", \"Ministral3ForCausalLM\"),\n         (\"mistral\", \"MistralForCausalLM\"),\n         (\"mixtral\", \"MixtralForCausalLM\"),\n         (\"mllama\", \"MllamaForCausalLM\"),\n@@ -1254,6 +1256,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"megatron-bert\", \"MegatronBertForSequenceClassification\"),\n         (\"minimax\", \"MiniMaxForSequenceClassification\"),\n         (\"ministral\", \"MinistralForSequenceClassification\"),\n+        (\"ministral3\", \"Ministral3ForSequenceClassification\"),\n         (\"mistral\", \"MistralForSequenceClassification\"),\n         (\"mixtral\", \"MixtralForSequenceClassification\"),\n         (\"mobilebert\", \"MobileBertForSequenceClassification\"),\n@@ -1349,6 +1352,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"megatron-bert\", \"MegatronBertForQuestionAnswering\"),\n         (\"minimax\", \"MiniMaxForQuestionAnswering\"),\n         (\"ministral\", \"MinistralForQuestionAnswering\"),\n+        (\"ministral3\", \"Ministral3ForQuestionAnswering\"),\n         (\"mistral\", \"MistralForQuestionAnswering\"),\n         (\"mixtral\", \"MixtralForQuestionAnswering\"),\n         (\"mobilebert\", \"MobileBertForQuestionAnswering\"),\n@@ -1461,6 +1465,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"megatron-bert\", \"MegatronBertForTokenClassification\"),\n         (\"minimax\", \"MiniMaxForTokenClassification\"),\n         (\"ministral\", \"MinistralForTokenClassification\"),\n+        (\"ministral3\", \"Ministral3ForTokenClassification\"),\n         (\"mistral\", \"MistralForTokenClassification\"),\n         (\"mixtral\", \"MixtralForTokenClassification\"),\n         (\"mobilebert\", \"MobileBertForTokenClassification\"),"
        },
        {
            "sha": "31c6a783726bc80419a770af8091c3bdcaa3ddc8",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -212,12 +212,30 @@\n         (\"metaclip_2\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"mgp-str\", \"MgpstrTokenizer\"),\n         (\"minimax\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\n+            \"ministral3\",\n+            (\n+                \"MistralCommonBackend\"\n+                if is_mistral_common_available()\n+                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n+            ),\n+        ),\n         (\n             \"mistral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n             else (\"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n         ),\n+        (\n+            \"mistral3\",\n+            (\n+                \"MistralCommonBackend\"\n+                if is_mistral_common_available()\n+                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n+            ),\n+        ),\n         (\n             \"mixtral\",\n             \"MistralCommonBackend\"\n@@ -384,7 +402,10 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n     for module_name, tokenizer_class in TOKENIZER_MAPPING_NAMES.items():\n         if tokenizer_class == class_name:\n             module_name = model_type_to_module_name(module_name)\n-            if module_name in [\"mistral\", \"mixtral\", \"ministral\"] and class_name == \"MistralCommonBackend\":\n+            if (\n+                module_name in [\"mistral\", \"mistral3\", \"mixtral\", \"ministral\", \"ministral3\", \"pixtral\", \"voxtral\"]\n+                and class_name == \"MistralCommonTokenizer\"\n+            ):\n                 module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n             else:\n                 module = importlib.import_module(f\".{module_name}\", \"transformers.models\")"
        },
        {
            "sha": "59fdb90b750abe0e66e8c3f04d7cab3fec74affc",
            "filename": "src/transformers/models/ministral3/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2F__init__.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 Mistral AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_ministral3 import *\n+    from .modeling_ministral3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c1eb58dcd634bb2bf550a1fd67c31cd03af7c6dc",
            "filename": "src/transformers/models/ministral3/configuration_ministral3.py",
            "status": "added",
            "additions": 201,
            "deletions": 0,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fconfiguration_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fconfiguration_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fconfiguration_ministral3.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,201 @@\n+# coding=utf-8\n+# Copyright 2025 Mistral AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Ministral model configuration\"\"\"\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Ministral3Config(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Ministral3Model`]. It is used to instantiate an\n+    Mistral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the mistralai/Ministral-3-8B-Base-2512, mistralai/Ministral-3-8B-Instruct-2512 or mistralai/Ministral-3-8B-Reasoning-2512.\n+\n+    [mistralai/Ministral-3-8B-Base-2512](https://huggingface.co/mistralai/Ministral-3-8B-Base-2512)\n+    [mistralai/Ministral-3-8B-Instruct-2512](https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512)\n+    [mistralai/Ministral-3-8B-Reasoning-2512](https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512)\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`Optional`, *optional*, defaults to 131072):\n+            Vocabulary size of the Ministral3 model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`Ministral3Model`].\n+        hidden_size (`Optional`, *optional*, defaults to 4096):\n+            Dimensionality of the embeddings and hidden states.\n+        intermediate_size (`Optional`, *optional*, defaults to 14336):\n+            Dimensionality of the intermediate (feed-forward) layer.\n+        num_hidden_layers (`Optional`, *optional*, defaults to 34):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`Optional`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`Optional`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA); if\n+            `num_key_value_heads=1`, the model will use Multi Query Attention (MQA); otherwise GQA is used.\n+        head_dim (`Optional`, *optional*, defaults to 128):\n+            The attention head dimension. If not specified, will default to `hidden_size // num_attention_heads`.\n+        hidden_act (`Optional`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`Optional`, *optional*, defaults to 262144):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`Optional`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`Optional`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`Optional`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`Optional`, *optional*, defaults to 11):\n+            The id of the padding token.\n+        bos_token_id (`Optional`, *optional*, defaults to 1):\n+            The id of the \"beginning-of-sequence\" token.\n+        eos_token_id (`Optional`, *optional*, defaults to 2):\n+            The id of the \"end-of-sequence\" token.\n+        tie_word_embeddings (`Optional`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_parameters (`Union`, *optional*, defaults to `{'type': 'yarn', 'rope_theta': 1000000.0, 'factor': 16.0, 'original_max_position_embeddings': 16384, 'beta_fast': 32.0, 'beta_slow': 1.0, 'mscale_all_dim': 1.0, 'mscale': 1.0, 'llama_4_scaling_beta': 0.1}`):\n+            Dictionary containing the configuration parameters for the RoPE embeddings, including optional Yarn scaling\n+            settings such as `factor`, `original_max_position_embeddings`, `mscale`, and `llama_4_scaling_beta`.\n+        sliding_window (`Optional`, *optional*):\n+            Sliding window attention window size. If `None`, full attention is used.\n+        attention_dropout (`Optional`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Ministral3Config, Ministral3ForCausalLM, Mistral3Config, Mistral3ForConditionalGeneration, PixtralVisionConfig\n+\n+    >>> # Initializing a Pixtral-vision config\n+    >>> vision_config = PixtralVisionConfig()\n+\n+    >>> # Initializing a Ministral3 config\n+    >>> text_config = Ministral3Config()\n+\n+    >>> # Initializing a Mistral3 configuration\n+    >>> configuration = Mistral3Config(vision_config, text_config)\n+\n+    >>> # Initializing a model from the Ministral3 configuration\n+    >>> text_model = Ministral3ForCausalLM(text_config)\n+\n+    >>> # Initializing a model from the Mistral3 configuration\n+    >>> model = Mistral3ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ministral3\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `MistralModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 131072,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 34,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = 128,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 262144,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 11,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        sliding_window: Optional[int] = None,\n+        attention_dropout: Optional[float] = 0.0,\n+        **kwargs,\n+    ):\n+        if rope_parameters is None:\n+            rope_parameters = {\n+                \"type\": \"yarn\",\n+                \"rope_theta\": 1000000.0,\n+                \"factor\": 16.0,\n+                \"original_max_position_embeddings\": 16384,\n+                \"max_position_embeddings\": max_position_embeddings,\n+                \"beta_fast\": 32.0,\n+                \"beta_slow\": 1.0,\n+                \"mscale_all_dim\": 1.0,\n+                \"mscale\": 1.0,\n+                \"llama_4_scaling_beta\": 0.1,\n+            }\n+\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.sliding_window = sliding_window\n+        self.head_dim = head_dim if head_dim is not None else hidden_size // num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_dropout = attention_dropout\n+\n+        if \"layer_types\" in kwargs:\n+            logger.warning_once(\n+                \"Detected Mistral model with layer_types. Consider using AutoModel or Ministral classes instead to enable alternating attention compatibility.\"\n+            )\n+\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"llama_4_scaling_beta\"},\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Ministral3Config\"]"
        },
        {
            "sha": "29b267c888e2c42c8749631bb4f798a292e454ac",
            "filename": "src/transformers/models/ministral3/convert_ministral3_weights_to_hf.py",
            "status": "added",
            "additions": 321,
            "deletions": 0,
            "changes": 321,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,321 @@\n+# Copyright 2025 Mistral AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import json\n+import os\n+import re\n+\n+import torch\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    GenerationConfig,\n+    Ministral3Config,\n+    Ministral3ForCausalLM,\n+    Mistral3Config,\n+    Mistral3ForConditionalGeneration,\n+    PixtralImageProcessorFast,\n+    PixtralProcessor,\n+    PixtralVisionConfig,\n+)\n+from transformers.integrations.finegrained_fp8 import replace_with_fp8_linear\n+from transformers.integrations.mistral import convert_tekken_tokenizer\n+from transformers.quantizers.auto import AutoQuantizationConfig\n+\n+\n+# fmt: off\n+STATE_DICT_MAPPING = {\n+    # Text model keys\n+    r\"^output.weight\":                            r\"lm_head.weight\",\n+    r\"^norm.weight\":                              r\"model.language_model.norm.weight\",\n+    r\"^tok_embeddings.weight\":                    r\"model.language_model.embed_tokens.weight\",\n+    r\"^layers.(\\d+).attention_norm.weight\":       r\"model.language_model.layers.\\1.input_layernorm.weight\",\n+    r\"^layers.(\\d+).ffn_norm.weight\":             r\"model.language_model.layers.\\1.post_attention_layernorm.weight\",\n+    r\"^layers.(\\d+).attention.w(q|k|v|o).weight\": r\"model.language_model.layers.\\1.self_attn.\\2_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w1.weight\":      r\"model.language_model.layers.\\1.mlp.gate_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w2.weight\":      r\"model.language_model.layers.\\1.mlp.down_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w3.weight\":      r\"model.language_model.layers.\\1.mlp.up_proj.weight\",\n+    r\"^layers.(\\d+).attention.w(q|k|v|o).qscale_act\": r\"model.language_model.layers.\\1.self_attn.\\2_proj.activation_scale\",\n+    r\"^layers.(\\d+).feed_forward.w1.qscale_act\":      r\"model.language_model.layers.\\1.mlp.gate_proj.activation_scale\",\n+    r\"^layers.(\\d+).feed_forward.w2.qscale_act\":      r\"model.language_model.layers.\\1.mlp.down_proj.activation_scale\",\n+    r\"^layers.(\\d+).feed_forward.w3.qscale_act\":      r\"model.language_model.layers.\\1.mlp.up_proj.activation_scale\",\n+    r\"^layers.(\\d+).attention.w(q|k|v|o).qscale_weight\": r\"model.language_model.layers.\\1.self_attn.\\2_proj.weight_scale_inv\",\n+    r\"^layers.(\\d+).feed_forward.w1.qscale_weight\":      r\"model.language_model.layers.\\1.mlp.gate_proj.weight_scale_inv\",\n+    r\"^layers.(\\d+).feed_forward.w2.qscale_weight\":      r\"model.language_model.layers.\\1.mlp.down_proj.weight_scale_inv\",\n+    r\"^layers.(\\d+).feed_forward.w3.qscale_weight\":      r\"model.language_model.layers.\\1.mlp.up_proj.weight_scale_inv\",\n+\n+    # Vision model keys\n+    r\"vision_encoder.transformer.layers.(\\d+).attention_norm.weight\": r\"model.vision_tower.transformer.layers.\\1.attention_norm.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).ffn_norm.weight\": r\"model.vision_tower.transformer.layers.\\1.ffn_norm.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).attention.w(q|k|v|o).weight\": r\"model.vision_tower.transformer.layers.\\1.attention.\\2_proj.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w1.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.gate_proj.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w2.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.down_proj.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w3.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.up_proj.weight\",\n+    r\"^vision_language_adapter.w_in\": r\"model.multi_modal_projector.linear_1\",\n+    r\"^vision_language_adapter.w_out\": r\"model.multi_modal_projector.linear_2\",\n+    r\"^vision_encoder.ln_pre.weight\": r\"model.vision_tower.ln_pre.weight\",\n+    r\"^vision_encoder.patch_conv.weight\": r\"model.vision_tower.patch_conv.weight\",\n+    r\"^patch_merger.merging_layer.weight\": r\"model.multi_modal_projector.patch_merger.merging_layer.weight\",\n+    r\"^pre_mm_projector_norm.weight\": r\"model.multi_modal_projector.norm.weight\",\n+}\n+# fmt: on\n+\n+\n+def map_old_key_to_new(old_key):\n+    \"\"\"Map of a key of the original state dict to the equivalent key in HF format\"\"\"\n+    for pattern, replacement in STATE_DICT_MAPPING.items():\n+        new_key, n_replace = re.subn(pattern, replacement, old_key)\n+        # Early exit of the loop\n+        if n_replace > 0:\n+            return new_key\n+\n+    raise ValueError(f\"Key: {old_key} could not be mapped (check the mapping).\")\n+\n+\n+def read_json(path):\n+    with open(path, \"r\") as f:\n+        return json.load(f)\n+\n+\n+def permute_for_rope(tensor, n_heads, dim1, dim2):\n+    \"\"\"Permute the weights for the ROPE formulation.\"\"\"\n+    tensor = tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+    tensor = tensor.transpose(1, 2)\n+    tensor = tensor.reshape(dim1, dim2)\n+    return tensor\n+\n+\n+def convert_state_dict(original_state_dict: dict, config: Mistral3Config):\n+    \"\"\"Convert a state dict file, when a single `nn.Module` is never sharded in different files (usual case).\"\"\"\n+    new_dict = {}\n+\n+    for old_key, tensor in original_state_dict.items():\n+        if \"fake_quantizer\" in old_key:\n+            continue\n+\n+        new_key = map_old_key_to_new(old_key)\n+\n+        if \"vision\" in old_key:\n+            num_attention_heads = config.vision_config.num_attention_heads\n+            num_key_value_heads = num_attention_heads\n+            hidden_size = config.vision_config.hidden_size\n+            head_dim = config.vision_config.head_dim\n+            key_value_dim = head_dim * num_attention_heads\n+            query_dim = head_dim * num_attention_heads\n+        else:\n+            num_attention_heads = config.text_config.num_attention_heads\n+            hidden_size = config.text_config.hidden_size\n+            head_dim = config.text_config.head_dim\n+            num_key_value_heads = config.text_config.num_key_value_heads\n+            key_value_dim = head_dim * num_key_value_heads\n+            query_dim = head_dim * num_attention_heads\n+\n+        if \"q_proj\" in new_key and new_key.endswith(\"weight\"):\n+            tensor = permute_for_rope(tensor, num_attention_heads, query_dim, hidden_size)\n+        elif \"k_proj\" in new_key and new_key.endswith(\"weight\"):\n+            tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, hidden_size)\n+\n+        new_dict[new_key] = tensor\n+    return new_dict\n+\n+\n+def convert_config(original_config: dict, max_position_embeddings: int = 262144):\n+    original_vision_config = original_config.pop(\"vision_encoder\", None)\n+    original_text_config = original_config\n+\n+    # Text config\n+    text_key_mapping = {\n+        \"hidden_size\": \"dim\",\n+        \"num_hidden_layers\": \"n_layers\",\n+        \"intermediate_size\": \"hidden_dim\",\n+        \"num_attention_heads\": \"n_heads\",\n+        \"num_key_value_heads\": \"n_kv_heads\",\n+        \"rms_norm_eps\": \"norm_eps\",\n+    }\n+    similar_text_keys_to_keep = [\n+        \"head_dim\",\n+        \"vocab_size\",\n+    ]\n+\n+    new_text_config_kwargs = {k: original_text_config[v] for k, v in text_key_mapping.items()}\n+    new_text_config_kwargs.update({k: v for k, v in original_text_config.items() if k in similar_text_keys_to_keep})\n+    tie_word_embeddings = original_text_config.get(\"tied_embeddings\", False)\n+    new_text_config_kwargs[\"tie_word_embeddings\"] = tie_word_embeddings\n+    new_text_config_kwargs[\"rope_parameters\"] = {\n+        \"type\": \"yarn\",\n+        \"rope_theta\": original_config.get(\"rope_theta\", 1000000.0),\n+        \"factor\": float(original_config[\"yarn\"][\"factor\"]),\n+        \"original_max_position_embeddings\": original_config[\"yarn\"][\"original_max_position_embeddings\"],\n+        \"beta_fast\": float(original_config[\"yarn\"][\"beta\"]),\n+        \"beta_slow\": float(original_config[\"yarn\"][\"alpha\"]),\n+        \"mscale_all_dim\": 1.0,\n+        \"mscale\": 1.0,\n+        \"llama_4_scaling_beta\": original_config[\"llama_4_scaling\"][\"beta\"],\n+    }\n+\n+    # These are not always defined depending on `params.json`\n+    new_text_config_kwargs[\"sliding_window\"] = original_text_config.get(\"sliding_window\", None)\n+    new_text_config_kwargs[\"max_position_embeddings\"] = original_text_config.get(\n+        \"max_position_embeddings\", original_text_config.get(\"max_seq_len\", max_position_embeddings)\n+    )\n+    # This may sometimes be a string in `params.json`\n+    if new_text_config_kwargs[\"sliding_window\"] is not None:\n+        new_text_config_kwargs[\"sliding_window\"] = int(new_text_config_kwargs[\"sliding_window\"])\n+\n+    new_text_config = Ministral3Config(**new_text_config_kwargs)\n+\n+    # No vision\n+    if original_vision_config is None:\n+        return new_text_config\n+\n+    # Vision config\n+    new_vision_config = original_vision_config\n+    adapter_bias = new_vision_config.pop(\"adapter_bias\", False)\n+    _ = new_vision_config.pop(\"mm_projector_id\", None)\n+    _ = new_vision_config.pop(\"add_pre_mm_projector_layer_norm\", None)\n+    spatial_merge_size = new_vision_config.pop(\"spatial_merge_size\")\n+    image_token_id = new_vision_config.pop(\"image_token_id\", 10)\n+    _ = new_vision_config.pop(\"image_break_token_id\", 12)\n+    _ = new_vision_config.pop(\"image_end_token_id\", 13)\n+    _ = new_vision_config.pop(\"max_image_size\")\n+    new_vision_config = PixtralVisionConfig(hidden_act=\"silu\", **new_vision_config)\n+\n+    kwargs = {}\n+    if original_config.get(\"quantization\", {}).get(\"qformat_weight\") == \"fp8_e4m3\":\n+        assert original_config[\"quantization\"][\"qscheme_act\"] == \"TENSOR\"\n+        quantization_config = {\n+            \"activation_scheme\": \"static\",\n+            \"modules_to_not_convert\": [\"model.vision_tower\", \"model.multi_modal_projector\"],\n+            \"quant_method\": \"fp8\",\n+            \"weight_block_size\": None,\n+        }\n+        kwargs[\"quantization_config\"] = AutoQuantizationConfig.from_dict(quantization_config)\n+\n+    new_config = Mistral3Config(\n+        vision_config=new_vision_config,\n+        text_config=new_text_config,\n+        multimodal_projector_bias=adapter_bias,\n+        image_token_id=image_token_id,\n+        spatial_merge_size=spatial_merge_size,\n+        vision_feature_layer=-1,\n+        **kwargs,\n+    )\n+    return new_config\n+\n+\n+def convert_and_write_model(input_dir: str, output_dir: str, max_position_embeddings: int):\n+    \"\"\"Convert the model and save it (this implicitly save the config as well).\"\"\"\n+    params = read_json(os.path.join(input_dir, \"params.json\"))\n+\n+    config = convert_config(params, max_position_embeddings)\n+\n+    full_state_dict = {}\n+    # The model may be split between different files, but a single nn.Module is always fully present in a single file\n+    shards = [file for file in os.listdir(input_dir) if file.endswith(\".safetensors\")]\n+    for shard_file in shards:\n+        original_state_dict = load_file(os.path.join(input_dir, shard_file))\n+        new_dict = convert_state_dict(original_state_dict, config)\n+        full_state_dict.update(new_dict)\n+\n+    if config.text_config.tie_word_embeddings:\n+        full_state_dict[\"lm_head.weight\"] = full_state_dict[\"model.language_model.embed_tokens.weight\"]\n+\n+    # Load weights into model and resave them\n+    with torch.device(\"meta\"):\n+        if isinstance(config, Mistral3Config):\n+            model = Mistral3ForConditionalGeneration(config)\n+        elif isinstance(config, Ministral3Config):\n+            model = Ministral3ForCausalLM(config)\n+        else:\n+            raise ValueError(f\"Unknown config type {type(config)}.\")\n+\n+        # let's swap nn.Linear to FP8 Linear before loading\n+        if hasattr(model.config, \"quantization_config\"):\n+            model = replace_with_fp8_linear(\n+                model, model.config.quantization_config.modules_to_not_convert, model.config.quantization_config\n+            )\n+\n+    model.load_state_dict(full_state_dict, strict=True, assign=True)\n+    model.save_pretrained(output_dir)\n+    return config\n+\n+\n+def convert_and_write_processor_and_tokenizer(\n+    input_dir: str, output_dir: str, model_config: Mistral3Config | Ministral3ForCausalLM\n+):\n+    \"\"\"Convert the tokenizer and save it.\"\"\"\n+    from mistral_common.tokens.tokenizers.tekken import Tekkenizer\n+\n+    tokenizer_file = os.path.join(input_dir, \"tekken.json\")\n+    tokenizer = convert_tekken_tokenizer(tokenizer_file)\n+    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n+\n+    # No vision\n+    if isinstance(model_config, Ministral3Config):\n+        tokenizer.save_pretrained(output_dir)\n+        return\n+\n+    tekkenizer = Tekkenizer.from_file(tokenizer_file)\n+    config = read_json(os.path.join(input_dir, \"params.json\"))\n+    patch_size = config[\"vision_encoder\"][\"patch_size\"]\n+    spatial_merge_size = config[\"vision_encoder\"][\"spatial_merge_size\"]\n+    max_image_size = config[\"vision_encoder\"][\"max_image_size\"]\n+    image_processor = PixtralImageProcessorFast(patch_size=patch_size, size={\"longest_edge\": max_image_size})\n+\n+    processor = PixtralProcessor(\n+        tokenizer=tokenizer,\n+        image_processor=image_processor,\n+        image_token=\"[IMG]\",\n+        patch_size=patch_size,\n+        spatial_merge_size=spatial_merge_size,\n+    )\n+\n+    # Finally save it\n+    processor.save_pretrained(output_dir)\n+\n+    generation_config = GenerationConfig(\n+        eos_token_id=tekkenizer.eos_id,\n+        bos_token_id=tekkenizer.bos_id,\n+        pad_token_id=tekkenizer.pad_id,\n+        max_length=model_config.text_config.max_position_embeddings,\n+    )\n+\n+    generation_config.save_pretrained(output_dir)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"input_dir\",\n+        help=\"Location of Mistral weights, which contains tokenizer.model and model folders\",\n+    )\n+    parser.add_argument(\n+        \"output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--max_position_embeddings\",\n+        type=int,\n+        default=262144,\n+        help=\"`max_position_embeddings` field in the config. This needs to be manually passed (not present anywhere otherwise).\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    config = convert_and_write_model(args.input_dir, args.output_dir, args.max_position_embeddings)\n+    convert_and_write_processor_and_tokenizer(args.input_dir, args.output_dir, config)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "2d9a8de2428395c5aac07fab01d7db39dcd45826",
            "filename": "src/transformers/models/ministral3/modeling_ministral3.py",
            "status": "added",
            "additions": 520,
            "deletions": 0,
            "changes": 520,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,520 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/ministral3/modular_ministral3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ministral3.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers.utils.generic import check_model_inputs\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from .configuration_ministral3 import Ministral3Config\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def _get_llama_4_attn_scale(positions_ids: torch.Tensor, beta: float, max_position_embeddings: int) -> torch.Tensor:\n+    scaling = 1 + beta * torch.log(1 + torch.floor(positions_ids / max_position_embeddings))\n+    return scaling.unsqueeze(-1)\n+\n+\n+class Ministral3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Ministral3Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = query_states * _get_llama_4_attn_scale(\n+            cache_position,\n+            self.config.rope_parameters.get(\"llama_4_scaling_beta\"),\n+            self.config.rope_parameters.get(\"original_max_position_embeddings\"),\n+        ).to(query_states.dtype)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Ministral3MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Ministral3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Ministral3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Ministral3DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Ministral3Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = Ministral3Attention(config=config, layer_idx=layer_idx)\n+        self.mlp = Ministral3MLP(config)\n+        self.input_layernorm = Ministral3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Ministral3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Ministral3PreTrainedModel(PreTrainedModel):\n+    config: Ministral3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Ministral3DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Ministral3DecoderLayer,\n+        \"attentions\": Ministral3Attention,\n+    }\n+\n+\n+class Ministral3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Ministral3Config, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Ministral3Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring\n+class Ministral3Model(Ministral3PreTrainedModel):\n+    def __init__(self, config: Ministral3Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Ministral3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Ministral3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Ministral3RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+@auto_docstring\n+class Ministral3ForCausalLM(Ministral3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Ministral3Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Ministral3ForCausalLM\n+\n+        >>> model = Ministral3ForCausalLM.from_pretrained(\"meta-ministral3/Ministral3-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-ministral3/Ministral3-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class Ministral3ForTokenClassification(GenericForTokenClassification, Ministral3PreTrainedModel):\n+    pass\n+\n+\n+class Ministral3ForSequenceClassification(GenericForSequenceClassification, Ministral3PreTrainedModel):\n+    pass\n+\n+\n+class Ministral3ForQuestionAnswering(GenericForQuestionAnswering, Ministral3PreTrainedModel):\n+    pass\n+\n+\n+__all__ = [\n+    \"Ministral3ForCausalLM\",\n+    \"Ministral3ForQuestionAnswering\",\n+    \"Ministral3Model\",\n+    \"Ministral3PreTrainedModel\",\n+    \"Ministral3ForSequenceClassification\",\n+    \"Ministral3ForTokenClassification\",\n+]"
        },
        {
            "sha": "fe35e378cd1ca168dde99be6dc1306741d289592",
            "filename": "src/transformers/models/ministral3/modular_ministral3.py",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodular_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodular_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodular_ministral3.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,124 @@\n+from collections.abc import Callable\n+from typing import Optional\n+\n+import torch\n+\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, logging\n+from ..mistral.modeling_mistral import (\n+    MistralAttention,\n+    MistralDecoderLayer,\n+    MistralForCausalLM,\n+    MistralModel,\n+    MistralPreTrainedModel,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _get_llama_4_attn_scale(positions_ids: torch.Tensor, beta: float, max_position_embeddings: int) -> torch.Tensor:\n+    scaling = 1 + beta * torch.log(1 + torch.floor(positions_ids / max_position_embeddings))\n+    return scaling.unsqueeze(-1)\n+\n+\n+class Ministral3Attention(MistralAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = query_states * _get_llama_4_attn_scale(\n+            cache_position,\n+            self.config.rope_parameters.get(\"llama_4_scaling_beta\"),\n+            self.config.rope_parameters.get(\"original_max_position_embeddings\"),\n+        ).to(query_states.dtype)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Ministral3DecoderLayer(MistralDecoderLayer):\n+    pass\n+\n+\n+@auto_docstring\n+class Ministral3PreTrainedModel(MistralPreTrainedModel):\n+    pass\n+\n+\n+@auto_docstring\n+class Ministral3Model(MistralModel):\n+    pass\n+\n+\n+@auto_docstring\n+class Ministral3ForCausalLM(MistralForCausalLM):\n+    pass\n+\n+\n+class Ministral3ForTokenClassification(GenericForTokenClassification, Ministral3PreTrainedModel):\n+    pass\n+\n+\n+class Ministral3ForSequenceClassification(GenericForSequenceClassification, Ministral3PreTrainedModel):\n+    pass\n+\n+\n+class Ministral3ForQuestionAnswering(GenericForQuestionAnswering, Ministral3PreTrainedModel):\n+    pass\n+\n+\n+__all__ = [\n+    \"Ministral3ForCausalLM\",\n+    \"Ministral3ForQuestionAnswering\",\n+    \"Ministral3Model\",\n+    \"Ministral3PreTrainedModel\",\n+    \"Ministral3ForSequenceClassification\",\n+    \"Ministral3ForTokenClassification\",\n+]"
        },
        {
            "sha": "b63531a6c1d5d0fa49cfef87e39152ff5b9533e5",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -2007,11 +2007,11 @@ def post_init(self):\n         Safety checker that arguments are correct\n         \"\"\"\n         self.activation_scheme = self.activation_scheme.lower()\n-        if self.activation_scheme != \"dynamic\":\n+        if self.activation_scheme not in [\"dynamic\", \"static\"]:\n             raise ValueError(f\"Activation scheme {self.activation_scheme} not supported\")\n-        if len(self.weight_block_size) != 2:\n+        if self.weight_block_size is not None and len(self.weight_block_size) != 2:\n             raise ValueError(\"weight_block_size must be a tuple of two integers\")\n-        if self.weight_block_size[0] <= 0 or self.weight_block_size[1] <= 0:\n+        if self.weight_block_size is not None and (self.weight_block_size[0] <= 0 or self.weight_block_size[1] <= 0):\n             raise ValueError(\"weight_block_size must be a tuple of two positive integers\")\n \n     def get_loading_attributes(self):"
        },
        {
            "sha": "4d6a450d79760e1eacde8d258a02f1c435a741ef",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -439,7 +439,13 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n         _set_config_rope_params(\n-            config, {\"rope_type\": \"default\", \"rope_theta\": 10_000.0, \"partial_rotary_factor\": partial_rotary_factor}\n+            config,\n+            {\n+                \"rope_type\": \"default\",\n+                \"rope_theta\": 10_000.0,\n+                \"partial_rotary_factor\": partial_rotary_factor,\n+                \"original_max_position_embeddings\": 16384,\n+            },\n         )\n         original_model = self.model_tester_class.base_model_class(config)\n         original_model.to(torch_device)\n@@ -649,7 +655,9 @@ def _config_supports_rope_scaling(config: PreTrainedConfig) -> bool:\n \n def _set_config_rope_params(config: PreTrainedConfig, rope_params: dict) -> bool:\n     \"\"\"Recursively sets RoPE parameters on configs and subconfigs, by duplicating the same RoPE values.\"\"\"\n-    config.rope_parameters = rope_params\n+    config.rope_parameters = getattr(config, \"rope_parameters\", {}) or {}\n+    config.rope_parameters.update(rope_params)\n+\n     if any(name in config.__class__.__name__.lower() for name in [\"gemma3\", \"modernbert\"]):\n         config.rope_parameters = {layer_type: config.rope_parameters.copy() for layer_type in config.layer_types}\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/ministral3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/tests%2Fmodels%2Fministral3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/tests%2Fmodels%2Fministral3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral3%2F__init__.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce"
        },
        {
            "sha": "192f53f64d601064fd0c84b1583b53d627d48523",
            "filename": "tests/models/ministral3/test_modeling_ministral3.py",
            "status": "added",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf3f0ae70d0e902efab4b8517fce88f6697636ce/tests%2Fmodels%2Fministral3%2Ftest_modeling_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf3f0ae70d0e902efab4b8517fce88f6697636ce/tests%2Fmodels%2Fministral3%2Ftest_modeling_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral3%2Ftest_modeling_ministral3.py?ref=bf3f0ae70d0e902efab4b8517fce88f6697636ce",
            "patch": "@@ -0,0 +1,118 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Ministral3 model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import pytest\n+\n+from transformers import AutoTokenizer, Mistral3ForConditionalGeneration, is_torch_available\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    cleanup,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Ministral3Model,\n+    )\n+\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class Ministral3ModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        base_model_class = Ministral3Model\n+\n+\n+@require_torch\n+class Ministral3ModelTest(CausalLMModelTest, unittest.TestCase):\n+    _is_stateful = True\n+    model_split_percents = [0.5, 0.6]\n+    model_tester_class = Ministral3ModelTester\n+\n+    # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        return True\n+\n+    @require_flash_attn\n+    @require_torch_accelerator\n+    @pytest.mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        self.skipTest(reason=\"Ministral3 flash attention does not support right padding\")\n+\n+\n+@require_torch\n+class Ministral3IntegrationTest(unittest.TestCase):\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_3b_logits(self):\n+        input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n+        model = Mistral3ForConditionalGeneration.from_pretrained(\n+            \"mistralai/Ministral-3-3B-Instruct-2512\", device_map=\"auto\"\n+        )\n+        input_ids = torch.tensor([input_ids]).to(model.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor([[-1.1503, -1.9935, -0.4457, -1.0717, -1.9182, -1.1431, -0.9697, -1.7098]])\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @slow\n+    def test_model_3b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = (\n+            \"My favourite condiment is 100% pure olive oil. It's a staple in my kitchen and I use it in\"\n+        )\n+        prompt = \"My favourite condiment is \"\n+        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Ministral-3-3B-Instruct-2512\")\n+        model = Mistral3ForConditionalGeneration.from_pretrained(\n+            \"mistralai/Ministral-3-3B-Instruct-2512\", device_map=\"auto\"\n+        )\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()"
        }
    ],
    "stats": {
        "total": 1555,
        "additions": 1522,
        "deletions": 33
    }
}