{
    "author": "ArthurZucker",
    "message": "run model debugging with forward arg (#39905)\n\n* run model debugging a lot simpler\n\n* fixup\n\n* Update src/transformers/utils/generic.py\n\n* fixup\n\n* mode syle?\n\n* guard a bit",
    "sha": "dedcbd6e3d52b9e49031df7ade1eafe75fca2f60",
    "files": [
        {
            "sha": "8a3cc7a19ac335a8ddfac061871dc77d42d9a8f3",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/dedcbd6e3d52b9e49031df7ade1eafe75fca2f60/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dedcbd6e3d52b9e49031df7ade1eafe75fca2f60/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=dedcbd6e3d52b9e49031df7ade1eafe75fca2f60",
            "patch": "@@ -21,26 +21,23 @@\n from io import StringIO\n from typing import Optional\n \n-from safetensors.torch import save_file\n-\n-from transformers.utils.import_utils import requires\n-\n-from .utils import is_torch_available\n+from .utils.import_utils import is_torch_available, requires\n \n \n if is_torch_available():\n     import torch\n     import torch.distributed.tensor\n+    from safetensors.torch import save_file\n \n-\n+    # Note to code inspectors: this toolbox is intended for people who add models to `transformers`.\n+    _torch_distributed_available = torch.distributed.is_available()\n+else:\n+    _torch_distributed_available = False\n from .utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n-# Note to code inspectors: this toolbox is intended for people who add models to `transformers`.\n-_torch_distributed_available = torch.distributed.is_available()\n-\n \n def _is_rank_zero():\n     \"\"\"Return True if rank=0 or we aren't running distributed.\"\"\""
        },
        {
            "sha": "d778532e33e9ffc8c867d26c2b50e3d893996b94",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/dedcbd6e3d52b9e49031df7ade1eafe75fca2f60/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dedcbd6e3d52b9e49031df7ade1eafe75fca2f60/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=dedcbd6e3d52b9e49031df7ade1eafe75fca2f60",
            "patch": "@@ -52,6 +52,8 @@\n     # required for @can_return_tuple decorator to work with torchdynamo\n     import torch  # noqa: F401\n \n+    from ..model_debugging_utils import model_addition_debugger_context\n+\n \n class cached_property(property):\n     \"\"\"\n@@ -1032,7 +1034,13 @@ def make_capture_wrapper(module, orig_forward, key, index):\n             def wrapped_forward(*args, **kwargs):\n                 if key == \"hidden_states\" and len(collected_outputs[key]) == 0:\n                     collected_outputs[key] += (args[0],)\n-                output = orig_forward(*args, **kwargs)\n+                if kwargs.get(\"debug_io\", False):\n+                    with model_addition_debugger_context(\n+                        module, kwargs.get(\"debug_io_dir\", \"~/model_debug\"), kwargs.get(\"prune_layers\")\n+                    ):\n+                        output = orig_forward(*args, **kwargs)\n+                else:\n+                    output = orig_forward(*args, **kwargs)\n                 if not isinstance(output, tuple):\n                     collected_outputs[key] += (output,)\n                 elif output[index] is not None:"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 15,
        "deletions": 10
    }
}