{
    "author": "ydshieh",
    "message": "Fix `qwen_2_5 omni` (#38658)\n\n* fix\n\n* fix\n\n* break style\n\n* break style\n\n* Apply style fixes\n\n* break style\n\n* Apply style fixes\n\n* fix modular\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "d4e7aa5526309b748e5a79a59991e147d0a75f93",
    "files": [
        {
            "sha": "313d69e0edbe2e62f808d40b8baf1f87bd26ebc1",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/d4e7aa5526309b748e5a79a59991e147d0a75f93/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d4e7aa5526309b748e5a79a59991e147d0a75f93/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=d4e7aa5526309b748e5a79a59991e147d0a75f93",
            "patch": "@@ -4029,70 +4029,70 @@ def generate(\n             return thinker_result\n \n         # 2. Generate speech tokens from talker module\n-        embeds_to_talker = thinker_result.hidden_states[0][0].clone().to(self.talker.device)\n+        embeds_to_talker = thinker_result.hidden_states[0][0].clone().to(input_ids.device)\n         if thinker_kwargs.get(\"input_features\", None) is not None:\n             audio_ids_mask = input_ids == self.config.thinker_config.audio_token_index\n-            audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             audio_mask_tensor = torch.zeros(\n                 [audio_ids_mask.sum(), embeds_to_talker.shape[-1]],\n                 dtype=embeds_to_talker.dtype,\n-                device=self.talker.device,\n+                device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(audio_mask, audio_mask_tensor)\n         if thinker_kwargs.get(\"pixel_values\", None) is not None:\n             image_ids_mask = input_ids == self.config.thinker_config.image_token_index\n-            image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             image_mask_tensor = torch.zeros(\n                 [image_ids_mask.sum(), embeds_to_talker.shape[-1]],\n                 dtype=embeds_to_talker.dtype,\n-                device=self.talker.device,\n+                device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(image_mask, image_mask_tensor)\n         if thinker_kwargs.get(\"pixel_values_videos\", None) is not None:\n             video_ids_mask = input_ids == self.config.thinker_config.video_token_index\n-            video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             video_mask_tensor = torch.zeros(\n                 [video_ids_mask.sum(), embeds_to_talker.shape[-1]],\n                 dtype=embeds_to_talker.dtype,\n-                device=self.talker.device,\n+                device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(video_mask, video_mask_tensor)\n \n         processed_thinker_hidden = (\n             (embeds_to_talker,) + thinker_result.hidden_states[0][1:],\n         ) + thinker_result.hidden_states[1:]\n-        thinker_generate_ids = thinker_result.sequences[:, input_ids.size(1) :].to(self.talker.device)\n+        thinker_generate_ids = thinker_result.sequences[:, input_ids.size(1) :].to(input_ids.device)\n         thinker_token_embeds = [\n-            token_hidden_states[0].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n+            token_hidden_states[0].to(input_ids.device) for token_hidden_states in processed_thinker_hidden\n         ]\n         thinker_hidden_states = [\n-            token_hidden_states[-1].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n+            token_hidden_states[-1].to(input_ids.device) for token_hidden_states in processed_thinker_hidden\n         ]\n \n         talker_text_bos_token = speaker_params[\"bos_token\"]\n         talker_input_text_ids = torch.cat(\n             [\n-                input_ids.to(self.talker.device),\n-                torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=self.talker.device),\n+                input_ids,\n+                torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=input_ids.device),\n                 thinker_generate_ids[:, :1],\n             ],\n             dim=-1,\n         )\n \n         talker_input_ids = torch.cat(\n             [\n-                torch.full_like(input_ids, fill_value=self.talker.codec_mask_token, device=self.talker.device),\n-                torch.tensor([[self.talker.codec_pad_token]], dtype=torch.long, device=self.talker.device),\n-                torch.tensor([[self.talker.codec_bos_token]], dtype=torch.long, device=self.talker.device),\n+                torch.full_like(input_ids, fill_value=self.talker.codec_mask_token),\n+                torch.tensor([[self.talker.codec_pad_token]], dtype=torch.long, device=input_ids.device),\n+                torch.tensor([[self.talker.codec_bos_token]], dtype=torch.long, device=input_ids.device),\n             ],\n             dim=1,\n         )\n \n         thinker_embed_tokens = self.thinker.get_input_embeddings()\n         thinker_reply_part = torch.cat(thinker_hidden_states[1:], dim=1) + torch.cat(thinker_token_embeds[1:], dim=1)\n         talker_inputs_embeds = thinker_hidden_states[0] + thinker_token_embeds[0]\n-        talker_text_bos_token = torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=self.thinker.device)\n-        talker_text_bos_embed = thinker_embed_tokens(talker_text_bos_token).to(self.talker.device)\n+        talker_text_bos_token = torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=input_ids.device)\n+        talker_text_bos_embed = thinker_embed_tokens(talker_text_bos_token).to(input_ids.device)\n         talker_inputs_embeds = torch.cat(\n             [\n                 talker_inputs_embeds,\n@@ -4103,12 +4103,12 @@ def generate(\n         )\n \n         eos_embedding = thinker_embed_tokens(\n-            torch.tensor([[self.talker.text_eos_token]], dtype=torch.long, device=self.thinker.device)\n-        ).to(self.talker.device)\n+            torch.tensor([[self.talker.text_eos_token]], dtype=torch.long, device=input_ids.device)\n+        )\n \n         pad_embedding = thinker_embed_tokens(\n-            torch.tensor([[self.talker.text_pad_token]], dtype=torch.long, device=self.thinker.device)\n-        ).to(self.talker.device)\n+            torch.tensor([[self.talker.text_pad_token]], dtype=torch.long, device=input_ids.device)\n+        )\n \n         thinker_reply_part = torch.cat(\n             [\n@@ -4123,7 +4123,7 @@ def generate(\n         if \"attention_mask\" in kwargs:\n             talker_attention_mask = torch.cat(\n                 [kwargs[\"attention_mask\"], kwargs[\"attention_mask\"].new_ones((1, 2))], dim=1\n-            ).to(self.talker.device)\n+            ).to(input_ids.device)\n \n         talker_result = self.talker.generate(\n             input_ids=talker_input_ids,\n@@ -4132,7 +4132,7 @@ def generate(\n             inputs_embeds=talker_inputs_embeds,\n             attention_mask=talker_attention_mask,\n             suppress_tokens=[self.talker.codec_bos_token],\n-            **{k: (v.to(self.talker.device) if torch.is_tensor(v) else v) for k, v in talker_kwargs.items()},\n+            **{k: (v.to(input_ids.device) if torch.is_tensor(v) else v) for k, v in talker_kwargs.items()},\n         )\n         talker_generate_codes = talker_result[:, talker_input_ids.shape[1] : -1]\n \n@@ -4141,9 +4141,9 @@ def generate(\n             self.token2wav.float()\n \n         wav = self.token2wav(\n-            talker_generate_codes.to(self.token2wav.device),\n-            conditioning=speaker_params[\"cond\"].to(self.token2wav.device).float(),\n-            reference_mel=speaker_params[\"ref_mel\"].to(self.token2wav.device).float(),\n+            talker_generate_codes.to(input_ids.device),\n+            conditioning=speaker_params[\"cond\"].to(input_ids.device).float(),\n+            reference_mel=speaker_params[\"ref_mel\"].to(input_ids.device).float(),\n             **token2wav_kwargs,\n         )\n "
        },
        {
            "sha": "61f767d87e8f514f0f758c642114b7280bc0c81e",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/d4e7aa5526309b748e5a79a59991e147d0a75f93/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d4e7aa5526309b748e5a79a59991e147d0a75f93/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=d4e7aa5526309b748e5a79a59991e147d0a75f93",
            "patch": "@@ -4295,70 +4295,70 @@ def generate(\n             return thinker_result\n \n         # 2. Generate speech tokens from talker module\n-        embeds_to_talker = thinker_result.hidden_states[0][0].clone().to(self.talker.device)\n+        embeds_to_talker = thinker_result.hidden_states[0][0].clone().to(input_ids.device)\n         if thinker_kwargs.get(\"input_features\", None) is not None:\n             audio_ids_mask = input_ids == self.config.thinker_config.audio_token_index\n-            audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             audio_mask_tensor = torch.zeros(\n                 [audio_ids_mask.sum(), embeds_to_talker.shape[-1]],\n                 dtype=embeds_to_talker.dtype,\n-                device=self.talker.device,\n+                device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(audio_mask, audio_mask_tensor)\n         if thinker_kwargs.get(\"pixel_values\", None) is not None:\n             image_ids_mask = input_ids == self.config.thinker_config.image_token_index\n-            image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             image_mask_tensor = torch.zeros(\n                 [image_ids_mask.sum(), embeds_to_talker.shape[-1]],\n                 dtype=embeds_to_talker.dtype,\n-                device=self.talker.device,\n+                device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(image_mask, image_mask_tensor)\n         if thinker_kwargs.get(\"pixel_values_videos\", None) is not None:\n             video_ids_mask = input_ids == self.config.thinker_config.video_token_index\n-            video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             video_mask_tensor = torch.zeros(\n                 [video_ids_mask.sum(), embeds_to_talker.shape[-1]],\n                 dtype=embeds_to_talker.dtype,\n-                device=self.talker.device,\n+                device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(video_mask, video_mask_tensor)\n \n         processed_thinker_hidden = (\n             (embeds_to_talker,) + thinker_result.hidden_states[0][1:],\n         ) + thinker_result.hidden_states[1:]\n-        thinker_generate_ids = thinker_result.sequences[:, input_ids.size(1) :].to(self.talker.device)\n+        thinker_generate_ids = thinker_result.sequences[:, input_ids.size(1) :].to(input_ids.device)\n         thinker_token_embeds = [\n-            token_hidden_states[0].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n+            token_hidden_states[0].to(input_ids.device) for token_hidden_states in processed_thinker_hidden\n         ]\n         thinker_hidden_states = [\n-            token_hidden_states[-1].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n+            token_hidden_states[-1].to(input_ids.device) for token_hidden_states in processed_thinker_hidden\n         ]\n \n         talker_text_bos_token = speaker_params[\"bos_token\"]\n         talker_input_text_ids = torch.cat(\n             [\n-                input_ids.to(self.talker.device),\n-                torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=self.talker.device),\n+                input_ids,\n+                torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=input_ids.device),\n                 thinker_generate_ids[:, :1],\n             ],\n             dim=-1,\n         )\n \n         talker_input_ids = torch.cat(\n             [\n-                torch.full_like(input_ids, fill_value=self.talker.codec_mask_token, device=self.talker.device),\n-                torch.tensor([[self.talker.codec_pad_token]], dtype=torch.long, device=self.talker.device),\n-                torch.tensor([[self.talker.codec_bos_token]], dtype=torch.long, device=self.talker.device),\n+                torch.full_like(input_ids, fill_value=self.talker.codec_mask_token),\n+                torch.tensor([[self.talker.codec_pad_token]], dtype=torch.long, device=input_ids.device),\n+                torch.tensor([[self.talker.codec_bos_token]], dtype=torch.long, device=input_ids.device),\n             ],\n             dim=1,\n         )\n \n         thinker_embed_tokens = self.thinker.get_input_embeddings()\n         thinker_reply_part = torch.cat(thinker_hidden_states[1:], dim=1) + torch.cat(thinker_token_embeds[1:], dim=1)\n         talker_inputs_embeds = thinker_hidden_states[0] + thinker_token_embeds[0]\n-        talker_text_bos_token = torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=self.thinker.device)\n-        talker_text_bos_embed = thinker_embed_tokens(talker_text_bos_token).to(self.talker.device)\n+        talker_text_bos_token = torch.tensor([[talker_text_bos_token]], dtype=torch.long, device=input_ids.device)\n+        talker_text_bos_embed = thinker_embed_tokens(talker_text_bos_token).to(input_ids.device)\n         talker_inputs_embeds = torch.cat(\n             [\n                 talker_inputs_embeds,\n@@ -4369,12 +4369,12 @@ def generate(\n         )\n \n         eos_embedding = thinker_embed_tokens(\n-            torch.tensor([[self.talker.text_eos_token]], dtype=torch.long, device=self.thinker.device)\n-        ).to(self.talker.device)\n+            torch.tensor([[self.talker.text_eos_token]], dtype=torch.long, device=input_ids.device)\n+        )\n \n         pad_embedding = thinker_embed_tokens(\n-            torch.tensor([[self.talker.text_pad_token]], dtype=torch.long, device=self.thinker.device)\n-        ).to(self.talker.device)\n+            torch.tensor([[self.talker.text_pad_token]], dtype=torch.long, device=input_ids.device)\n+        )\n \n         thinker_reply_part = torch.cat(\n             [\n@@ -4389,7 +4389,7 @@ def generate(\n         if \"attention_mask\" in kwargs:\n             talker_attention_mask = torch.cat(\n                 [kwargs[\"attention_mask\"], kwargs[\"attention_mask\"].new_ones((1, 2))], dim=1\n-            ).to(self.talker.device)\n+            ).to(input_ids.device)\n \n         talker_result = self.talker.generate(\n             input_ids=talker_input_ids,\n@@ -4398,7 +4398,7 @@ def generate(\n             inputs_embeds=talker_inputs_embeds,\n             attention_mask=talker_attention_mask,\n             suppress_tokens=[self.talker.codec_bos_token],\n-            **{k: (v.to(self.talker.device) if torch.is_tensor(v) else v) for k, v in talker_kwargs.items()},\n+            **{k: (v.to(input_ids.device) if torch.is_tensor(v) else v) for k, v in talker_kwargs.items()},\n         )\n         talker_generate_codes = talker_result[:, talker_input_ids.shape[1] : -1]\n \n@@ -4407,9 +4407,9 @@ def generate(\n             self.token2wav.float()\n \n         wav = self.token2wav(\n-            talker_generate_codes.to(self.token2wav.device),\n-            conditioning=speaker_params[\"cond\"].to(self.token2wav.device).float(),\n-            reference_mel=speaker_params[\"ref_mel\"].to(self.token2wav.device).float(),\n+            talker_generate_codes.to(input_ids.device),\n+            conditioning=speaker_params[\"cond\"].to(input_ids.device).float(),\n+            reference_mel=speaker_params[\"ref_mel\"].to(input_ids.device).float(),\n             **token2wav_kwargs,\n         )\n "
        },
        {
            "sha": "e075c412ef780e6ac85059df84a9b53042ff9171",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 53,
            "deletions": 23,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/d4e7aa5526309b748e5a79a59991e147d0a75f93/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d4e7aa5526309b748e5a79a59991e147d0a75f93/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=d4e7aa5526309b748e5a79a59991e147d0a75f93",
            "patch": "@@ -33,6 +33,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_flash_attn,\n     require_torch,\n@@ -555,13 +556,13 @@ def tearDown(self):\n     @slow\n     def test_small_model_integration_test(self):\n         model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.float32, device_map=\"auto\"\n+            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n-            text=[text], audio=[self.raw_audio], images=[self.raw_image], return_tensors=\"pt\", padding=True\n-        )\n+            text=text, audio=[self.raw_audio], images=[self.raw_image], return_tensors=\"pt\", padding=True\n+        ).to(torch.bfloat16)\n \n         expected_input_ids = torch.tensor(\n             [\n@@ -581,7 +582,7 @@ def test_small_model_integration_test(self):\n                 198,\n                 151647,\n                 151646,\n-                151648,\n+                151646,\n             ]\n         )\n         assert torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n@@ -595,17 +596,19 @@ def test_small_model_integration_test(self):\n                 [1.3902, 1.4048, 1.4194],\n                 [1.5216, 1.5362, 1.5362],\n             ],\n-            dtype=torch.float32,\n+            dtype=torch.bfloat16,\n             device=\"cpu\",\n         )\n         assert torch.allclose(expected_pixel_slice, inputs.pixel_values[:6, :3], atol=3e-3)\n \n         # verify generation\n         inputs = inputs.to(torch_device)\n \n-        output = model.generate(**inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False)\n+        output = model.generate(\n+            **inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False, thinker_max_new_tokens=20\n+        )\n \n-        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\"\n+        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\"\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -615,23 +618,34 @@ def test_small_model_integration_test(self):\n     @slow\n     def test_small_model_integration_test_batch(self):\n         model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.float32, device_map=\"auto\"\n+            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n-            text=[text, text],\n+            text=text * 2,\n             audio=[self.raw_audio, self.raw_audio],\n             images=[self.raw_image, self.raw_image],\n             return_tensors=\"pt\",\n             padding=True,\n-        ).to(torch_device)\n+        ).to(torch_device, dtype=torch.bfloat16)\n \n-        output = model.generate(**inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False)\n+        output = model.generate(\n+            **inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False, thinker_max_new_tokens=20\n+        )\n \n-        EXPECTED_DECODED_TEXT = [\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\",\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\",\n-        ]\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7) : [\n+                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is of glass shattering, and the dog in the picture is a Labrador Retriever\",\n+                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is of glass shattering, and the dog in the picture is a Labrador Retriever\",\n+                ],\n+                (\"cuda\", 8): [\n+                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+                ],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n \n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -641,7 +655,7 @@ def test_small_model_integration_test_batch(self):\n     @slow\n     def test_small_model_integration_test_multiturn(self):\n         model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.float32, device_map=\"auto\"\n+            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         messages = [\n@@ -666,14 +680,16 @@ def test_small_model_integration_test_multiturn(self):\n \n         text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n-            text=[text],\n+            text=text,\n             audio=[self.raw_audio, self.raw_audio_additional],\n             images=[self.raw_image],\n             return_tensors=\"pt\",\n             padding=True,\n-        ).to(torch_device)\n+        ).to(torch_device, dtype=torch.bfloat16)\n \n-        output = model.generate(**inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False)\n+        output = model.generate(\n+            **inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False, thinker_max_new_tokens=20\n+        )\n \n         EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a cough.\"\n \n@@ -685,7 +701,7 @@ def test_small_model_integration_test_multiturn(self):\n     @slow\n     def test_small_model_integration_test_w_audio(self):\n         model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.float32, device_map=\"auto\"\n+            \"Qwen/Qwen2.5-Omni-7B\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"\n \n@@ -707,11 +723,25 @@ def test_small_model_integration_test_w_audio(self):\n         audio, _ = librosa.load(BytesIO(urlopen(audio_url).read()), sr=self.processor.feature_extractor.sampling_rate)\n \n         text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=text, audio=[audio], return_tensors=\"pt\", padding=True).to(torch_device)\n+        inputs = self.processor(text=text, audio=[audio], return_tensors=\"pt\", padding=True).to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n \n-        output = model.generate(**inputs, thinker_temperature=0, thinker_do_sample=False)\n+        output = model.generate(\n+            **inputs,\n+            thinker_temperature=0,\n+            thinker_do_sample=False,\n+            thinker_max_new_tokens=20,\n+            talker_max_new_tokens=10,\n+        )\n \n-        EXPECTED_DECODED_TEXT = \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can't really guess your age and gender just from your voice. There are so many factors that can affect how a voice sounds, like the environment you're in, how you're feeling at the moment, and even the microphone you're using. But if you want to share more about your voice, like if it's high - pitched or low - pitched, that might give me a bit of an idea. So, what can you tell me about your voice?\"\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can try. But it's not always that accurate. I might be able to make\",\n+                (\"cuda\", 8): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can't really guess your age and gender just from your voice. There are are a\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n \n         self.assertEqual(\n             self.processor.decode(output[0][0], skip_special_tokens=True),"
        }
    ],
    "stats": {
        "total": 180,
        "additions": 105,
        "deletions": 75
    }
}