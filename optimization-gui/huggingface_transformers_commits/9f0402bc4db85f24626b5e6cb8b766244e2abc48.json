{
    "author": "Cyrilvallez",
    "message": "Fix all import errors based on older torch versions (#38370)\n\n* Update masking_utils.py\n\n* fix\n\n* fix\n\n* fix\n\n* Update masking_utils.py\n\n* Update executorch.py\n\n* fix",
    "sha": "9f0402bc4db85f24626b5e6cb8b766244e2abc48",
    "files": [
        {
            "sha": "c42a1917ce5e5d3c91905f6760e9c077a1637485",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f0402bc4db85f24626b5e6cb8b766244e2abc48/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f0402bc4db85f24626b5e6cb8b766244e2abc48/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=9f0402bc4db85f24626b5e6cb8b766244e2abc48",
            "patch": "@@ -25,11 +25,16 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n     from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n-\n+else:\n+    # Register a fake type to avoid crashing for annotations and `isinstance` checks\n+    BlockMask = torch.Tensor\n \n _is_torch_greater_or_equal_than_2_5 = is_torch_greater_or_equal(\"2.5\", accept_dev=True)\n+_is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n+\n+if _is_torch_greater_or_equal_than_2_6:\n+    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n \n \n def and_masks(*mask_functions: list[Callable]) -> Callable:\n@@ -415,14 +420,14 @@ def sdpa_mask_older_torch(\n \n     # Due to a bug in versions of torch<2.5, we need to update the mask in case a query is not attending to any\n     # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n-    if allow_torch_fix:\n+    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n         causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n     return causal_mask\n \n \n # We use the version with newer torch whenever possible, as it is more general and can handle arbitrary mask functions\n # (especially mask_function indexing a tensor, such as the padding mask function)\n-sdpa_mask = sdpa_mask_recent_torch if is_torch_flex_attn_available() else sdpa_mask_older_torch\n+sdpa_mask = sdpa_mask_recent_torch if _is_torch_greater_or_equal_than_2_6 else sdpa_mask_older_torch\n \n \n def eager_mask(\n@@ -522,7 +527,7 @@ def flex_attention_mask(\n     mask_function: Callable = causal_mask_function,\n     attention_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n-) -> \"BlockMask\":\n+) -> BlockMask:\n     \"\"\"\n     Create a 4D block mask which is a compressed representation of the full 4D block causal mask. BlockMask is essential\n     for performant computation of flex attention. See: https://pytorch.org/blog/flexattention/\n@@ -652,7 +657,7 @@ def create_causal_mask(\n     past_key_values: Optional[Cache],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n+) -> Optional[Union[torch.Tensor, BlockMask]]:\n     \"\"\"\n     Create a standard causal mask based on the attention implementation used (stored in the config). If `past_key_values`\n     has an HybridCache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n@@ -700,12 +705,12 @@ def create_causal_mask(\n \n     # Allow slight deviations from causal mask\n     if or_mask_function is not None:\n-        if not _is_torch_greater_or_equal_than_2_5:\n+        if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_causal_skip = False\n     if and_mask_function is not None:\n-        if not _is_torch_greater_or_equal_than_2_5:\n+        if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_causal_skip = False\n@@ -733,7 +738,7 @@ def create_sliding_window_causal_mask(\n     past_key_values: Optional[Cache],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n+) -> Optional[Union[torch.Tensor, BlockMask]]:\n     \"\"\"\n     Create a sliding window causal mask based on the attention implementation used (stored in the config). This type\n     of attention pattern was mostly democratized by Mistral. If `past_key_values` has an HybridCache structure, this\n@@ -786,12 +791,12 @@ def create_sliding_window_causal_mask(\n \n     # Allow slight deviations from sliding causal mask\n     if or_mask_function is not None:\n-        if not _is_torch_greater_or_equal_than_2_5:\n+        if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_causal_skip = False\n     if and_mask_function is not None:\n-        if not _is_torch_greater_or_equal_than_2_5:\n+        if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_causal_skip = False\n@@ -820,7 +825,7 @@ def create_chunked_causal_mask(\n     past_key_values: Optional[Cache],\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n+) -> Optional[Union[torch.Tensor, BlockMask]]:\n     \"\"\"\n     Create a chunked attention causal mask based on the attention implementation used (stored in the config). This type\n     of attention pattern was mostly democratized by Llama4. If `past_key_values` has an HybridCache structure, this\n@@ -880,12 +885,12 @@ def create_chunked_causal_mask(\n \n     # Allow slight deviations from chunked causal mask\n     if or_mask_function is not None:\n-        if not _is_torch_greater_or_equal_than_2_5:\n+        if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_causal_skip = False\n     if and_mask_function is not None:\n-        if not _is_torch_greater_or_equal_than_2_5:\n+        if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_causal_skip = False"
        },
        {
            "sha": "f426d10ccd9265541528d0a8eb09d99eabda3e16",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f0402bc4db85f24626b5e6cb8b766244e2abc48/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f0402bc4db85f24626b5e6cb8b766244e2abc48/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9f0402bc4db85f24626b5e6cb8b766244e2abc48",
            "patch": "@@ -2078,7 +2078,7 @@ def post_init(self):\n             if plan := getattr(module, \"_tp_plan\", None):\n                 self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n \n-        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.3\"):\n+        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.5\"):\n             for _, v in self._tp_plan.items():\n                 if v not in ALL_PARALLEL_STYLES:\n                     raise ValueError("
        }
    ],
    "stats": {
        "total": 35,
        "additions": 20,
        "deletions": 15
    }
}