{
    "author": "yonigozlan",
    "message": "Improve BatchFeature (.to() works on lists/nested lists of tensors, auto-skip non array conversion) (#42884)\n\n* improve .to() to work on lists or nested lists of tensors, automatically skip converting non array structure\n\n* consider empty list or nested list as array-like",
    "sha": "9f583b1b70953ed0d5f05b90e05b413382faa107",
    "files": [
        {
            "sha": "20d46a0d58e54aceaafcf824807fe17baf295dcc",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f583b1b70953ed0d5f05b90e05b413382faa107/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f583b1b70953ed0d5f05b90e05b413382faa107/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=9f583b1b70953ed0d5f05b90e05b413382faa107",
            "patch": "@@ -30,6 +30,7 @@\n     PROCESSOR_NAME,\n     PushToHubMixin,\n     TensorType,\n+    _is_tensor_or_array_like,\n     copy_func,\n     is_numpy_array,\n     is_torch_available,\n@@ -167,6 +168,11 @@ def convert_to_tensors(\n                 `None`, no modification is done.\n             skip_tensor_conversion (`list[str]` or `set[str]`, *optional*):\n                 List or set of keys that should NOT be converted to tensors, even when `tensor_type` is specified.\n+\n+        Note:\n+            Values that don't have an array-like structure (e.g., strings, dicts, lists of strings) are\n+            automatically skipped and won't be converted to tensors. Ragged arrays (lists of arrays with\n+            different lengths) are still attempted, though they may raise errors during conversion.\n         \"\"\"\n         if tensor_type is None:\n             return self\n@@ -179,6 +185,10 @@ def convert_to_tensors(\n             if skip_tensor_conversion and key in skip_tensor_conversion:\n                 continue\n \n+            # Skip values that are not array-like\n+            if not _is_tensor_or_array_like(value):\n+                continue\n+\n             try:\n                 if not is_tensor(value):\n                     tensor = as_tensor(value)\n@@ -233,12 +243,15 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n \n         # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n         def maybe_to(v):\n-            # check if v is a floating point\n+            # check if v is a floating point tensor\n             if isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n                 # cast and send to device\n                 return v.to(*args, **kwargs)\n             elif isinstance(v, torch.Tensor) and device is not None:\n                 return v.to(device=device, non_blocking=non_blocking)\n+            # recursively handle lists and tuples\n+            elif isinstance(v, (list, tuple)):\n+                return type(v)(maybe_to(item) for item in v)\n             else:\n                 return v\n "
        },
        {
            "sha": "2a0383bca9a8148add17151017a1a8523120a8af",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f583b1b70953ed0d5f05b90e05b413382faa107/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f583b1b70953ed0d5f05b90e05b413382faa107/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=9f583b1b70953ed0d5f05b90e05b413382faa107",
            "patch": "@@ -49,6 +49,7 @@\n     PaddingStrategy,\n     TensorType,\n     TransformersKwargs,\n+    _is_tensor_or_array_like,\n     can_return_loss,\n     can_return_tuple,\n     expand_dims,"
        },
        {
            "sha": "dee901a395ed4723e468fb4f576501d6353bf7e2",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f583b1b70953ed0d5f05b90e05b413382faa107/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f583b1b70953ed0d5f05b90e05b413382faa107/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=9f583b1b70953ed0d5f05b90e05b413382faa107",
            "patch": "@@ -155,6 +155,26 @@ def is_torch_dtype(x):\n     return isinstance(x, torch.dtype)\n \n \n+def _is_tensor_or_array_like(value):\n+    \"\"\"\n+    Check if a value is array-like (includes ragged arrays)\n+    \"\"\"\n+    if is_numpy_array(value):\n+        return True\n+    if is_torch_tensor(value):\n+        return True\n+    if isinstance(value, (int, float, bool, np.number)):\n+        return True\n+\n+    if isinstance(value, (list, tuple)):\n+        if len(value) == 0:\n+            # consider empty list or nested list as array-like\n+            return True\n+        return _is_tensor_or_array_like(value[0])\n+\n+    return False\n+\n+\n def maybe_autocast(\n     device_type: str,\n     dtype: Optional[\"_dtype\"] = None,"
        },
        {
            "sha": "8291fd0e7462b2b0ab7804b3d81e2e5baa2ce509",
            "filename": "tests/utils/test_feature_extraction_utils.py",
            "status": "modified",
            "additions": 48,
            "deletions": 5,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f583b1b70953ed0d5f05b90e05b413382faa107/tests%2Futils%2Ftest_feature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f583b1b70953ed0d5f05b90e05b413382faa107/tests%2Futils%2Ftest_feature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_feature_extraction_utils.py?ref=9f583b1b70953ed0d5f05b90e05b413382faa107",
            "patch": "@@ -135,11 +135,28 @@ def test_batch_feature_error_handling(self):\n         self.assertIn(\"inhomogeneous\", error_msg.lower())\n         self.assertIn(\"return_tensors=None\", error_msg)\n \n-        # Unconvertible type (dict)\n-        data_dict = {\"values\": [[1, 2]], \"metadata\": {\"key\": \"val\"}}\n-        with self.assertRaises(ValueError) as context:\n-            BatchFeature(data_dict, tensor_type=\"pt\")\n-        self.assertIn(\"metadata\", str(context.exception))\n+    @require_torch\n+    def test_batch_feature_auto_skip_non_array_like(self):\n+        \"\"\"Test that non-array-like values are automatically skipped during tensor conversion.\"\"\"\n+        data = {\n+            \"values\": [[1, 2]],\n+            \"metadata\": {\"key\": \"val\"},\n+            \"image_path\": \"/path/to/image.jpg\",\n+            \"tags\": [\"tag1\", \"tag2\"],\n+            \"extra\": None,\n+        }\n+        batch = BatchFeature(data, tensor_type=\"pt\")\n+\n+        # values should be converted\n+        self.assertIsInstance(batch[\"values\"], torch.Tensor)\n+\n+        # Non-array-like values should remain unchanged\n+        self.assertIsInstance(batch[\"metadata\"], dict)\n+        self.assertEqual(batch[\"metadata\"], {\"key\": \"val\"})\n+        self.assertIsInstance(batch[\"image_path\"], str)\n+        self.assertIsInstance(batch[\"tags\"], list)\n+        self.assertEqual(batch[\"tags\"], [\"tag1\", \"tag2\"])\n+        self.assertIsNone(batch[\"extra\"])\n \n     @require_torch\n     def test_batch_feature_skip_tensor_conversion(self):\n@@ -169,6 +186,32 @@ def test_batch_feature_convert_to_tensors_method(self):\n         self.assertIsInstance(batch[\"input_values\"], torch.Tensor)\n         self.assertIsInstance(batch[\"metadata\"], list)\n \n+    @require_torch\n+    def test_batch_feature_to_with_nested_tensors(self):\n+        \"\"\"Test .to() method works recursively with nested lists and tuples of tensors.\"\"\"\n+        batch = BatchFeature(\n+            {\n+                \"list_tensors\": [torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0])],\n+                \"nested_list\": [[torch.tensor([1.0]), torch.tensor([2.0])]],\n+                \"tuple_tensors\": (torch.tensor([5.0]), torch.tensor([6.0])),\n+            }\n+        )\n+\n+        batch_fp16 = batch.to(torch.float16)\n+\n+        # Check lists of tensors are converted\n+        self.assertIsInstance(batch_fp16[\"list_tensors\"], list)\n+        self.assertEqual(batch_fp16[\"list_tensors\"][0].dtype, torch.float16)\n+        self.assertEqual(batch_fp16[\"list_tensors\"][1].dtype, torch.float16)\n+\n+        # Check nested lists are converted\n+        self.assertIsInstance(batch_fp16[\"nested_list\"][0], list)\n+        self.assertEqual(batch_fp16[\"nested_list\"][0][0].dtype, torch.float16)\n+\n+        # Check tuples are preserved and converted\n+        self.assertIsInstance(batch_fp16[\"tuple_tensors\"], tuple)\n+        self.assertEqual(batch_fp16[\"tuple_tensors\"][0].dtype, torch.float16)\n+\n \n class FeatureExtractorUtilTester(unittest.TestCase):\n     def test_cached_files_are_used_when_internet_is_down(self):"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 83,
        "deletions": 6
    }
}