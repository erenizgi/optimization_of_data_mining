{
    "author": "Rocketknight1",
    "message": "Image pipelines spec compliance (#33899)\n\n* Update many similar visual pipelines\r\n\r\n* Add input tests\r\n\r\n* Add ImageToText as well\r\n\r\n* Add output tests\r\n\r\n* Add output tests\r\n\r\n* Add output tests\r\n\r\n* OutputElement -> Output\r\n\r\n* Correctly test elements\r\n\r\n* make fixup\r\n\r\n* fix typo in the task list\r\n\r\n* Fix VQA testing\r\n\r\n* Add copyright to image_classification.py\r\n\r\n* Revert changes to VQA pipeline because outputs have differences - will move to another PR\r\n\r\n* make fixup\r\n\r\n* Remove deprecation warnings",
    "sha": "3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
    "files": [
        {
            "sha": "f70f8d85c15db84fd64e90460962de8bda2746e6",
            "filename": "src/transformers/pipelines/depth_estimation.py",
            "status": "modified",
            "additions": 20,
            "deletions": 8,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -1,3 +1,4 @@\n+import warnings\n from typing import List, Union\n \n import numpy as np\n@@ -50,12 +51,12 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, \"vision\")\n         self.check_model_type(MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES)\n \n-    def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs):\n+    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):\n         \"\"\"\n         Predict the depth(s) of the image(s) passed as inputs.\n \n         Args:\n-            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n+            inputs (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 The pipeline handles three types of images:\n \n                 - A string containing a http link pointing to an image\n@@ -65,9 +66,10 @@ def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Imag\n                 The pipeline accepts either a single image or a batch of images, which must then be passed as a string.\n                 Images in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\n                 images.\n-            timeout (`float`, *optional*, defaults to None):\n-                The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n-                the call may block forever.\n+            parameters (`Dict`, *optional*):\n+                A dictionary of argument names to parameter values, to control pipeline behaviour.\n+                The only parameter available right now is `timeout`, which is the length of time, in seconds,\n+                that the pipeline should wait before giving up on trying to download an image.\n \n         Return:\n             A dictionary or a list of dictionaries containing result. If the input is a single image, will return a\n@@ -79,12 +81,22 @@ def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Imag\n             - **predicted_depth** (`torch.Tensor`) -- The predicted depth by the model as a `torch.Tensor`.\n             - **depth** (`PIL.Image`) -- The predicted depth by the model as a `PIL.Image`.\n         \"\"\"\n-        return super().__call__(images, **kwargs)\n-\n-    def _sanitize_parameters(self, timeout=None, **kwargs):\n+        # After deprecation of this is completed, remove the default `None` value for `images`\n+        if \"images\" in kwargs:\n+            inputs = kwargs.pop(\"images\")\n+        if inputs is None:\n+            raise ValueError(\"Cannot call the depth-estimation pipeline without an inputs argument!\")\n+        return super().__call__(inputs, **kwargs)\n+\n+    def _sanitize_parameters(self, timeout=None, parameters=None, **kwargs):\n         preprocess_params = {}\n         if timeout is not None:\n+            warnings.warn(\n+                \"The `timeout` argument is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n+            )\n             preprocess_params[\"timeout\"] = timeout\n+        if isinstance(parameters, dict) and \"timeout\" in parameters:\n+            preprocess_params[\"timeout\"] = parameters[\"timeout\"]\n         return preprocess_params, {}, {}\n \n     def preprocess(self, image, timeout=None):"
        },
        {
            "sha": "20ad72e79055e20126006f7fd17efea5263177c5",
            "filename": "src/transformers/pipelines/image_classification.py",
            "status": "modified",
            "additions": 25,
            "deletions": 6,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fimage_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fimage_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_classification.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -1,3 +1,17 @@\n+# Copyright 2023 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import warnings\n from typing import List, Union\n \n import numpy as np\n@@ -99,6 +113,9 @@ def __init__(self, *args, **kwargs):\n     def _sanitize_parameters(self, top_k=None, function_to_apply=None, timeout=None):\n         preprocess_params = {}\n         if timeout is not None:\n+            warnings.warn(\n+                \"The `timeout` argument is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n+            )\n             preprocess_params[\"timeout\"] = timeout\n         postprocess_params = {}\n         if top_k is not None:\n@@ -109,12 +126,12 @@ def _sanitize_parameters(self, top_k=None, function_to_apply=None, timeout=None)\n             postprocess_params[\"function_to_apply\"] = function_to_apply\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs):\n+    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):\n         \"\"\"\n         Assign labels to the image(s) passed as inputs.\n \n         Args:\n-            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n+            inputs (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 The pipeline handles three types of images:\n \n                 - A string containing a http link pointing to an image\n@@ -142,9 +159,6 @@ def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Imag\n             top_k (`int`, *optional*, defaults to 5):\n                 The number of top labels that will be returned by the pipeline. If the provided number is higher than\n                 the number of labels available in the model configuration, it will default to the number of labels.\n-            timeout (`float`, *optional*, defaults to None):\n-                The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n-                the call may block forever.\n \n         Return:\n             A dictionary or a list of dictionaries containing result. If the input is a single image, will return a\n@@ -156,7 +170,12 @@ def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Imag\n             - **label** (`str`) -- The label identified by the model.\n             - **score** (`int`) -- The score attributed by the model for that label.\n         \"\"\"\n-        return super().__call__(images, **kwargs)\n+        # After deprecation of this is completed, remove the default `None` value for `images`\n+        if \"images\" in kwargs:\n+            inputs = kwargs.pop(\"images\")\n+        if inputs is None:\n+            raise ValueError(\"Cannot call the image-classification pipeline without an inputs argument!\")\n+        return super().__call__(inputs, **kwargs)\n \n     def preprocess(self, image, timeout=None):\n         image = load_image(image, timeout=timeout)"
        },
        {
            "sha": "0ac653fd1e872536ed16a635cbc01e77e0e1f2ca",
            "filename": "src/transformers/pipelines/image_segmentation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fimage_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fimage_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_segmentation.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -1,3 +1,4 @@\n+import warnings\n from typing import Any, Dict, List, Union\n \n import numpy as np\n@@ -90,16 +91,19 @@ def _sanitize_parameters(self, **kwargs):\n         if \"overlap_mask_area_threshold\" in kwargs:\n             postprocess_kwargs[\"overlap_mask_area_threshold\"] = kwargs[\"overlap_mask_area_threshold\"]\n         if \"timeout\" in kwargs:\n+            warnings.warn(\n+                \"The `timeout` argument is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n+            )\n             preprocess_kwargs[\"timeout\"] = kwargs[\"timeout\"]\n \n         return preprocess_kwargs, {}, postprocess_kwargs\n \n-    def __call__(self, images, **kwargs) -> Union[Predictions, List[Prediction]]:\n+    def __call__(self, inputs=None, **kwargs) -> Union[Predictions, List[Prediction]]:\n         \"\"\"\n         Perform segmentation (detect masks & classes) in the image(s) passed as inputs.\n \n         Args:\n-            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n+            inputs (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 The pipeline handles three types of images:\n \n                 - A string containing an HTTP(S) link pointing to an image\n@@ -118,9 +122,6 @@ def __call__(self, images, **kwargs) -> Union[Predictions, List[Prediction]]:\n                 Threshold to use when turning the predicted masks into binary values.\n             overlap_mask_area_threshold (`float`, *optional*, defaults to 0.5):\n                 Mask overlap threshold to eliminate small, disconnected segments.\n-            timeout (`float`, *optional*, defaults to None):\n-                The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n-                the call may block forever.\n \n         Return:\n             A dictionary or a list of dictionaries containing the result. If the input is a single image, will return a\n@@ -136,7 +137,12 @@ def __call__(self, images, **kwargs) -> Union[Predictions, List[Prediction]]:\n             - **score** (*optional* `float`) -- Optionally, when the model is capable of estimating a confidence of the\n               \"object\" described by the label and the mask.\n         \"\"\"\n-        return super().__call__(images, **kwargs)\n+        # After deprecation of this is completed, remove the default `None` value for `images`\n+        if \"images\" in kwargs:\n+            inputs = kwargs.pop(\"images\")\n+        if inputs is None:\n+            raise ValueError(\"Cannot call the image-classification pipeline without an inputs argument!\")\n+        return super().__call__(inputs, **kwargs)\n \n     def preprocess(self, image, subtask=None, timeout=None):\n         image = load_image(image, timeout=timeout)"
        },
        {
            "sha": "4beaa4819200547d4c47377d428208e906156f03",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -13,6 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import warnings\n from typing import List, Union\n \n from ..utils import (\n@@ -80,6 +81,9 @@ def _sanitize_parameters(self, max_new_tokens=None, generate_kwargs=None, prompt\n         if prompt is not None:\n             preprocess_params[\"prompt\"] = prompt\n         if timeout is not None:\n+            warnings.warn(\n+                \"The `timeout` argument is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n+            )\n             preprocess_params[\"timeout\"] = timeout\n \n         if max_new_tokens is not None:\n@@ -94,12 +98,12 @@ def _sanitize_parameters(self, max_new_tokens=None, generate_kwargs=None, prompt\n \n         return preprocess_params, forward_params, {}\n \n-    def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs):\n+    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):\n         \"\"\"\n         Assign labels to the image(s) passed as inputs.\n \n         Args:\n-            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n+            inputs (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 The pipeline handles three types of images:\n \n                 - A string containing a HTTP(s) link pointing to an image\n@@ -113,16 +117,18 @@ def __call__(self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Imag\n \n             generate_kwargs (`Dict`, *optional*):\n                 Pass it to send all of these arguments directly to `generate` allowing full control of this function.\n-            timeout (`float`, *optional*, defaults to None):\n-                The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n-                the call may block forever.\n \n         Return:\n             A list or a list of list of `dict`: Each result comes as a dictionary with the following key:\n \n             - **generated_text** (`str`) -- The generated text.\n         \"\"\"\n-        return super().__call__(images, **kwargs)\n+        # After deprecation of this is completed, remove the default `None` value for `images`\n+        if \"images\" in kwargs:\n+            inputs = kwargs.pop(\"images\")\n+        if inputs is None:\n+            raise ValueError(\"Cannot call the image-to-text pipeline without an inputs argument!\")\n+        return super().__call__(inputs, **kwargs)\n \n     def preprocess(self, image, prompt=None, timeout=None):\n         image = load_image(image, timeout=timeout)"
        },
        {
            "sha": "c135b1e131acb95424d3b21a08dbd79f8a2bf2fc",
            "filename": "src/transformers/pipelines/object_detection.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fobject_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fobject_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fobject_detection.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -1,3 +1,4 @@\n+import warnings\n from typing import Any, Dict, List, Union\n \n from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging, requires_backends\n@@ -63,6 +64,9 @@ def __init__(self, *args, **kwargs):\n     def _sanitize_parameters(self, **kwargs):\n         preprocess_params = {}\n         if \"timeout\" in kwargs:\n+            warnings.warn(\n+                \"The `timeout` argument is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n+            )\n             preprocess_params[\"timeout\"] = kwargs[\"timeout\"]\n         postprocess_kwargs = {}\n         if \"threshold\" in kwargs:\n@@ -74,7 +78,7 @@ def __call__(self, *args, **kwargs) -> Union[Predictions, List[Prediction]]:\n         Detect objects (bounding boxes & classes) in the image(s) passed as inputs.\n \n         Args:\n-            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n+            inputs (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 The pipeline handles three types of images:\n \n                 - A string containing an HTTP(S) link pointing to an image\n@@ -85,9 +89,6 @@ def __call__(self, *args, **kwargs) -> Union[Predictions, List[Prediction]]:\n                 same format: all as HTTP(S) links, all as local paths, or all as PIL images.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The probability necessary to make a prediction.\n-            timeout (`float`, *optional*, defaults to None):\n-                The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n-                the call may block forever.\n \n         Return:\n             A list of dictionaries or a list of list of dictionaries containing the result. If the input is a single\n@@ -100,7 +101,9 @@ def __call__(self, *args, **kwargs) -> Union[Predictions, List[Prediction]]:\n             - **score** (`float`) -- The score attributed by the model for that label.\n             - **box** (`List[Dict[str, int]]`) -- The bounding box of detected object in image's original size.\n         \"\"\"\n-\n+        # After deprecation of this is completed, remove the default `None` value for `images`\n+        if \"images\" in kwargs and \"inputs\" not in kwargs:\n+            kwargs[\"inputs\"] = kwargs.pop(\"images\")\n         return super().__call__(*args, **kwargs)\n \n     def preprocess(self, image, timeout=None):"
        },
        {
            "sha": "253c684fcbbdadb0e32d9ed31a553994d87f141e",
            "filename": "src/transformers/pipelines/zero_shot_image_classification.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -1,3 +1,4 @@\n+import warnings\n from collections import UserDict\n from typing import List, Union\n \n@@ -73,12 +74,12 @@ def __init__(self, **kwargs):\n             else MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES\n         )\n \n-    def __call__(self, images: Union[str, List[str], \"Image\", List[\"Image\"]], **kwargs):\n+    def __call__(self, image: Union[str, List[str], \"Image\", List[\"Image\"]] = None, **kwargs):\n         \"\"\"\n         Assign labels to the image(s) passed as inputs.\n \n         Args:\n-            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n+            image (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 The pipeline handles three types of images:\n \n                 - A string containing a http link pointing to an image\n@@ -93,31 +94,36 @@ def __call__(self, images: Union[str, List[str], \"Image\", List[\"Image\"]], **kwar\n                 replacing the placeholder with the candidate_labels. Pass \"{}\" if *candidate_labels* are\n                 already formatted.\n \n-            timeout (`float`, *optional*, defaults to None):\n-                The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n-                the call may block forever.\n-\n-            tokenizer_kwargs (`dict`, *optional*):\n-                Additional dictionary of keyword arguments passed along to the tokenizer.\n-\n         Return:\n             A list of dictionaries containing one entry per proposed label. Each dictionary contains the\n             following keys:\n             - **label** (`str`) -- One of the suggested *candidate_labels*.\n             - **score** (`float`) -- The score attributed by the model to that label. It is a value between\n                 0 and 1, computed as the `softmax` of `logits_per_image`.\n         \"\"\"\n-        return super().__call__(images, **kwargs)\n+        # After deprecation of this is completed, remove the default `None` value for `image`\n+        if \"images\" in kwargs:\n+            image = kwargs.pop(\"images\")\n+        if image is None:\n+            raise ValueError(\"Cannot call the zero-shot-image-classification pipeline without an images argument!\")\n+        return super().__call__(image, **kwargs)\n \n     def _sanitize_parameters(self, tokenizer_kwargs=None, **kwargs):\n         preprocess_params = {}\n         if \"candidate_labels\" in kwargs:\n             preprocess_params[\"candidate_labels\"] = kwargs[\"candidate_labels\"]\n         if \"timeout\" in kwargs:\n+            warnings.warn(\n+                \"The `timeout` argument is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n+            )\n             preprocess_params[\"timeout\"] = kwargs[\"timeout\"]\n         if \"hypothesis_template\" in kwargs:\n             preprocess_params[\"hypothesis_template\"] = kwargs[\"hypothesis_template\"]\n         if tokenizer_kwargs is not None:\n+            warnings.warn(\n+                \"The `tokenizer_kwargs` argument is deprecated and will be removed in version 5 of Transformers\",\n+                FutureWarning,\n+            )\n             preprocess_params[\"tokenizer_kwargs\"] = tokenizer_kwargs\n \n         return preprocess_params, {}, {}"
        },
        {
            "sha": "03cc86abec34d88ab16761b8129e02eef70fee1c",
            "filename": "tests/pipelines/test_pipelines_depth_estimation.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -14,11 +14,13 @@\n \n import unittest\n \n+from huggingface_hub import DepthEstimationOutput\n from huggingface_hub.utils import insecure_hashlib\n \n from transformers import MODEL_FOR_DEPTH_ESTIMATION_MAPPING, is_torch_available, is_vision_available\n from transformers.pipelines import DepthEstimationPipeline, pipeline\n from transformers.testing_utils import (\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n     require_tf,\n@@ -94,6 +96,9 @@ def run_pipeline_test(self, depth_estimator, examples):\n             outputs,\n         )\n \n+        for single_output in outputs:\n+            compare_pipeline_output_to_hub_spec(single_output, DepthEstimationOutput)\n+\n     @require_tf\n     @unittest.skip(reason=\"Depth estimation is not implemented in TF\")\n     def test_small_model_tf(self):"
        },
        {
            "sha": "8c954892531c04d8b3006981961f8c7e1b340d13",
            "filename": "tests/pipelines/test_pipelines_image_classification.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_classification.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -14,6 +14,8 @@\n \n import unittest\n \n+from huggingface_hub import ImageClassificationOutputElement\n+\n from transformers import (\n     MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n     TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n@@ -23,6 +25,7 @@\n )\n from transformers.pipelines import ImageClassificationPipeline, pipeline\n from transformers.testing_utils import (\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n     require_tf,\n@@ -121,6 +124,10 @@ def run_pipeline_test(self, image_classifier, examples):\n             ],\n         )\n \n+        for single_output in outputs:\n+            for output_element in single_output:\n+                compare_pipeline_output_to_hub_spec(output_element, ImageClassificationOutputElement)\n+\n     @require_torch\n     def test_small_model_pt(self):\n         small_model = \"hf-internal-testing/tiny-random-vit\""
        },
        {
            "sha": "c62909331f8a1ae379fd6d443702d9561bbaf2bc",
            "filename": "tests/pipelines/test_pipelines_image_segmentation.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -20,6 +20,7 @@\n import numpy as np\n import requests\n from datasets import load_dataset\n+from huggingface_hub import ImageSegmentationOutputElement\n from huggingface_hub.utils import insecure_hashlib\n \n from transformers import (\n@@ -36,6 +37,7 @@\n     pipeline,\n )\n from transformers.testing_utils import (\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n     require_tf,\n@@ -168,6 +170,10 @@ def run_pipeline_test(self, image_segmenter, examples):\n             f\"Expected [{n}, {n}, {n}, {n}, {n}], got {[len(item) for item in outputs]}\",\n         )\n \n+        for single_output in outputs:\n+            for output_element in single_output:\n+                compare_pipeline_output_to_hub_spec(output_element, ImageSegmentationOutputElement)\n+\n     @require_tf\n     @unittest.skip(reason=\"Image segmentation not implemented in TF\")\n     def test_small_model_tf(self):"
        },
        {
            "sha": "70331643249e9a1008ca16fa977992523d859aea",
            "filename": "tests/pipelines/test_pipelines_image_to_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -15,10 +15,12 @@\n import unittest\n \n import requests\n+from huggingface_hub import ImageToTextOutput\n \n from transformers import MODEL_FOR_VISION_2_SEQ_MAPPING, TF_MODEL_FOR_VISION_2_SEQ_MAPPING, is_vision_available\n from transformers.pipelines import ImageToTextPipeline, pipeline\n from transformers.testing_utils import (\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     require_tf,\n     require_torch,\n@@ -103,6 +105,9 @@ def test_small_model_tf(self):\n             [{\"generated_text\": \"growth\"}],\n         )\n \n+        for single_output in outputs:\n+            compare_pipeline_output_to_hub_spec(single_output, ImageToTextOutput)\n+\n     @require_torch\n     def test_small_model_pt(self):\n         pipe = pipeline(\"image-to-text\", model=\"hf-internal-testing/tiny-random-vit-gpt2\")"
        },
        {
            "sha": "448257bd05495daf4b0dbfd265e97a8297517969",
            "filename": "tests/pipelines/test_pipelines_object_detection.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_object_detection.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -14,6 +14,8 @@\n \n import unittest\n \n+from huggingface_hub import ObjectDetectionOutputElement\n+\n from transformers import (\n     MODEL_FOR_OBJECT_DETECTION_MAPPING,\n     AutoFeatureExtractor,\n@@ -22,7 +24,8 @@\n     is_vision_available,\n     pipeline,\n )\n-from transformers.testing_utils import (\n+from transformers.testing_utils import (  #\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n     require_pytesseract,\n@@ -101,6 +104,7 @@ def run_pipeline_test(self, object_detector, examples):\n                         \"box\": {\"xmin\": ANY(int), \"ymin\": ANY(int), \"xmax\": ANY(int), \"ymax\": ANY(int)},\n                     },\n                 )\n+                compare_pipeline_output_to_hub_spec(detected_object, ObjectDetectionOutputElement)\n \n     @require_tf\n     @unittest.skip(reason=\"Object detection not implemented in TF\")"
        },
        {
            "sha": "bbeaeff3c17496b8f396d67cde1d858d1dc7096a",
            "filename": "tests/pipelines/test_pipelines_zero_shot_image_classification.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -14,9 +14,12 @@\n \n import unittest\n \n+from huggingface_hub import ZeroShotImageClassificationOutputElement\n+\n from transformers import is_vision_available\n from transformers.pipelines import pipeline\n from transformers.testing_utils import (\n+    compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n     require_tf,\n@@ -127,6 +130,9 @@ def test_small_model_pt(self, torch_dtype=\"float32\"):\n             ],\n         )\n \n+        for single_output in output:\n+            compare_pipeline_output_to_hub_spec(single_output, ZeroShotImageClassificationOutputElement)\n+\n     @require_torch\n     def test_small_model_pt_fp16(self):\n         self.test_small_model_pt(torch_dtype=\"float16\")"
        },
        {
            "sha": "329d5057230bf9bc46f489d33b50ecbea77ab720",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 26,
            "deletions": 2,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b44d2f0424ce46411fd0b6beb72aaafdf10c361/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=3b44d2f0424ce46411fd0b6beb72aaafdf10c361",
            "patch": "@@ -25,9 +25,27 @@\n from textwrap import dedent\n from typing import get_args\n \n-from huggingface_hub import AudioClassificationInput, AutomaticSpeechRecognitionInput\n+from huggingface_hub import (\n+    AudioClassificationInput,\n+    AutomaticSpeechRecognitionInput,\n+    DepthEstimationInput,\n+    ImageClassificationInput,\n+    ImageSegmentationInput,\n+    ImageToTextInput,\n+    ObjectDetectionInput,\n+    ZeroShotImageClassificationInput,\n+)\n \n-from transformers.pipelines import AudioClassificationPipeline, AutomaticSpeechRecognitionPipeline\n+from transformers.pipelines import (\n+    AudioClassificationPipeline,\n+    AutomaticSpeechRecognitionPipeline,\n+    DepthEstimationPipeline,\n+    ImageClassificationPipeline,\n+    ImageSegmentationPipeline,\n+    ImageToTextPipeline,\n+    ObjectDetectionPipeline,\n+    ZeroShotImageClassificationPipeline,\n+)\n from transformers.testing_utils import (\n     is_pipeline_test,\n     require_decord,\n@@ -105,6 +123,12 @@\n     # task spec in the HF Hub\n     \"audio-classification\": (AudioClassificationPipeline, AudioClassificationInput),\n     \"automatic-speech-recognition\": (AutomaticSpeechRecognitionPipeline, AutomaticSpeechRecognitionInput),\n+    \"depth-estimation\": (DepthEstimationPipeline, DepthEstimationInput),\n+    \"image-classification\": (ImageClassificationPipeline, ImageClassificationInput),\n+    \"image-segmentation\": (ImageSegmentationPipeline, ImageSegmentationInput),\n+    \"image-to-text\": (ImageToTextPipeline, ImageToTextInput),\n+    \"object-detection\": (ObjectDetectionPipeline, ObjectDetectionInput),\n+    \"zero-shot-image-classification\": (ZeroShotImageClassificationPipeline, ZeroShotImageClassificationInput),\n }\n \n for task, task_info in pipeline_test_mapping.items():"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 153,
        "deletions": 44
    }
}