{
    "author": "NielsRogge",
    "message": "[CLIPSeg] Make interpolate_pos_encoding default to True (#34419)\n\n* Remove interpolate_pos_encoding\r\n\r\n* Make fixup\r\n\r\n* Make interpolate_pos_encoding default to True\r\n\r\n* Reuse existing interpolation\r\n\r\n* Add integration test",
    "sha": "df8640cedb5dd3266ebf8958731f9f1315507182",
    "files": [
        {
            "sha": "4ead68032b603405879f521b57beb71c3f4c8bb0",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/df8640cedb5dd3266ebf8958731f9f1315507182/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df8640cedb5dd3266ebf8958731f9f1315507182/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=df8640cedb5dd3266ebf8958731f9f1315507182",
            "patch": "@@ -205,7 +205,7 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n \n         return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n-    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=True) -> torch.Tensor:\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n@@ -535,7 +535,7 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `True`):\n             Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n@@ -574,7 +574,7 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `True`):\n             Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n@@ -845,14 +845,13 @@ def __init__(self, config: CLIPSegVisionConfig):\n \n     @add_start_docstrings_to_model_forward(CLIPSEG_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPSegVisionConfig)\n-    # Copied from transformers.models.clip.modeling_clip.CLIPVisionTransformer.forward\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor],\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n+        interpolate_pos_encoding: Optional[bool] = True,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n@@ -864,9 +863,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n@@ -912,7 +908,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n+        interpolate_pos_encoding: Optional[bool] = True,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1035,7 +1031,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: bool = False,\n+        interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1091,7 +1087,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: bool = False,\n+        interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPSegOutput]:\n         r\"\"\"\n@@ -1397,7 +1393,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: bool = False,\n+        interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPSegOutput]:\n         r\"\"\""
        },
        {
            "sha": "75ffa7ad23c2ff3c8664f00c4521b95b6e3ff567",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/df8640cedb5dd3266ebf8958731f9f1315507182/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df8640cedb5dd3266ebf8958731f9f1315507182/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=df8640cedb5dd3266ebf8958731f9f1315507182",
            "patch": "@@ -796,15 +796,15 @@ def test_inference_image_segmentation(self):\n \n         # forward pass\n         with torch.no_grad():\n-            outputs = model(**inputs, interpolate_pos_encoding=True)\n+            outputs = model(**inputs)\n \n         # verify the predicted masks\n         self.assertEqual(\n             outputs.logits.shape,\n             torch.Size((3, 352, 352)),\n         )\n         expected_masks_slice = torch.tensor(\n-            [[-7.4613, -7.4785, -7.3627], [-7.3268, -7.0898, -7.1333], [-6.9838, -6.7900, -6.8913]]\n+            [[-7.4613, -7.4785, -7.3628], [-7.3268, -7.0899, -7.1333], [-6.9838, -6.7900, -6.8913]]\n         ).to(torch_device)\n \n         self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_masks_slice, atol=1e-3))"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 11,
        "deletions": 15
    }
}