{
    "author": "ydshieh",
    "message": "All CI jobs with A10 (#39119)\n\nall a10\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
    "files": [
        {
            "sha": "adafd5c046bdf2586677a74d33769034f343f64b",
            "filename": ".github/workflows/check_failed_tests.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fcheck_failed_tests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fcheck_failed_tests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcheck_failed_tests.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -41,7 +41,7 @@ jobs:\n   check_new_failures:\n     name: \" \"\n     runs-on:\n-      group: aws-g4dn-4xlarge-cache\n+      group: aws-g5-4xlarge-cache\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "86368b0f4ca3ed195e943cc2d6d2cda0de0d5899",
            "filename": ".github/workflows/doctest_job.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fdoctest_job.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fdoctest_job.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctest_job.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -28,7 +28,7 @@ jobs:\n       matrix:\n         split_keys: ${{ fromJson(inputs.split_keys) }}\n     runs-on: \n-      group: aws-g4dn-4xlarge-cache\n+      group: aws-g5-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "925701a4c10a12a3404e1723a0a2fd1d6fafbf60",
            "filename": ".github/workflows/doctests.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fdoctests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fdoctests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctests.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -15,7 +15,7 @@ jobs:\n   setup:\n     name: Setup\n     runs-on: \n-      group: aws-g4dn-4xlarge-cache\n+      group: aws-g5-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "b7818d798d5b3c54580e931bbf74e458bcc82570",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -107,9 +107,9 @@ jobs:\n         run: |\n           echo \"${{ inputs.machine_type }}\"\n \n-          if [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ inputs.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ inputs.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ inputs.machine_type }}"
        },
        {
            "sha": "e9f07cd591a41be6ac7c474332b4800589ae19f4",
            "filename": ".github/workflows/self-comment-ci.yml",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fself-comment-ci.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fself-comment-ci.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-comment-ci.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -185,7 +185,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.get-tests.outputs.models) }}\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n        group: '${{ matrix.machine_type }}'\n     container:\n@@ -239,9 +239,9 @@ jobs:\n         shell: bash\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -292,7 +292,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.get-tests.outputs.quantizations) }}\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -338,9 +338,9 @@ jobs:\n         shell: bash\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}"
        },
        {
            "sha": "3ce1ae55992eaed30840c4e5c3fb2f93d126bdaf",
            "filename": ".github/workflows/self-push.yml",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fself-push.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fself-push.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-push.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -31,7 +31,7 @@ jobs:\n     name: Setup\n     strategy:\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -131,7 +131,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g4dn-2xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -169,9 +169,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -244,7 +244,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -282,9 +282,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -357,7 +357,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -395,9 +395,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -467,7 +467,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -505,9 +505,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}"
        },
        {
            "sha": "7a2b7470e154c315cf243319ddeb20827dbddf12",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/539c6c2fa8abc9bb97218ac0b3c3d143ba800e05/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=539c6c2fa8abc9bb97218ac0b3c3d143ba800e05",
            "patch": "@@ -50,7 +50,7 @@ jobs:\n     name: Setup\n     strategy:\n       matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -128,7 +128,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n         slice_id: [0, 1]\n     uses: ./.github/workflows/model_jobs.yml\n     with:\n@@ -145,7 +145,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -179,9 +179,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -213,7 +213,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -247,9 +247,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -282,7 +282,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -344,9 +344,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}\n@@ -381,7 +381,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.quantization_matrix) }}\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -424,9 +424,9 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n           else\n             machine_type=${{ matrix.machine_type }}"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 38,
        "deletions": 38
    }
}