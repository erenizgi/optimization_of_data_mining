{
    "author": "qubvel",
    "message": "Add V-JEPA 2 (#38746)\n\n* adding model and conversion scripts\n\n* add imports to test vjepa conversion\n\n* fix imports and make conversion work\n\n* fix computation for short side\n\n* replace attention with library attention function\n\n* cleanup more attention classes\n\n* remove config overrides\n\n* add test cases, fix some of the failing ones\n\n* fix the model outputs\n\n* fix outputs of the model per review\n\n* fix too big model test case\n\n* fix styling __init__.py\n\n* fix initialization test\n\n* remove all asserts per review\n\n* update sorting unsorting logic as per feedback\n\n* remove is_video per review\n\n* remove another is_video segment\n\n* remove unwanted stuff\n\n* small fixes\n\n* add docstrings for the model\n\n* revert adding vjepa2 config here\n\n* update styling\n\n* add config docstrings (wip)\n\n* fix dpr issue\n\n* removed test failing issues\n\n* update styles\n\n* merge predictor configs into main config\n\n* remove processing code, add video processor\n\n* remove permute which is not necessary now\n\n* fix styles\n\n* updated vjepa2 to be in video_processing_auto\n\n* update comment for preprocessing\n\n* test integration test and fix the outputs\n\n* update test values, change test to look at repeated frames for a given image\n\n* add a simple video processing test\n\n* refactoring pixel_values_videos and upload ckpts to original\n\n* fix torch_fx test cases\n\n* remove unused config\n\n* add all config docstrings\n\n* add more integration tests\n\n* add basic doc\n\n* revert unwanted styling changes\n\n* working make fixup\n\n* Fix model_type in config\n\n* update attention implementation to fit new hf standards\n\n* fix the preprocessing logic, ensure it matches the original model\n\n* remove use_rope logic, cleanup\n\n* fix docstrings\n\n* Further cleanup, update doc\n\n* Fix model prefix\n\n* fix get_vision_features\n\n* VJEPA2Embeddings style refactor\n\n* nit, style comment\n\n* change modules default values\n\n* Only `str` activation in config\n\n* GradientCheckpointingLayer\n\n* fixup\n\n* fix conversion script\n\n* Remove return_dict\n\n* remove None return typehint\n\n* Refactor VJEPA2Layer, remove use_SiLU\n\n* Fix fx tests\n\n* dpr -> drop_path_rates\n\n* move *ModelOutput on top\n\n* format docs bit\n\n* update docs\n\n* update docs\n\n* update doc example\n\n* remove prune_heads from model\n\n* remove unused config params\n\n* refactor embed signature\n\n* Add vjepa to docs\n\n* Fix config docstring\n\n* update defaults\n\n* Update docs/source/en/model_doc/vjepa2.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/model_doc/vjepa2.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Fix import\n\n* Min refactoring\n\n* Update HUB_SOURCE and HUB_REPO in conversion script\n\n* Add missing headers\n\n* VJEPA -> V-JEPA in docs\n\n* Add image to doc\n\n* fix style\n\n* fix init weights\n\n* change checkpoint name in modeling tests\n\n---------\n\nCo-authored-by: Koustuv Sinha <koustuv.sinha@mail.mcgill.ca>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: Koustuv Sinha <koustuvsinha@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>",
    "sha": "84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
    "files": [
        {
            "sha": "7916dd9a06acfc77070ea41a59be8abdbfda7ff5",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -905,6 +905,8 @@\n     - sections:\n       - local: model_doc/timesformer\n         title: TimeSformer\n+      - local: model_doc/vjepa2\n+        title: V-JEPA 2\n       - local: model_doc/videomae\n         title: VideoMAE\n       - local: model_doc/vivit"
        },
        {
            "sha": "5ad02ae274baeaa14a9438514313ffe306e460c5",
            "filename": "docs/source/en/model_doc/vjepa2.md",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,82 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+# V-JEPA 2\n+\n+V-JEPA 2 is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vjepa.gif\" alt=\"drawing\" width=\"600\"/>\n+</div>\n+\n+You can find all original V-JEPA2 checkpoints under the [V-JEPA 2](https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6) collection.\n+\n+\n+This model was contributed by [koustuvs](https://huggingface.co/koustuvs), [yonigozlan](https://huggingface.co/yonigozlan) and [qubvel](https://huggingface.co/qubvel-hf). The original code can be found [here](https://github.com/facebookresearch/vjepa2).\n+\n+## Usage example\n+\n+The snippet below shows how to load the V-JEPA 2 model using the `AutoModel` class.\n+\n+```py\n+import torch\n+from torchcodec.decoders import VideoDecoder\n+import numpy as np\n+\n+processor = AutoVideoProcessor.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")\n+model = AutoModel.from_pretrained(\n+    \"facebook/vjepa2-vitl-fpc64-256\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+\n+video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/archery/-Qz25rXdMjE_000014_000024.mp4\"\n+\n+vr = VideoDecoder(video_url)\n+frame_idx = np.arange(0, 64) # choosing some frames. here, you can define more complex sampling strategy\n+video = vr.get_frames_at(indices=frame_idx).data  # T x C x H x W\n+video = processor(video, return_tensors=\"pt\").to(model.device)\n+outputs = model(**video)\n+\n+# V-JEPA 2 encoder outputs, same as calling `model.get_vision_features()`\n+encoder_outputs = outputs.last_hidden_state\n+\n+# V-JEPA 2 predictor outputs\n+predictor_outputs = outputs.predictor_output.last_hidden_state\n+```\n+\n+## VJEPA2Config\n+\n+[[autodoc]] VJEPA2Config\n+\n+## VJEPA2Model\n+\n+[[autodoc]] VJEPA2Model\n+    - forward\n+\n+## VJEPA2VideoProcessor\n+\n+[[autodoc]] VJEPA2VideoProcessor"
        },
        {
            "sha": "dea3c98c38e1ab5ee9a1bb30195c51adc70f09b6",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -323,6 +323,7 @@\n     from .vitpose_backbone import *\n     from .vits import *\n     from .vivit import *\n+    from .vjepa2 import *\n     from .wav2vec2 import *\n     from .wav2vec2_bert import *\n     from .wav2vec2_conformer import *"
        },
        {
            "sha": "fe8a889b5d30855a78816d2ab9cd73fb3f4883a1",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -365,6 +365,7 @@\n         (\"vitpose_backbone\", \"VitPoseBackboneConfig\"),\n         (\"vits\", \"VitsConfig\"),\n         (\"vivit\", \"VivitConfig\"),\n+        (\"vjepa2\", \"VJEPA2Config\"),\n         (\"wav2vec2\", \"Wav2Vec2Config\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2BertConfig\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2ConformerConfig\"),\n@@ -750,6 +751,7 @@\n         (\"vitpose_backbone\", \"ViTPoseBackbone\"),\n         (\"vits\", \"VITS\"),\n         (\"vivit\", \"ViViT\"),\n+        (\"vjepa2\", \"VJEPA2Model\"),\n         (\"wav2vec2\", \"Wav2Vec2\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2-BERT\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2-Conformer\"),"
        },
        {
            "sha": "bcd56483c113e67f910d2e5db409163dbc7916dd",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -336,6 +336,7 @@\n         (\"vitdet\", \"VitDetModel\"),\n         (\"vits\", \"VitsModel\"),\n         (\"vivit\", \"VivitModel\"),\n+        (\"vjepa2\", \"VJEPA2Model\"),\n         (\"wav2vec2\", \"Wav2Vec2Model\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2BertModel\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2ConformerModel\"),"
        },
        {
            "sha": "0f48dcdac7eb1e709c2a395db8520a358cb927ea",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -56,6 +56,7 @@\n             (\"qwen2_vl\", \"Qwen2VLVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n+            (\"vjepa2\", \"VJEPA2VideoProcessor\"),\n         ]\n     )\n "
        },
        {
            "sha": "f184d058a8e4fe8c88bc179d72903679461a1fe1",
            "filename": "src/transformers/models/vjepa2/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2F__init__.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_vjepa2 import *\n+    from .modeling_vjepa2 import *\n+    from .video_processing_vjepa2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4571b886021f1cf3c1ab989d19665e3db8436016",
            "filename": "src/transformers/models/vjepa2/configuration_vjepa2.py",
            "status": "added",
            "additions": 146,
            "deletions": 0,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconfiguration_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconfiguration_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconfiguration_vjepa2.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,146 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"VJEPA 2 model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class VJEPA2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`VJEPA2Model`]. It is used to instantiate an\n+    VJEPA2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the VJEPA2\n+    [facebook/vjepa2-vitl-fpc64-256](https://huggingface.co/facebook/vjepa2-vitl-fpc64-256) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        crop_size (`int`, *optional*, defaults to 256):\n+            Input resolution of the model\n+        frames_per_clip (`int`, *optional*, defaults to 64):\n+            The number of frames the model has been pretrained with. Does not impact inference.\n+        tubelet_size (`int`, *optional*, defaults to 2):\n+            The number of temporal frames used for a single rastor, check paper for more information.\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers\n+        in_chans (`int`, *optional*, defaults to 3):\n+            The number of input channels\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Encoder\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            The number of hidden layers\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            Stochastic depth rate per sample (when applied in the main path of residual layers).\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Ratio of the hidden size of the MLPs used in Encoder relative to the `hidden_size`.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for attentions.\n+            The dropout probability for all fully connected layers.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        pred_hidden_size (`int`, *optional*, defaults to 384):\n+            Dimensionality of the predictor layers\n+        pred_num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Predictor\n+        pred_num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Predictor\n+        pred_num_mask_tokens (`int`, *optional*, defaults to 10):\n+            Define the number of mask tokens to use in the Predictor\n+        pred_zero_init_mask_tokens (`bool`, *optional*, defaults to `True`):\n+            Initialize the mask tokens in the predictor with 0.\n+        pred_mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Ratio of the hidden size of the MLPs used in Predictor relative to the `pred_hidden_size`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import VJEPA2Config, VJEPA2Model\n+\n+    >>> # Initializing a VJEPA2 vjepa2-vitl-fpc64-256 style configuration\n+    >>> configuration = VJEPA2Config()\n+\n+    >>> # Initializing a model (with random weights) from the vjepa2-vitl-fpc64-256  style configuration\n+    >>> model = VJEPA2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"vjepa2\"\n+\n+    def __init__(\n+        self,\n+        patch_size=16,\n+        crop_size=256,\n+        frames_per_clip=64,\n+        tubelet_size=2,\n+        hidden_size=1024,\n+        in_chans=3,\n+        num_attention_heads=16,\n+        num_hidden_layers=24,\n+        drop_path_rate=0.0,\n+        mlp_ratio=4.0,\n+        layer_norm_eps=1e-6,\n+        qkv_bias=True,\n+        attention_probs_dropout_prob=0.0,\n+        hidden_act=\"gelu\",\n+        initializer_range=0.02,\n+        # predictor params\n+        pred_hidden_size=384,\n+        pred_num_attention_heads=12,\n+        pred_num_hidden_layers=12,\n+        pred_num_mask_tokens=10,\n+        pred_zero_init_mask_tokens=True,\n+        pred_mlp_ratio=4.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.crop_size = crop_size\n+        self.frames_per_clip = frames_per_clip\n+        self.patch_size = patch_size\n+        self.tubelet_size = tubelet_size\n+        self.hidden_size = hidden_size\n+        self.in_chans = in_chans\n+        self.num_attention_heads = num_attention_heads\n+        self.num_hidden_layers = num_hidden_layers\n+        self.drop_path_rate = drop_path_rate\n+        self.mlp_ratio = mlp_ratio\n+        self.layer_norm_eps = layer_norm_eps\n+        self.qkv_bias = qkv_bias\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.image_size = crop_size\n+        # predictor params\n+        self.pred_hidden_size = pred_hidden_size\n+        self.pred_num_attention_heads = pred_num_attention_heads\n+        self.pred_num_hidden_layers = pred_num_hidden_layers\n+        self.pred_num_mask_tokens = pred_num_mask_tokens\n+        self.pred_zero_init_mask_tokens = pred_zero_init_mask_tokens\n+        self.pred_mlp_ratio = pred_mlp_ratio\n+\n+\n+__all__ = [\"VJEPA2Config\"]"
        },
        {
            "sha": "527dbc35d99b317e3c698a2d92c5531ffd5cb8f1",
            "filename": "src/transformers/models/vjepa2/convert_vjepa2_to_hf.py",
            "status": "added",
            "additions": 346,
            "deletions": 0,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_to_hf.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,346 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import os\n+import tempfile\n+from pathlib import Path\n+\n+import numpy as np\n+import requests\n+import torch\n+from huggingface_hub import HfApi\n+from PIL import Image\n+\n+from transformers import VJEPA2Config, VJEPA2Model, VJEPA2VideoProcessor\n+from transformers.models.vjepa2.modeling_vjepa2 import apply_masks\n+\n+\n+HUB_REPO = \"https://github.com/facebookresearch/vjepa2\"\n+HUB_SOURCE = \"github\"\n+\n+HUB_MODELS = {\n+    \"vit_large\": \"facebook/vjepa2-vitl-fpc64-256\",\n+    \"vit_huge\": \"facebook/vjepa2-vith-fpc64-256\",\n+    \"vit_giant\": \"facebook/vjepa2-vitg-fpc64-256\",\n+    \"vit_giant_384\": \"facebook/vjepa2-vitg-fpc64-384\",\n+}\n+\n+S3_MODELS = {\n+    \"vit_large\": \"https://dl.fbaipublicfiles.com/vjepa2/vitl.pt\",\n+    \"vit_huge\": \"https://dl.fbaipublicfiles.com/vjepa2/vith.pt\",\n+    \"vit_giant\": \"https://dl.fbaipublicfiles.com/vjepa2/vitg.pt\",\n+    \"vit_giant_384\": \"https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt\",\n+}\n+\n+TOKEN = os.environ.get(\"HF_TOKEN\", None)\n+\n+\n+def get_vjepa2_config(model_name):\n+    # size of the architecture\n+    if model_name == \"vit_large\":\n+        return VJEPA2Config(\n+            crop_size=256,\n+            frames_per_clip=64,\n+            hidden_size=1024,\n+            num_attention_heads=16,\n+            num_hidden_layers=24,\n+            mlp_ratio=4,\n+            pred_hidden_size=384,\n+            pred_num_attention_heads=12,\n+            pred_num_hidden_layers=12,\n+            pred_num_mask_tokens=10,\n+        )\n+    elif model_name == \"vit_huge\":\n+        return VJEPA2Config(\n+            crop_size=256,\n+            frames_per_clip=64,\n+            hidden_size=1280,\n+            num_attention_heads=16,\n+            num_hidden_layers=32,\n+            mlp_ratio=4,\n+            pred_hidden_size=384,\n+            pred_num_attention_heads=12,\n+            pred_num_hidden_layers=12,\n+            pred_num_mask_tokens=10,\n+        )\n+    elif model_name == \"vit_giant\":\n+        return VJEPA2Config(\n+            crop_size=256,\n+            frames_per_clip=64,\n+            hidden_size=1408,\n+            num_attention_heads=22,\n+            num_hidden_layers=40,\n+            mlp_ratio=48 / 11,\n+            pred_hidden_size=384,\n+            pred_num_attention_heads=12,\n+            pred_num_hidden_layers=12,\n+            pred_num_mask_tokens=10,\n+        )\n+    elif model_name == \"vit_giant_384\":\n+        return VJEPA2Config(\n+            crop_size=384,\n+            frames_per_clip=64,\n+            hidden_size=1408,\n+            num_attention_heads=22,\n+            num_hidden_layers=40,\n+            mlp_ratio=48 / 11,\n+            pred_hidden_size=384,\n+            pred_num_attention_heads=12,\n+            pred_num_hidden_layers=12,\n+            pred_num_mask_tokens=10,\n+        )\n+    else:\n+        raise ValueError(\"Model not supported\")\n+\n+\n+def convert_encoder_keys(model_state_dict, og_encoder_state_dict, config):\n+    emb_dim = config.hidden_size\n+    for key, val in og_encoder_state_dict.copy().items():\n+        val = og_encoder_state_dict.pop(key)\n+        key = key.replace(\"module.backbone.\", \"\")\n+        if key.startswith(\"blocks.\"):\n+            key = key.replace(\"blocks.\", \"encoder.layer.\")\n+        if \"attn.\" in key:\n+            key = key.replace(\"attn.\", \"attention.\")\n+        if key == \"pos_embed\":\n+            key = \"encoder.embeddings.position_embeddings\"\n+        if \"patch_embed.\" in key:\n+            key = key.replace(\"patch_embed.\", \"encoder.embeddings.patch_embeddings.\")\n+        if key.startswith(\"norm.\"):\n+            key = key.replace(\"norm.\", \"encoder.layernorm.\")\n+        if \"qkv.\" in key:\n+            prefix, suffix = key.split(\"qkv\")\n+            if \"bias\" in suffix:\n+                q_e, k_e, v_e = (\n+                    val[0:emb_dim],\n+                    val[emb_dim : emb_dim * 2],\n+                    val[emb_dim * 2 :],\n+                )\n+            else:\n+                q_e, k_e, v_e = (\n+                    val[0:emb_dim, :],\n+                    val[emb_dim : emb_dim * 2, :],\n+                    val[emb_dim * 2 :, :],\n+                )\n+            og_encoder_state_dict[prefix + \"query\" + suffix] = q_e\n+            og_encoder_state_dict[prefix + \"key\" + suffix] = k_e\n+            og_encoder_state_dict[prefix + \"value\" + suffix] = v_e\n+        else:\n+            og_encoder_state_dict[key] = val\n+    return og_encoder_state_dict\n+\n+\n+def convert_predictor_keys(model_state_dict, og_predictor_state_dict, config):\n+    emb_dim = config.pred_hidden_size\n+    if \"predictor_pos_embed\" in og_predictor_state_dict:\n+        del og_predictor_state_dict[\"predictor_pos_embed\"]\n+    # update predictor weights\n+    mask_tokens = {}\n+    mask_token_keys_to_delete = []\n+    for key, val in og_predictor_state_dict.copy().items():\n+        val = og_predictor_state_dict.pop(key)\n+        key = key.replace(\"module.backbone.\", \"\")\n+        if key.startswith(\"predictor_blocks.\"):\n+            key = key.replace(\"predictor_blocks.\", \"predictor.layer.\")\n+        if \"attn.\" in key:\n+            key = key.replace(\"attn.\", \"attention.\")\n+        if key == \"predictor_pos_embed\":\n+            key = \"predictor.embeddings.position_embeddings\"\n+        if \"predictor_embed.\" in key:\n+            key = key.replace(\"predictor_embed.\", \"predictor.embeddings.predictor_embeddings.\")\n+        if \"mask_tokens.\" in key:\n+            mask_tokens[key.split(\"mask_tokens.\")[-1]] = val\n+            mask_token_keys_to_delete.append(key)\n+            # key = key.replace(\"mask_tokens.\", \"predictor.embeddings.mask_tokens.\")\n+        if key.startswith(\"predictor_norm.\"):\n+            key = key.replace(\"predictor_norm.\", \"predictor.layernorm.\")\n+        if key.startswith(\"predictor_proj.\"):\n+            key = key.replace(\"predictor_proj.\", \"predictor.proj.\")\n+        if \"qkv.\" in key:\n+            prefix, suffix = key.split(\"qkv\")\n+            if \"bias\" in suffix:\n+                q_e, k_e, v_e = (\n+                    val[0:emb_dim],\n+                    val[emb_dim : emb_dim * 2],\n+                    val[emb_dim * 2 :],\n+                )\n+            else:\n+                q_e, k_e, v_e = (\n+                    val[0:emb_dim, :],\n+                    val[emb_dim : emb_dim * 2, :],\n+                    val[emb_dim * 2 :, :],\n+                )\n+            og_predictor_state_dict[prefix + \"query\" + suffix] = q_e\n+            og_predictor_state_dict[prefix + \"key\" + suffix] = k_e\n+            og_predictor_state_dict[prefix + \"value\" + suffix] = v_e\n+        else:\n+            og_predictor_state_dict[key] = val\n+    mask_tokens = torch.stack([mask_tokens[f\"{i}\"] for i in range(len(mask_tokens))], dim=0)\n+    for k in mask_token_keys_to_delete:\n+        del og_predictor_state_dict[k]\n+    og_predictor_state_dict[\"predictor.embeddings.mask_tokens\"] = mask_tokens\n+    return og_predictor_state_dict\n+\n+\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+    return image\n+\n+\n+def upload_original_ckpts(model_name):\n+    hf_repo = HUB_MODELS[model_name]\n+    original_ckpt = S3_MODELS[model_name]\n+    print(f\"Uploading original checkpoint for vjepa2 {model_name} to {hf_repo}/original/\")\n+    with tempfile.NamedTemporaryFile() as fn:\n+        local_path = fn.name\n+        torch.hub.download_url_to_file(original_ckpt, local_path)\n+        api = HfApi()\n+        api.upload_file(\n+            repo_id=hf_repo,\n+            path_or_fileobj=local_path,\n+            path_in_repo=\"original/model.pth\",\n+            repo_type=\"model\",\n+            token=TOKEN,\n+        )\n+        print(\"Uploading complete\")\n+\n+\n+@torch.no_grad()\n+def convert_and_test_vjepa2_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=False):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our VJEPA2 structure.\n+    \"\"\"\n+    config = get_vjepa2_config(model_name)\n+\n+    # load original model from torch hub\n+    original_encoder, original_predictor = torch.hub.load(HUB_REPO, \"vjepa2_\" + model_name, source=HUB_SOURCE)\n+    original_encoder.eval()\n+    original_predictor.eval()\n+    original_preprocessor = torch.hub.load(\n+        HUB_REPO, \"vjepa2_preprocessor\", source=HUB_SOURCE, crop_size=config.crop_size\n+    )\n+\n+    # load state_dict of original model, remove and rename some keys\n+    encoder_state_dict = original_encoder.state_dict()\n+    decoder_state_dict = original_predictor.state_dict()\n+\n+    model = VJEPA2Model(config).eval()\n+    state_dict = model.state_dict()\n+\n+    og_encoder_sd = convert_encoder_keys(state_dict, encoder_state_dict, config)\n+    og_predictor_sd = convert_predictor_keys(state_dict, decoder_state_dict, config)\n+\n+    og_state_dict = og_encoder_sd\n+    og_state_dict.update(og_predictor_sd)\n+    model.load_state_dict(og_state_dict)\n+\n+    # load image\n+    image = prepare_img()\n+    image = torch.Tensor(np.array(image)).unsqueeze(0).permute(0, 3, 1, 2)\n+    print(\"Input shape: \", image.shape)\n+\n+    crop_size = config.crop_size\n+    processor = VJEPA2VideoProcessor(crop_size=crop_size)\n+    pr_out = processor(image, return_tensors=\"pt\")\n+    pixel_values_videos = pr_out.pixel_values_videos\n+    # run original preprocessor\n+    original_pixel_values = original_preprocessor(image)\n+    assert original_pixel_values[0].permute(1, 0, 2, 3).shape == pixel_values_videos[0].shape\n+    assert torch.allclose(original_pixel_values[0].permute(1, 0, 2, 3), pixel_values_videos[0], atol=1e-3)\n+\n+    with torch.no_grad():\n+        # reshape and move to gpu\n+        if pixel_values_videos.size(1) == 1:\n+            pixel_values_videos = pixel_values_videos.repeat(1, config.frames_per_clip, 1, 1, 1)\n+        # pixel_values_videos = pixel_values_videos.permute(0, 2, 1, 3, 4)  # B x C x T x H x W\n+        pixel_values_videos = pixel_values_videos.to(device=\"cuda\", dtype=torch.float32)\n+        original_encoder = original_encoder.to(device=\"cuda\", dtype=torch.float32)\n+        original_predictor = original_predictor.to(device=\"cuda\", dtype=torch.float32)\n+        model = model.to(device=\"cuda\", dtype=torch.float32)\n+        # forward\n+        original_encoder_outputs = original_encoder(pixel_values_videos.permute(0, 2, 1, 3, 4))\n+        B, N, _ = original_encoder_outputs.shape\n+        # test full mask\n+        context_mask = [torch.arange(N, device=pixel_values_videos.device).unsqueeze(0).repeat((B, 1))]\n+        predictor_mask = context_mask\n+        original_predictor_outputs = original_predictor(original_encoder_outputs, context_mask, predictor_mask)\n+        outputs = model(pixel_values_videos, context_mask=context_mask, target_mask=predictor_mask)\n+        assert torch.allclose(outputs.last_hidden_state, original_encoder_outputs, atol=1e-3)\n+        predictor_outputs = outputs.predictor_output\n+        assert torch.allclose(predictor_outputs.last_hidden_state, original_predictor_outputs, atol=1e-3)\n+        # test partial mask\n+        window_size = 256\n+        mask = torch.arange(N, device=pixel_values_videos.device).unsqueeze(0)\n+        context_mask = [mask[:, :window_size].repeat((B, 1))]\n+        predictor_mask = [mask[:, window_size : window_size * 2].repeat((B, 1))]\n+        original_predictor_outputs = original_predictor(\n+            apply_masks(original_encoder_outputs, context_mask),\n+            context_mask,\n+            predictor_mask,\n+        )\n+        outputs = model(pixel_values_videos, context_mask=context_mask, target_mask=predictor_mask)\n+        assert torch.allclose(outputs.last_hidden_state, original_encoder_outputs, atol=1e-3)\n+        predictor_outputs = outputs.predictor_output\n+        assert torch.allclose(predictor_outputs.last_hidden_state, original_predictor_outputs, atol=1e-3)\n+\n+    print(\"Looks ok!\")\n+\n+    if pytorch_dump_folder_path is not None:\n+        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+        print(f\"Saving model {model_name} to {pytorch_dump_folder_path}\")\n+        model.save_pretrained(pytorch_dump_folder_path)\n+        print(f\"Saving image processor to {pytorch_dump_folder_path}\")\n+        processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        name = HUB_MODELS[model_name]\n+        model.push_to_hub(name, private=True)\n+        processor.push_to_hub(name, private=True)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"vit_large\",\n+        type=str,\n+        choices=[\n+            \"vit_large\",\n+            \"vit_huge\",\n+            \"vit_giant\",\n+            \"vit_giant_384\",\n+        ],\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=None,\n+        type=str,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the ðŸ¤— hub.\",\n+    )\n+    parser.add_argument(\"--upload_original\", action=\"store_true\", help=\"upload the original checkpoint\")\n+\n+    args = parser.parse_args()\n+    convert_and_test_vjepa2_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)\n+    if args.upload_original:\n+        upload_original_ckpts(args.model_name)"
        },
        {
            "sha": "7a3a95b12982bcbf292ce98bb3fc7749217eb6f8",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "added",
            "additions": 903,
            "deletions": 0,
            "changes": 903,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,903 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, dataclass\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n+from .configuration_vjepa2 import VJEPA2Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@dataclass\n+class VJEPA2WithMaskedInputPredictorOutput(ModelOutput):\n+    \"\"\"\n+    VJEPA Predictor outputs that also contains the masked encoder outputs\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        masked_hidden_state (`torch.FloatTensor`), *optional*, returned when `context_mask` is provided which is applied on VJEPA2Encoder outputs\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        target_hidden_state (`torch.FloatTensor`), *optional*):\n+            Returned when `target_mask` is provided which is applied on VJEPA2Encoder outputs.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor\n+    masked_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    target_hidden_state: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+class VJEPA2WithMaskedInputModelOutput(ModelOutput):\n+    \"\"\"\n+    VJEPA outputs that also contains the masked encoder outputs\n+    Optionally contains the predictor outputs\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        masked_hidden_state (`torch.FloatTensor`), *optional*):\n+            Returned when `context_mask` is provided which is applied on VJEPA2Encoder outputs.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        predictor_output (`VJEPA2WithMaskedInputPredictorOutput`, *optional*):\n+            Returns the output from the Predictor module\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor\n+    masked_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    predictor_output: Optional[VJEPA2WithMaskedInputPredictorOutput] = None\n+\n+    def to_tuple(self):\n+        output = list(super().to_tuple())\n+        if isinstance(output[-1], VJEPA2WithMaskedInputPredictorOutput):\n+            output[-1] = output[-1].to_tuple()\n+        return tuple(output)\n+\n+\n+class VJEPA2PatchEmbeddings3D(nn.Module):\n+    \"\"\"\n+    Image to Patch Embedding\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: VJEPA2Config,\n+        hidden_size: int = 1024,\n+    ):\n+        super().__init__()\n+        self.patch_size = config.patch_size\n+        self.tubelet_size = config.tubelet_size\n+        self.hidden_size = hidden_size\n+\n+        self.proj = nn.Conv3d(\n+            in_channels=config.in_chans,\n+            out_channels=hidden_size,\n+            kernel_size=(config.tubelet_size, config.patch_size, config.patch_size),\n+            stride=(config.tubelet_size, config.patch_size, config.patch_size),\n+        )\n+\n+    @staticmethod\n+    def num_patches(config):\n+        return (\n+            (config.frames_per_clip // config.tubelet_size)\n+            * (config.crop_size // config.patch_size)\n+            * (config.crop_size // config.patch_size)\n+        )\n+\n+    def forward(self, pixel_values_videos: torch.Tensor) -> torch.Tensor:\n+        x = self.proj(pixel_values_videos).flatten(2).transpose(1, 2)\n+        return x\n+\n+\n+class VJEPA2Embeddings(nn.Module):\n+    \"\"\"\n+    Construct mask token, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: VJEPA2Config, hidden_size: int = 1024):\n+        super().__init__()\n+\n+        self.config = config\n+        self.hidden_size = hidden_size\n+        self.patch_embeddings = VJEPA2PatchEmbeddings3D(config, hidden_size=hidden_size)\n+\n+        self.num_patches = self.patch_embeddings.num_patches\n+        self.patch_size = config.patch_size\n+\n+    def forward(self, pixel_values_videos: torch.Tensor) -> torch.Tensor:\n+        num_frames = pixel_values_videos.shape[1]\n+\n+        # Swap `frames` and `channels` dims, the result is:\n+        # (batch_size, channels, num_frames, height, width)\n+        pixel_values_videos = pixel_values_videos.permute(0, 2, 1, 3, 4)\n+\n+        # For some cases, if the input vision (image/video) consists of num_frames < tubelet_size,\n+        # then embedding lookup fails. In these cases, we duplicate the frames.\n+        if num_frames < self.config.tubelet_size:\n+            pixel_values_videos = pixel_values_videos.repeat(1, 1, self.config.tubelet_size, 1, 1)\n+\n+        target_dtype = self.patch_embeddings.proj.weight.dtype\n+        pixel_values_videos = pixel_values_videos.to(dtype=target_dtype)\n+        embeddings = self.patch_embeddings(pixel_values_videos)\n+\n+        return embeddings\n+\n+\n+# Adapted from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def rotate_queries_or_keys(x, pos):\n+    B, num_heads, N, D = x.size()\n+\n+    # similar to inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+    # they are computing this every time. instead HF style is to compute the inv_freq once and store it\n+    # -- compute angle for each position\n+    omega = torch.arange(D // 2, dtype=x.dtype, device=x.device)\n+    omega /= D / 2.0\n+    omega = 1.0 / 10000**omega  # (D/2,)\n+    freq = torch.einsum(\"..., f -> ... f\", pos, omega)  # (..., N, D/2), outer product\n+\n+    # -- build rotation matrix and apply\n+    emb_sin = freq.sin()  # (..., N, D/2)\n+    emb_cos = freq.cos()  # (..., N, D/2)\n+\n+    emb_sin = emb_sin.squeeze(-1).repeat(1, 1, 1, 2)\n+    emb_cos = emb_cos.squeeze(-1).repeat(1, 1, 1, 2)\n+\n+    # --\n+    y = x.unflatten(-1, (-1, 2))\n+    y1, y2 = y.unbind(dim=-1)\n+\n+    y = torch.stack((-y2, y1), dim=-1)\n+    y = y.flatten(-2)\n+    return (x * emb_cos) + (y * emb_sin)\n+\n+\n+class VJEPA2RopeAttention(nn.Module):\n+    def __init__(\n+        self,\n+        config: VJEPA2Config,\n+        hidden_size: int = 1024,\n+        num_attention_heads: int = 16,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        if hidden_size % num_attention_heads != 0:\n+            raise ValueError(\n+                f\"The hidden size {(hidden_size,)} is not a multiple of the number of attention \"\n+                f\"heads {num_attention_heads}.\"\n+            )\n+\n+        self.attention_head_size = int(hidden_size / num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.key = nn.Linear(hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.value = nn.Linear(hidden_size, self.all_head_size, bias=config.qkv_bias)\n+\n+        self.proj = nn.Linear(hidden_size, hidden_size)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.dropout = nn.Dropout(self.dropout_prob)\n+\n+        self.grid_size = self.config.crop_size // self.config.patch_size\n+        self.grid_depth = self.config.frames_per_clip // self.config.tubelet_size\n+\n+        self.d_dim = int(2 * ((self.attention_head_size // 3) // 2))\n+        self.h_dim = int(2 * ((self.attention_head_size // 3) // 2))\n+        self.w_dim = int(2 * ((self.attention_head_size // 3) // 2))\n+\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (\n+            self.num_attention_heads,\n+            self.attention_head_size,\n+        )\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def _get_frame_pos(self, ids):\n+        tokens_per_frame = int(self.grid_size * self.grid_size)\n+        return ids // tokens_per_frame\n+\n+    def _get_height_pos(self, ids):\n+        # Remove frame component from ids\n+        tokens_per_frame = int(self.grid_size * self.grid_size)\n+        frame_ids = self._get_frame_pos(ids)\n+        ids = ids - tokens_per_frame * frame_ids\n+        # --\n+        tokens_per_row = self.grid_size\n+        return ids // tokens_per_row\n+\n+    def get_position_ids(self, x, masks=None):\n+        device = x.device\n+        token_size = x.size(1)\n+\n+        # Note: when masks is none, we use a 1d id instead of Bxnum_attention_heads mask,\n+        # as 1d vector is broadcasted to the correct shapes.\n+        if masks is not None:\n+            ids = masks.unsqueeze(1).repeat(1, self.num_attention_heads, 1)\n+        else:\n+            ids = torch.arange(token_size, device=device)\n+        # change to allow for extrapolation\n+        tokens_per_frame = int(self.grid_size * self.grid_size)\n+        frame_ids = self._get_frame_pos(ids)\n+        # --\n+        tokens_per_row = self.grid_size\n+        height_ids = self._get_height_pos(ids)\n+        # --\n+        # Remove frame component from ids (1st term) and height component (2nd term)\n+        width_ids = (ids - tokens_per_frame * frame_ids) - tokens_per_row * height_ids\n+        return frame_ids, height_ids, width_ids\n+\n+    def apply_rotary_embeddings(self, qk, pos_ids):\n+        d_mask, h_mask, w_mask = pos_ids\n+        s = 0\n+        qkd = rotate_queries_or_keys(qk[..., s : s + self.d_dim], pos=d_mask)\n+        s += self.d_dim\n+        qkh = rotate_queries_or_keys(qk[..., s : s + self.h_dim], pos=h_mask)\n+        s += self.h_dim\n+        qkw = rotate_queries_or_keys(qk[..., s : s + self.w_dim], pos=w_mask)\n+        s += self.w_dim\n+        # Combine rotated dimension\n+        if s < self.attention_head_size:\n+            qkr = qk[..., s:]\n+            qk = torch.cat([qkd, qkh, qkw, qkr], dim=-1)\n+        else:\n+            qk = torch.cat([qkd, qkh, qkw], dim=-1)\n+        return qk\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        position_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        head_mask: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        pos_ids = self.get_position_ids(hidden_states, masks=position_mask)\n+        key_layer = self.apply_rotary_embeddings(key_layer, pos_ids)\n+        query_layer = self.apply_rotary_embeddings(query_layer, pos_ids)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        context_layer, attention_probs = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n+        )\n+\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = self.proj(context_layer.reshape(new_context_layer_shape))\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n+\n+\n+# Adapted from transformers.models.beit.modeling_dinov2.drop_path\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n+    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n+    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n+    argument.\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+# Adapted from transformers.models.beit.modeling_beit.BeitDropPath\n+class VJEPA2DropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None):\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return \"p={}\".format(self.drop_prob)\n+\n+\n+class VJEPA2MLP(nn.Module):\n+    def __init__(self, config: VJEPA2Config, hidden_size: int = 1024, mlp_ratio: float = 4.0):\n+        super().__init__()\n+        in_features = out_features = hidden_size\n+        hidden_features = int(hidden_size * mlp_ratio)\n+        self.fc1 = nn.Linear(in_features, hidden_features, bias=True)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.fc2 = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.fc1(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.fc2(hidden_state)\n+        return hidden_state\n+\n+\n+class VJEPA2Layer(GradientCheckpointingLayer):\n+    \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n+\n+    def __init__(\n+        self,\n+        config: VJEPA2Config,\n+        drop_path_rate: float = 0.0,\n+        hidden_size: int = 1024,\n+        num_attention_heads: int = 16,\n+        mlp_ratio: float = 4.0,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_ratio = mlp_ratio\n+\n+        self.norm1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_eps)\n+        self.attention = VJEPA2RopeAttention(config, hidden_size, num_attention_heads)\n+        self.drop_path = VJEPA2DropPath(drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        self.norm2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = VJEPA2MLP(config, hidden_size=hidden_size, mlp_ratio=mlp_ratio)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, ...]:\n+        # Self-Attention\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states)\n+        self_attention_outputs = self.attention(\n+            hidden_states,\n+            position_mask=position_mask,  # position mask for context/target selection\n+            head_mask=head_mask,  # head mask is applied at F.scaled_dot_product_attention\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self_attention_outputs[0]\n+        hidden_states = self.drop_path(attention_output) + residual\n+\n+        # MLP\n+        residual = hidden_states\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.drop_path(hidden_states) + residual\n+\n+        # Add self attentions if we output attention weights\n+        outputs = self_attention_outputs[1:]\n+        outputs = (hidden_states,) + outputs\n+\n+        return outputs\n+\n+\n+class VJEPA2Encoder(nn.Module):\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.config = config\n+\n+        self.embeddings = VJEPA2Embeddings(config, hidden_size=config.hidden_size)\n+        drop_path_rates = [\n+            (config.drop_path_rate * i / (config.num_hidden_layers - 1) if config.num_hidden_layers > 1 else 0.0)\n+            for i in range(config.num_hidden_layers)\n+        ]\n+        self.layer = nn.ModuleList(\n+            [\n+                VJEPA2Layer(\n+                    config,\n+                    drop_path_rate=drop_path_rates[i],\n+                    hidden_size=config.hidden_size,\n+                    num_attention_heads=config.num_attention_heads,\n+                    mlp_ratio=config.mlp_ratio,\n+                )\n+                for i in range(config.num_hidden_layers)\n+            ]\n+        )\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values_videos: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> BaseModelOutput:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        hidden_states = self.embeddings(pixel_values_videos)\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+            layer_outputs = layer_module(hidden_states, None, layer_head_mask, output_attentions)\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        hidden_states = self.layernorm(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+def apply_masks(x, masks) -> torch.Tensor:\n+    \"\"\"\n+    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n+    :param masks: list of tensors of shape [B, K] containing indices of K patches in [N] to keep\n+    \"\"\"\n+    all_x = []\n+    for m in masks:\n+        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n+        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n+\n+    return torch.cat(all_x, dim=0)\n+\n+\n+class VJEPA2PredictorEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct mask token, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+\n+        self.config = config\n+        self.predictor_embeddings = nn.Linear(config.hidden_size, config.pred_hidden_size)\n+        self.num_mask_tokens = 0\n+        self.zero_init_mask_tokens = config.pred_zero_init_mask_tokens\n+        self.num_mask_tokens = config.pred_num_mask_tokens\n+        self.mask_tokens = nn.Parameter(torch.zeros(self.num_mask_tokens, 1, 1, config.pred_hidden_size))\n+\n+        self.patch_size = config.patch_size\n+        self.config = config\n+\n+    @staticmethod\n+    def num_patches(config):\n+        if config.frames_per_clip > 1:\n+            return (\n+                (config.frames_per_clip // config.tubelet_size)\n+                * (config.crop_size // config.patch_size)\n+                * (config.crop_size // config.patch_size)\n+            )\n+        else:\n+            return (config.crop_size // config.patch_size) * (config.crop_size // config.patch_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        context_mask: List[torch.Tensor],\n+        target_mask: List[torch.Tensor],\n+        mask_index: int = 1,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        hidden_states : encoder outputs (context)\n+        context_mask: tokens of the context (outputs from the encoder)\n+        target_mask: tokens to predict\n+        mask_index: index of the target mask to choose (useful for multiclip?)\n+        \"\"\"\n+\n+        B = hidden_states.size(0)\n+        context = self.predictor_embeddings(hidden_states)\n+\n+        # Make target tokens\n+        mask_index = mask_index % self.num_mask_tokens\n+        target = self.mask_tokens[mask_index]\n+\n+        # Note: this is problematic if the config isn't initialized with the right frames_per_clip value,\n+        # e.g. for scenarios if we want to run predictor for more tokens than in the config.\n+        # target = target.repeat(B, self.num_patches(self.config), 1)\n+        # Remedy: use the provided target mask to get the max patch num\n+        max_patch_num = target_mask[0].max() + 1  # one extra to include the last patch\n+        target = target.repeat(B, max_patch_num, 1)\n+        target = apply_masks(target, target_mask)\n+\n+        # Concatenate context & target tokens\n+        context = context.repeat(len(context_mask), 1, 1)\n+        embeddings = torch.cat([context, target], dim=1)\n+\n+        # Positions of context & target tokens\n+        cm = torch.cat(context_mask, dim=0)\n+        tm = torch.cat(target_mask, dim=0)\n+        masks = torch.cat([cm, tm], dim=1)\n+\n+        return embeddings, masks\n+\n+\n+class VJEPA2Predictor(nn.Module):\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.config = config\n+        self.gradient_checkpointing = False\n+        self.embeddings = VJEPA2PredictorEmbeddings(config)\n+        drop_path_rates = [\n+            (\n+                config.drop_path_rate * i / (config.pred_num_hidden_layers - 1)\n+                if config.pred_num_hidden_layers > 1\n+                else 0.0\n+            )\n+            for i in range(config.pred_num_hidden_layers)\n+        ]\n+        self.layer = nn.ModuleList(\n+            [\n+                VJEPA2Layer(\n+                    config,\n+                    drop_path_rate=drop_path_rates[i],\n+                    hidden_size=config.pred_hidden_size,\n+                    num_attention_heads=config.pred_num_attention_heads,\n+                    mlp_ratio=config.pred_mlp_ratio,\n+                )\n+                for i in range(config.pred_num_hidden_layers)\n+            ]\n+        )\n+        self.layernorm = nn.LayerNorm(config.pred_hidden_size, eps=config.layer_norm_eps)\n+        self.proj = nn.Linear(config.pred_hidden_size, config.hidden_size, bias=True)\n+\n+    def sort_tokens(self, hidden_states, position_masks, argsort, head_mask=None):\n+        position_masks = torch.gather(position_masks, dim=1, index=argsort)\n+        hidden_states = torch.gather(\n+            hidden_states,\n+            dim=1,\n+            index=argsort.unsqueeze(-1).expand(-1, -1, hidden_states.size(-1)),\n+        )\n+        if head_mask is not None and head_mask[0] is not None:\n+            head_mask = head_mask.permute(1, 0, 2, 3, 4)\n+            argsort_4d = (\n+                argsort.unsqueeze(1)\n+                .unsqueeze(1)\n+                .expand(-1, head_mask.size(1), head_mask.size(2), -1)\n+                .unsqueeze(-1)\n+                .expand(-1, -1, -1, -1, head_mask.size(-1))\n+            )\n+            head_mask = torch.gather(head_mask, dim=3, index=argsort_4d)\n+            argsort_5d = (\n+                argsort.unsqueeze(1)\n+                .unsqueeze(1)\n+                .unsqueeze(1)\n+                .expand(-1, head_mask.size(1), head_mask.size(2), head_mask.size(3), -1)\n+            )\n+            head_mask = torch.gather(head_mask, dim=4, index=argsort_5d)\n+            head_mask = head_mask.permute(1, 0, 2, 3, 4)\n+        return hidden_states, position_masks, head_mask\n+\n+    def unsort_tokens(self, hidden_states, argsort):\n+        reverse_argsort = torch.argsort(argsort, dim=1)\n+        hidden_states = torch.gather(\n+            hidden_states,\n+            dim=1,\n+            index=reverse_argsort.unsqueeze(-1).expand(-1, -1, hidden_states.size(-1)),\n+        )\n+        return hidden_states\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        encoder_hidden_states: torch.Tensor,\n+        context_mask: List[torch.Tensor],\n+        target_mask: List[torch.Tensor],\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> BaseModelOutput:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        # mask out the encoder hidden states\n+        # this is implemented here as in VJEPA training a separate encoder is used for target\n+        encoder_hidden_states = apply_masks(encoder_hidden_states, context_mask)\n+        _, N_ctxt, D = encoder_hidden_states.shape\n+        hidden_states, position_masks = self.embeddings(encoder_hidden_states, context_mask, target_mask)\n+\n+        # Put tokens in sorted order\n+        argsort = torch.argsort(position_masks, dim=1)  # [B, N]\n+        hidden_states, position_masks, head_mask = self.sort_tokens(hidden_states, position_masks, argsort, head_mask)\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+            layer_outputs = layer_module(hidden_states, position_masks, layer_head_mask, output_attentions)\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.layernorm(hidden_states)\n+        # unsort and extract the predicted tokens\n+        hidden_states = self.unsort_tokens(hidden_states, argsort)\n+        hidden_states = hidden_states[:, N_ctxt:]\n+        # projection\n+        hidden_states = self.proj(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+@auto_docstring\n+class VJEPA2PreTrainedModel(PreTrainedModel):\n+    config_class = VJEPA2Config\n+    base_model_prefix = \"vjepa2\"\n+    main_input_name = \"pixel_values_videos\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"VJEPA2Layer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+\n+    def _init_weights(\n+        self,\n+        module: Union[\n+            nn.Linear,\n+            nn.Conv2d,\n+            nn.LayerNorm,\n+            VJEPA2Embeddings,\n+            VJEPA2PredictorEmbeddings,\n+        ],\n+    ):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv3d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, VJEPA2PredictorEmbeddings):\n+            if not module.zero_init_mask_tokens:\n+                module.mask_token = nn.init.trunc_normal_(\n+                    module.mask_token.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.mask_token.dtype)\n+            else:\n+                module.mask_tokens.data.zero_()\n+\n+\n+def _convert_head_mask_to_5d(head_mask, num_hidden_layers):\n+    \"\"\"\n+    Inputs:\n+        - head_mask: bsz x seq_length x seq_length | None\n+    Returns\n+        - [num_hidden_layers x batch x num_heads x seq_length x seq_length] | [num_hidden_layers]\n+    \"\"\"\n+    if head_mask is not None:\n+        head_mask = head_mask.unsqueeze(1).unsqueeze(0)\n+        head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n+    else:\n+        head_mask = [None] * num_hidden_layers\n+    return head_mask\n+\n+\n+@auto_docstring\n+class VJEPA2Model(VJEPA2PreTrainedModel):\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.encoder = VJEPA2Encoder(config)\n+        self.predictor = VJEPA2Predictor(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> VJEPA2PatchEmbeddings3D:\n+        return self.encoder.embeddings.patch_embeddings\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values_videos: torch.Tensor,\n+        context_head_mask: Optional[torch.Tensor] = None,\n+        context_mask: Optional[List[torch.Tensor]] = None,\n+        target_head_mask: Optional[torch.Tensor] = None,\n+        target_mask: Optional[List[torch.Tensor]] = None,\n+        skip_predictor: bool = False,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> VJEPA2WithMaskedInputModelOutput:\n+        r\"\"\"\n+        pixel_values_videos (`torch.Tensor` with shape `[batch size x num_frames x num_channels x height x width]`, required):\n+            The input video pixels which is processed by VJEPA2VideoProcessor.\n+        context_head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n+            The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard) for the context.\n+        target_head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n+            The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard) for the target.\n+        context_mask (`torch.Tensor` with shape `[batch_size, patch_size, 1]`, *optional*):\n+            The mask position ids indicating which encoder output patches are going to be exposed to the predictor.\n+            By default, this mask is created as torch.arange(N).unsqueeze(0).repeat(B,1), indicating full context\n+            available to the predictor.\n+        target_mask (`torch.Tensor` with shape `[batch_size, patch_size, 1]`, *optional*):\n+            The mask position ids indicating which encoder output patches are going to be used as a prediction target\n+            for the predictor. By default, this mask is created as torch.arange(N).unsqueeze(0).repeat(B,1), indicating\n+            that the predictor should predict all encoder patches.\n+        skip_predictor (bool):\n+            flag to skip the predictor forward, useful if you just need the encoder outputs\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if pixel_values_videos is None:\n+            raise ValueError(\"You have to specify pixel_values_videos\")\n+\n+        # Prepare head mask if needed\n+        context_head_mask = _convert_head_mask_to_5d(context_head_mask, self.config.num_hidden_layers)\n+        target_head_mask = _convert_head_mask_to_5d(target_head_mask, self.config.pred_num_hidden_layers)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            pixel_values_videos=pixel_values_videos,\n+            head_mask=context_head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+        sequence_output = encoder_outputs.last_hidden_state\n+\n+        if context_mask is None and target_mask is None:\n+            B = pixel_values_videos.size(0)\n+            N = sequence_output.size(1)  # ensure we are using dynamic patch size\n+            context_mask = [torch.arange(N, device=pixel_values_videos.device).unsqueeze(0).repeat((B, 1))]\n+            target_mask = [torch.arange(N, device=pixel_values_videos.device).unsqueeze(0).repeat((B, 1))]\n+\n+        if not skip_predictor:\n+            predictor_outputs: BaseModelOutput = self.predictor(\n+                encoder_hidden_states=sequence_output,\n+                context_mask=context_mask,\n+                target_mask=target_mask,\n+                head_mask=target_head_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            predictor_output = VJEPA2WithMaskedInputPredictorOutput(\n+                last_hidden_state=predictor_outputs.last_hidden_state,\n+                target_hidden_state=apply_masks(sequence_output, target_mask),\n+                hidden_states=predictor_outputs.hidden_states,\n+                attentions=predictor_outputs.attentions,\n+            )\n+        else:\n+            predictor_output = None\n+\n+        encoder_output = VJEPA2WithMaskedInputModelOutput(\n+            last_hidden_state=sequence_output,\n+            masked_hidden_state=apply_masks(sequence_output, context_mask),\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+            predictor_output=predictor_output,\n+        )\n+\n+        return encoder_output\n+\n+    def get_vision_features(self, pixel_values_videos) -> torch.Tensor:\n+        encoder_output = self.forward(pixel_values_videos)\n+        return encoder_output.last_hidden_state\n+\n+\n+__all__ = [\"VJEPA2Model\", \"VJEPA2PreTrainedModel\"]"
        },
        {
            "sha": "2df100f7eb782293c40b8fcd8e19d9a1ff98fc94",
            "filename": "src/transformers/models/vjepa2/video_processing_vjepa2.py",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,59 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Video processor class for VJEPA2.\"\"\"\n+\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+)\n+from ...processing_utils import Unpack, VideosKwargs\n+from ...utils import is_vision_available\n+from ...utils.import_utils import requires\n+from ...video_processing_utils import BaseVideoProcessor\n+\n+\n+if is_vision_available():\n+    from ...image_utils import PILImageResampling\n+\n+\n+class VJEPA2VideoProcessorInitKwargs(VideosKwargs): ...\n+\n+\n+@requires(backends=(\"torchvision\",))\n+class VJEPA2VideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": int(256 * 256 / 224)}\n+    crop_size = 256\n+    do_resize = True\n+    do_rescale = True\n+    do_center_crop = True\n+    do_normalize = True\n+    valid_kwargs = VJEPA2VideoProcessorInitKwargs\n+    model_input_names = [\"pixel_values_videos\"]\n+\n+    def __init__(self, **kwargs: Unpack[VJEPA2VideoProcessorInitKwargs]):\n+        crop_size = kwargs.get(\"crop_size\", 256)\n+        if not isinstance(crop_size, int):\n+            if not isinstance(crop_size, dict) or \"height\" not in crop_size:\n+                raise ValueError(\"crop_size must be an integer or a dictionary with a 'height' key\")\n+            crop_size = crop_size[\"height\"]\n+        resize_size = int(crop_size * 256 / 224)\n+        kwargs[\"size\"] = {\"shortest_edge\": resize_size}\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"VJEPA2VideoProcessor\"]"
        },
        {
            "sha": "884dab4720ba1bb577cd0905d4695af6d4c52d79",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -166,6 +166,7 @@ def _generate_supported_model_class_names(\n     \"t5\",\n     \"trocr\",\n     \"vit\",\n+    \"vjepa2\",\n     \"xglm\",\n     \"wav2vec2\",\n     #    \"xlnet\","
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/vjepa2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/tests%2Fmodels%2Fvjepa2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/tests%2Fmodels%2Fvjepa2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvjepa2%2F__init__.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d"
        },
        {
            "sha": "8a4b55ad6aa133daf3743a5a09cfd759f1780fd1",
            "filename": "tests/models/vjepa2/test_modeling_vjepa2.py",
            "status": "added",
            "additions": 345,
            "deletions": 0,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -0,0 +1,345 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch V-JEPA2 model.\"\"\"\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import VJEPA2Config\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...test_video_processing_common import (\n+    prepare_video_inputs,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import VJEPA2Model\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoVideoProcessor\n+\n+VJEPA_HF_MODEL = \"facebook/vjepa2-vitl-fpc64-256\"\n+\n+\n+class VJEPA2ModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        image_size=16,\n+        patch_size=16,\n+        num_channels=3,\n+        hidden_size=32,\n+        num_hidden_layers=4,\n+        num_attention_heads=2,\n+        num_frames=2,\n+        mlp_ratio=1,\n+        pred_hidden_size=32,\n+        pred_num_attention_heads=2,\n+        pred_num_hidden_layers=2,\n+        pred_num_mask_tokens=10,\n+        is_training=False,\n+        attn_implementation=\"sdpa\",\n+        mask_ratio=0.5,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_frames = num_frames\n+        self.mlp_ratio = mlp_ratio\n+        self.pred_hidden_size = pred_hidden_size\n+        self.pred_num_attention_heads = pred_num_attention_heads\n+        self.pred_num_hidden_layers = pred_num_hidden_layers\n+        self.pred_num_mask_tokens = pred_num_mask_tokens\n+        self.attn_implementation = attn_implementation\n+        self.is_training = is_training\n+        self.mask_ratio = mask_ratio\n+\n+        num_patches = ((image_size // patch_size) ** 2) * (num_frames // 2)\n+        self.seq_length = num_patches\n+        self.num_masks = int(self.mask_ratio * self.seq_length)\n+        self.mask_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values_videos = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_frames,\n+                self.num_channels,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values_videos\n+\n+    def get_config(self):\n+        return VJEPA2Config(\n+            crop_size=self.image_size,\n+            frames_per_clip=self.num_frames,\n+            hidden_size=self.hidden_size,\n+            num_attention_heads=self.num_attention_heads,\n+            num_hidden_layers=self.num_hidden_layers,\n+            mlp_ratio=self.mlp_ratio,\n+            pred_hidden_size=self.pred_hidden_size,\n+            pred_num_attention_heads=self.pred_num_attention_heads,\n+            pred_num_hidden_layers=self.pred_num_hidden_layers,\n+            pred_num_mask_tokens=self.pred_num_mask_tokens,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values_videos):\n+        model = VJEPA2Model(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values_videos)\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (self.batch_size, self.seq_length, self.hidden_size),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values_videos,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values_videos\": pixel_values_videos}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class VJEPA2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as VJEPA2 does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    test_torch_exportable = True\n+\n+    all_model_classes = (VJEPA2Model,) if is_torch_available() else ()\n+\n+    fx_compatible = True\n+\n+    pipeline_model_mapping = {}\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = VJEPA2ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=VJEPA2Config, has_text_modality=False, hidden_size=37)\n+\n+    @is_flaky(max_attempts=3, description=\"`torch.nn.init.trunc_normal_` is flaky.\")\n+    def test_initialization(self):\n+        super().test_initialization()\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"VJEPA2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"VJEPA2 does not support feedforward chunking yet\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model = VJEPA2Model.from_pretrained(VJEPA_HF_MODEL)\n+        self.assertIsNotNone(model)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+def prepare_random_video(image_size=256):\n+    videos = prepare_video_inputs(\n+        batch_size=1,\n+        num_frames=16,\n+        num_channels=3,\n+        min_resolution=image_size,\n+        max_resolution=image_size,\n+        equal_resolution=True,\n+        return_tensors=\"torch\",\n+    )\n+    return videos\n+\n+\n+@require_torch\n+@require_vision\n+class VJEPA2ModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_video_processor(self):\n+        return AutoVideoProcessor.from_pretrained(VJEPA_HF_MODEL) if is_vision_available() else None\n+\n+    @slow\n+    def test_inference_image(self):\n+        model = VJEPA2Model.from_pretrained(VJEPA_HF_MODEL).to(torch_device)\n+\n+        video_processor = self.default_video_processor\n+        image = prepare_img()\n+        inputs = video_processor(torch.Tensor(np.array(image)), return_tensors=\"pt\").to(torch_device)\n+        pixel_values_videos = inputs.pixel_values_videos\n+        pixel_values_videos = pixel_values_videos.repeat(1, model.config.frames_per_clip, 1, 1, 1)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(pixel_values_videos)\n+\n+        # verify the last hidden states\n+        expected_shape = torch.Size((1, 8192, 1024))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.0061, -1.8365, 2.7343], [-2.5938, -2.7181, -0.1663], [-1.7993, -2.2430, -1.1388]],\n+            device=torch_device,\n+        )\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n+\n+    @slow\n+    def test_inference_video(self):\n+        model = VJEPA2Model.from_pretrained(VJEPA_HF_MODEL).to(torch_device)\n+\n+        video_processor = self.default_video_processor\n+        video = prepare_random_video()\n+        inputs = video_processor(video, return_tensors=\"pt\").to(torch_device)\n+        pixel_values_videos = inputs.pixel_values_videos\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(pixel_values_videos)\n+\n+        # verify the last hidden states\n+        expected_shape = torch.Size((1, 2048, 1024))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+    @slow\n+    def test_predictor_outputs(self):\n+        model = VJEPA2Model.from_pretrained(VJEPA_HF_MODEL).to(torch_device)\n+\n+        video_processor = self.default_video_processor\n+        video = prepare_random_video()\n+        inputs = video_processor(video, return_tensors=\"pt\").to(torch_device)\n+        pixel_values_videos = inputs.pixel_values_videos\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(pixel_values_videos)\n+\n+        # verify the last hidden states\n+        expected_shape = torch.Size((1, 2048, 1024))\n+        self.assertEqual(outputs.predictor_output.last_hidden_state.shape, expected_shape)\n+\n+    @slow\n+    def test_predictor_full_mask(self):\n+        model = VJEPA2Model.from_pretrained(VJEPA_HF_MODEL).to(torch_device)\n+\n+        video_processor = self.default_video_processor\n+        video = prepare_random_video()\n+        inputs = video_processor(video, return_tensors=\"pt\").to(torch_device)\n+        pixel_values_videos = inputs.pixel_values_videos\n+\n+        # forward pass\n+        with torch.no_grad():\n+            context_mask = [torch.arange(2048, device=pixel_values_videos.device).unsqueeze(0)]\n+            predictor_mask = context_mask\n+            outputs = model(pixel_values_videos, context_mask=context_mask, target_mask=predictor_mask)\n+\n+        # verify the last hidden states\n+        expected_shape = torch.Size((1, 2048, 1024))\n+        self.assertEqual(outputs.predictor_output.last_hidden_state.shape, expected_shape)\n+\n+    @slow\n+    def test_predictor_partial_mask(self):\n+        model = VJEPA2Model.from_pretrained(VJEPA_HF_MODEL).to(torch_device)\n+\n+        video_processor = self.default_video_processor\n+        video = prepare_random_video()\n+        inputs = video_processor(video, return_tensors=\"pt\").to(torch_device)\n+        pixel_values_videos = inputs.pixel_values_videos\n+\n+        num_patches = 2048\n+        num_masks = 100\n+        # forward pass\n+        with torch.no_grad():\n+            pos_ids = torch.arange(num_patches, device=pixel_values_videos.device)\n+            context_mask = [pos_ids[0 : num_patches - num_masks].unsqueeze(0)]\n+            predictor_mask = [pos_ids[num_patches - num_masks :].unsqueeze(0)]\n+            outputs = model(pixel_values_videos, context_mask=context_mask, target_mask=predictor_mask)\n+\n+        # verify the last hidden states\n+        expected_shape = torch.Size((1, num_masks, 1024))\n+        self.assertEqual(outputs.predictor_output.last_hidden_state.shape, expected_shape)"
        },
        {
            "sha": "4aab7a69ebd30355aa0b30f89c6d883c18b840e6",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84710a4291c3ca4d4b3d65d5a011ff83af243c1d/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=84710a4291c3ca4d4b3d65d5a011ff83af243c1d",
            "patch": "@@ -1301,6 +1301,7 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                     \"input_values\",\n                     \"inputs_embeds\",\n                     \"pixel_values\",\n+                    \"pixel_values_videos\",\n                     \"token_type_ids\",\n                     \"visual_feats\",\n                     \"visual_pos\","
        }
    ],
    "stats": {
        "total": 1919,
        "additions": 1919,
        "deletions": 0
    }
}