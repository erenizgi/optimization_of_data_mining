{
    "author": "zucchini-nlp",
    "message": "[Cohere2Vision] remove unused arg (#40103)\n\n* remove unused arg\n\n* remove the arg from test as well",
    "sha": "252364fd8e88f14fe588912b0992f3d016495355",
    "files": [
        {
            "sha": "dd15d86d815dd2e93e4eee49236d310506333d3f",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 26,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/252364fd8e88f14fe588912b0992f3d016495355/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252364fd8e88f14fe588912b0992f3d016495355/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=252364fd8e88f14fe588912b0992f3d016495355",
            "patch": "@@ -174,19 +174,13 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model\n \n-    def get_image_features(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        image_num_patches: torch.Tensor,\n-    ):\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            image_num_patches (`torch.Tensor` of shape `(num_images)`)\n-                Number of patches for each image.\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n@@ -227,7 +221,6 @@ def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         pixel_values: torch.FloatTensor = None,\n-        image_num_patches: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -236,18 +229,14 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, Cohere2VisionModelOutputWithPast]:\n-        r\"\"\"\n-        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n-            Number of patches per input image.\n-        \"\"\"\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            image_features = self.get_image_features(pixel_values, image_num_patches=image_num_patches)\n+            image_features = self.get_image_features(pixel_values)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             special_image_mask = self.get_placeholder_mask(\n                 input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n@@ -303,15 +292,8 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def get_image_features(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        image_num_patches: torch.Tensor,\n-    ):\n-        return self.model.get_image_features(\n-            pixel_values=pixel_values,\n-            image_num_patches=image_num_patches,\n-        )\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        return self.model.get_image_features(pixel_values=pixel_values)\n \n     # Make modules available throught conditional class for BC\n     @property\n@@ -332,7 +314,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        image_num_patches: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -345,8 +326,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Cohere2VisionCausalLMOutputWithPast]:\n         r\"\"\"\n-        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n-            Number of patches per input image.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -384,7 +363,6 @@ def forward(\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n-            image_num_patches=image_num_patches,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "9cbe84f26d31dba972657bd42ec74106e45c7c28",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 26,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/252364fd8e88f14fe588912b0992f3d016495355/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252364fd8e88f14fe588912b0992f3d016495355/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=252364fd8e88f14fe588912b0992f3d016495355",
            "patch": "@@ -94,19 +94,13 @@ class Cohere2VisionCausalLMOutputWithPast(AyaVisionCausalLMOutputWithPast):\n class Cohere2VisionModel(AyaVisionModel):\n     _checkpoint_conversion_mapping = {}\n \n-    def get_image_features(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        image_num_patches: torch.Tensor,\n-    ):\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            image_num_patches (`torch.Tensor` of shape `(num_images)`)\n-                Number of patches for each image.\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n@@ -123,7 +117,6 @@ def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         pixel_values: torch.FloatTensor = None,\n-        image_num_patches: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -132,18 +125,14 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, Cohere2VisionModelOutputWithPast]:\n-        r\"\"\"\n-        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n-            Number of patches per input image.\n-        \"\"\"\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            image_features = self.get_image_features(pixel_values, image_num_patches=image_num_patches)\n+            image_features = self.get_image_features(pixel_values)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             special_image_mask = self.get_placeholder_mask(\n                 input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n@@ -172,23 +161,15 @@ def forward(\n class Cohere2VisionForConditionalGeneration(AyaVisionForConditionalGeneration):\n     _checkpoint_conversion_mapping = {}\n \n-    def get_image_features(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        image_num_patches: torch.Tensor,\n-    ):\n-        return self.model.get_image_features(\n-            pixel_values=pixel_values,\n-            image_num_patches=image_num_patches,\n-        )\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        return self.model.get_image_features(pixel_values=pixel_values)\n \n     @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        image_num_patches: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -201,8 +182,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Cohere2VisionCausalLMOutputWithPast]:\n         r\"\"\"\n-        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n-            Number of patches per input image.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -240,7 +219,6 @@ def forward(\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n-            image_num_patches=image_num_patches,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "0db774dea659a1f914494773f6ee7287a1979464",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/252364fd8e88f14fe588912b0992f3d016495355/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252364fd8e88f14fe588912b0992f3d016495355/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=252364fd8e88f14fe588912b0992f3d016495355",
            "patch": "@@ -120,13 +120,12 @@ def get_config(self):\n     def prepare_config_and_inputs(self):\n         config = self.get_config()\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-        image_num_patches = torch.tensor([1] * self.batch_size).to(torch_device)\n \n-        return config, pixel_values, image_num_patches\n+        return config, pixel_values\n \n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, image_num_patches = config_and_inputs\n+        config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n@@ -136,7 +135,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"pixel_values\": pixel_values,\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"image_num_patches\": image_num_patches,\n         }\n         return config, inputs_dict\n "
        }
    ],
    "stats": {
        "total": 66,
        "additions": 10,
        "deletions": 56
    }
}