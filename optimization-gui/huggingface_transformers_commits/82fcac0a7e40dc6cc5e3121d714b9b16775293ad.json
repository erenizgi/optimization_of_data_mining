{
    "author": "MekkCyber",
    "message": "Adding logger.info about update_torch_dtype in some quantizers (#35046)\n\nadding logger.info",
    "sha": "82fcac0a7e40dc6cc5e3121d714b9b16775293ad",
    "files": [
        {
            "sha": "4dd818f6465df92df078113c3f92042ed29a1dbc",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/82fcac0a7e40dc6cc5e3121d714b9b16775293ad/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82fcac0a7e40dc6cc5e3121d714b9b16775293ad/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=82fcac0a7e40dc6cc5e3121d714b9b16775293ad",
            "patch": "@@ -87,6 +87,7 @@ def validate_environment(self, device_map, **kwargs):\n     def update_torch_dtype(self, torch_dtype):\n         if torch_dtype is None:\n             torch_dtype = torch.float16\n+            logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `torch_dtype` manually.\")\n         elif torch_dtype != torch.float16:\n             logger.warning(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\")\n         return torch_dtype"
        },
        {
            "sha": "bf5079435d63b2377d89e13928e68014d6013e92",
            "filename": "src/transformers/quantizers/quantizer_gptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/82fcac0a7e40dc6cc5e3121d714b9b16775293ad/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82fcac0a7e40dc6cc5e3121d714b9b16775293ad/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py?ref=82fcac0a7e40dc6cc5e3121d714b9b16775293ad",
            "patch": "@@ -64,6 +64,7 @@ def validate_environment(self, *args, **kwargs):\n     def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if torch_dtype is None:\n             torch_dtype = torch.float16\n+            logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `torch_dtype` manually.\")\n         elif torch_dtype != torch.float16:\n             logger.info(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency with GPTQ.\")\n         return torch_dtype"
        },
        {
            "sha": "bcc9c57dfa006d0444db68b28962d079e48c1ec5",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/82fcac0a7e40dc6cc5e3121d714b9b16775293ad/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82fcac0a7e40dc6cc5e3121d714b9b16775293ad/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=82fcac0a7e40dc6cc5e3121d714b9b16775293ad",
            "patch": "@@ -114,6 +114,9 @@ def update_torch_dtype(self, torch_dtype):\n                 torch_dtype = torch.bfloat16\n         if self.quantization_config.quant_type == \"int8_dynamic_activation_int8_weight\":\n             if torch_dtype is None:\n+                logger.info(\n+                    \"Setting torch_dtype to torch.float32 for int8_dynamic_activation_int8_weight quantization as no torch_dtype was specified in from_pretrained\"\n+                )\n                 # we need to set the torch_dtype, otherwise we have dtype mismatch when performing the quantized linear op\n                 torch_dtype = torch.float32\n         return torch_dtype"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}