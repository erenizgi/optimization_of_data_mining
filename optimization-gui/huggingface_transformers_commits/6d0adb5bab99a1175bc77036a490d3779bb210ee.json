{
    "author": "hmellor",
    "message": "Only call `torch.autocast` if it will have an effect (#42747)\n\n* Only call `torch.autocast` if it will have an effect\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* whitespace\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* fixup\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* fix copies\n\n---------\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "6d0adb5bab99a1175bc77036a490d3779bb210ee",
    "files": [
        {
            "sha": "0d7f330d7918267394cd51094ecd6395233679d5",
            "filename": "src/transformers/models/afmoe/modeling_afmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_afmoe import AfmoeConfig\n \n \n@@ -97,7 +97,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "b6d0aa94cfbc4363f38dceb5605a7406d08f65c1",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,7 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_apertus import ApertusConfig\n \n \n@@ -131,7 +131,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "900a530a34b71ef67cf140191665399f5365e4ac",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -43,7 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_arcee import ArceeConfig\n \n \n@@ -138,7 +138,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "a9ef9ea7cb28121c2dbf7440fda4192c4494fd8b",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ..auto import AutoModel\n from .configuration_aria import AriaConfig, AriaTextConfig\n \n@@ -675,7 +675,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "9a5790c1fb6ab4b87c2c536dd7d4a23303db0ecf",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import maybe_autocast\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -250,7 +251,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "76532cd109824cbbfd27e3bebbd65b29abb59a32",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,7 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_bitnet import BitNetConfig\n \n \n@@ -326,7 +326,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "b640a1b3db09fdb2005b1c8ce91629ce0ac88253",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_blt import (\n     BltConfig,\n     BltGlobalTransformerConfig,\n@@ -141,7 +141,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "c8c0812b00c12743d56245d24f5d6e1b281c582d",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from ..cohere2.modeling_cohere2 import rotate_half  # noqa: F401\n from ..llama.modeling_llama import LlamaRotaryEmbedding\n from ..mllama.modeling_mllama import (\n@@ -277,7 +277,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "04abf453c8153bfc59b30ccd37afa415cc3ef450",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,6 +38,7 @@\n     can_return_tuple,\n     logging,\n )\n+from ...utils.generic import maybe_autocast\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n \n \n@@ -122,7 +123,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "10c9c845695f97340f4e47b86b4449a52ae313a4",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,7 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_cohere import CohereConfig\n \n \n@@ -122,7 +122,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "f3bce4003e6a9907a515055a1e2ce459449473d8",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.generic import maybe_autocast\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaForCausalLM,\n@@ -75,7 +76,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "e398e363409b0f16763a77d2f2976521bb232849",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,7 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_cohere2 import Cohere2Config\n \n \n@@ -96,7 +96,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "2fa8c10bbc1fc093ff7102e32e37818d66de48a7",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.generic import maybe_autocast\n from ..cohere.modeling_cohere import (\n     CohereAttention,\n     CohereDecoderLayer,\n@@ -222,7 +223,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "d27a1cd196717d37a18068a227536bcdf1484172",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import maybe_autocast\n from ...utils.import_utils import is_torchdynamo_compiling\n from ..auto import AutoModel\n from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n@@ -174,7 +175,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "4bfa7842294eb3ac9d93cc5698add3d6e5fd751d",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_cwm import CwmConfig\n \n \n@@ -97,7 +97,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "459941267397ad321637c3b10d21e44e635513fb",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_dbrx import DbrxConfig\n \n \n@@ -97,7 +97,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "ecce72e40aa8bdea648fc553424d24edfe4ec831",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -34,6 +34,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.generic import maybe_autocast\n from .configuration_decision_transformer import DecisionTransformerConfig\n \n \n@@ -141,7 +142,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n             scale_factor /= float(self.layer_idx + 1)\n \n         # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n-        with torch.autocast(query.device.type, enabled=False):\n+        with maybe_autocast(query.device.type, enabled=False):\n             q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n             attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n             attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)"
        },
        {
            "sha": "c8de73dba2932f73876ce4d29fbf15377be2a9c7",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n \n@@ -223,7 +223,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.to(x.device) @ position_ids_expanded).transpose(1, 2)\n             freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # Convert to complex representation\n             freqs_cis = freqs_cis * self.attention_scaling"
        },
        {
            "sha": "d39c8db600368f63eb3a903b730fe0d20e0efe48",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -25,6 +25,7 @@\n from ...modeling_rope_utils import RopeParameters, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import logging\n+from ...utils.generic import maybe_autocast\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -303,7 +304,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.to(x.device) @ position_ids_expanded).transpose(1, 2)\n             freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # Convert to complex representation\n             freqs_cis = freqs_cis * self.attention_scaling"
        },
        {
            "sha": "0f6e93abefe299eb97e2ff009d36fa8f8940226d",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n \n@@ -110,7 +110,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "cf665cc3c47370a65db2eba78fbb80d8b6d306cb",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n from .generation_dia import DiaGenerationMixin\n \n@@ -184,7 +185,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "673925f7d8198471d121e823fb9a19df929aa621",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -46,7 +46,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_diffllama import DiffLlamaConfig\n \n \n@@ -125,7 +125,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "2827524127e9a4b5a687f1b3f72fb703b1cd4eb4",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,7 +36,7 @@\n from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.backbone_utils import BackboneMixin\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_dinov3_vit import DINOv3ViTConfig\n \n \n@@ -156,7 +156,7 @@ def forward(self, pixel_values: torch.Tensor) -> tuple[torch.Tensor, torch.Tenso\n         device = pixel_values.device\n         device_type = device.type if isinstance(device.type, str) and device.type != \"mps\" else \"cpu\"\n \n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             # Although we could precompute static patch_coords from image_size and patch_size in the config,\n             # the model was trained with random_scale, so it can process images of varying sizes.\n             # Therefore, it's better to compute patch_coords dynamically (with lru_cache)."
        },
        {
            "sha": "1f6848269966f96be882be4a6a81608c352dd5a7",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.backbone_utils import BackboneMixin\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_dinov3_vit import DINOv3ViTConfig\n \n \n@@ -163,7 +163,7 @@ def forward(self, pixel_values: torch.Tensor) -> tuple[torch.Tensor, torch.Tenso\n         device = pixel_values.device\n         device_type = device.type if isinstance(device.type, str) and device.type != \"mps\" else \"cpu\"\n \n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             # Although we could precompute static patch_coords from image_size and patch_size in the config,\n             # the model was trained with random_scale, so it can process images of varying sizes.\n             # Therefore, it's better to compute patch_coords dynamically (with lru_cache)."
        },
        {
            "sha": "ccc3911b8c9560766075289c075b3340e1b4c8da",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -42,7 +42,7 @@\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_doge import DogeConfig\n \n \n@@ -127,7 +127,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "c80090877c0f14eb7a8e9970dfd4623db0dcd05c",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_dots1 import Dots1Config\n \n \n@@ -119,7 +119,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "3e464d7c22fd37e4dc88ae1d26772c20b6ef9e95",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -33,7 +33,7 @@\n     can_return_tuple,\n     torch_int,\n )\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_efficientloftr import EfficientLoFTRConfig\n \n \n@@ -147,7 +147,7 @@ def forward(\n         embed_height = (feats_height - self.config.q_aggregation_kernel_size) // self.config.q_aggregation_stride + 1\n         embed_width = (feats_width - self.config.q_aggregation_kernel_size) // self.config.q_aggregation_stride + 1\n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             emb = compute_embeddings(self.inv_freq, embed_height, embed_width, self.config.hidden_size)\n             sin = emb.sin()\n             cos = emb.cos()"
        },
        {
            "sha": "3b3d7513e7eef249679a20fff16269fc1c234fff",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n \n@@ -1167,7 +1167,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "2a04ca804c49623fdc6d9ac5f9863683c7dba2c9",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_ernie4_5 import Ernie4_5Config\n \n \n@@ -95,7 +95,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "51c5d03a8a8b72c85f6fdec096a19d7652f94aac",
            "filename": "src/transformers/models/ernie4_5/modular_ernie4_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -18,6 +18,7 @@\n \n from ...modeling_rope_utils import dynamic_rope_update\n from ...utils import auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from ..glm.modeling_glm import rotate_half\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -36,7 +37,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "11c4ef6b0a72a8b6ac3cb9be8f232554dc011f96",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_ernie4_5_moe import Ernie4_5_MoeConfig\n \n \n@@ -135,7 +135,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling\n@@ -371,7 +371,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n             else \"cpu\"\n         )\n \n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             router_logits = F.linear(hidden_states.float(), self.weight)\n             router_logits = F.softmax(router_logits, dim=1, dtype=torch.float)\n             router_top_value, router_indices = torch.topk(self.moe_statics(router_logits), self.top_k, dim=-1)"
        },
        {
            "sha": "2c27e1ad81de830d5e2f60d1719eb981934fa71e",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -26,7 +26,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from ..ernie4_5.modeling_ernie4_5 import Ernie4_5RotaryEmbedding, apply_rotary_pos_emb, rotate_half  # noqa: F401\n from ..llama.modeling_llama import LlamaAttention, LlamaRMSNorm\n from ..mixtral.modeling_mixtral import (\n@@ -146,7 +146,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n             else \"cpu\"\n         )\n \n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             router_logits = F.linear(hidden_states.float(), self.weight)\n             router_logits = F.softmax(router_logits, dim=1, dtype=torch.float)\n             router_top_value, router_indices = torch.topk(self.moe_statics(router_logits), self.top_k, dim=-1)"
        },
        {
            "sha": "dc00161d478c1e4c70ba4467f2438bd04bb2a9d0",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -32,6 +32,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.generic import maybe_autocast\n from .modeling_esm import EsmModel, EsmPreTrainedModel\n from .openfold_utils import (\n     OFProtein,\n@@ -267,7 +268,7 @@ def __init__(self, c_in, eps=1e-5):\n     def forward(self, x):\n         d = x.dtype\n         if d is torch.bfloat16 and not is_deepspeed_initialized():\n-            with torch.autocast(device_type=\"cuda\", enabled=False):\n+            with maybe_autocast(device_type=\"cuda\", enabled=False):\n                 out = nn.functional.layer_norm(x, self.c_in, self.weight.to(dtype=d), self.bias.to(dtype=d), self.eps)\n         else:\n             out = nn.functional.layer_norm(x, self.c_in, self.weight, self.bias, self.eps)\n@@ -282,7 +283,7 @@ def softmax_no_cast(t: torch.Tensor, dim: int = -1) -> torch.Tensor:\n     \"\"\"\n     d = t.dtype\n     if d is torch.bfloat16 and not is_deepspeed_initialized():\n-        with torch.autocast(device_type=\"cuda\", enabled=False):\n+        with maybe_autocast(device_type=\"cuda\", enabled=False):\n             s = torch.nn.functional.softmax(t, dim=dim)\n     else:\n         s = torch.nn.functional.softmax(t, dim=dim)\n@@ -868,7 +869,7 @@ def forward(\n \n         device_type = a.device.type if a.device.type != \"mps\" else \"cpu\"\n         if is_fp16_enabled(device_type):\n-            with torch.autocast(device_type=device_type, enabled=False):\n+            with maybe_autocast(device_type=device_type, enabled=False):\n                 x = self._combine_projections(a.float(), b.float())\n         else:\n             x = self._combine_projections(a, b)\n@@ -1491,7 +1492,7 @@ def forward(\n         # [*, H, N_res, N_res]\n         device_type = q.device.type if q.device.type != \"mps\" else \"cpu\"\n         if is_fp16_enabled(device_type):\n-            with torch.autocast(device_type=device_type, enabled=False):\n+            with maybe_autocast(device_type=device_type, enabled=False):\n                 a = torch.matmul(\n                     permute_final_dims(q.float(), (1, 0, 2)),  # [*, H, N_res, C_hidden]\n                     permute_final_dims(k.float(), (1, 2, 0)),  # [*, H, C_hidden, N_res]"
        },
        {
            "sha": "597e64e2370de43e727aa7f1e38233c41a0ebce1",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,7 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n \n@@ -1019,7 +1019,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "40e557c5fa6b10fa50e4f28c66c76320c4b2b171",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from .configuration_exaone4 import Exaone4Config\n \n \n@@ -124,7 +125,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "32380fb4df54dc258234e871e80e6314c21db15e",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -48,6 +48,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.generic import maybe_autocast\n from .configuration_falcon import FalconConfig\n \n \n@@ -160,7 +161,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "a52daaab13526aaaff081d718dbafb7a2d37a4a4",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,6 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.generic import maybe_autocast\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n@@ -279,7 +280,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "dd57fe1994863087dfb7fbdc71f70c7e420bd8fe",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_flex_olmo import FlexOlmoConfig\n \n \n@@ -119,7 +119,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "d70e2be87843f566ccb6a8457f9eb2029abca676",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_gemma import GemmaConfig\n \n \n@@ -137,7 +137,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "c2c8c72f3605733c6e4eda36ad03377b96c7e187",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -42,7 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_gemma2 import Gemma2Config\n \n \n@@ -138,7 +138,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "3150dd1772b2080f9595f233f2c874148b403d73",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.generic import maybe_autocast\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -252,7 +253,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "60e24e26a0ca72359f26293dd02d00353b593ac8",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n \n@@ -214,7 +214,7 @@ def forward(self, x, position_ids, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * attention_scaling"
        },
        {
            "sha": "2bf81619f1e5de0d3890cdeac318fad9d27cb120",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import maybe_autocast\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -437,7 +438,7 @@ def forward(self, x, position_ids, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * attention_scaling"
        },
        {
            "sha": "8d95a5a367e132974326cbd85fcc0c0f3bca2159",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ..auto import AutoModel\n from .configuration_gemma3n import Gemma3nAudioConfig, Gemma3nConfig, Gemma3nTextConfig, Gemma3nVisionConfig\n \n@@ -1525,7 +1525,7 @@ def forward(self, x, position_ids, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * attention_scaling"
        },
        {
            "sha": "51072f7a67c6fad0d523bbaf013d5add9d165af2",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_glm import GlmConfig\n \n \n@@ -120,7 +120,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "3b1261a1ef27ec4726ca1c6c4ba44379e7e1c24b",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_glm4 import Glm4Config\n \n \n@@ -325,7 +325,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "4b34c598756971719a2fbcebb5b81e88e051a40a",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_glm4_moe import Glm4MoeConfig\n \n \n@@ -101,7 +101,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "78c4c95819944f647ea780a073ae8ec0f7a9e96b",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_glm4v import Glm4vConfig, Glm4vTextConfig, Glm4vVisionConfig\n \n \n@@ -446,7 +446,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "c6e832048cd52f7b2de729658c7859e237870a1a",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,7 +36,7 @@\n from ...processing_utils import Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ...video_utils import VideoInput\n from ..glm4.modeling_glm4 import Glm4MLP, Glm4RMSNorm, Glm4RotaryEmbedding, eager_attention_forward\n from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n@@ -511,7 +511,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "491ce7c3d3f48b7cd36a3b39b0c5f15b6e51aa6d",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_glm4v_moe import Glm4vMoeConfig, Glm4vMoeTextConfig, Glm4vMoeVisionConfig\n \n \n@@ -150,7 +150,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "de9f9530d55c10979db43a83e0575a8247ce291c",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,6 +45,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.generic import maybe_autocast\n from .configuration_gpt2 import GPT2Config\n \n \n@@ -150,7 +151,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n             scale_factor /= float(self.layer_idx + 1)\n \n         # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n-        with torch.autocast(query.device.type, enabled=False):\n+        with maybe_autocast(query.device.type, enabled=False):\n             q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n             attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n             attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)"
        },
        {
            "sha": "ad4805cb5633d444f8a1f64a7d10edf04ebb94cd",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_gpt_neox import GPTNeoXConfig\n \n \n@@ -107,7 +107,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "078a40c8de1f5a9b1f00cb0204f2bc079c3e5579",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_gpt_neox_japanese import GPTNeoXJapaneseConfig\n \n \n@@ -116,7 +117,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "91a8546e0c9ad16ea67b220fe3517143d45b9df0",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_gpt_oss import GptOssConfig\n \n \n@@ -236,7 +236,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = freqs\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "7a5227946fe537512fe0a6359a24d110b52574e8",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -34,7 +34,7 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n     LlamaPreTrainedModel,\n@@ -185,7 +185,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = freqs\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "66f9c38e783205a53565c0284cc01626d8f15d2e",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -36,7 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_granite import GraniteConfig\n \n \n@@ -376,7 +376,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "75d34787c5a1e56f506c0f15a062c860ec7b5b72",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.generic import can_return_tuple, check_model_inputs\n+from ...utils.generic import can_return_tuple, check_model_inputs, maybe_autocast\n from .configuration_granitemoe import GraniteMoeConfig\n \n \n@@ -119,7 +119,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "e96944c45f359dd75e159c88c59fef2e7c62debc",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n \n@@ -954,7 +954,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "24cbe16f2052f480a2a6d59be51d7bf363e27bbb",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.generic import can_return_tuple, check_model_inputs\n+from ...utils.generic import can_return_tuple, check_model_inputs, maybe_autocast\n from .configuration_granitemoeshared import GraniteMoeSharedConfig\n \n \n@@ -533,7 +533,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "e46e2f57fe05d4cd84a9b35f58ed0e91371a08cf",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_helium import HeliumConfig\n \n \n@@ -118,7 +118,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "6079a5aeedc3ecb8d666b5058920b944b710c934",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_hunyuan_v1_dense import HunYuanDenseV1Config\n \n \n@@ -359,7 +359,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "458bd0f5723d780d595fe5dbff579e2df9b156e2",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_hunyuan_v1_moe import HunYuanMoEV1Config\n \n \n@@ -452,7 +452,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "288d10748be061dc6e56b6e4fb82cbfee2b76bcc",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,6 +38,7 @@\n     logging,\n     torch_float,\n )\n+from ...utils.generic import maybe_autocast\n from .configuration_imagegpt import ImageGPTConfig\n \n \n@@ -150,7 +151,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n             scale_factor /= float(self.layer_idx + 1)\n \n         # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n-        with torch.autocast(query.device.type, enabled=False):\n+        with maybe_autocast(query.device.type, enabled=False):\n             q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n             attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n             attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)"
        },
        {
            "sha": "2a33f09bc6bd825e3a2ee752dd06ea423598a9cf",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_jetmoe import JetMoeConfig\n \n \n@@ -122,7 +122,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "7e8c379199579631b3baaf50e5ffd7cda9da39b1",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.generic import maybe_autocast\n from ..auto import AutoModel\n from .configuration_kyutai_speech_to_text import KyutaiSpeechToTextConfig\n \n@@ -315,7 +316,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "df66da296f7cfa2844bdebff7369a8f261c6a849",
            "filename": "src/transformers/models/lasr/modeling_lasr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_lasr import LasrCTCConfig, LasrEncoderConfig\n \n \n@@ -123,7 +123,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "d7d4716500ae26519ddf47cca11aa075768786b0",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -34,7 +34,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ...utils.import_utils import is_causal_conv1d_available, is_torchdynamo_compiling\n from .configuration_lfm2 import Lfm2Config\n \n@@ -122,7 +122,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "f90c3c207c39ce8f0dd75aafb78a64b7b6758f02",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ...utils.import_utils import is_causal_conv1d_available, is_torchdynamo_compiling\n from .configuration_lfm2_moe import Lfm2MoeConfig\n \n@@ -123,7 +123,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "54ea8e0da271cb7f08a884e0a9ee649549714954",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -42,7 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_llama import LlamaConfig\n \n \n@@ -126,7 +126,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "9d45fa3f6c08dcc6d98121c9c23cf19f7703cb0b",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_llama4 import Llama4Config, Llama4TextConfig\n \n \n@@ -228,7 +228,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.to(x.device) @ position_ids_expanded).transpose(1, 2)\n             freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # Convert to complex representation\n             freqs_cis = freqs_cis * self.attention_scaling"
        },
        {
            "sha": "2246fbc71756956d95553c8dd303371696779d37",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_longcat_flash import LongcatFlashConfig\n \n \n@@ -121,7 +121,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "be06fe36261c337bc6efd75060ef4a843dcb7261",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_mimi import MimiConfig\n \n \n@@ -559,7 +560,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "a7c53588800739159b286b6558a02f9043de4557",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,7 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_minimax import MiniMaxConfig\n \n \n@@ -310,7 +310,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "7b3aa97ff5ee4cce502d9428359d45a7783cc0db",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -27,7 +27,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_ministral import MinistralConfig\n \n \n@@ -328,7 +328,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "871d333f343002fd2abf96f2afb9b212ce1c7c9d",
            "filename": "src/transformers/models/ministral3/modeling_ministral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from .configuration_ministral3 import Ministral3Config\n \n \n@@ -333,7 +334,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "bcb8b007400d9d5b62e499047b77a9fc7cab2d87",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from .configuration_mistral import MistralConfig\n \n \n@@ -323,7 +324,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "e41870213a4af75a2acbb242154bc06690a36983",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -51,7 +51,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder\n+from ...utils.generic import OutputRecorder, maybe_autocast\n from .configuration_mixtral import MixtralConfig\n \n \n@@ -208,7 +208,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "d84e22c4d2be43fbf9197dee2d7cc7492d412122",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n \n@@ -781,7 +781,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "993d7e63461457b136cf3c8bab61fd6ebaea5a91",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,6 +45,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_flash_attn_2_available, logging\n+from ...utils.generic import maybe_autocast\n from ...utils.import_utils import is_triton_available\n from .configuration_modernbert import ModernBertConfig\n \n@@ -316,7 +317,7 @@ def forward(self, x, position_ids, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * attention_scaling"
        },
        {
            "sha": "066cd0e0539b81ceb3e9570901a1050f5383c3ad",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_modernbert_decoder import ModernBertDecoderConfig\n \n \n@@ -168,7 +168,7 @@ def forward(self, x, position_ids, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * attention_scaling"
        },
        {
            "sha": "3e9d585f6ea66f27359fef62f1dbaa352cefc8a9",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -46,6 +46,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from .configuration_moonshine import MoonshineConfig\n \n \n@@ -138,7 +139,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "a825cc53379ff690bcc0ffde2b8b087e278c525e",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import maybe_autocast\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n@@ -327,7 +328,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "f7d0d9261a9f1e660fc8c1eb795233037336eeb3",
            "filename": "src/transformers/models/nanochat/modeling_nanochat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_nanochat import NanoChatConfig\n \n \n@@ -113,7 +113,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "00a1d2f3ca18283466d6a7419c545223dcb272d9",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,6 +45,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_nemotron import NemotronConfig\n \n \n@@ -87,7 +88,7 @@ def forward(self, input: Tensor) -> Tensor:\n         args = _cast_if_autocast_enabled(\n             device_type, input, self.normalized_shape, self.weight + 1, self.bias, self.eps\n         )\n-        with torch.autocast(device_type=input.device.type, enabled=False):\n+        with maybe_autocast(device_type=input.device.type, enabled=False):\n             return F.layer_norm(*args)\n \n \n@@ -151,7 +152,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "8df79456c60ae7cf77128826717907b05f0d754a",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -42,7 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_olmo import OlmoConfig\n \n \n@@ -132,7 +132,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "5f60d4d57c57125ca52c8973bf43525824702dfa",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...utils import logging\n+from ...utils.generic import maybe_autocast\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -77,7 +78,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "6c0772e145568482550e4f1a1ea67dfc6a41a9ff",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -43,7 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_olmo2 import Olmo2Config\n \n \n@@ -124,7 +124,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "61c9096d7748d666294e697031f216483adfb813",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_olmo3 import Olmo3Config\n \n \n@@ -332,7 +332,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "4e41a75fea5f4c234ba669e7d0166ddd2a17e3c1",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_olmoe import OlmoeConfig\n \n \n@@ -116,7 +116,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "292638ee3f1419cb7737530519b97a7b5e6b28f3",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,6 +39,7 @@\n     requires_backends,\n )\n from ...utils.backbone_utils import load_backbone\n+from ...utils.generic import maybe_autocast\n from .configuration_oneformer import OneFormerConfig\n \n \n@@ -322,7 +323,7 @@ def forward(self, masks_queries_logits, class_queries_logits, mask_labels, class\n                 align_corners=False,\n             ).squeeze(1)\n \n-            with torch.autocast(device_type=\"cuda\", enabled=False):\n+            with maybe_autocast(device_type=\"cuda\", enabled=False):\n                 pred_mask = pred_mask.float()\n                 target_mask = target_mask.float()\n "
        },
        {
            "sha": "58909c249bb68e98fb249784732535b8af28effe",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n \n \n@@ -88,7 +88,7 @@ def forward(self, hidden_states: torch.Tensor):\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             sin = freqs.sin()\n             cos = freqs.cos()"
        },
        {
            "sha": "bdafbfc3507f923c64c5ccac08de47d0955cc633",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ..fastspeech2_conformer.modeling_fastspeech2_conformer import FastSpeech2ConformerConvolutionModule\n from ..llama.modeling_llama import LlamaAttention, eager_attention_forward\n from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n@@ -84,7 +84,7 @@ def forward(self, hidden_states: torch.Tensor):\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             sin = freqs.sin()\n             cos = freqs.cos()"
        },
        {
            "sha": "2eb827778e3730908008fc336061e71d800c61b1",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -46,6 +46,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_persimmon import PersimmonConfig\n \n \n@@ -118,7 +119,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "f58f3be5347c6229c0685533351ec74e6e885006",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -25,7 +25,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_phi import PhiConfig\n \n \n@@ -90,7 +90,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "aef959f4272045e43929682379f696faaae3947f",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from .configuration_phi3 import Phi3Config\n \n \n@@ -123,7 +124,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "b37888669ee9a53265d7e555b4427da3f0d68f60",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -47,7 +47,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, torch_int\n-from ...utils.generic import TransformersKwargs, check_model_inputs\n+from ...utils.generic import TransformersKwargs, check_model_inputs, maybe_autocast\n from .configuration_phi4_multimodal import Phi4MultimodalAudioConfig, Phi4MultimodalConfig, Phi4MultimodalVisionConfig\n \n \n@@ -602,7 +602,7 @@ def forward(\n \n         # Temporarily disable autocast to avoid issue on bf16 tensors\n         # Ref: https://github.com/pytorch/pytorch/issues/132715\n-        with torch.autocast(device_type=inputs_embeds.device.type, enabled=False):\n+        with maybe_autocast(device_type=inputs_embeds.device.type, enabled=False):\n             image_embeds = inputs_embeds.index_put(\n                 indices=positions_tuple, values=merged_img_set_tensor, accumulate=False\n             )\n@@ -1116,7 +1116,7 @@ def forward(\n         merged_audio_embeds = merged_audio_embeds.to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n         # Temporarily disable autocast to avoid issue on bf16 tensors\n         # Ref: https://github.com/pytorch/pytorch/issues/132715\n-        with torch.autocast(device_type=inputs_embeds.device.type, enabled=False):\n+        with maybe_autocast(device_type=inputs_embeds.device.type, enabled=False):\n             audio_embeds = inputs_embeds.index_put(\n                 indices=positions_tuple, values=merged_audio_embeds, accumulate=False\n             )\n@@ -1500,7 +1500,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "2f34aa160b25cc0604f62dd15133c6e4bd8f2a4a",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n-from ...utils.generic import TransformersKwargs, check_model_inputs\n+from ...utils.generic import TransformersKwargs, check_model_inputs, maybe_autocast\n from ..phi3.configuration_phi3 import Phi3Config\n from ..phi3.modeling_phi3 import (\n     Phi3DecoderLayer,\n@@ -844,7 +844,7 @@ def forward(\n \n         # Temporarily disable autocast to avoid issue on bf16 tensors\n         # Ref: https://github.com/pytorch/pytorch/issues/132715\n-        with torch.autocast(device_type=inputs_embeds.device.type, enabled=False):\n+        with maybe_autocast(device_type=inputs_embeds.device.type, enabled=False):\n             image_embeds = inputs_embeds.index_put(\n                 indices=positions_tuple, values=merged_img_set_tensor, accumulate=False\n             )\n@@ -1358,7 +1358,7 @@ def forward(\n         merged_audio_embeds = merged_audio_embeds.to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n         # Temporarily disable autocast to avoid issue on bf16 tensors\n         # Ref: https://github.com/pytorch/pytorch/issues/132715\n-        with torch.autocast(device_type=inputs_embeds.device.type, enabled=False):\n+        with maybe_autocast(device_type=inputs_embeds.device.type, enabled=False):\n             audio_embeds = inputs_embeds.index_put(\n                 indices=positions_tuple, values=merged_audio_embeds, accumulate=False\n             )"
        },
        {
            "sha": "60b0ba979eeba8a0d3551548ac741eeb9bcfe94f",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_phimoe import PhimoeConfig\n \n \n@@ -113,7 +113,7 @@ def forward(self, x, position_ids=None, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * mscale"
        },
        {
            "sha": "774190308c87a931f6c223dbd0c6c65746ed1d4d",
            "filename": "src/transformers/models/phimoe/modular_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -24,7 +24,7 @@\n     GenericForSequenceClassification,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...utils.generic import OutputRecorder\n+from ...utils.generic import OutputRecorder, maybe_autocast\n from ..llama.modeling_llama import LlamaAttention\n from ..mixtral.modeling_mixtral import (\n     MixtralDecoderLayer,\n@@ -74,7 +74,7 @@ def forward(self, x, position_ids=None, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * mscale"
        },
        {
            "sha": "64cf923b11cd013f4b76ae3da5a54b9cb87da404",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -28,6 +28,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_pixtral import PixtralVisionConfig\n \n \n@@ -125,7 +126,7 @@ def compute_default_rope_parameters(\n     def forward(self, x, position_ids):\n         freqs = self.inv_freq[position_ids]\n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             emb = freqs\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "a2d67f93a9be7f481acf1635af2b996967837216",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -27,7 +27,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_qwen2 import Qwen2Config\n \n \n@@ -103,7 +103,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "6564716176779144e384d9b39b1cd8c427149792",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -43,6 +43,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, check_torch_load_is_safe, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import maybe_autocast\n from ...utils.hub import cached_file\n from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_omni import (\n@@ -1291,7 +1292,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling\n@@ -2559,7 +2560,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "80a39b96b411d7add93d4131969c82e23e61ae73",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import maybe_autocast\n from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n@@ -547,7 +548,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "cc67efff6bd9ac62582286f11dc5e9ef0b978213",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -48,7 +48,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n@@ -129,7 +129,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "f99d6b1c4990d9dea410eb0ac7dfa97437d21495",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -44,6 +44,7 @@\n     can_return_tuple,\n     logging,\n )\n+from ...utils.generic import maybe_autocast\n from ..qwen2.modeling_qwen2 import (\n     Qwen2RMSNorm,\n )\n@@ -164,7 +165,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "bac3690fcef1caa399e49423aa4d03c0654ebf88",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -42,7 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_qwen3 import Qwen3Config\n \n \n@@ -139,7 +139,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "5700813e5dacc7912cfce5ef6f54866f0604d589",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -44,7 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n \n@@ -440,7 +440,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "f67d83282696964d4affd0ea0a925eebf65d2c8f",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -44,7 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_flash_linear_attention_available,\n@@ -233,7 +233,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "dc9c02824ebb9aab0470c877854280579491f106",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -50,7 +50,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs\n+from ...utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs, maybe_autocast\n from .configuration_qwen3_omni_moe import (\n     Qwen3OmniMoeAudioEncoderConfig,\n     Qwen3OmniMoeCode2WavConfig,\n@@ -1291,7 +1291,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n             emb = torch.cat((freqs, freqs), dim=-1)\n@@ -2516,7 +2516,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "41438c1a37b23b3c70491bfe52f73c8b84a5188b",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig\n \n \n@@ -337,7 +337,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n             emb = torch.cat((freqs, freqs), dim=-1)"
        },
        {
            "sha": "8bf9572baca1389a869fd0ecfc8978cb2c1a19f6",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -35,7 +35,7 @@\n from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from ...video_utils import VideoInput\n from ..llama.modeling_llama import LlamaRotaryEmbedding\n from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n@@ -389,7 +389,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n             emb = torch.cat((freqs, freqs), dim=-1)"
        },
        {
            "sha": "ca59338c1ae6ebc79e609a564e106e7b527a6ad0",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_qwen3_vl_moe import Qwen3VLMoeConfig, Qwen3VLMoeTextConfig, Qwen3VLMoeVisionConfig\n \n \n@@ -860,7 +860,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n             emb = torch.cat((freqs, freqs), dim=-1)"
        },
        {
            "sha": "2da5adcb27f06262cfe04ffd6804600d5c1001fd",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -31,6 +31,7 @@\n from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.generic import maybe_autocast\n from ...utils.import_utils import is_tracing\n from .configuration_recurrent_gemma import RecurrentGemmaConfig\n \n@@ -121,7 +122,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "c3dd76357e2e15e48a4ed5852adbcafe49cf2e0a",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_seed_oss import SeedOssConfig\n \n \n@@ -350,7 +350,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "2a9107bc67e99c307b597f2f334675446c36de5b",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -42,7 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_smollm3 import SmolLM3Config\n \n \n@@ -102,7 +102,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "2328585f34b197792f9cb7c340997cd507d2c3ff",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,6 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.generic import maybe_autocast\n from .configuration_stablelm import StableLmConfig\n \n \n@@ -117,7 +118,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "dcd71a74d47c296104becdf118c1419de43bb079",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -48,6 +48,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import maybe_autocast\n from .configuration_starcoder2 import Starcoder2Config\n \n \n@@ -327,7 +328,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "ca07431ec3fd21d7fbfa0534f2ba820f1f3a146c",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -45,7 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n \n \n@@ -147,7 +147,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "4f5cca0edec530d651072580d083ba0864cc8f63",
            "filename": "src/transformers/models/t5gemma2/modeling_t5gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -46,7 +46,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from ..auto import AutoModel\n from .configuration_t5gemma2 import T5Gemma2Config, T5Gemma2DecoderConfig, T5Gemma2EncoderConfig, T5Gemma2TextConfig\n \n@@ -162,7 +162,7 @@ def forward(self, x, position_ids, layer_type=None):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * attention_scaling"
        },
        {
            "sha": "95171197fdd8aa42fe63bb575336c1d047f6b8da",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -38,7 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_vaultgemma import VaultGemmaConfig\n \n \n@@ -336,7 +336,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "a6bec1ff724b290bd56217d25f65845e6ca6c924",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n+from ...utils.generic import maybe_autocast\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_zamba2 import Zamba2Config\n \n@@ -263,7 +264,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling"
        },
        {
            "sha": "fc3b04549414a546d27a96109be68bcacbe9b98a",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d0adb5bab99a1175bc77036a490d3779bb210ee/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=6d0adb5bab99a1175bc77036a490d3779bb210ee",
            "patch": "@@ -21,7 +21,7 @@\n import warnings\n from collections import OrderedDict, UserDict, defaultdict\n from collections.abc import Callable, Iterable, MutableMapping\n-from contextlib import AbstractContextManager, ExitStack\n+from contextlib import AbstractContextManager, ExitStack, nullcontext\n from dataclasses import dataclass, fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n@@ -42,6 +42,7 @@\n if is_torch_available():\n     # required for @can_return_tuple decorator to work with torchdynamo\n     import torch\n+    from torch.types import _dtype\n \n     from ..model_debugging_utils import model_addition_debugger_context\n \n@@ -154,6 +155,28 @@ def is_torch_dtype(x):\n     return isinstance(x, torch.dtype)\n \n \n+def maybe_autocast(\n+    device_type: str,\n+    dtype: Optional[\"_dtype\"] = None,\n+    enabled: bool = True,\n+    cache_enabled: Optional[bool] = None,\n+):\n+    \"\"\"\n+    Context manager that only autocasts if:\n+\n+    - `autocast` is already enabled in this context\n+    - Or this call to `maybe_autocast` has `enabled=True`\n+\n+    This prevents `autocast` being added to the graph when it is effectively a no-op.\n+    Which makes graph splitting in `torch.compile` more flexible as it removes the\n+    requirement that partition IDs be monotonically increasing.\n+    \"\"\"\n+    if torch.is_autocast_enabled(device_type) or enabled:\n+        return torch.autocast(device_type, dtype=dtype, enabled=enabled, cache_enabled=cache_enabled)\n+    else:\n+        return nullcontext()\n+\n+\n def _is_mlx(x):\n     import mlx.core as mx\n "
        }
    ],
    "stats": {
        "total": 491,
        "additions": 276,
        "deletions": 215
    }
}