{
    "author": "OmarManzoor",
    "message": "Add sdpa support for Albert (#32092)\n\n* Add sdpa support for Albert\r\n\r\n* [run_slow] albert\r\n\r\n* Add benchmarks and PR suggestion\r\n\r\n* Fix quality\r\n\r\n* Fix\r\n\r\n* [run_slow] albert",
    "sha": "03c12d0d63f781eda525900978f69eebc3df7478",
    "files": [
        {
            "sha": "d195203615de83640f6a1411f21dc7a207b245cb",
            "filename": "docs/source/en/model_doc/albert.md",
            "status": "modified",
            "additions": 46,
            "deletions": 1,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/03c12d0d63f781eda525900978f69eebc3df7478/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03c12d0d63f781eda525900978f69eebc3df7478/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md?ref=03c12d0d63f781eda525900978f69eebc3df7478",
            "patch": "@@ -59,7 +59,52 @@ This model was contributed by [lysandre](https://huggingface.co/lysandre). This\n - Layers are split in groups that share parameters (to save memory).\n Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n \n-\n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```\n+from transformers import AlbertModel\n+model = AlbertModel.from_pretrained(\"albert/albert-base-v1\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+On a local benchmark (GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16`, we saw the \n+following speedups during training and inference.\n+\n+#### Training for 100 iterations\n+\n+|batch_size|seq_len|Time per batch (eager - s)| Time per batch (sdpa - s)| Speedup (%)| Eager peak mem (MB)| sdpa peak mem (MB)| Mem saving (%)|\n+|----------|-------|--------------------------|--------------------------|------------|--------------------|-------------------|---------------|\n+|2         |256    |0.028                     |0.024                     |14.388      |358.411             |321.088            |11.624         |\n+|2         |512    |0.049                     |0.041                     |17.681      |753.458             |602.660            |25.022         |\n+|4         |256    |0.044                     |0.039                     |12.246      |679.534             |602.660            |12.756         |\n+|4         |512    |0.090                     |0.076                     |18.472      |1434.820            |1134.140           |26.512         |\n+|8         |256    |0.081                     |0.072                     |12.664      |1283.825            |1134.140           |13.198         |\n+|8         |512    |0.170                     |0.143                     |18.957      |2820.398            |2219.695           |27.062         |\n+\n+#### Inference with 50 batches\n+\n+|batch_size|seq_len|Per token latency eager (ms)|Per token latency SDPA (ms)|Speedup (%) |Mem eager (MB)|Mem BT (MB)|Mem saved (%)|\n+|----------|-------|----------------------------|---------------------------|------------|--------------|-----------|-------------|\n+|4         |128    |0.083                       |0.071                      |16.967      |48.319        |48.45      |-0.268       |\n+|4         |256    |0.148                       |0.127                      |16.37       |63.4          |63.922     |-0.817       |\n+|4         |512    |0.31                        |0.247                      |25.473      |110.092       |94.343     |16.693       |\n+|8         |128    |0.137                       |0.124                      |11.102      |63.4          |63.66      |-0.409       |\n+|8         |256    |0.271                       |0.231                      |17.271      |91.202        |92.246     |-1.132       |\n+|8         |512    |0.602                       |0.48                       |25.47       |186.159       |152.564    |22.021       |\n+|16        |128    |0.252                       |0.224                      |12.506      |91.202        |91.722     |-0.567       |\n+|16        |256    |0.526                       |0.448                      |17.604      |148.378       |150.467    |-1.388       |\n+|16        |512    |1.203                       |0.96                       |25.365      |338.293       |271.102    |24.784       |\n \n This model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT)."
        },
        {
            "sha": "dabe91a9d4cd2bc7e41c412a9a1ab67c6e26d520",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/03c12d0d63f781eda525900978f69eebc3df7478/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03c12d0d63f781eda525900978f69eebc3df7478/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=03c12d0d63f781eda525900978f69eebc3df7478",
            "patch": "@@ -201,6 +201,7 @@ FlashAttention is more memory efficient, meaning you can train on much larger se\n PyTorch's [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) can also call FlashAttention and memory-efficient attention kernels under the hood. SDPA support is currently being added natively in Transformers and is used by default for `torch>=2.1.1` when an implementation is available. You may also set `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n For now, Transformers supports SDPA inference and training for the following architectures:\n+* [Albert](https://huggingface.co/docs/transformers/model_doc/albert#transformers.AlbertModel)\n * [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)\n * [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n * [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)"
        },
        {
            "sha": "6ccb266009e193855892b608e34f29cb7ef9fe8d",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 91,
            "deletions": 7,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/03c12d0d63f781eda525900978f69eebc3df7478/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/03c12d0d63f781eda525900978f69eebc3df7478/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=03c12d0d63f781eda525900978f69eebc3df7478",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -34,7 +35,12 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import (\n+    apply_chunking_to_forward,\n+    find_pruneable_heads_and_indices,\n+    is_torch_greater_or_equal_than_2_2,\n+    prune_linear_layer,\n+)\n from ...utils import (\n     ModelOutput,\n     add_code_sample_docstrings,\n@@ -358,6 +364,66 @@ def forward(\n         return (layernormed_context_layer, attention_probs) if output_attentions else (layernormed_context_layer,)\n \n \n+class AlbertSdpaAttention(AlbertAttention):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = not is_torch_greater_or_equal_than_2_2\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            logger.warning(\n+                \"AlbertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the eager attention implementation, but specifying the eager implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(hidden_states, attention_mask, head_mask, output_attentions)\n+\n+        batch_size, seq_len, _ = hidden_states.size()\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        attention_output = torch.nn.functional.scaled_dot_product_attention(\n+            query=query_layer,\n+            key=key_layer,\n+            value=value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+        )\n+\n+        attention_output = attention_output.transpose(1, 2)\n+        attention_output = attention_output.reshape(batch_size, seq_len, self.all_head_size)\n+\n+        projected_context_layer = self.dense(attention_output)\n+        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n+        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n+        return (layernormed_context_layer,)\n+\n+\n+ALBERT_ATTENTION_CLASSES = {\n+    \"eager\": AlbertAttention,\n+    \"sdpa\": AlbertSdpaAttention,\n+}\n+\n+\n class AlbertLayer(nn.Module):\n     def __init__(self, config: AlbertConfig):\n         super().__init__()\n@@ -366,7 +432,7 @@ def __init__(self, config: AlbertConfig):\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n         self.full_layer_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.attention = AlbertAttention(config)\n+        self.attention = ALBERT_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)\n         self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)\n         self.activation = ACT2FN[config.hidden_act]\n@@ -496,6 +562,7 @@ class AlbertPreTrainedModel(PreTrainedModel):\n     config_class = AlbertConfig\n     load_tf_weights = load_tf_weights_in_albert\n     base_model_prefix = \"albert\"\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n@@ -635,6 +702,9 @@ def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n             self.pooler = None\n             self.pooler_activation = None\n \n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -708,14 +778,28 @@ def forward(\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n-        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n-        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n         embedding_output = self.embeddings(\n             input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n         )\n+\n+        use_sdpa_attention_mask = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        if use_sdpa_attention_mask:\n+            extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                attention_mask, embedding_output.dtype, tgt_len=seq_length\n+            )\n+        else:\n+            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n+            extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n+            extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n+\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n         encoder_outputs = self.encoder(\n             embedding_output,\n             extended_attention_mask,"
        }
    ],
    "stats": {
        "total": 146,
        "additions": 138,
        "deletions": 8
    }
}