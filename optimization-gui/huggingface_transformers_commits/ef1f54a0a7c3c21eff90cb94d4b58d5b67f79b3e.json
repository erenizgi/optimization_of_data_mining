{
    "author": "ji-huazhong",
    "message": "add bnb support for Ascend NPU (#31512)\n\n* add bnb support for Ascend NPU\n\n* delete comment",
    "sha": "ef1f54a0a7c3c21eff90cb94d4b58d5b67f79b3e",
    "files": [
        {
            "sha": "8657bda166254df45217519c5119f7eff3f1566e",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef1f54a0a7c3c21eff90cb94d4b58d5b67f79b3e/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef1f54a0a7c3c21eff90cb94d4b58d5b67f79b3e/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=ef1f54a0a7c3c21eff90cb94d4b58d5b67f79b3e",
            "patch": "@@ -29,6 +29,7 @@\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_torch_available,\n+    is_torch_npu_available,\n     is_torch_xpu_available,\n     logging,\n )\n@@ -171,6 +172,9 @@ def create_quantized_param(\n \n         old_value = getattr(module, tensor_name)\n \n+        # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\n+        if isinstance(target_device, int) and is_torch_npu_available():\n+            target_device = f\"npu:{target_device}\"\n         if tensor_name == \"bias\":\n             if param_value is None:\n                 new_value = old_value.to(target_device)\n@@ -259,11 +263,12 @@ def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             torch_dtype = torch.float16\n         return torch_dtype\n \n-    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.update_device_map\n     def update_device_map(self, device_map):\n         if device_map is None:\n             if torch.cuda.is_available():\n                 device_map = {\"\": torch.cuda.current_device()}\n+            elif is_torch_npu_available():\n+                device_map = {\"\": f\"npu:{torch.npu.current_device()}\"}\n             elif is_torch_xpu_available():\n                 device_map = {\"\": f\"xpu:{torch.xpu.current_device()}\"}\n             else:"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 6,
        "deletions": 1
    }
}