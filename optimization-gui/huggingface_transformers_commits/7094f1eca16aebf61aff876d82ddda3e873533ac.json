{
    "author": "AnMakc",
    "message": "Disable loss rounding in training stats log (#42104)\n\n* Disable loss rounding in training stats log\n\n* Format metrics in console logs.",
    "sha": "7094f1eca16aebf61aff876d82ddda3e873533ac",
    "files": [
        {
            "sha": "ed77e1c79384b034c6197341547c84ca1a073ca7",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7094f1eca16aebf61aff876d82ddda3e873533ac/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7094f1eca16aebf61aff876d82ddda3e873533ac/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=7094f1eca16aebf61aff876d82ddda3e873533ac",
            "patch": "@@ -3004,7 +3004,7 @@ def _maybe_log_save_evaluate(\n             # reset tr_loss to zero\n             tr_loss -= tr_loss\n \n-            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n+            logs[\"loss\"] = tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged)\n             if grad_norm is not None:\n                 logs[\"grad_norm\"] = grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm\n             if learning_rate is not None:"
        },
        {
            "sha": "6fe3470d0441cc59722b600e7a2f2e860bd7dcce",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7094f1eca16aebf61aff876d82ddda3e873533ac/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7094f1eca16aebf61aff876d82ddda3e873533ac/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=7094f1eca16aebf61aff876d82ddda3e873533ac",
            "patch": "@@ -665,12 +665,12 @@ def on_log(self, args, state, control, logs=None, **kwargs):\n                         f\"[String too long to display, length: {len(v)} > {self.max_str_len}. \"\n                         \"Consider increasing `max_str_len` if needed.]\"\n                     )\n+                if isinstance(v, float):\n+                    # Format floats for better readability\n+                    shallow_logs[k] = f\"{v:.4g}\"\n                 else:\n                     shallow_logs[k] = v\n             _ = shallow_logs.pop(\"total_flos\", None)\n-            # round numbers so that it looks better in console\n-            if \"epoch\" in shallow_logs:\n-                shallow_logs[\"epoch\"] = round(shallow_logs[\"epoch\"], 2)\n             self.training_bar.write(str(shallow_logs))\n \n     def on_train_end(self, args, state, control, **kwargs):\n@@ -687,6 +687,8 @@ class PrinterCallback(TrainerCallback):\n     def on_log(self, args, state, control, logs=None, **kwargs):\n         _ = logs.pop(\"total_flos\", None)\n         if state.is_local_process_zero:\n+            if logs is not None:\n+                logs = {k: (f\"{v:.4g}\" if isinstance(v, float) else v) for k, v in logs.items()}\n             print(logs)\n \n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}