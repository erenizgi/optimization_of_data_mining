{
    "author": "ydshieh",
    "message": "Update `MllamaForConditionalGenerationIntegrationTest` (#37750)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\n* fix 5\n\n* fix 6\n\n* trigger CI\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "0f7940bb3febf49dec9febb6573eb9830ec18c4e",
    "files": [
        {
            "sha": "34e4d4e48963cbc36743f06cfd651aca57055d94",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f7940bb3febf49dec9febb6573eb9830ec18c4e/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f7940bb3febf49dec9febb6573eb9830ec18c4e/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=0f7940bb3febf49dec9febb6573eb9830ec18c4e",
            "patch": "@@ -544,7 +544,7 @@ def test_11b_model_integration_generate(self):\n         expected_input_ids_all = Expectations(\n             {\n                 (\"xpu\", 3): torch.tensor([[128000, 128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n-                (\"cuda\", 7): torch.tensor([[128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n+                (\"cuda\", 7): torch.tensor([[128000, 128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n                 (\"cuda\", 8): torch.tensor([[128000, 128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n             }\n         )  # fmt: skip\n@@ -564,7 +564,7 @@ def test_11b_model_integration_generate(self):\n         expected_outputs = Expectations(\n                 {\n                     (\"xpu\", 3): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n-                    (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nI'm not a poet.\\\\nBut I'm a photographer.\\\\nAnd I'm a\",\n+                    (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nA dock in the lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n                     (\"cuda\", 8): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n                 }\n             )  # fmt: skip\n@@ -591,7 +591,7 @@ def test_11b_model_integration_generate_text_only(self):\n         expected_input_ids_all = Expectations(\n             {\n                 (\"xpu\", 3): [128000, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n-                (\"cuda\", 7): [128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n+                (\"cuda\", 7): [128000, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n                 (\"cuda\", 8): [128000, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n             }\n         )\n@@ -611,7 +611,7 @@ def test_11b_model_integration_generate_text_only(self):\n         expected_outputs = Expectations(\n                 {\n                     (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy tapestry\\n Threads of joy and sorrow\\nWeft of memories\",\n-                    (\"cuda\", 7): \"If I had to write a haiku about my life, I think it would be something like:\\n\\\"Life is a messy stream\\nTwists and turns, ups\",\n+                    (\"cuda\", 7): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 8): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                 }\n             )  # fmt: skip\n@@ -650,7 +650,7 @@ def test_11b_model_integration_forward(self):\n         expected_logits_all = Expectations(\n             {\n                 (\"xpu\", 3): torch.tensor([9.1562, 8.9141, 5.0664, 1.6855, 3.2324]),\n-                (\"cuda\", 7): torch.tensor([8.3594, 7.7148, 4.7266, 0.7803, 3.1504]),\n+                (\"cuda\", 7): torch.tensor([9.0781, 8.8750, 5.0781, 1.6221, 3.2207]),\n                 (\"cuda\", 8): torch.tensor([9.0703, 8.8750, 5.0781, 1.6279, 3.2207]),\n             }\n         )\n@@ -695,7 +695,7 @@ def test_11b_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n                 {\n                     (\"xpu\", 3): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n-                    (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nI'm not a poet.\\\\nBut I'm a photographer.\\\\nAnd I'm a\",\n+                    (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n                     (\"cuda\", 8): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n                  }\n             )  # fmt: skip\n@@ -712,7 +712,7 @@ def test_11b_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n                 {\n                     (\"xpu\", 3): \"This image shows\\nI'm not able to provide information on the person in this image. I can give you an idea of what's happening\",\n-                    (\"cuda\", 7): \"This image shows is a photograph of a stop sign in front of a Chinese archway. The stop sign is red with white letters and is\",\n+                    (\"cuda\", 7): \"This image shows\\nI'm not able to provide information on the person in this image. I can give you an idea of what's happening\",\n                     (\"cuda\", 8): \"This image shows\\nI'm not able to provide information on the person in this image. I can give you an idea of what's happening\",\n                 }\n             )  # fmt: skip"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}