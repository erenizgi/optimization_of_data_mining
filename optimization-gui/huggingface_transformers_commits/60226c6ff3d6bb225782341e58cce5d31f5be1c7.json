{
    "author": "Cyrilvallez",
    "message": "TP initialization module-by-module (#35996)\n\n* module-by-module loading!\n\n* Update modeling_utils.py\n\n* dtyle and comments\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update test\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update test_tp.py\n\n* Update test_tp.py\n\n* Update modeling_utils.py\n\n* re-trigger CIs\n\n* re-trigger CIs",
    "sha": "60226c6ff3d6bb225782341e58cce5d31f5be1c7",
    "files": [
        {
            "sha": "2918d3a44e7724e254ad7172429ae7d72efec157",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 73,
            "deletions": 20,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/60226c6ff3d6bb225782341e58cce5d31f5be1c7/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60226c6ff3d6bb225782341e58cce5d31f5be1c7/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=60226c6ff3d6bb225782341e58cce5d31f5be1c7",
            "patch": "@@ -787,6 +787,7 @@ def _load_state_dict_into_meta_model(\n     keep_in_fp32_modules=None,\n     unexpected_keys=None,  # passing `unexpected` for cleanup from quantization items\n     pretrained_model_name_or_path=None,  # for flagging the user when the model contains renamed keys\n+    device_mesh=None,\n ):\n     \"\"\"\n     This is somewhat similar to `_load_state_dict_into_model`, but deals with a model that has some or all of its\n@@ -796,6 +797,8 @@ def _load_state_dict_into_meta_model(\n     `start_prefix` is used for models which insert their name into model keys, e.g. `bert` in\n     `bert.pooler.dense.weight`\n \n+    It also initialize tensor parallelism for each module if needed.\n+\n     \"\"\"\n \n     # XXX: remaining features to implement to be fully compatible with _load_state_dict_into_model\n@@ -809,6 +812,12 @@ def _load_state_dict_into_meta_model(\n \n     is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n \n+    # we need this later to initialize tensor parallelism\n+    if device_mesh is not None:\n+        full_tp_plan = model.config.base_model_tp_plan\n+        for submodule in model.modules():\n+            full_tp_plan.update(getattr(submodule, \"_tp_plan\", {}))\n+\n     for param_name, param in state_dict.items():\n         if param_name not in expected_keys:\n             continue\n@@ -912,6 +921,37 @@ def _load_state_dict_into_meta_model(\n                 setattr(module, tensor_name, value)\n             # TODO: consider removing used param_parts from state_dict before return\n \n+        # In this case, let's parallelize the modules!\n+        if device_mesh is not None:\n+            # Immediate parent\n+            split_parent_module_name = param_name.split(\".\")[:-1]\n+            parent_module_name = \".\".join(split_parent_module_name)\n+            parent_module = model\n+            for name in split_parent_module_name:\n+                parent_module = getattr(parent_module, name)\n+\n+            # Check if we are part of the tp_plan\n+            current_module_plan = None\n+            for param, plan in full_tp_plan.items():\n+                # \"*\" are a placeholder for layer indices, so we replace them by \"[0-9]+\" in the regex pattern\n+                pattern = param.replace(\"*\", \"[0-9]+\")\n+                if re.search(pattern, parent_module_name):\n+                    current_module_plan = plan\n+                    break\n+\n+            # We can only apply the tp_plan after all parameters of the current module have been correctly initialized (e.g.\n+            # if we have bias, we need both `weights` and `bias` of a nn.Linear to be initialized)\n+            process_device = list(device_map.values())[0]\n+            all_module_parameters_initialized = all(\n+                m.device == process_device for m in parent_module.parameters(recurse=False)\n+            ) and all(m.device == process_device for m in parent_module.buffers(recurse=False))\n+            if current_module_plan is not None and all_module_parameters_initialized:\n+                torch.distributed.tensor.parallel.parallelize_module(\n+                    parent_module,\n+                    device_mesh=device_mesh,\n+                    parallelize_plan=translate_to_torch_parallel_style(current_module_plan),\n+                )\n+\n     return error_msgs, offload_index, state_dict_index\n \n \n@@ -3489,12 +3529,11 @@ def from_pretrained(\n             )\n \n         # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n-        # `device_map` pointing to the correct device. If we don't, torch will use the default device (index 0) for all\n-        # childs processes at parallelization time, resulting in excessive memory usage on device 0 and OOMs.\n-        # And temporarily setting the default device to current process rank result in the following error\n-        # `torch.distributed.DistBackendError: Attempt to perform collective on tensor not on device passed to init_process_group`\n-        tp_device = None\n+        # `device_map` pointing to the correct device\n+        device_mesh = None\n         if tp_plan is not None:\n+            if not is_torch_greater_or_equal(\"2.5\"):\n+                raise EnvironmentError(\"tensor parallel is only supported for `torch>=2.5`.\")\n             if not torch.distributed.is_initialized():\n                 raise ValueError(\"Tensor Parallel requires torch.distributed to be initialized first.\")\n \n@@ -3506,6 +3545,10 @@ def from_pretrained(\n             # This is the easiest way to dispatch to the current process device\n             device_map = tp_device\n \n+            # Assuming sharding the model onto the world\n+            world_size = torch.distributed.get_world_size()\n+            device_mesh = torch.distributed.init_device_mesh(tp_device.type, (world_size,))\n+\n         if is_fsdp_enabled():\n             low_cpu_mem_usage = True\n \n@@ -3600,7 +3643,7 @@ def from_pretrained(\n             if low_cpu_mem_usage is None:\n                 low_cpu_mem_usage = True\n             elif not low_cpu_mem_usage:\n-                raise ValueError(\"Passing along a `device_map` requires `low_cpu_mem_usage=True`\")\n+                raise ValueError(\"Passing along a `device_map` or a `tp_plan` requires `low_cpu_mem_usage=True`\")\n \n         if low_cpu_mem_usage:\n             if is_deepspeed_zero3_enabled():\n@@ -3609,7 +3652,7 @@ def from_pretrained(\n                 )\n             elif not is_accelerate_available():\n                 raise ImportError(\n-                    f\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n+                    f\"Using `low_cpu_mem_usage=True`, a `device_map` or a `tp_plan` requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n                 )\n \n         # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\n@@ -4186,6 +4229,9 @@ def from_pretrained(\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)\n \n+        if device_mesh is not None and not model.supports_tp_plan:\n+            raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n+\n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n \n@@ -4336,6 +4382,7 @@ def from_pretrained(\n                 keep_in_fp32_modules=keep_in_fp32_modules,\n                 gguf_path=gguf_path,\n                 weights_only=weights_only,\n+                device_mesh=device_mesh,\n             )\n \n         # make sure token embedding weights are still tied if needed\n@@ -4370,8 +4417,9 @@ def from_pretrained(\n                 )\n                 pass\n \n-        # Dispatch model with hooks on all devices if necessary\n-        if device_map is not None:\n+        # Dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n+        # harm performances)\n+        if device_map is not None and device_mesh is None:\n             device_map_kwargs = {\n                 \"device_map\": device_map,\n                 \"offload_dir\": offload_folder,\n@@ -4398,6 +4446,13 @@ def from_pretrained(\n             if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n                 dispatch_model(model, **device_map_kwargs)\n \n+        # This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\n+        # not part of the state_dict (persistent=False)\n+        if device_mesh is not None:\n+            for buffer in model.buffers():\n+                if buffer.device != tp_device:\n+                    buffer.data = buffer.to(tp_device)\n+\n         if hf_quantizer is not None:\n             hf_quantizer.postprocess_model(model, config=config)\n             model.hf_quantizer = hf_quantizer\n@@ -4420,16 +4475,6 @@ def from_pretrained(\n                 }\n             return model, loading_info\n \n-        if tp_plan is not None:\n-            assert tp_device is not None, \"tp_device not set!\"\n-            if not model.supports_tp_plan:\n-                raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n-            # Assuming sharding the model onto the world\n-            world_size = torch.distributed.get_world_size()\n-            device_mesh = torch.distributed.init_device_mesh(tp_device.type, (world_size,))\n-            # Apply Tensor Parallelism\n-            model.tensor_parallel(device_mesh)\n-\n         return model\n \n     @staticmethod\n@@ -4523,6 +4568,7 @@ def _load_pretrained_model(\n         keep_in_fp32_modules=None,\n         gguf_path=None,\n         weights_only=True,\n+        device_mesh=None,\n     ):\n         is_safetensors = False\n         is_quantized = hf_quantizer is not None\n@@ -4822,6 +4868,7 @@ def _find_mismatched_keys(\n                     is_safetensors=is_safetensors,\n                     keep_in_fp32_modules=keep_in_fp32_modules,\n                     unexpected_keys=unexpected_keys,\n+                    device_mesh=device_mesh,\n                 )\n             else:\n                 # Sharded checkpoint or whole but low_cpu_mem_usage==True\n@@ -4911,6 +4958,7 @@ def _find_mismatched_keys(\n                             is_safetensors=is_safetensors,\n                             keep_in_fp32_modules=keep_in_fp32_modules,\n                             unexpected_keys=unexpected_keys,\n+                            device_mesh=device_mesh,\n                         )\n                         error_msgs += new_error_msgs\n                 else:\n@@ -5188,7 +5236,12 @@ def supports_tp_plan(self):\n \n     def tensor_parallel(self, device_mesh):\n         \"\"\"\n-        Tensor parallelize the model across the given device mesh.\n+        Tensor parallelize the model across the given device mesh. This function is a helper to be called after the model\n+        was already loaded in memory, note however that this means that each process will first initialize the whole model,\n+        then parallelize it accross devices. Thus there is a huge waste of GPU memory, and this can lead to OOM at loading time.\n+\n+        Calling `from_pretrained(..., tp_plan=\"auto\")` is prefered, and will parallelize module-by-module during initialization,\n+        so that the expected per-device memory spike at loading time is not larger than the final model size on each device.\n \n         Args:\n             device_mesh (`torch.distributed.DeviceMesh`):"
        },
        {
            "sha": "6a564e552428381cd4881203f5c220a462f54485",
            "filename": "tests/tp/test_tp.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/60226c6ff3d6bb225782341e58cce5d31f5be1c7/tests%2Ftp%2Ftest_tp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60226c6ff3d6bb225782341e58cce5d31f5be1c7/tests%2Ftp%2Ftest_tp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftp%2Ftest_tp.py?ref=60226c6ff3d6bb225782341e58cce5d31f5be1c7",
            "patch": "@@ -81,17 +81,13 @@ def test_loading_memory_consumption(self):\n             model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, tp_plan=\"auto\")\n             torch.distributed.barrier()\n \n-            # The expected full model memory footprint\n-            expected_model_memory = 16\n+            # The expected model memory footprint. We add 1 as not all the modules are split (e.g. the embeddings)\n+            expected_model_memory_per_device = (16 / world_size) + 1\n             overhead_factor = 1.2\n \n-            # Assert we did not use more than the full model expected memory (with some overhead)\n-            if not torch.cuda.max_memory_allocated(device) / 1024**3 < expected_model_memory * overhead_factor:\n-                raise ValueError(\"Loading the model used more than the full model size\")\n-\n-            # Assert we correctly handled the sharding between devices\n-            if not torch.cuda.memory_allocated(device) / 1024**3 < (expected_model_memory / world_size) * overhead_factor:\n-                raise ValueError(\"Each model shard is larger than what is expected.\")\n+            # Check that we do not use more than the expected sharded size during initialization\n+            if torch.cuda.max_memory_allocated(device) / 1024**3 > expected_model_memory_per_device * overhead_factor:\n+                raise ValueError(\"Loading the model used more than the expected fraction of model size per device\")\n \n             torch.distributed.barrier()\n             torch.distributed.destroy_process_group()"
        }
    ],
    "stats": {
        "total": 107,
        "additions": 78,
        "deletions": 29
    }
}