{
    "author": "McPatate",
    "message": "feat: add cache retention for requests (#38446)\n\n* feat: add cache retention for requests\n\n* fix: propagate `manual_eviction` param & refactor `finish_request`\n\n`finish_request` now only takes `request_id: str` as an input rather\nthan the full `RequestState`, which was not needed and simplifies\ncalling from `ContinuousBatchingManager::evict_request_from_cache`\n\n* refactor: pop req from `active_requests`\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "8010f3cf616c98964ec38dde78de54b25f2a64af",
    "files": [
        {
            "sha": "0165742cef365e364dc1415ba38642185ebd54fa",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 55,
            "deletions": 19,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/8010f3cf616c98964ec38dde78de54b25f2a64af/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8010f3cf616c98964ec38dde78de54b25f2a64af/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=8010f3cf616c98964ec38dde78de54b25f2a64af",
            "patch": "@@ -305,11 +305,12 @@ class Scheduler(ABC):\n     It is expected that cache allocation and scheduling logic will be implemented in subclasses.\n     \"\"\"\n \n-    def __init__(self, cache: PagedAttentionCache):\n+    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False):\n         self.active_requests: Dict[str, RequestState] = {}\n         self.waiting_requests: Dict[str, RequestState] = {}\n         self.waiting_requests_order: Deque[str] = deque()\n         self.cache = cache\n+        self.retain_cache_on_finish = retain_cache_on_finish\n \n     @abstractmethod\n     def add_waiting_request(self, state: RequestState):\n@@ -326,7 +327,7 @@ def has_pending_requests(self) -> bool:\n         return self.active_requests or self.waiting_requests\n \n     @abstractmethod\n-    def finish_request(self, state: RequestState):\n+    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n         \"\"\"Finish processing a request and free its allocated blocks.\"\"\"\n         pass\n \n@@ -385,6 +386,11 @@ def _prepare_request_for_processing(\n     @traced\n     def add_waiting_request(self, state: RequestState):\n         \"\"\"Add a request to the waiting list.\"\"\"\n+        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n+            old_state = self.active_requests.pop(state.request_id)\n+            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]\n+            state.allocated_blocks = old_state.allocated_blocks\n+            state.position_offset = old_state.position_offset\n         self.waiting_requests[state.request_id] = state\n         self.waiting_requests_order.append(state.request_id)\n \n@@ -444,11 +450,11 @@ def _remove_from_waiting_requests(state: RequestState):\n         return scheduled_requests\n \n     @traced\n-    def finish_request(self, state: RequestState):\n-        request_id = state.request_id\n-        self.cache.free_blocks(request_id)\n-        if request_id in self.active_requests:\n-            del self.active_requests[request_id]\n+    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n+        if evict_from_cache:\n+            self.cache.free_blocks(request_id)\n+            if request_id in self.active_requests:\n+                del self.active_requests[request_id]\n \n \n @attach_tracer()\n@@ -499,6 +505,11 @@ def _prepare_request_for_processing(\n     @traced\n     def add_waiting_request(self, state: RequestState):\n         \"\"\"Add a request to the waiting list.\"\"\"\n+        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n+            old_state = self.active_requests.pop(state.request_id)\n+            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]  # XXX: check for indexing error?\n+            state.allocated_blocks = old_state.allocated_blocks\n+            state.position_offset = old_state.position_offset\n         self.waiting_requests[state.request_id] = state\n         self.waiting_requests_order.append(state.request_id)\n \n@@ -558,11 +569,11 @@ def _remove_from_waiting_requests(state: RequestState):\n         return scheduled_requests\n \n     @traced\n-    def finish_request(self, state: RequestState):\n-        request_id = state.request_id\n-        self.cache.free_blocks(request_id)\n-        if request_id in self.active_requests:\n-            del self.active_requests[request_id]\n+    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n+        if evict_from_cache:\n+            self.cache.free_blocks(request_id)\n+            if request_id in self.active_requests:\n+                del self.active_requests[request_id]\n \n \n @traced(standalone=True)\n@@ -717,6 +728,7 @@ def __init__(\n         model_dtype: torch.dtype,\n         scheduler: Scheduler,\n         streaming: bool = False,\n+        manual_eviction: bool = False,\n     ):\n         \"\"\"Initialize the continuous batch processor.\n \n@@ -740,6 +752,7 @@ def __init__(\n         self.model_dtype = model_dtype\n         self.scheduler = scheduler\n         self.streaming = streaming\n+        self.manual_eviction = manual_eviction\n \n         self.requests_in_batch: List[RequestState] = []\n \n@@ -1002,7 +1015,7 @@ def update_batch(self):\n                 state.prompt_ids = [token]\n                 if state.update_with_token(token):\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n-                    self.scheduler.finish_request(state)\n+                    self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n                     finished_request_ids.append(req_id)\n                 self._maybe_send_output(state, token)\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n@@ -1019,7 +1032,7 @@ def handle_batch_error(self, error):\n         failed_reqs = self.requests_in_batch\n         for req in failed_reqs:\n             self._handle_request_error(error, req)\n-            self.scheduler.finish_request(req)\n+            self.scheduler.finish_request(req.request_id)\n \n     @traced\n     def fail_all_requests(self, error):\n@@ -1030,7 +1043,7 @@ def fail_all_requests(self, error):\n         \"\"\"\n         for state in self.scheduler.active_requests.values():\n             self._handle_request_error(error, state)\n-            self.scheduler.finish_request(state)\n+            self.scheduler.finish_request(state.request_id)\n \n         # Also fail any requests in the waiting queue\n         for req_id in list(self.scheduler.waiting_requests.keys()):\n@@ -1056,7 +1069,14 @@ class ContinuousBatchingManager:\n     retrieving results, and managing the background generation thread.\n     \"\"\"\n \n-    def __init__(self, model, generation_config: GenerationConfig, max_queue_size=0, streaming: bool = True):\n+    def __init__(\n+        self,\n+        model,\n+        generation_config: GenerationConfig,\n+        manual_eviction: bool = False,\n+        max_queue_size=0,\n+        streaming: bool = True,\n+    ):\n         \"\"\"Initialize the continuous batching manager.\n \n         Args:\n@@ -1080,6 +1100,8 @@ def __init__(self, model, generation_config: GenerationConfig, max_queue_size=0,\n         self.logit_processor = self.model._get_logits_processor(self.model.generation_config)\n         self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n         self.profile = getattr(generation_config, \"profile\", False)\n+        self.manual_eviction = manual_eviction\n+        self.batch_processor: Optional[ContinuousBatchProcessor] = None\n \n     @traced\n     def start(self):\n@@ -1262,9 +1284,11 @@ def _run_generation_loop(self):\n                 self.stop_event,\n                 self.model.device,\n                 self.model.dtype,\n-                scheduler(paged_attention_cache),\n+                scheduler(paged_attention_cache, self.manual_eviction),\n                 self.streaming,\n+                self.manual_eviction,\n             )\n+            self.batch_processor = batch_processor\n             is_first = True\n \n             if self.profile:\n@@ -1346,15 +1370,23 @@ def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatc\n         if batch_processor is not None:\n             batch_processor.fail_all_requests(error)\n \n+    @traced\n+    def evict_request_from_cache(self, request_id: str):\n+        \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n+        if not self.manual_eviction:\n+            raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n+        if self.batch_processor is not None:\n+            self.batch_processor.scheduler.finish_request(request_id)\n+\n \n class ContinuousMixin:\n     \"\"\"Mixin class for models to add continuous batching capabilities.\"\"\"\n \n     def init_continuous_batching(\n         self,\n         generation_config: Optional[GenerationConfig] = None,\n+        manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        scheduler: str = \"fifo\",\n         streaming: bool = False,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n@@ -1380,7 +1412,11 @@ def init_continuous_batching(\n \n         # Create and return the manager\n         return ContinuousBatchingManager(\n-            model=self, generation_config=gen_config, max_queue_size=max_queue_size, streaming=streaming\n+            model=self,\n+            generation_config=gen_config,\n+            manual_eviction=manual_eviction,\n+            max_queue_size=max_queue_size,\n+            streaming=streaming,\n         )\n \n     @traced"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 55,
        "deletions": 19
    }
}