{
    "author": "BlackSamorez",
    "message": "HIGGS Quantization Support (#34997)\n\n* higgs init\r\n\r\n* working with crunches\r\n\r\n* per-model workspaces\r\n\r\n* style\r\n\r\n* style 2\r\n\r\n* tests and style\r\n\r\n* higgs tests passing\r\n\r\n* protecting torch import\r\n\r\n* removed torch.Tensor type annotations\r\n\r\n* torch.nn.Module inheritance fix maybe\r\n\r\n* hide inputs inside quantizer calls\r\n\r\n* style structure something\r\n\r\n* Update src/transformers/quantizers/quantizer_higgs.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* reworked num_sms\r\n\r\n* Update src/transformers/integrations/higgs.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* revamped device checks\r\n\r\n* docstring upd\r\n\r\n* Update src/transformers/quantizers/quantizer_higgs.py\r\n\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\r\n\r\n* edited tests and device map assertions\r\n\r\n* minor edits\r\n\r\n* updated flute cuda version in docker\r\n\r\n* Added p=1 and 2,3bit HIGGS\r\n\r\n* flute version check update\r\n\r\n* incorporated `modules_to_not_convert`\r\n\r\n* less hardcoding\r\n\r\n* Fixed comment\r\n\r\n* Added docs\r\n\r\n* Fixed gemma support\r\n\r\n* example in docs\r\n\r\n* fixed torch_dtype for HIGGS\r\n\r\n* Update docs/source/en/quantization/higgs.md\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Collection link\r\n\r\n* dequantize interface\r\n\r\n* newer flute version, torch.compile support\r\n\r\n* unittest message fix\r\n\r\n* docs update compile\r\n\r\n* isort\r\n\r\n* ValueError instead of assert\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
    "files": [
        {
            "sha": "44d1ceb2bfdd5ee059bfae9c8d096c09cd1d3b2d",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -69,6 +69,10 @@ RUN python3 -m pip install --no-cache-dir optimum-quanto\n # Add eetq for quantization testing\n RUN python3 -m pip install git+https://github.com/NetEase-FuXi/EETQ.git\n \n+# Add flute-kernel and fast_hadamard_transform for quantization testing\n+RUN python3 -m pip install --no-cache-dir flute-kernel==0.3.0 -i https://flute-ai.github.io/whl/cu118\n+RUN python3 -m pip install --no-cache-dir fast_hadamard_transform==1.0.4.post1\n+\n # When installing in editable mode, `transformers` is not recognized as a package.\n # this line must be added in order for python to be aware of transformers.\n RUN cd transformers && python3 setup.py develop"
        },
        {
            "sha": "de21cd1408a31c7175a68f36e6108e21550b2789",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -173,6 +173,8 @@\n     title: Quanto\n   - local: quantization/eetq\n     title: EETQ\n+  - local: quantization/higgs\n+    title: HIGGS\n   - local: quantization/hqq\n     title: HQQ\n   - local: quantization/fbgemm_fp8"
        },
        {
            "sha": "037660d0638cbdafd4bbd8b64904bb93435c45f6",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -57,6 +57,10 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n \n [[autodoc]] quantizers.base.HfQuantizer\n \n+## HiggsConfig\n+\n+[[autodoc]] HiggsConfig\n+\n ## HqqConfig\n \n [[autodoc]] HqqConfig"
        },
        {
            "sha": "d2aa9c9dc497d562fa6b74d044e34044cc5a02b1",
            "filename": "docs/source/en/quantization/higgs.md",
            "status": "added",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2Fquantization%2Fhiggs.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2Fquantization%2Fhiggs.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fhiggs.md?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -0,0 +1,66 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# HIGGS\n+\n+HIGGS is a 0-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and SOTA performance. You can find more information in the paper [arxiv.org/abs/2411.17525](https://arxiv.org/abs/2411.17525).\n+\n+Runtime support for HIGGS is implemented through [FLUTE](https://arxiv.org/abs/2407.10960), and its [library](https://github.com/HanGuo97/flute).\n+\n+## Quantization Example\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"google/gemma-2-9b-it\",\n+    quantization_config=HiggsConfig(bits=4),\n+    device_map=\"auto\",\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n+\n+tokenizer.decode(model.generate(\n+    **tokenizer(\"Hi,\", return_tensors=\"pt\").to(model.device),\n+    temperature=0.5,\n+    top_p=0.80,\n+)[0])\n+```\n+\n+## Pre-quantized models\n+\n+Some pre-quantized models can be found in the [official collection](https://huggingface.co/collections/ISTA-DASLab/higgs-675308e432fd56b7f6dab94e) on Hugging Face Hub.\n+\n+## Current Limitations\n+\n+**Architectures**\n+\n+Currently, FLUTE, and HIGGS by extension, **only support Llama 3 and 3.0 of 8B, 70B and 405B parameters, as well as Gemma-2 9B and 27B**. We're working on allowing to run more diverse models as well as allow arbitrary models by modifying the FLUTE compilation procedure.\n+\n+**torch.compile**\n+\n+HIGGS is fully compatible with `torch.compile`. Compiling `model.forward`, as described [here](../perf_torch_compile.md), here're the speedups it provides on RTX 4090 for `Llama-3.1-8B-Instruct` (forward passes/sec):\n+\n+| Batch Size | BF16 (With `torch.compile`) | HIGGS 4bit (No `torch.compile`) | HIGGS 4bit (With `torch.compile`) |\n+|------------|-----------------------------|----------------------------------|-----------------------------------|\n+| 1          | 59                          | 41                               | 124                               |\n+| 4          | 57                          | 42                               | 123                               |\n+| 16         | 56                          | 41                               | 120                               |\n+\n+\n+**Quantized training**\n+\n+Currently, HIGGS doesn't support quantized training (and backward passes in general). We're working on adding support for it.\n\\ No newline at end of file"
        },
        {
            "sha": "48840fad646fd0ed86cb618bc34d39ba0c120800",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -54,6 +54,7 @@ Use the table below to help you decide which quantization method to use.\n | [EETQ](./eetq)                                | 游릭                       | 游댮   | 游릭        | 游댮              | 游댮         | 游댮                     | ?                       | 8              | 游릭                                   | 游릭            | 游릭                      | https://github.com/NetEase-FuXi/EETQ        |\n | GGUF / GGML (llama.cpp)             | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游댮                       | 1 - 8          | 游댮                                   | [See GGUF section](../gguf)                | [See GGUF section](../gguf)                      | https://github.com/ggerganov/llama.cpp      |\n | [GPTQ](./gptq)                                | 游댮                       | 游댮   | 游릭        | 游릭              | 游댮                     | 游댮         | 游댮                       | 2 - 3 - 4 - 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n+| [HIGGS](./higgs)                             | 游릭                       | 游댮    | 游릭        | 游댮              | 游댮                     | 游댮         | 游릭                       | 2 - 4          | 游댮                                   | 游릭            | 游릭                      | https://github.com/HanGuo97/flute           |       \n | [HQQ](./hqq)                                 | 游릭                       | 游릭    | 游릭        | 游댮              | 游댮                     | 游댮         | 游릭                       | 1 - 8          | 游릭                                   | 游댮            | 游릭                      | https://github.com/mobiusml/hqq/            |\n | [optimum-quanto](./quanto)                              | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游릭                       | 2 / 4 / 8      | 游댮                                   | 游댮            | 游릭                      | https://github.com/huggingface/optimum-quanto       |\n | [FBGEMM_FP8](./fbgemm_fp8.md)                              | 游릭                       | 游댮    | 游릭        | 游댮              | 游댮                      | 游댮         | 游댮                        | 8      | 游댮                                   | 游릭            | 游릭                      | https://github.com/pytorch/FBGEMM       |"
        },
        {
            "sha": "ef140cc6d3a843c88c0e1ec9951fe30202542755",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -998,6 +998,7 @@\n         \"EetqConfig\",\n         \"FbgemmFp8Config\",\n         \"GPTQConfig\",\n+        \"HiggsConfig\",\n         \"HqqConfig\",\n         \"QuantoConfig\",\n         \"TorchAoConfig\",\n@@ -6023,6 +6024,7 @@\n         EetqConfig,\n         FbgemmFp8Config,\n         GPTQConfig,\n+        HiggsConfig,\n         HqqConfig,\n         QuantoConfig,\n         TorchAoConfig,"
        },
        {
            "sha": "e0149decde31014da626e6e42c10ef1496997c54",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -63,6 +63,7 @@\n         \"load_dequant_gguf_tensor\",\n         \"load_gguf\",\n     ],\n+    \"higgs\": [\"HiggsLinear\", \"dequantize_higgs\", \"quantize_with_higgs\", \"replace_with_higgs_linear\"],\n     \"hqq\": [\"prepare_for_hqq_linear\"],\n     \"integration_utils\": [\n         \"INTEGRATION_TO_CALLBACK\",\n@@ -166,6 +167,7 @@\n         load_dequant_gguf_tensor,\n         load_gguf,\n     )\n+    from .higgs import HiggsLinear, dequantize_higgs, quantize_with_higgs, replace_with_higgs_linear\n     from .hqq import prepare_for_hqq_linear\n     from .integration_utils import (\n         INTEGRATION_TO_CALLBACK,"
        },
        {
            "sha": "5a8f6537bb2bd57f097030eb2eb21211666541e7",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "added",
            "additions": 657,
            "deletions": 0,
            "changes": 657,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -0,0 +1,657 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"HIGGS through FLUTE (Flexible Lookup Table Engine for LUT-quantized LLMs) integration file\"\n+\n+from math import sqrt\n+\n+from ..utils import (\n+    is_flute_available,\n+    is_hadamard_available,\n+    is_torch_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+if is_flute_available():\n+    import flute.utils\n+\n+if is_hadamard_available():\n+    from fast_hadamard_transform import hadamard_transform\n+\n+if is_flute_available():\n+    import flute.utils\n+    from flute.integrations.higgs import prepare_data_transposed\n+\n+\n+def pad_to_block(tensor, dims, had_block_size, value=0):\n+    pad_dims = [0 for _ in range(2 * len(tensor.shape))]\n+    for dim in dims:\n+        size = tensor.shape[dim]\n+        next_multiple_of_1024 = ((size - 1) // had_block_size + 1) * had_block_size\n+        delta = next_multiple_of_1024 - size\n+        pad_dims[-2 * dim - 1] = delta\n+\n+    return nn.functional.pad(tensor, pad_dims, \"constant\", value)\n+\n+\n+def get_higgs_grid(p: int, n: int):\n+    if (p, n) == (2, 256):\n+        return torch.tensor(\n+            [\n+                [-2.501467704772949, 0.17954708635807037],\n+                [-0.6761789321899414, 1.2728623151779175],\n+                [-1.8025816679000854, 0.7613157629966736],\n+                [-0.538287878036499, -2.6028504371643066],\n+                [0.8415029644966125, -0.8600977659225464],\n+                [0.7023013234138489, 3.3138747215270996],\n+                [0.5699077844619751, 2.5782253742218018],\n+                [3.292393207550049, -0.6016128063201904],\n+                [0.5561617016792297, -1.7723814249038696],\n+                [-2.1012380123138428, 0.020958125591278076],\n+                [0.46085724234580994, 0.8428705334663391],\n+                [1.4548040628433228, -0.6156039237976074],\n+                [3.210029363632202, 0.3546904921531677],\n+                [0.8893890976905823, -0.5967988967895508],\n+                [0.8618854284286499, -3.2061192989349365],\n+                [1.1360996961593628, -0.23852407932281494],\n+                [1.6646337509155273, -0.9265465140342712],\n+                [1.4767773151397705, 1.2476022243499756],\n+                [-1.0511897802352905, 1.94503915309906],\n+                [-1.56318998336792, -0.3264186680316925],\n+                [-0.1829211413860321, 0.2922491431236267],\n+                [-0.8950616717338562, -1.3887052536010742],\n+                [-0.08206957578659058, -1.329533576965332],\n+                [-0.487422913312912, 1.4817842245101929],\n+                [-1.6769757270812988, -2.8269758224487305],\n+                [-1.5057679414749146, 1.8905963897705078],\n+                [1.8335362672805786, 1.0515104532241821],\n+                [0.3273945450782776, 1.0491033792495728],\n+                [-3.295924186706543, -0.7021600008010864],\n+                [-1.8428784608840942, -1.2315762042999268],\n+                [-0.8575026392936707, -1.7005949020385742],\n+                [-1.120667815208435, 0.6467998027801514],\n+                [-0.1588846743106842, -1.804071068763733],\n+                [-0.8539647459983826, 0.5645008683204651],\n+                [-1.4192019701004028, -0.6175029873847961],\n+                [1.0799058675765991, 1.7871345281600952],\n+                [1.171311855316162, 0.7511613965034485],\n+                [2.162078380584717, 0.8044339418411255],\n+                [1.3969420194625854, -1.243762493133545],\n+                [-0.23818807303905487, 0.053944624960422516],\n+                [2.304199457168579, -1.2667627334594727],\n+                [1.4225027561187744, 0.568610668182373],\n+                [0.376836895942688, -0.7134661674499512],\n+                [2.0404467582702637, 0.4087389409542084],\n+                [0.7639489769935608, -1.1367933750152588],\n+                [0.3622530400753021, -1.4827953577041626],\n+                [0.4100743532180786, 0.36108437180519104],\n+                [-1.5867475271224976, -1.618212342262268],\n+                [-2.2769672870635986, -1.2132309675216675],\n+                [0.9184022545814514, -0.34428009390830994],\n+                [-0.3902314603328705, 0.21785245835781097],\n+                [3.120687484741211, 1.3077973127365112],\n+                [1.587440848350525, -1.6506884098052979],\n+                [-1.718808889389038, -0.038405973464250565],\n+                [-0.6888407468795776, -0.8402308821678162],\n+                [-0.7981445789337158, -1.1117373704910278],\n+                [-2.4124443531036377, 1.3419722318649292],\n+                [-0.6611530184745789, 0.9939885139465332],\n+                [-0.33103418350219727, -0.16702833771705627],\n+                [-2.4091389179229736, -2.326857566833496],\n+                [1.6610108613967896, -2.159703254699707],\n+                [0.014884627424180508, 0.3887578248977661],\n+                [0.029668325558304787, 1.8786455392837524],\n+                [1.180362582206726, 2.699317216873169],\n+                [1.821286678314209, -0.5960053205490112],\n+                [-0.44835323095321655, 3.327436685562134],\n+                [-0.3714401423931122, -2.1466753482818604],\n+                [-1.1103475093841553, -2.4536871910095215],\n+                [-0.39110705256462097, 0.6670510172843933],\n+                [0.474752813577652, -1.1959707736968994],\n+                [-0.013110585510730743, -2.52519154548645],\n+                [-2.0836575031280518, -1.703289270401001],\n+                [-1.1077687740325928, -0.1252644956111908],\n+                [-0.4138077199459076, 1.1837692260742188],\n+                [-1.977599024772644, 1.688241720199585],\n+                [-1.659559965133667, -2.1387736797332764],\n+                [0.03242531046271324, 0.6526556015014648],\n+                [0.9127950072288513, 0.6099498867988586],\n+                [-0.38478314876556396, 0.433487206697464],\n+                [0.27454206347465515, -0.27719801664352417],\n+                [0.10388526320457458, 2.2812814712524414],\n+                [-0.014394169673323631, -3.177137613296509],\n+                [-1.2871228456497192, -0.8961855173110962],\n+                [0.5720916986465454, -0.921597957611084],\n+                [1.1159656047821045, -0.7609877586364746],\n+                [2.4383342266082764, -2.2983546257019043],\n+                [-0.294057160615921, -0.9770799875259399],\n+                [-0.9342701435089111, 1.107579231262207],\n+                [-1.549338698387146, 3.090520143508911],\n+                [2.6076579093933105, 2.051239013671875],\n+                [-0.9259037375450134, 1.407211184501648],\n+                [-0.1747353971004486, 0.540488600730896],\n+                [-0.8963701725006104, 0.8271111249923706],\n+                [0.6480194926261902, 1.0128909349441528],\n+                [0.980783998966217, -0.06156221032142639],\n+                [-0.16883476078510284, 1.0601658821105957],\n+                [0.5839992761611938, 0.004697148688137531],\n+                [-0.34228450059890747, -1.2423977851867676],\n+                [2.500824451446533, 0.3665279746055603],\n+                [-0.17641609907150269, 1.3529551029205322],\n+                [0.05378641560673714, 2.817232847213745],\n+                [-1.2391047477722168, 2.354328155517578],\n+                [0.630434513092041, -0.668536365032196],\n+                [1.7576488256454468, 0.6738647818565369],\n+                [0.4435231387615204, 0.6000469326972961],\n+                [-0.08794835954904556, -0.11511358618736267],\n+                [1.6540337800979614, 0.33995017409324646],\n+                [-0.04202975332736969, -0.5375117063522339],\n+                [-0.4247745871543884, -0.7897617220878601],\n+                [0.06695003807544708, 1.2000739574432373],\n+                [-3.2508881092071533, 0.28734830021858215],\n+                [-1.613816261291504, 0.4944162368774414],\n+                [1.3598989248275757, 0.26117825508117676],\n+                [2.308382511138916, 1.3462618589401245],\n+                [-1.2137469053268433, -1.9254342317581177],\n+                [-0.4889402985572815, 1.8136259317398071],\n+                [-0.1870335340499878, -0.3480615019798279],\n+                [1.0766386985778809, -1.0627082586288452],\n+                [0.4651014506816864, 2.131748914718628],\n+                [-0.1306295394897461, -0.7811847925186157],\n+                [0.06433182954788208, -1.5397958755493164],\n+                [-0.2894323468208313, -0.5789554715156555],\n+                [-0.6081662178039551, 0.4845278263092041],\n+                [2.697964668273926, -0.18515698611736298],\n+                [0.1277363896369934, -0.7221432328224182],\n+                [0.8700758218765259, 0.35042452812194824],\n+                [0.22088994085788727, 0.495242178440094],\n+                [-2.5843818187713623, -0.8000828623771667],\n+                [0.6732649803161621, -1.4362232685089111],\n+                [-1.5286413431167603, 1.0417330265045166],\n+                [-1.1222513914108276, -0.6269875764846802],\n+                [-0.9752035140991211, -0.8750635385513306],\n+                [-2.6369473934173584, 0.6918523907661438],\n+                [0.14478731155395508, -0.041986867785453796],\n+                [-1.5629483461380005, 1.4369450807571411],\n+                [0.38952457904815674, -2.16428804397583],\n+                [-0.16885095834732056, 0.7976621985435486],\n+                [-3.12416934967041, 1.256506085395813],\n+                [0.6843105554580688, -0.4203019142150879],\n+                [1.9345275163650513, 1.934950351715088],\n+                [0.012184220366179943, -2.1080918312072754],\n+                [-0.6350273489952087, 0.7358828186988831],\n+                [-0.837304949760437, -0.6214472651481628],\n+                [0.08211923390626907, -0.9472538232803345],\n+                [2.9332995414733887, -1.4956780672073364],\n+                [1.3806978464126587, -0.2916182279586792],\n+                [0.06773144006729126, 0.9285762310028076],\n+                [-1.1943119764328003, 1.5963770151138306],\n+                [1.6395620107650757, -0.32285431027412415],\n+                [-1.390851378440857, -0.08273141086101532],\n+                [1.816330909729004, -1.2812227010726929],\n+                [0.7921574711799622, -2.1135804653167725],\n+                [0.5817914605140686, 1.2644577026367188],\n+                [1.929347038269043, -0.2386285960674286],\n+                [0.8877345323562622, 1.190008521080017],\n+                [1.4732073545455933, 0.8935023546218872],\n+                [-2.8518524169921875, -1.5478795766830444],\n+                [0.2439267635345459, 0.7576767802238464],\n+                [0.5246709585189819, -2.606659412384033],\n+                [1.150876760482788, 1.4073830842971802],\n+                [-0.2643202245235443, 2.0634236335754395],\n+                [1.555483341217041, -0.0023102816194295883],\n+                [2.0830578804016113, -1.7225427627563477],\n+                [-0.5424830317497253, -1.070199728012085],\n+                [0.9168899655342102, 0.8955540060997009],\n+                [-0.8120972514152527, 2.696739912033081],\n+                [-0.29908373951911926, -1.5310651063919067],\n+                [1.2320337295532227, -1.556247353553772],\n+                [1.8612544536590576, 0.08704725652933121],\n+                [0.22133447229862213, -1.8091708421707153],\n+                [-0.4403655230998993, -0.38571012020111084],\n+                [-1.88539457321167, 1.192205786705017],\n+                [2.239687919616699, 0.004709010478109121],\n+                [1.139495611190796, 0.45733731985092163],\n+                [-1.507995367050171, 0.19716016948223114],\n+                [0.46986445784568787, 1.5422041416168213],\n+                [-1.2573751211166382, -0.35984551906585693],\n+                [-1.7415345907211304, -0.6020717024803162],\n+                [1.0751984119415283, 0.19006384909152985],\n+                [2.24186635017395, -0.46343153715133667],\n+                [0.3610347509384155, -0.07658443599939346],\n+                [-1.3111497163772583, 0.432013601064682],\n+                [0.6164408326148987, 0.24538464844226837],\n+                [-1.9266542196273804, -0.3256155550479889],\n+                [-0.5870336890220642, -0.1879584938287735],\n+                [-1.0476511716842651, 0.3677721917629242],\n+                [-1.229940414428711, 1.2433830499649048],\n+                [0.18550436198711395, 0.22753673791885376],\n+                [-0.017921989783644676, 0.12625974416732788],\n+                [1.1659504175186157, -0.5020995736122131],\n+                [-0.5983408093452454, -1.40438973903656],\n+                [0.7519024014472961, -0.16282692551612854],\n+                [0.9920787811279297, -1.344896912574768],\n+                [-0.8103678226470947, 0.3064485788345337],\n+                [0.6956969499588013, 1.8208192586898804],\n+                [-2.7830491065979004, -0.2299390584230423],\n+                [-0.34681546688079834, 2.4890666007995605],\n+                [-1.4452646970748901, -1.2216600179672241],\n+                [-2.1872897148132324, 0.8926076292991638],\n+                [1.706072211265564, -2.8440372943878174],\n+                [1.1119003295898438, -2.4923460483551025],\n+                [-2.582794666290283, 2.0973289012908936],\n+                [0.04987720400094986, -0.2964983284473419],\n+                [-2.063807487487793, -0.7847916483879089],\n+                [-0.4068813621997833, 0.9135897755622864],\n+                [-0.9814359545707703, -0.3874954879283905],\n+                [-1.4227229356765747, 0.7337291240692139],\n+                [0.3065044581890106, 1.3125417232513428],\n+                [1.2160996198654175, -1.9643305540084839],\n+                [-1.2163853645324707, 0.14608727395534515],\n+                [-2.3030710220336914, -0.37558120489120483],\n+                [0.9232977628707886, 2.1843791007995605],\n+                [-0.1989777386188507, 1.651851773262024],\n+                [-0.714374840259552, -0.39365994930267334],\n+                [-0.7805715799331665, -2.099881887435913],\n+                [0.9015759229660034, -1.7053706645965576],\n+                [0.1033422127366066, 1.5256654024124146],\n+                [-1.8773194551467896, 2.324174165725708],\n+                [1.9227174520492554, 2.7441604137420654],\n+                [-0.5994020104408264, 0.23984014987945557],\n+                [1.3496100902557373, -0.9126054644584656],\n+                [-0.8765304088592529, -3.1877026557922363],\n+                [-1.2040035724639893, -1.5169521570205688],\n+                [1.4261796474456787, 2.150200128555298],\n+                [1.463774561882019, 1.6656692028045654],\n+                [0.20364105701446533, -0.4988172650337219],\n+                [0.5195154547691345, -0.24067887663841248],\n+                [-1.1116786003112793, -1.1599653959274292],\n+                [-0.8490808606147766, -0.1681060940027237],\n+                [0.3189965784549713, -0.9641751646995544],\n+                [-0.5664751529693604, -0.5951744318008423],\n+                [-1.6347930431365967, -0.9137664437294006],\n+                [0.44048091769218445, -0.47259435057640076],\n+                [-2.147747039794922, 0.47442489862442017],\n+                [1.834734320640564, 1.4462147951126099],\n+                [1.1777573823928833, 1.0659226179122925],\n+                [-0.9568989872932434, 0.09495053440332413],\n+                [-1.838529348373413, 0.2950586676597595],\n+                [-0.4800611734390259, 0.014894310384988785],\n+                [-0.5235516428947449, -1.7687653303146362],\n+                [2.0735011100769043, -0.8825281262397766],\n+                [2.637502431869507, 0.8455678224563599],\n+                [2.606602907180786, -0.7848446369171143],\n+                [-1.1886937618255615, 0.9330510497093201],\n+                [0.38082656264305115, 0.13328030705451965],\n+                [0.6847941875457764, 0.7384101152420044],\n+                [1.2638574838638306, -0.007309418171644211],\n+                [0.18292222917079926, -1.22371244430542],\n+                [0.8143821954727173, 1.4976691007614136],\n+                [0.6571850776672363, 0.48368802666664124],\n+                [-0.6991601586341858, 2.150190830230713],\n+                [0.8101756572723389, 0.10206498205661774],\n+                [-0.08768226951360703, -1.084917664527893],\n+                [-0.7208092212677002, 0.03657956421375275],\n+                [0.3211449086666107, 1.803687334060669],\n+                [-0.7835946083068848, 1.6869111061096191],\n+            ]\n+        )\n+    if (p, n) == (2, 64):\n+        return torch.tensor(\n+            [\n+                [-2.7216711044311523, 0.14431366324424744],\n+                [-0.766914427280426, 1.7193410396575928],\n+                [-2.2575762271881104, 1.2476624250411987],\n+                [1.233758807182312, -2.3560616970062256],\n+                [0.8701965808868408, -0.2649352252483368],\n+                [1.4506438970565796, 2.1776366233825684],\n+                [-0.06305818259716034, 1.9049758911132812],\n+                [2.536226511001587, 0.563927412033081],\n+                [0.4599496126174927, -1.8745561838150024],\n+                [-1.900517225265503, -0.30703988671302795],\n+                [0.09386251866817474, 0.8755807280540466],\n+                [1.946500539779663, -0.6743080615997314],\n+                [2.1338934898376465, 1.4581491947174072],\n+                [0.9429940581321716, -0.8038390278816223],\n+                [2.0697755813598633, -1.614896535873413],\n+                [0.772676408290863, 0.22017823159694672],\n+                [1.0689979791641235, -1.525044322013855],\n+                [0.6813604831695557, 1.1345642805099487],\n+                [0.4706456661224365, 2.606626272201538],\n+                [-1.294018030166626, -0.4372096061706543],\n+                [-0.09134224057197571, 0.4610418677330017],\n+                [-0.7907772064208984, -0.48412787914276123],\n+                [0.060459110885858536, -0.9172890186309814],\n+                [-0.5855047702789307, 2.56172513961792],\n+                [0.11484206467866898, -2.659848213195801],\n+                [-1.5893300771713257, 2.188580274581909],\n+                [1.6750942468643188, 0.7089915871620178],\n+                [-0.445697546005249, 0.7452405095100403],\n+                [-1.8539940118789673, -1.8377939462661743],\n+                [-1.5791912078857422, -1.017285943031311],\n+                [-1.030419945716858, -1.5746369361877441],\n+                [-1.9511750936508179, 0.43696075677871704],\n+                [-0.3446580767631531, -1.8953213691711426],\n+                [-1.4219647645950317, 0.7676230669021606],\n+                [-0.9191089272499084, 0.5021472573280334],\n+                [0.20464491844177246, 1.3684605360031128],\n+                [0.5402919054031372, 0.6699410676956177],\n+                [1.8903915882110596, 0.03638288006186485],\n+                [0.4723062515258789, -0.6216739416122437],\n+                [-0.41345009207725525, -0.22752176225185394],\n+                [2.7119064331054688, -0.5111885070800781],\n+                [1.065286636352539, 0.6950305700302124],\n+                [0.40629103779792786, -0.14339995384216309],\n+                [1.2815024852752686, 0.17108257114887238],\n+                [0.01785222627222538, -0.43778058886528015],\n+                [0.054590027779340744, -1.4225547313690186],\n+                [0.3076786696910858, 0.30697619915008545],\n+                [-0.9498570561408997, -0.9576997756958008],\n+                [-2.4640724658966064, -0.9660449028015137],\n+                [1.3714425563812256, -0.39760473370552063],\n+                [-0.4857747256755829, 0.2386789172887802],\n+                [1.2797833681106567, 1.3097363710403442],\n+                [0.5508887767791748, -1.1777795553207397],\n+                [-1.384316325187683, 0.1465839296579361],\n+                [-0.46556955575942993, -1.2442727088928223],\n+                [-0.3915477693080902, -0.7319604158401489],\n+                [-1.4005504846572876, 1.3890998363494873],\n+                [-0.8647305965423584, 1.0617644786834717],\n+                [-0.8901953101158142, -0.01650036871433258],\n+                [-0.9893633723258972, -2.4662880897521973],\n+                [1.445534110069275, -1.049334168434143],\n+                [-0.041650623083114624, 0.012734669260680676],\n+                [-0.3302375078201294, 1.26217782497406],\n+                [0.6934980154037476, 1.7714335918426514],\n+            ]\n+        )\n+    elif (p, n) == (2, 16):\n+        return torch.tensor(\n+            [\n+                [-0.8996632695198059, -1.6360418796539307],\n+                [-0.961183488368988, 1.5999565124511719],\n+                [-1.882026195526123, 0.678778350353241],\n+                [0.36300793290138245, -1.9667866230010986],\n+                [-0.6814072728157043, -0.576818585395813],\n+                [0.7270012497901917, 0.6186859607696533],\n+                [0.3359416127204895, 1.8371193408966064],\n+                [1.859930396080017, 0.036668598651885986],\n+                [0.17208248376846313, -0.9401724338531494],\n+                [-1.7599700689315796, -0.6244229674339294],\n+                [-0.8993809223175049, 0.32267823815345764],\n+                [0.839488685131073, -0.3017036020755768],\n+                [1.5314953327178955, 1.2942044734954834],\n+                [-0.0011779458727687597, 0.00022069070837460458],\n+                [1.4274526834487915, -1.207889199256897],\n+                [-0.16123905777931213, 0.8787511587142944],\n+            ]\n+        )\n+    elif (p, n) == (1, 16):\n+        return torch.tensor(\n+            [\n+                [-2.7325894832611084],\n+                [-2.069017171859741],\n+                [-1.6180464029312134],\n+                [-1.2562311887741089],\n+                [-0.9423404335975647],\n+                [-0.6567591428756714],\n+                [-0.38804829120635986],\n+                [-0.12839503586292267],\n+                [0.12839503586292267],\n+                [0.38804829120635986],\n+                [0.6567591428756714],\n+                [0.9423404335975647],\n+                [1.2562311887741089],\n+                [1.6180464029312134],\n+                [2.069017171859741],\n+                [2.7325894832611084],\n+            ]\n+        )\n+    elif (p, n) == (1, 8):\n+        return torch.tensor(\n+            [\n+                [-2.1519455909729004],\n+                [-1.3439092636108398],\n+                [-0.7560052871704102],\n+                [-0.2450941801071167],\n+                [0.2450941801071167],\n+                [0.7560052871704102],\n+                [1.3439092636108398],\n+                [2.1519455909729004],\n+            ]\n+        )\n+    elif (p, n) == (1, 4):\n+        return torch.tensor([[-1.5104175806045532], [-0.4527800381183624], [0.4527800381183624], [1.5104175806045532]])\n+    else:\n+        raise NotImplementedError(f\"Unsupported p={p}, n={n}\")\n+\n+\n+def quantize_with_higgs(weight, bits: int = 4, p: int = 2, group_size: int = 256, hadamard_size: int = 1024):\n+    assert len(weight.shape) == 2, \"Only 2D weights are supported for now\"\n+\n+    grid = get_higgs_grid(p, 2 ** (p * bits)).to(weight.device)\n+    grid_norm_2 = torch.linalg.norm(grid, axis=-1) ** 2\n+\n+    device = weight.device\n+    dtype = weight.dtype\n+    weight = weight.clone().float()\n+    # Pad to Hadamard transform size\n+    weight = pad_to_block(weight, [1], hadamard_size)\n+\n+    # Scale and Hadamard transform\n+    mult = weight.shape[1] // hadamard_size\n+    weight = weight.reshape(-1, mult, hadamard_size)\n+    scales = torch.linalg.norm(weight, axis=-1)\n+    weight = hadamard_transform(weight, 1) / scales[:, :, None]\n+\n+    # Pad to edenn_d and project\n+    weight = pad_to_block(weight, [2], p).reshape(weight.shape[0], mult, -1, p)\n+\n+    # Quantize\n+    codes = torch.empty(weight.shape[:-1], device=device, dtype=torch.uint8)\n+    for i in range(0, weight.shape[0], 64):\n+        codes[i : i + 64] = torch.argmax(2 * weight[i : i + 64] @ grid.T - grid_norm_2, dim=-1).to(torch.uint8)\n+    del weight\n+\n+    codes = codes.reshape(codes.shape[0], -1)\n+    scales = scales / sqrt(hadamard_size)\n+\n+    weight, scales, tables, tables2 = prepare_data_transposed(\n+        codes,\n+        torch.repeat_interleave(scales.to(dtype), hadamard_size // group_size, dim=1),\n+        grid.to(dtype),\n+        num_bits=bits,\n+        group_size=group_size,\n+        vector_size=p,\n+        dtype=dtype,\n+        device=device,\n+    )\n+\n+    return {\n+        \"weight\": weight,\n+        \"scales\": scales,\n+        \"tables\": tables,\n+        \"tables2\": tables2.view(dtype=torch.float16),\n+    }\n+\n+\n+class HiggsLinear(torch.nn.Module):\n+    def __init__(\n+        self,\n+        in_features: int,\n+        out_features: int,\n+        num_bits: int,\n+        bias=True,\n+        dtype: torch.dtype = None,\n+        device: torch.device = None,\n+        group_size: int = 256,\n+        hadamard_size: int = 1024,\n+    ):\n+        super().__init__()\n+        self.in_features = in_features\n+        self.out_features = out_features\n+        self.num_bits = num_bits\n+        self.group_size = group_size\n+        self.hadamard_size = hadamard_size\n+        self.num_sms_packed = nn.Parameter(torch.tensor(-1, dtype=torch.int32, device=device), requires_grad=False)\n+\n+        assert in_features % group_size == 0\n+        assert num_bits in [2, 3, 4]\n+\n+        self.weight = nn.Parameter(\n+            torch.empty((out_features * num_bits // 16, in_features), dtype=torch.int16, device=device),\n+            requires_grad=False,\n+        )\n+        self.scales = nn.Parameter(\n+            torch.empty((out_features, in_features // group_size), dtype=dtype, device=device), requires_grad=False\n+        )\n+        self.tables = nn.Parameter(torch.empty((2**num_bits,), dtype=dtype, device=device), requires_grad=False)\n+        self.tables2 = nn.Parameter(\n+            torch.empty((2**num_bits, 2**num_bits, 2), dtype=dtype, device=device), requires_grad=False\n+        )\n+\n+        if bias:\n+            self.bias = nn.Parameter(torch.empty(out_features, device=device, dtype=dtype), requires_grad=False)\n+        else:\n+            self.register_parameter(\"bias\", None)\n+\n+        self.workspace = None  # must be set externally to be reused among layers\n+\n+    def forward(self, x):\n+        x = pad_to_block(x, [-1], self.hadamard_size)\n+\n+        if self.workspace is None:\n+            raise Exception(\"Workspace must be set before calling forward\")\n+\n+        return flute.qgemm_hadamard(\n+            x,\n+            self.weight,\n+            self.scales,\n+            self.tables,\n+            self.tables2.view(dtype=torch.float32),\n+            self.workspace,\n+            self.num_bits,\n+            self.group_size,\n+            self.hadamard_size,\n+        )\n+\n+\n+def replace_with_higgs_linear(\n+    model,\n+    quantization_config=None,\n+    current_key_name=None,\n+    has_been_replaced=False,\n+):\n+    \"\"\"\n+    Public method that recursively replaces the Linear layers of the given model with HIGGS quantized layers.\n+    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n+    conversion has been successfull or not.\n+\n+    Args:\n+        model (`torch.nn.Module`):\n+            The model to convert, can be any `torch.nn.Module` instance.\n+        quantization_config (`HiggsConfig`):\n+            The quantization config object that contains the quantization parameters.\n+        current_key_name (`list`, *optional*):\n+            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n+        has_been_replaced (`bool`, *optional*):\n+            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n+            should not be passed by the user.\n+    \"\"\"\n+\n+    from accelerate import init_empty_weights\n+\n+    for name, module in model.named_children():\n+        if current_key_name is None:\n+            current_key_name = []\n+        current_key_name.append(name)\n+\n+        if isinstance(module, nn.Linear):\n+            # Check if the current key is not in the `quantization_config.modules_to_not_convert`\n+            current_key_name_str = \".\".join(current_key_name)\n+            if not any(current_key_name_str.endswith(key) for key in quantization_config.modules_to_not_convert):\n+                with init_empty_weights():\n+                    in_features = module.in_features\n+                    out_features = module.out_features\n+\n+                    model._modules[name] = HiggsLinear(\n+                        in_features,\n+                        out_features,\n+                        bias=module.bias is not None,\n+                        num_bits=quantization_config.bits,\n+                        hadamard_size=quantization_config.hadamard_size,\n+                        group_size=quantization_config.group_size,\n+                    )\n+                    has_been_replaced = True\n+\n+                    # Store the module class in case we need to transpose the weight later\n+                    model._modules[name].source_cls = type(module)\n+                    # Force requires grad to False to avoid unexpected errors\n+                    model._modules[name].requires_grad_(False)\n+        if len(list(module.children())) > 0:\n+            _, has_been_replaced = replace_with_higgs_linear(\n+                module,\n+                quantization_config=quantization_config,\n+                current_key_name=current_key_name,\n+                has_been_replaced=has_been_replaced,\n+            )\n+        # Remove the last key for recursion\n+        current_key_name.pop(-1)\n+    return model, has_been_replaced\n+\n+\n+def dequantize_higgs(model, current_key_name=None):\n+    \"\"\"\n+    Dequantizes the HiggsLinear layers in the given model by replacing them with standard torch.nn.Linear layers.\n+    Args:\n+        model (torch.nn.Module): The model containing HiggsLinear layers to be dequantized.\n+        current_key_name (list, optional): A list to keep track of the current module names during recursion. Defaults to None.\n+    Returns:\n+        torch.nn.Module: The model with HiggsLinear layers replaced by torch.nn.Linear layers.\n+    \"\"\"\n+\n+    with torch.no_grad():\n+        for name, module in model.named_children():\n+            if current_key_name is None:\n+                current_key_name = []\n+            current_key_name.append(name)\n+\n+            if isinstance(module, HiggsLinear):\n+                in_features = module.in_features\n+                out_features = module.out_features\n+\n+                model._modules[name] = torch.nn.Linear(\n+                    in_features,\n+                    out_features,\n+                    bias=module.bias is not None,\n+                    device=module.scales.device,\n+                    dtype=module.scales.dtype,\n+                )\n+\n+                model._modules[name].weight.data = module(\n+                    torch.eye(in_features, device=module.scales.device, dtype=module.scales.dtype)\n+                ).T.contiguous()\n+\n+            if len(list(module.children())) > 0:\n+                _ = dequantize_higgs(\n+                    module,\n+                    current_key_name=current_key_name,\n+                )\n+            # Remove the last key for recursion\n+            current_key_name.pop(-1)\n+        return model"
        },
        {
            "sha": "d5b51d038ab8bbe3231e05295fe3844dee0b1ca7",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -24,6 +24,7 @@\n     EetqConfig,\n     FbgemmFp8Config,\n     GPTQConfig,\n+    HiggsConfig,\n     HqqConfig,\n     QuantizationConfigMixin,\n     QuantizationMethod,\n@@ -40,6 +41,7 @@\n from .quantizer_eetq import EetqHfQuantizer\n from .quantizer_fbgemm_fp8 import FbgemmFp8HfQuantizer\n from .quantizer_gptq import GptqHfQuantizer\n+from .quantizer_higgs import HiggsHfQuantizer\n from .quantizer_hqq import HqqHfQuantizer\n from .quantizer_quanto import QuantoHfQuantizer\n from .quantizer_torchao import TorchAoHfQuantizer\n@@ -54,6 +56,7 @@\n     \"aqlm\": AqlmHfQuantizer,\n     \"quanto\": QuantoHfQuantizer,\n     \"eetq\": EetqHfQuantizer,\n+    \"higgs\": HiggsHfQuantizer,\n     \"hqq\": HqqHfQuantizer,\n     \"compressed-tensors\": CompressedTensorsHfQuantizer,\n     \"fbgemm_fp8\": FbgemmFp8HfQuantizer,\n@@ -73,6 +76,7 @@\n     \"hqq\": HqqConfig,\n     \"compressed-tensors\": CompressedTensorsConfig,\n     \"fbgemm_fp8\": FbgemmFp8Config,\n+    \"higgs\": HiggsConfig,\n     \"torchao\": TorchAoConfig,\n     \"bitnet\": BitNetConfig,\n     \"vptq\": VptqConfig,"
        },
        {
            "sha": "f33e2f21e98fd8f9636e040a0c4f3f37992b574c",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "added",
            "additions": 232,
            "deletions": 0,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -0,0 +1,232 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional\n+\n+from .base import HfQuantizer\n+from .quantizers_utils import get_module_from_name\n+\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+\n+from ..utils import is_accelerate_available, is_flute_available, is_hadamard_available, is_torch_available, logging\n+from ..utils.quantization_config import QuantizationConfigMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_num_sms_from_device(device):\n+    target_device_cc = torch.cuda.get_device_capability(device=device)\n+    if target_device_cc == (8, 6):\n+        return 84\n+    elif target_device_cc == (8, 0):\n+        return 108\n+    elif target_device_cc == (8, 9):\n+        return 128\n+    else:\n+        raise NotImplementedError(\n+            f\"Device capability {target_device_cc} not supported for FLUTE (yet?) to verify your device capability check out https://developer.nvidia.com/cuda-gpus\"\n+        )\n+\n+\n+class HiggsHfQuantizer(HfQuantizer):\n+    \"\"\"\n+    Quantizer of the HIGGS method. Enables the loading of prequantized models and in-flight quantization of full-precision models.\n+    \"\"\"\n+\n+    requires_calibration = False\n+    requires_parameters_quantization = True\n+    required_packages = [\"flute-kernel\", \"fast_hadamard_transform\"]\n+\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+        self.quantization_config = quantization_config\n+\n+    def validate_environment(self, device_map, **kwargs):\n+        if not torch.cuda.is_available():\n+            raise NotImplementedError(\"HIGGS quantization is only supported on GPU. Please use a different quantizer.\")\n+\n+        if not is_accelerate_available():\n+            raise ImportError(\"Using `higgs` quantization requires Accelerate: `pip install accelerate`\")\n+\n+        if not is_flute_available():\n+            raise ImportError(\"Using `higgs` quantization requires FLUTE: `pip install flute-kernel>=0.3.0`\")\n+\n+        if not is_hadamard_available():\n+            raise ImportError(\n+                \"Using `higgs` quantization requires fast_hadamard_transform: `pip install fast_hadamard_transform`\"\n+            )\n+\n+        if device_map is None:\n+            raise ValueError(\n+                \"You are attempting to load a HIGGS model without setting device_map.\"\n+                \" Please set device_map comprised of 'cuda' devices.\"\n+            )\n+        elif isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+            raise ValueError(\n+                \"You are attempting to load a HIGGS model with a device_map that contains a CPU or disk device.\"\n+                \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n+            )\n+\n+    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        if torch_dtype is None:\n+            logger.info(\"`torch_dtype` is None. Setting `torch_dtype=torch.float16` for FLUTE compatibility.\")\n+            torch_dtype = torch.float16\n+        elif torch_dtype != torch.float16 and torch_dtype != torch.bfloat16:\n+            raise ValueError(\n+                f\"Invalid `torch_dtype` {torch_dtype}. HIGGS quantization only supports `torch_dtype=torch.float16` or `torch_dtype=torch.bfloat16`.\"\n+            )\n+\n+        return torch_dtype\n+\n+    def create_quantized_param(\n+        self,\n+        model: \"PreTrainedModel\",\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        target_device: \"torch.device\",\n+        state_dict: Dict[str, Any],\n+        unexpected_keys: Optional[List[str]] = None,\n+    ):\n+        from ..integrations import quantize_with_higgs\n+\n+        \"\"\"\n+        Quantizes weights into weight and weight_scale\n+        \"\"\"\n+        flute_dict = quantize_with_higgs(\n+            param_value.to(target_device),\n+            self.quantization_config.bits,\n+            self.quantization_config.p,\n+            self.quantization_config.group_size,\n+            self.quantization_config.hadamard_size,\n+        )\n+\n+        del param_value\n+\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        for key, value in flute_dict.items():\n+            if key in module._parameters:\n+                module._parameters[key] = torch.nn.Parameter(value, requires_grad=False)\n+            elif key in module._buffers:\n+                module._buffers[key] = torch.nn.Buffer(value)\n+            else:\n+                raise ValueError(f\"Unexpected key {key} in module {module}\")\n+\n+        if unexpected_keys is not None and param_name in unexpected_keys:\n+            unexpected_keys.remove(param_name)\n+\n+        module.num_sms_packed = torch.nn.Parameter(\n+            torch.tensor(get_num_sms_from_device(target_device), device=target_device, dtype=torch.int32),\n+            requires_grad=False,\n+        )\n+\n+    def _process_model_before_weight_loading(\n+        self,\n+        model: \"PreTrainedModel\",\n+        **kwargs,\n+    ):\n+        from ..integrations import replace_with_higgs_linear\n+\n+        replace_with_higgs_linear(\n+            model,\n+            quantization_config=self.quantization_config,\n+        )\n+        model.config.quantization_config = self.quantization_config\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        import flute.utils\n+\n+        from ..integrations import HiggsLinear\n+\n+        flute_workspaces = {}\n+        for name, module in model.named_modules():\n+            if isinstance(module, HiggsLinear):\n+                # Every HiggsLinear needs a \"workspace\": a buffer for the unpacking operation.\n+                # This buffer needs to be on the same device as the weights, but can be reused across modules otherwise.\n+                if module.weight.device not in flute_workspaces:\n+                    flute_workspaces[module.weight.device] = flute.utils.make_workspace_streamk(\n+                        device=module.weight.device\n+                    )\n+                module.workspace = flute_workspaces[module.weight.device]\n+\n+                # FLUTE weights are packed in a way that is optimized for a specific number of SMs (GPU streaming multiprocessors).\n+                # If the model is loaded on a different device than the one it was saved on, we need to repack the weights.\n+                if module.num_sms_packed.item() != get_num_sms_from_device(module.weight.device):\n+                    new_device = module.weight.device\n+                    new_num_sms = get_num_sms_from_device(new_device)\n+                    module.weight.data = flute.utils.pack(\n+                        flute.utils.unpack(\n+                            weight=module.weight.data,\n+                            scales=module.scales.data,\n+                            workspace=module.workspace,\n+                            num_bits=module.num_bits,\n+                            group_size=module.group_size,\n+                            num_sms_packed=module.num_sms_packed.item(),\n+                        ).T.contiguous(),\n+                        module.num_bits,\n+                        module.group_size,\n+                    )\n+                    module.num_sms_packed = torch.nn.Parameter(\n+                        torch.tensor(new_num_sms, device=new_device, dtype=torch.int32),\n+                        requires_grad=False,\n+                    )\n+\n+    def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n+        from ..integrations import HiggsLinear\n+\n+        not_missing_keys = []\n+        for name, module in model.named_modules():\n+            if isinstance(module, HiggsLinear):\n+                for missing in missing_keys:\n+                    if (\n+                        (name in missing or name in f\"{prefix}.{missing}\")\n+                        and not missing.endswith(\".weight\")\n+                        and not missing.endswith(\".bias\")\n+                    ):\n+                        not_missing_keys.append(missing)\n+        return [k for k in missing_keys if k not in not_missing_keys]\n+\n+    @property\n+    def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n+        return False\n+\n+    def is_serializable(self, safe_serialization=None):\n+        return True\n+\n+    def check_quantized_param(\n+        self,\n+        model: \"PreTrainedModel\",\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        state_dict: Dict[str, Any],\n+        **kwargs,\n+    ) -> bool:\n+        from ..integrations import HiggsLinear\n+\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        if isinstance(module, HiggsLinear) and tensor_name == \"weight\" and param_value.dtype != torch.int16:\n+            # Only quantize weights of HiggsLinear modules that are not already quantized\n+            return True\n+        else:\n+            return False\n+\n+    def _dequantize(self, model):\n+        from ..integrations import dequantize_higgs\n+\n+        model = dequantize_higgs(model)\n+        return model"
        },
        {
            "sha": "00a7ee59664df21987fd0951873828c9877bca39",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -79,12 +79,14 @@\n     is_fbgemm_gpu_available,\n     is_flash_attn_2_available,\n     is_flax_available,\n+    is_flute_available,\n     is_fsdp_available,\n     is_ftfy_available,\n     is_g2p_en_available,\n     is_galore_torch_available,\n     is_gguf_available,\n     is_grokadamw_available,\n+    is_hadamard_available,\n     is_ipex_available,\n     is_jieba_available,\n     is_jinja_available,\n@@ -1239,6 +1241,15 @@ def require_fbgemm_gpu(test_case):\n     return unittest.skipUnless(is_fbgemm_gpu_available(), \"test requires fbgemm-gpu\")(test_case)\n \n \n+def require_flute_hadamard(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires higgs and hadamard\n+    \"\"\"\n+    return unittest.skipUnless(\n+        is_flute_available() and is_hadamard_available(), \"test requires flute and fast_hadamard_transform\"\n+    )(test_case)\n+\n+\n def require_phonemizer(test_case):\n     \"\"\"\n     Decorator marking a test that requires phonemizer"
        },
        {
            "sha": "74b6d39fda52bb2767c6d2bea5b5f3e563066de4",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -140,12 +140,14 @@\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n     is_flax_available,\n+    is_flute_available,\n     is_fsdp_available,\n     is_ftfy_available,\n     is_g2p_en_available,\n     is_galore_torch_available,\n     is_gguf_available,\n     is_grokadamw_available,\n+    is_hadamard_available,\n     is_hqq_available,\n     is_in_notebook,\n     is_ipex_available,"
        },
        {
            "sha": "f880535dd6fedb6fc65fb43e29388f2f746c2561",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -128,6 +128,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n         _faiss_available = False\n _ftfy_available = _is_package_available(\"ftfy\")\n _g2p_en_available = _is_package_available(\"g2p_en\")\n+_hadamard_available = _is_package_available(\"fast_hadamard_transform\")\n _ipex_available, _ipex_version = _is_package_available(\"intel_extension_for_pytorch\", return_version=True)\n _jieba_available = _is_package_available(\"jieba\")\n _jinja_available = _is_package_available(\"jinja2\")\n@@ -332,6 +333,10 @@ def is_torch_deterministic():\n         return True\n \n \n+def is_hadamard_available():\n+    return _hadamard_available\n+\n+\n def is_hqq_available(min_version: str = HQQ_MIN_VERSION):\n     return _hqq_available and version.parse(_hqq_version) >= version.parse(min_version)\n \n@@ -615,6 +620,13 @@ def is_flax_available():\n     return _flax_available\n \n \n+def is_flute_available():\n+    try:\n+        return importlib.util.find_spec(\"flute\") is not None and importlib.metadata.version(\"flute-kernel\") >= \"0.3.0\"\n+    except importlib.metadata.PackageNotFoundError:\n+        return False\n+\n+\n def is_ftfy_available():\n     return _ftfy_available\n "
        },
        {
            "sha": "3160c3481da1d74250087abc82fef008e69f1e3d",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -42,6 +42,7 @@ class QuantizationMethod(str, Enum):\n     VPTQ = \"vptq\"\n     QUANTO = \"quanto\"\n     EETQ = \"eetq\"\n+    HIGGS = \"higgs\"\n     HQQ = \"hqq\"\n     COMPRESSED_TENSORS = \"compressed-tensors\"\n     FBGEMM_FP8 = \"fbgemm_fp8\"\n@@ -1340,6 +1341,58 @@ def get_loading_attributes(self):\n         return loading_attibutes_dict\n \n \n+@dataclass\n+class HiggsConfig(QuantizationConfigMixin):\n+    \"\"\"\n+    HiggsConfig is a configuration class for quantization using the HIGGS method.\n+\n+    Args:\n+        bits (int, *optional*, defaults to 4):\n+            Number of bits to use for quantization. Can be 2, 3 or 4. Default is 4.\n+        p (int, *optional*, defaults to 2):\n+            Quantization grid dimension. 1 and 2 are supported. 2 is always better in practice. Default is 2.\n+        modules_to_not_convert (`list`, *optional*, default to [\"lm_head\"]):\n+            List of linear layers that should not be quantized.\n+        hadamard_size (int, *optional*, defaults to 512):\n+            Hadamard size for the HIGGS method. Default is 512. Input dimension of matrices is padded to this value. Decreasing this below 512 will reduce the quality of the quantization.\n+        group_size (int, *optional*, defaults to 256):\n+            Group size for the HIGGS method. Can be 64, 128 or 256. Decreasing it barely affects the performance. Default is 256. Must be a divisor of hadamard_size.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        bits: int = 4,\n+        p: int = 2,\n+        modules_to_not_convert: Optional[List[str]] = None,\n+        hadamard_size: int = 512,\n+        group_size: int = 256,\n+        **kwargs,\n+    ):\n+        if modules_to_not_convert is None:\n+            modules_to_not_convert = [\"lm_head\"]\n+        self.quant_method = QuantizationMethod.HIGGS\n+        self.bits = bits\n+        self.p = p\n+        self.modules_to_not_convert = modules_to_not_convert\n+        self.hadamard_size = hadamard_size\n+        self.group_size = group_size\n+\n+        self.post_init()\n+\n+    def post_init(self):\n+        r\"\"\"\n+        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n+        \"\"\"\n+        if self.bits not in [2, 3, 4]:\n+            raise ValueError(\"bits must be 2, 3, or 4\")\n+        if self.p not in [1, 2]:\n+            raise ValueError(\"p must be 1 or 2. 2 is always better in practice\")\n+        if self.group_size not in [64, 128, 256]:\n+            raise ValueError(\"group_size must be 64, 128, or 256\")\n+        if self.hadamard_size % self.group_size != 0:\n+            raise ValueError(\"hadamard_size must be divisible by group_size\")\n+\n+\n @dataclass\n class TorchAoConfig(QuantizationConfigMixin):\n     \"\"\"This is a config class for torchao quantization/sparsity techniques."
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/higgs/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/tests%2Fquantization%2Fhiggs%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/tests%2Fquantization%2Fhiggs%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2F__init__.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d"
        },
        {
            "sha": "26ee6bc0564777e6ea9657392feefab3885874ab",
            "filename": "tests/quantization/higgs/test_higgs.py",
            "status": "added",
            "additions": 197,
            "deletions": 0,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py?ref=64c05eecd68712f2a67bb9f1fb1292eccf4b5b3d",
            "patch": "@@ -0,0 +1,197 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, HiggsConfig, OPTForCausalLM\n+from transformers.testing_utils import (\n+    require_accelerate,\n+    require_flute_hadamard,\n+    require_torch_gpu,\n+    require_torch_multi_gpu,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_accelerate_available, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n+\n+@require_torch_gpu\n+class HiggsConfigTest(unittest.TestCase):\n+    def test_to_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a config and converts it to a dict, the dict is the same as the config object\n+        \"\"\"\n+        quantization_config = HiggsConfig()\n+        config_to_dict = quantization_config.to_dict()\n+\n+        for key in config_to_dict:\n+            self.assertEqual(getattr(quantization_config, key), config_to_dict[key])\n+\n+    def test_from_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a dict and converts it to a config object, the config object is the same as the dict\n+        \"\"\"\n+        dict = {\"modules_to_not_convert\": [\"embed_tokens\", \"lm_head\"], \"quant_method\": \"higgs\"}\n+        quantization_config = HiggsConfig.from_dict(dict)\n+\n+        self.assertEqual(dict[\"modules_to_not_convert\"], quantization_config.modules_to_not_convert)\n+        self.assertEqual(dict[\"quant_method\"], quantization_config.quant_method)\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_flute_hadamard\n+@require_accelerate\n+# @require_read_token\n+class HiggsTest(unittest.TestCase):\n+    model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n+\n+    input_text = \"A quick brown fox jumps over the\"\n+    max_new_tokens = 2\n+\n+    EXPECTED_OUTPUT = \"A quick brown fox jumps over the lazy dog\"\n+\n+    device_map = \"cuda\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Setup quantized model\n+        \"\"\"\n+        quantization_config = HiggsConfig()\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n+        )\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_quantized_model_conversion(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model has been converted properly\n+        \"\"\"\n+\n+        from transformers.integrations import HiggsLinear, replace_with_higgs_linear\n+\n+        model_id = \"facebook/opt-350m\"\n+        config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n+        quantization_config = HiggsConfig()\n+\n+        with init_empty_weights():\n+            model = OPTForCausalLM(config)\n+\n+        nb_linears = 0\n+        for module in model.modules():\n+            if isinstance(module, torch.nn.Linear):\n+                nb_linears += 1\n+\n+        model, _ = replace_with_higgs_linear(model, quantization_config=quantization_config)\n+        nb_higgs_linear = 0\n+        for module in model.modules():\n+            if isinstance(module, HiggsLinear):\n+                nb_higgs_linear += 1\n+\n+        self.assertEqual(nb_linears - 1, nb_higgs_linear)\n+\n+        with init_empty_weights():\n+            model = OPTForCausalLM(config)\n+        quantization_config = HiggsConfig(modules_to_not_convert=[\"fc1\"])\n+        model, _ = replace_with_higgs_linear(model, quantization_config=quantization_config)\n+        nb_higgs_linear = 0\n+        for module in model.modules():\n+            if isinstance(module, HiggsLinear):\n+                nb_higgs_linear += 1\n+\n+        self.assertEqual(nb_linears - 24, nb_higgs_linear)\n+\n+    def test_quantized_model(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    def test_save_pretrained(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_quantized_model_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with multiple GPUs\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        quantization_config = HiggsConfig()\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, device_map=\"auto\", quantization_config=quantization_config\n+        )\n+        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_save_pretrained_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"auto\")\n+            self.assertTrue(set(model.hf_device_map.values()) == {0, 1})\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @unittest.skip(\"This will almost surely OOM. Enable when swithed to a smaller model\")\n+    def test_dequantize(self):\n+        \"\"\"\n+        Test the ability to dequantize a model\n+        \"\"\"\n+        self.quantized_model.dequantize()\n+\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)"
        }
    ],
    "stats": {
        "total": 1249,
        "additions": 1249,
        "deletions": 0
    }
}