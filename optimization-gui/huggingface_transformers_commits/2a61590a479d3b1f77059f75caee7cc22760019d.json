{
    "author": "Cyrilvallez",
    "message": "Fix broken models due to modeling/weight mapping (#42468)\n\n* lfm2\n\n* fix\n\n* config\n\n* fixes\n\n* fix prefixes\n\n* finally\n\n* copies\n\n* oupsi\n\n* rework key renaming\n\n* fixes\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix lfm2\n\n* fix\n\n* fix more bad mappings",
    "sha": "2a61590a479d3b1f77059f75caee7cc22760019d",
    "files": [
        {
            "sha": "159c503f0f562e43435c480f374eb139ba819595",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 49,
            "deletions": 7,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -70,11 +70,56 @@ def _build_checkpoint_conversion_mapping():\n                 operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n             ),\n             WeightConverter(\n-                source_patterns=[\"mlp.experts.*.down_proj.weight\"],\n+                source_patterns=\"mlp.experts.*.down_proj.weight\",\n                 target_patterns=\"mlp.experts.down_proj\",\n                 operations=[MergeModulelist(dim=0)],\n             ),\n         ],\n+        \"phimoe\": [\n+            WeightConverter(\n+                source_patterns=[\n+                    \"mlp.experts.*.w1.weight\",\n+                    \"mlp.experts.*.w3.weight\",\n+                ],\n+                target_patterns=\"mlp.experts.gate_up_proj\",\n+                operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n+            ),\n+            WeightConverter(\n+                source_patterns=\"mlp.experts.*.w2.weight\",\n+                target_patterns=\"mlp.experts.down_proj\",\n+                operations=[MergeModulelist(dim=0)],\n+            ),\n+        ],\n+        \"lfm2_moe\": [\n+            WeightConverter(\n+                source_patterns=[\n+                    \"feed_forward.experts.*.w1.weight\",\n+                    \"feed_forward.experts.*.w3.weight\",\n+                ],\n+                target_patterns=\"feed_forward.experts.gate_up_proj\",\n+                operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n+            ),\n+            WeightConverter(\n+                source_patterns=\"feed_forward.experts.*.w2.weight\",\n+                target_patterns=\"feed_forward.experts.down_proj\",\n+                operations=[MergeModulelist(dim=0)],\n+            ),\n+        ],\n+        \"jamba\": [\n+            WeightConverter(\n+                source_patterns=[\n+                    \"feed_forward.experts.*.gate_proj.weight\",\n+                    \"feed_forward.experts.*.up_proj.weight\",\n+                ],\n+                target_patterns=\"feed_forward.experts.gate_up_proj\",\n+                operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n+            ),\n+            WeightConverter(\n+                source_patterns=\"feed_forward.experts.*.down_proj.weight\",\n+                target_patterns=\"feed_forward.experts.down_proj\",\n+                operations=[MergeModulelist(dim=0)],\n+            ),\n+        ],\n         \"timm_wrapper\": [\n             # Simply add the prefix `timm_model`\n             # TODO: Would be probably much cleaner with a `add_prefix` argument in WeightRenaming\n@@ -117,16 +162,13 @@ def _build_checkpoint_conversion_mapping():\n             ),\n         ]\n \n-    mapping[\"phimoe\"] = mapping[\"mixtral\"].copy()\n     mapping[\"deepseek_v2\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"deepseek_v3\"] = mapping[\"qwen2_moe\"].copy()\n-    mapping[\"dot1\"] = mapping[\"qwen2_moe\"].copy()\n-    mapping[\"ernie_4_5_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"dots1\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"ernie4_5_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"glm4_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"glm4v_moe\"] = mapping[\"qwen2_moe\"].copy()\n-    mapping[\"jamba\"] = mapping[\"qwen2_moe\"].copy()\n-    mapping[\"lfm2_moe\"] = mapping[\"mixtral\"].copy()\n-    mapping[\"long_cat_flash\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"longcat_flash\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"qwen3_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"qwen3_omni_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"qwen3_next\"] = mapping[\"qwen2_moe\"].copy()"
        },
        {
            "sha": "b3cb6ebd1be4904d76c999190e40eb94cd28ba9a",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 69,
            "deletions": 99,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -48,27 +48,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def compile_glob_rule(source_glob: str, target_glob: str) -> tuple[re.Pattern, str]:\n-    \"\"\"\n-    Convert a glob-style source + target into a full regex + replacement.\n-\n-    Rules:\n-      - '*' in source_glob  →  (.*) capture group\n-      - '*' in target_glob  →  \\\\1, \\\\2, ... backrefs\n-    \"\"\"\n-    regex = re.compile(source_glob)\n-\n-    counter = 0\n-\n-    def _star_to_backref(_: re.Match) -> str:\n-        nonlocal counter\n-        counter += 1\n-        return rf\"\\{counter}\"\n-\n-    replacement = re.sub(r\"\\*\", _star_to_backref, target_glob)\n-    return regex, replacement\n-\n-\n def build_glob_alternation(\n     globs: list[Union[WeightRenaming, WeightConverter, str]],\n ) -> tuple[re.Pattern, dict[str, str], dict[str, str]]:\n@@ -303,6 +282,7 @@ def convert(\n class WeightTransform:\n     source_patterns: Union[str, list[str]] = field(init=True)\n     target_patterns: Union[str, list[str]] = field(init=True)\n+    compiled_sources: re.Pattern = field(init=False)\n \n     distributed_operation: Optional[TensorParallelLayer] = None\n     quantization_operation: Optional[ConversionOps] = None\n@@ -322,20 +302,27 @@ def __post_init__(self):\n         for i, pattern in enumerate(self.target_patterns):\n             # Some mapping contains `^` to notify start of string when matching -> remove it during reverse mapping\n             pattern = pattern.removeprefix(\"^\")\n-            # This is ugly but needed for reverse mapping of Qwen2.5!\n-            if r\"(?!\\.(language_model|visual))\" in pattern:\n-                pattern = pattern.replace(r\"(?!\\.(language_model|visual))\", \"\")\n-            # Allow capturing groups in patterns, i.e. to add a prefix to all keys (e.g. timm_wrapper)\n+            # Remove negative lookahead if any. This is ugly but needed for reverse mapping of Qwen2.5 and Sam3!\n+            pattern = re.sub(r\"\\(\\?!.+\\)\", \"\", pattern)\n+            # Allow capturing groups in patterns, i.e. to add/remove a prefix to all keys (e.g. timm_wrapper, sam3)\n             if r\"(.+)\" in pattern:\n-                pattern = pattern.replace(r\"(.+)\", \"\")\n+                pattern = pattern.replace(r\"(.+)\", r\"\\1\")\n             self.target_patterns[i] = pattern\n \n-        # We also need to check capturing groups in the sources during reverse mapping (e.g. timm_wrapper)\n+        # We also need to check capturing groups in the sources during reverse mapping (e.g. timm_wrapper, sam3)\n         for i, pattern in enumerate(self.source_patterns):\n             if r\"\\1\" in pattern:\n-                pattern = pattern.replace(r\"\\1\", \"\")\n+                pattern = pattern.replace(r\"\\1\", r\"(.+)\")\n             self.source_patterns[i] = pattern\n \n+        # Construct the regex we will use to rename keys from the sources to the targets\n+        branches = []\n+        for i, source_pattern in enumerate(self.source_patterns):\n+            group_name = f\"g{i}\"\n+            pattern = source_pattern.replace(\".*.\", r\"\\..*\\.\")\n+            branches.append(f\"(?P<{group_name}>{pattern})\")\n+        self.compiled_sources = re.compile(\"|\".join(branches))\n+\n     def add_tensor(self, target_key: str, source_key: str, source_pattern: str, future: Future):\n         self.collected_tensors[source_pattern].append(future)\n         self.layer_targets[target_key].add(source_key)\n@@ -344,6 +331,32 @@ def reset(self) -> None:\n         \"\"\"Clean-up the collected tensors to make sure we don't keep references to past tensors in memory.\"\"\"\n         self.collected_tensors = defaultdict(list)\n \n+    def rename_source_key(self, source_key: str) -> tuple[str, str | None]:\n+        \"\"\"\n+        Return a tuple (renamed_key, source_pattern_producing_the_match).\n+        Try renaming `source_key` according to the source and target patterns of the current WeightTransform.\n+        In case of a one-to-many transform, i.e. we have several target patterns, the matching source pattern\n+        will be replaced by the first of all the target patterns (they are then correctly expanded in the Operations).\n+        \"\"\"\n+        # Try matching one of the alternation branches\n+        match_object = self.compiled_sources.search(source_key)\n+        if match_object is None:\n+            return source_key, None\n+        # Find the source that produced the match (it's the first group that matched, as the search stops after first branch match)\n+        matching_group_name = next(name for name, val in match_object.groupdict().items() if val is not None)\n+        source_pattern_that_matched = self.source_patterns[int(matching_group_name[1:])]\n+        # If we matched, we always replace with the first target pattern, in case we have several (one to many transform)\n+        replacement = self.target_patterns[0]\n+        # # Allow capturing groups in patterns, i.e. to add a prefix to all keys (e.g. timm_wrapper, sam3)\n+        if r\"\\1\" in replacement:\n+            # The index of the internal group we need to replace is the index of the matched named group as it comes\n+            # inside that matched named group\n+            replaced_group_idx = self.compiled_sources.groupindex[matching_group_name] + 1\n+            replacement = replacement.replace(r\"\\1\", match_object.group(replaced_group_idx))\n+        renamed_key = source_key.replace(match_object.group(0), replacement)\n+\n+        return renamed_key, source_pattern_that_matched\n+\n     def reverse_transform(self) -> WeightTransform:\n         \"\"\"Reverse the current `WeightTransform` instance, to be able to save with the opposite weight transformations.\"\"\"\n         # TODO: check this and relax when quantizer have `reverse_op`\n@@ -613,54 +626,30 @@ class SkipLayer(Exception):\n     pass\n \n \n-def repl(m, repl_map: dict[str, str]) -> str:\n-    # Collect all groups that matched\n-    matched_groups = [name for name, val in m.groupdict().items() if val]\n-\n-    if len(matched_groups) == 0:\n-        # Should never happen\n-        return m.group(0)\n-\n-    if len(matched_groups) > 1:\n-        raise ValueError(\n-            \"only a single match should happen, your regex patterns are tangled: \"\n-            f\"groups matched = {matched_groups} for the patternsL {repl_map.keys()}\"\n-        )\n-\n-    # Exactly one match => return replacement\n-    name = matched_groups[0]\n-    replacement = repl_map[name]\n-    # Allow capturing groups in patterns, i.e. to add a prefix to all keys (e.g. timm_wrapper)\n-    if r\"\\1\" in replacement and len(m.groups()) > 1:\n-        replacement = replacement.replace(r\"\\1\", m.group(1))\n-\n-    return replacement\n-\n-\n def rename_source_key(\n     source_key: str,\n-    rename_alternation: re.Pattern,\n-    rename_by_group: dict,\n-    weight_pattern_alternation: re.Pattern | None,\n-    weight_pattern_by_group: dict | None,\n+    weight_renamings: list[WeightRenaming],\n+    weight_converters: list[WeightConverter],\n     prefix: str | None = None,\n     meta_state_dict: dict | None = None,\n-) -> tuple[str, re.Match | None]:\n+) -> tuple[str, str | None]:\n     \"\"\"\n     Rename a source key given all the renaming and weight conversion patterns we have. Also takes care of adding/removing\n     the base model prefix during loading if necesary.\n     \"\"\"\n-    # 1. apply all renamings\n-    renamed_key = rename_alternation.sub(lambda m: repl(m, rename_by_group), source_key).replace(\"\\\\\", \"\")\n-\n-    # 2. apply renaming through weight conversions on the key if we have any WeightConverter\n-    matched_converter_pattern = (\n-        weight_pattern_alternation.search(renamed_key) if weight_pattern_alternation is not None else None\n-    )\n-    if matched_converter_pattern is not None:\n-        renamed_key = weight_pattern_alternation.sub(lambda m: repl(m, weight_pattern_by_group), renamed_key).replace(\n-            \"\\\\\", \"\"\n-        )\n+    renamed_key = source_key\n+    # 1. apply all renamings in turns (if multiple match, it's the responsibility of the mappings to make sure they\n+    # are coherent)\n+    for renaming in weight_renamings:\n+        renamed_key, _ = renaming.rename_source_key(renamed_key)\n+\n+    # 2. apply renaming through weight conversions on the key if we have any WeightConverter (here we stop after\n+    # the first match, as we assume only 1 converter can match any source key)\n+    source_pattern = None\n+    for converter in weight_converters:\n+        renamed_key, source_pattern = converter.rename_source_key(renamed_key)\n+        if source_pattern is not None:\n+            break\n \n     # 3. check if we need to add or remove prefix if necesary (only during loading, not saving)\n     if prefix is not None and meta_state_dict is not None:\n@@ -672,7 +661,7 @@ def rename_source_key(\n         elif meta_state_dict.get(f\"{prefix}.{renamed_key}\") is not None:\n             renamed_key = f\"{prefix}.{renamed_key}\"\n \n-    return renamed_key, matched_converter_pattern\n+    return renamed_key, source_pattern\n \n \n def convert_and_load_state_dict_in_model(\n@@ -799,10 +788,6 @@ def convert_and_load_state_dict_in_model(\n \n     # build '(?P<g0>.*.*\\\\.block_sparse_moe\\\\..*)' and group to source {'g0': '*.block_sparse_moe.'}\n     # and target to source {'g0': '*.mlp.'}. This allows us to quickly find which pattern matched.\n-    rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n-    weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = None, None, None\n-    if converters != []:\n-        weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = build_glob_alternation(converters)\n     if tp_plan != {}:\n         tp_plan_alt, tp_plan_by_group_name, _ = build_glob_alternation(list(tp_plan.keys()))\n     if dtype_plan != {}:\n@@ -813,24 +798,19 @@ def convert_and_load_state_dict_in_model(\n     state_dict = sorted(state_dict.items(), key=lambda kv: dot_natural_key(kv[0]))\n     for original_key, tensor in state_dict:\n         # 1. Rename the key according to all renaming pattern and optional weight converter patterns\n-        renamed_key, matched_pattern = rename_source_key(\n-            original_key,\n-            rename_alt,\n-            rename_by_group,\n-            weight_pattern_alt,\n-            tgt_group_to_glob,\n-            prefix,\n-            meta_model_state_dict,\n+        renamed_key, source_pattern = rename_source_key(\n+            original_key, renamings, converters, prefix, meta_model_state_dict\n         )\n \n         # 2. finally, collect the tensor into the proper converter\n         if renamed_key in missing_keys:\n             empty_param = meta_model_state_dict.get(renamed_key)\n-            if matched_pattern:\n-                new_converter = deepcopy(pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]])\n+            # If we enter here, we have a WeightConverter operation to perform\n+            if source_pattern is not None:\n+                new_converter = deepcopy(pattern_to_converter[source_pattern])\n                 # each target key gets its own converter instance\n                 mapping = param_name_to_load.setdefault(renamed_key, new_converter)\n-                source_pattern = src_group_to_glob[matched_pattern.lastgroup]\n+            # Otherwise, only potential renaming\n             else:\n                 mapping = param_name_to_load.setdefault(renamed_key, WeightRenaming(original_key, renamed_key))\n                 source_pattern = original_key\n@@ -882,8 +862,8 @@ def convert_and_load_state_dict_in_model(\n                 future = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n \n             mapping.add_tensor(renamed_key, original_key, source_pattern, future)\n-        elif matched_pattern:  # add all target keys as unexpected\n-            mapping = pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]]\n+        elif source_pattern is not None:  # add all target keys as unexpected\n+            mapping = pattern_to_converter[source_pattern]\n             for k in mapping.target_patterns:\n                 unexpected_keys.add(renamed_key.replace(mapping.target_patterns[0], k))\n         else:\n@@ -964,24 +944,14 @@ def revert_weight_conversion(model: PreTrainedModel, state_dict: dict[str, torch\n     pattern_to_converter = {k: converter for converter in converters for k in converter.source_patterns}\n     conversion_mapping = {}\n \n-    # build '(?P<g0>.*.*\\\\.block_sparse_moe\\\\..*)' and group to source {'g0': '*.block_sparse_moe.'}\n-    # and target to source {'g0': '*.mlp.'}. This allows us to quickly find which pattern matched.\n-    rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n-    weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = None, None, None\n-    if converters != []:\n-        weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = build_glob_alternation(converters)\n-\n     state_dict = sorted(state_dict.items(), key=lambda kv: dot_natural_key(kv[0]))\n     for original_key, tensor in state_dict:\n         # Rename the key according to all renaming pattern and optional weight converter patterns\n-        renamed_key, matched_pattern = rename_source_key(\n-            original_key, rename_alt, rename_by_group, weight_pattern_alt, tgt_group_to_glob\n-        )\n-        if matched_pattern is not None:\n-            new_converter = deepcopy(pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]])\n+        renamed_key, source_pattern = rename_source_key(original_key, renamings, converters)\n+        if source_pattern is not None:\n+            new_converter = deepcopy(pattern_to_converter[source_pattern])\n             # each target key gets its own converter instance\n             mapping = conversion_mapping.setdefault(renamed_key, new_converter)\n-            source_pattern = src_group_to_glob[matched_pattern.lastgroup]\n         else:\n             mapping = conversion_mapping.setdefault(renamed_key, WeightRenaming(original_key, renamed_key))\n             source_pattern = original_key"
        },
        {
            "sha": "70142f2bf2968e3d307ba935a3ad7ec24083cff4",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -480,18 +480,15 @@ def accelerate_disk_offload(\n     renamed) will be mapped to where they already reside on disk. Otherwise, the parameters will be resaved inside\n     `disk_offload_folder` during loading.\n     \"\"\"\n-    from ..core_model_loading import WeightRenaming, build_glob_alternation, repl\n+    from ..core_model_loading import WeightRenaming, rename_source_key\n \n     if disk_offload_folder is not None:\n         os.makedirs(disk_offload_folder, exist_ok=True)\n     is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n \n-    rename = False\n+    renamings = []\n     if weight_mapping is not None:\n         renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]\n-        if len(renamings) > 0:\n-            rename = True\n-            rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n \n     # In this case, the offload index is simply the existing safetensors (except if using custom weight loading\n     # Operation, e.g. the MoE models, where we need to resave the weights that were changed at loading time)\n@@ -505,10 +502,7 @@ def accelerate_disk_offload(\n             weight_map = {k: os.path.join(folder, v) for k, v in sharded_metadata[\"weight_map\"].items()}\n \n         # Update the weight names according to the `weight_mapping`\n-        weight_renaming_map = {\n-            rename_alt.sub(lambda m: repl(m, rename_by_group), k).replace(\"\\\\\", \"\") if rename else k: k\n-            for k in weight_map\n-        }\n+        weight_renaming_map = {rename_source_key(k, renamings, [])[0]: k for k in weight_map}\n \n         # Prepare the index using existing safetensors files\n         disk_offload_index = {"
        },
        {
            "sha": "4c1c12a4058e2f77bbe77349b0197f54db89bf6c",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -17,7 +17,7 @@\n import os\n from typing import Any, Literal\n \n-from ..core_model_loading import WeightRenaming, build_glob_alternation, repl\n+from ..core_model_loading import WeightRenaming, rename_source_key\n from ..utils import (\n     CONFIG_NAME,\n     cached_file,\n@@ -294,6 +294,9 @@ def load_adapter(\n             adapter_state_dict = load_peft_weights(peft_model_id, token=token, device=device, **adapter_kwargs)\n \n         # We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility\n+        renamings = []\n+        if key_mapping:\n+            renamings = [entry for entry in key_mapping if isinstance(entry, WeightRenaming)]\n         processed_adapter_state_dict = {}\n         prefix = \"base_model.model.\"\n         for key, value in adapter_state_dict.items():\n@@ -302,10 +305,7 @@ def load_adapter(\n             else:\n                 new_key = key\n \n-            if key_mapping:\n-                renamings = [entry for entry in key_mapping if isinstance(entry, WeightRenaming)]\n-                rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n-                new_key = rename_alt.sub(lambda m: repl(m, rename_by_group), new_key).replace(\"\\\\\", \"\")\n+            new_key = rename_source_key(new_key, renamings, [])[0]\n \n             # For hotswapping, we need the adapter name to be present in the state dict keys\n             if hotswap:"
        },
        {
            "sha": "93b6ad208abec5ddf26cc0b451343aea93cf21e3",
            "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -54,6 +54,8 @@ class Lfm2MoeConfig(PreTrainedConfig):\n             with longer `max_position_embeddings`.\n         max_position_embeddings (`int`, *optional*, defaults to 128000):\n             The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -118,6 +120,7 @@ def __init__(\n         tie_word_embeddings: bool = True,\n         rope_parameters: RopeParameters = None,\n         max_position_embeddings: int = 128_000,\n+        initializer_range: float = 0.02,\n         use_cache: bool = True,\n         norm_eps: float = 0.00001,\n         num_attention_heads: int = 32,\n@@ -138,6 +141,7 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n         self.use_cache = use_cache\n         self.norm_eps = norm_eps\n "
        },
        {
            "sha": "0d35c2420e9a3c420296c3b3ac6f58c783ec58a7",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -25,7 +25,6 @@\n from torch import nn\n \n from ... import initialization as init\n-from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n@@ -156,7 +155,6 @@ def __init__(self, config):\n         self.intermediate_dim = config.moe_intermediate_size\n         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n-        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n         self,\n@@ -165,22 +163,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n-            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = F.silu(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "f4549b2e6aa2212e44bb97c62e287a5b6000ae22",
            "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
            "status": "modified",
            "additions": 30,
            "deletions": 1,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -14,6 +14,7 @@\n from typing import Optional\n \n import torch\n+import torch.nn.functional as F\n from torch import nn\n \n from ... import initialization as init\n@@ -69,7 +70,35 @@ def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = Non\n \n \n class Lfm2MoeExperts(Qwen2MoeExperts):\n-    pass\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.act_fn\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n+    ) -> torch.Tensor:\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+\n+        for expert_idx in expert_hit:\n+            expert_idx = expert_idx[0]\n+            if expert_idx == self.num_experts:\n+                continue\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = F.silu(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n+        return final_hidden_states\n \n \n class Lfm2MoeSparseMoeBlock(nn.Module):"
        },
        {
            "sha": "5a407efe58bbb98cafa9cc5c010d5eecf6079cda",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -2089,7 +2089,9 @@ def _embed_pixels(\n \n class Sam3Model(Sam3PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n-    _checkpoint_conversion_mapping = {\"detector_model.\": \"\"}\n+    _checkpoint_conversion_mapping = {\n+        r\"detector_model.(.+)\": r\"\\1\"  # the regex allows to remove the prefix, and add it back in revert mode\n+    }\n     _keys_to_ignore_on_load_unexpected = [\n         r\"^tracker_model.\",\n         r\"^tracker_neck.\","
        },
        {
            "sha": "8a1a77eaafd9f8dba24617a4d183bfc6a7c55afc",
            "filename": "src/transformers/models/sam3_tracker/modeling_sam3_tracker.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -768,7 +768,7 @@ class Sam3TrackerModel(Sam3TrackerPreTrainedModel):\n         \"occlusion_spatial_embedding_parameter\",\n     ]\n     _checkpoint_conversion_mapping = {\n-        \"tracker_model.\": \"\",\n+        r\"tracker_model.(.+)\": r\"\\1\",  # the regex allows to remove the prefix, and add it back in revert mode\n         \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n         \"tracker_neck.\": \"vision_encoder.neck.\",\n     }"
        },
        {
            "sha": "18937f5455f645baafeb5e3c729d22bba5b1deec",
            "filename": "src/transformers/models/sam3_tracker/modular_sam3_tracker.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -180,7 +180,7 @@ class Sam3TrackerMaskDecoder(Sam2MaskDecoder):\n \n class Sam3TrackerModel(Sam2Model):\n     _checkpoint_conversion_mapping = {\n-        \"tracker_model.\": \"\",\n+        r\"tracker_model.(.+)\": r\"\\1\",  # the regex allows to remove the prefix, and add it back in revert mode\n         \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n         \"tracker_neck.\": \"vision_encoder.neck.\",\n     }"
        },
        {
            "sha": "125b86bacea0a4449af17f524809f7f5fc95794b",
            "filename": "src/transformers/models/sam3_tracker_video/modeling_sam3_tracker_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -1570,7 +1570,7 @@ class Sam3TrackerVideoModel(Sam3TrackerVideoPreTrainedModel):\n     _tied_weights_keys = {}\n     _keys_to_ignore_on_load_missing = []\n     _checkpoint_conversion_mapping = {\n-        \"tracker_model.\": \"\",\n+        r\"tracker_model.(.+)\": r\"\\1\",  # the regex allows to remove the prefix, and add it back in revert mode\n         \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n         \"tracker_neck.\": \"vision_encoder.neck.\",\n     }"
        },
        {
            "sha": "12e4bc8854e0de0ff8dafeb7641b1d1b5dae6dc5",
            "filename": "src/transformers/models/sam3_tracker_video/modular_sam3_tracker_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -456,7 +456,7 @@ class Sam3TrackerVideoMaskDecoder(Sam2VideoMaskDecoder):\n \n class Sam3TrackerVideoModel(Sam2VideoModel):\n     _checkpoint_conversion_mapping = {\n-        \"tracker_model.\": \"\",\n+        r\"tracker_model.(.+)\": r\"\\1\",  # the regex allows to remove the prefix, and add it back in revert mode\n         \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n         \"tracker_neck.\": \"vision_encoder.neck.\",\n     }"
        },
        {
            "sha": "d2b5e0949cac0cf59d690d78ffa16456a0940311",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -4058,7 +4058,6 @@ def test_tp_plan_matches_params(self):\n                 len(unused_entries) == 0, f\"The following entries of the TP-plan are not valid: {unused_entries}\"\n             )\n \n-    @unittest.skip(\"Some models have wrong mappings....\")\n     def test_reverse_loading_mapping(self):\n         \"\"\"Make sure we can load and save correctly the models having any weight renaming mapping or weight conversion\n         mapping.\n@@ -4073,7 +4072,7 @@ def test_reverse_loading_mapping(self):\n \n         #  Some MoE models alternate between a classic MLP and a MoE layer, in which case we want to have at\n         # lest one MoE layer here to check the mapping\n-        config_to_set = config.get_text_config()\n+        config_to_set = config.get_text_config(decoder=True)\n         config_to_set.first_k_dense_replace = 1  # means that the first layer (idx 0) will be MLP, then MoE\n         config_to_set.moe_layer_start_index = 1  # same as above but for Ernie 4.5...\n         config_to_set.mlp_only_layers = [0]  # same but for qwens\n@@ -4137,7 +4136,6 @@ def test_reverse_loading_mapping(self):\n                     # Make sure both saved state_dict are identical\n                     self.assertTrue(compare_state_dicts(state_dict_saved_from_init, state_dict_saved_from_pretrained))\n \n-    @unittest.skip(\"Some models have wrong mappings....\")\n     def test_can_load_from_already_mapped_keys(self):\n         \"\"\"Test that we can correctly reload a model if we chose `save_original_format=False` in `save_pretrained`,\n         i.e. we do not reapply weight conversions when reloading if it was saved correctly already."
        },
        {
            "sha": "6198702fe4a9e5f4f93b2507e3422057d110446a",
            "filename": "tests/utils/test_core_model_loading.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a61590a479d3b1f77059f75caee7cc22760019d/tests%2Futils%2Ftest_core_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a61590a479d3b1f77059f75caee7cc22760019d/tests%2Futils%2Ftest_core_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_core_model_loading.py?ref=2a61590a479d3b1f77059f75caee7cc22760019d",
            "patch": "@@ -27,7 +27,7 @@\n     WeightRenaming,\n     build_glob_alternation,\n     convert_and_load_state_dict_in_model,\n-    repl,\n+    rename_source_key,\n     revert_weight_conversion,\n )\n from transformers.utils.import_utils import is_triton_available\n@@ -138,23 +138,24 @@ def test_sub_key_rewrites_targets(self):\n             WeightRenaming(\"block_sparse_moe.experts.*.w2.weight\", \"mlp.experts.down_proj\"),\n             WeightRenaming(\"model.language_model.*\", \"language_model\"),\n         ]\n-        rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n \n-        def rename(original_key: str) -> str:\n-            return rename_alt.sub(lambda m: repl(m, rename_by_group), original_key).replace(\"\\\\\", \"\")\n-\n-        self.assertEqual(rename(\"foo.block_sparse_moe.experts.3.w1.weight\"), \"foo.mlp.experts.gate_up_proj\")\n-        self.assertEqual(rename(\"foo.block_sparse_moe.experts.3.w2.weight\"), \"foo.mlp.experts.down_proj\")\n-        self.assertEqual(rename(\"model.language_model.lm_head.weight\"), \"language_model\")\n+        self.assertEqual(\n+            rename_source_key(\"foo.block_sparse_moe.experts.3.w1.weight\", renamings, [])[0],\n+            \"foo.mlp.experts.gate_up_proj\",\n+        )\n+        self.assertEqual(\n+            rename_source_key(\"foo.block_sparse_moe.experts.3.w2.weight\", renamings, [])[0],\n+            \"foo.mlp.experts.down_proj\",\n+        )\n+        self.assertEqual(rename_source_key(\"model.language_model.lm_head.weight\", renamings, [])[0], \"language_model\")\n \n     def test_sub_key_no_match_returns_original(self):\n         renamings = [\n             WeightRenaming(\"block_sparse_moe.experts.*.w1.weight\", \"*.mlp.experts.gate_up_proj\"),\n         ]\n-        rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n \n         key = \"unrelated.key\"\n-        renamed_key = rename_alt.sub(lambda m: repl(m, rename_by_group), key).replace(\"\\\\\", \"\")\n+        renamed_key, _ = rename_source_key(key, renamings, [])\n         self.assertEqual(renamed_key, key)\n \n "
        }
    ],
    "stats": {
        "total": 331,
        "additions": 184,
        "deletions": 147
    }
}