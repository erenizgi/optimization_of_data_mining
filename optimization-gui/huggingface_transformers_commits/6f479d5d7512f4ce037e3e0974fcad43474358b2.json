{
    "author": "sywangyi",
    "message": "extend test_beam_search_early_stop_heuristic case to other device (#42078)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>",
    "sha": "6f479d5d7512f4ce037e3e0974fcad43474358b2",
    "files": [
        {
            "sha": "740ba073f329480c02ceb43d3744e4a38c34db09",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f479d5d7512f4ce037e3e0974fcad43474358b2/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f479d5d7512f4ce037e3e0974fcad43474358b2/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=6f479d5d7512f4ce037e3e0974fcad43474358b2",
            "patch": "@@ -2718,7 +2718,7 @@ def test_beam_search_early_stop_heuristic(self):\n         question = tokenizer.apply_chat_template(\n             question, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n         )\n-        inputs = tokenizer(question, return_tensors=\"pt\", padding=True).to(\"cuda\")\n+        inputs = tokenizer(question, return_tensors=\"pt\", padding=True).to(torch_device)\n         outputs = model.generate(**inputs, generation_config=generation_config)\n         responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         self.assertEqual(responses[0], EXPECTED_OUTPUT)\n@@ -2737,7 +2737,7 @@ def test_beam_search_early_stop_heuristic(self):\n         cot_question = tokenizer.apply_chat_template(\n             cot_question, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n         )\n-        inputs = tokenizer([question, cot_question], return_tensors=\"pt\", padding=True).to(\"cuda\")\n+        inputs = tokenizer([question, cot_question], return_tensors=\"pt\", padding=True).to(torch_device)\n \n         outputs = model.generate(**inputs, generation_config=generation_config)\n         responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}