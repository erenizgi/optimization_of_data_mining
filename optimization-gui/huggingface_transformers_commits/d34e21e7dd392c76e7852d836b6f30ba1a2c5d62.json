{
    "author": "manueldeprada",
    "message": "New cache tests and refactored Hybrid Cache (#37972)",
    "sha": "d34e21e7dd392c76e7852d836b6f30ba1a2c5d62",
    "files": [
        {
            "sha": "d24edd390c26cdb2c0df57075e15875bb2153d61",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 155,
            "deletions": 131,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/d34e21e7dd392c76e7852d836b6f30ba1a2c5d62/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d34e21e7dd392c76e7852d836b6f30ba1a2c5d62/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=d34e21e7dd392c76e7852d836b6f30ba1a2c5d62",
            "patch": "@@ -21,6 +21,104 @@\n logger = logging.get_logger(__name__)\n \n \n+# Utility functions for static/sliding cache update logic\n+def _static_cache_update(\n+    k_cache: torch.Tensor,\n+    v_cache: torch.Tensor,\n+    key_states: torch.Tensor,\n+    value_states: torch.Tensor,\n+    cache_position: Optional[torch.LongTensor],\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Updates the static cache tensors in place.\n+\n+    Args:\n+        k_cache (`torch.Tensor`): The key cache tensor to update.\n+        v_cache (`torch.Tensor`): The value cache tensor to update.\n+        key_states (`torch.Tensor`): The new key states to add.\n+        value_states (`torch.Tensor`): The new value states to add.\n+        cache_position (`Optional[torch.LongTensor]`): The position indices where the new states should be inserted.\n+                                                       If None, the entire cache is overwritten (prefill).\n+\n+    Returns:\n+        Tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value cache tensors (modified in-place).\n+    \"\"\"\n+    if cache_position is None:\n+        # Prefill phase where seq_len potentially equals max_cache_len. Directly copy.\n+        k_cache.copy_(key_states)\n+        v_cache.copy_(value_states)\n+    else:\n+        # Generation phase. Update specific positions.\n+        # Use index_copy_ for in-place update (compile-friendly).\n+        try:\n+            k_cache.index_copy_(2, cache_position, key_states)\n+            v_cache.index_copy_(2, cache_position, value_states)\n+        except NotImplementedError:\n+            # Fallback for devices like MPS where index_copy_ might not be supported.\n+            k_cache[:, :, cache_position] = key_states\n+            v_cache[:, :, cache_position] = value_states\n+    return k_cache, v_cache\n+\n+\n+def _sliding_cache_update(\n+    k_cache: torch.Tensor,\n+    v_cache: torch.Tensor,\n+    key_states: torch.Tensor,\n+    value_states: torch.Tensor,\n+    cache_position: torch.LongTensor,\n+    max_cache_len: int,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Updates the sliding window cache tensors, returning the potentially modified tensors.\n+\n+    Args:\n+        k_cache (`torch.Tensor`): The key cache tensor to update.\n+        v_cache (`torch.Tensor`): The value cache tensor to update.\n+        key_states (`torch.Tensor`): The new key states to add.\n+        value_states (`torch.Tensor`): The new value states to add.\n+        cache_position (`torch.LongTensor`): The position indices where the new states should be inserted.\n+        max_cache_len (`int`): The maximum length of the sliding window cache.\n+\n+    Returns:\n+        Tuple[`torch.Tensor`, `torch.Tensor`]: The key and value tensors representing the cache state after the update.\n+                                               For prefill > window, these are the full input states.\n+                                               Otherwise, they are the updated cache tensors.\n+    \"\"\"\n+    # Handle prefill phase when prompt length > sliding_window_size\n+    if cache_position.shape[0] > max_cache_len:\n+        new_k = key_states[:, :, -max_cache_len:, :]\n+        new_v = value_states[:, :, -max_cache_len:, :]\n+        k_cache.copy_(new_k)\n+        v_cache.copy_(new_v)\n+        return key_states, value_states\n+\n+    # Sliding window logic for generation phase or prefill < window\n+    slicing = torch.arange(max_cache_len, device=value_states.device)\n+    current_seq_len = cache_position[-1] + 1  # Use last position to determine current length\n+    to_shift = current_seq_len > max_cache_len\n+    indices = (slicing + to_shift.sum()) % max_cache_len\n+\n+    k_out_shifted = k_cache[:, :, indices]\n+    v_out_shifted = v_cache[:, :, indices]\n+\n+    # Clamp cache_position to determine the *target index* within the shifted cache view\n+    update_position = cache_position.clamp(min=0, max=max_cache_len - 1)\n+\n+    try:\n+        k_out_updated = k_out_shifted.index_copy(2, update_position, key_states)\n+        v_out_updated = v_out_shifted.index_copy(2, update_position, value_states)\n+    except NotImplementedError:\n+        # Fallback for MPS: clone and modify the clone\n+        k_out_updated = k_out_shifted.clone()\n+        v_out_updated = v_out_shifted.clone()\n+        k_out_updated[:, :, update_position] = key_states\n+        v_out_updated[:, :, update_position] = value_states\n+\n+    k_cache.copy_(k_out_updated)\n+    v_cache.copy_(v_out_updated)\n+    return k_out_updated, v_out_updated\n+\n+\n class Cache:\n     \"\"\"\n     Base, abstract class for all caches. The actual data structure is specific to each subclass.\n@@ -1264,28 +1362,16 @@ def update(\n         \"\"\"\n         if cache_kwargs is None:\n             cache_kwargs = {}\n-        cache_position = cache_kwargs.get(\"cache_position\")\n-        k_out = self.key_cache[layer_idx]\n-        v_out = self.value_cache[layer_idx]\n-        key_states = key_states.to(k_out.dtype)\n-        value_states = value_states.to(v_out.dtype)\n \n-        if cache_position is None:\n-            k_out.copy_(key_states)\n-            v_out.copy_(value_states)\n-        else:\n-            # Note: here we use `tensor.index_copy_(dim, index, tensor)` that is equivalent to\n-            # `tensor[:, :, index] = tensor`, but the first one is compile-friendly and it does explicitly an in-place\n-            # operation, that avoids copies and uses less memory.\n-            try:\n-                k_out.index_copy_(2, cache_position, key_states)\n-                v_out.index_copy_(2, cache_position, value_states)\n-            except NotImplementedError:\n-                # The operator 'aten::index_copy.out' is not currently implemented for the MPS device.\n-                k_out[:, :, cache_position] = key_states\n-                v_out[:, :, cache_position] = value_states\n-\n-        return k_out, v_out\n+        key_states = key_states.to(self.key_cache[layer_idx].dtype)\n+        value_states = value_states.to(self.value_cache[layer_idx].dtype)\n+        return _static_cache_update(\n+            self.key_cache[layer_idx],\n+            self.value_cache[layer_idx],\n+            key_states,\n+            value_states,\n+            cache_kwargs.get(\"cache_position\"),\n+        )\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states that were seen by the model.\"\"\"\n@@ -1314,7 +1400,7 @@ class SlidingWindowCache(StaticCache):\n \n     The `to_shift` is only true once we are above sliding_window. Thus with `sliding_window==64`:\n \n-    indices = (slicing + to_shift[-1].int()-1) % self.config.sliding_window\n+    indices = (slicing + to_shift[-1].sum()-1) % self.config.sliding_window\n     tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n@@ -1398,46 +1484,21 @@ def update(\n         if cache_kwargs is None:\n             cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n-        k_out = self.key_cache[layer_idx]\n-        v_out = self.value_cache[layer_idx]\n-        key_states = key_states.to(k_out.dtype)\n-        value_states = value_states.to(v_out.dtype)\n \n-        # assume this only happens in prefill phase when prompt length > sliding_window_size (= max_cache_len)\n-        if cache_position.shape[0] >= self.max_cache_len:\n-            k_out = key_states[:, :, -self.max_cache_len :, :]\n-            v_out = value_states[:, :, -self.max_cache_len :, :]\n-            # Assumption: caches are all zeros at this point, `+=` is equivalent to `=` but compile-friendly\n-            self.key_cache[layer_idx] += k_out\n-            self.value_cache[layer_idx] += v_out\n-            # we should return the whole states instead of k_out, v_out to take the whole prompt\n-            # into consideration when building kv cache instead of just throwing away tokens outside of the window\n-            return key_states, value_states\n-\n-        slicing = torch.ones(self.max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n-        to_shift = cache_position > self.max_cache_len - 1\n-        cache_position = cache_position.clamp(0, self.max_cache_len - 1)\n-        indices = (slicing + to_shift[-1].int() - 1) % self.max_cache_len\n-\n-        k_out = k_out[:, :, indices]\n-        v_out = v_out[:, :, indices]\n-\n-        try:\n-            k_out.index_copy_(2, cache_position, key_states)\n-            v_out.index_copy_(2, cache_position, value_states)\n-        except NotImplementedError:\n-            # The operator 'aten::index_copy.out' is not currently implemented for the MPS device.\n-            k_out[:, :, cache_position] = key_states\n-            v_out[:, :, cache_position] = value_states\n-\n-        # `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\n-        self.key_cache[layer_idx].zero_()\n-        self.value_cache[layer_idx].zero_()\n+        if cache_position is None:\n+            raise ValueError(\"`cache_position` must be provided for SlidingWindowCache.\")\n \n-        self.key_cache[layer_idx] += k_out\n-        self.value_cache[layer_idx] += v_out\n+        key_states = key_states.to(self.key_cache[layer_idx].dtype)\n+        value_states = value_states.to(self.value_cache[layer_idx].dtype)\n \n-        return k_out, v_out\n+        return _sliding_cache_update(\n+            self.key_cache[layer_idx],\n+            self.value_cache[layer_idx],\n+            key_states,\n+            value_states,\n+            cache_position,\n+            self.max_cache_len,\n+        )\n \n     def get_max_cache_shape(self) -> Optional[int]:\n         return self.max_cache_len\n@@ -1680,12 +1741,13 @@ def __init__(\n         super().__init__()\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n             raise ValueError(\n-                \"Setting `cache_implementation` to 'sliding_window' requires the model config supporting \"\n+                \"Setting `cache_implementation` to 'hybrid' requires the model config supporting \"\n                 \"sliding window attention, please check if there is a `sliding_window` field in the model \"\n                 \"config and it's not set to None.\"\n             )\n-        self.max_cache_len = max_cache_len\n-        self._sliding_window_max_len = min(config.sliding_window, max_cache_len)\n+        self.max_cache_len = max_cache_len if max_cache_len is not None else config.max_position_embeddings\n+        # Sliding layers can't be larger than the overall max cache len\n+        self.sliding_window_len = min(config.sliding_window, self.max_cache_len)\n         self.max_batch_size = max_batch_size\n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         self.head_dim = (\n@@ -1694,22 +1756,17 @@ def __init__(\n \n         self._dtype = dtype\n         self.num_key_value_heads = (\n-            config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n+            config.num_attention_heads\n+            if getattr(config, \"num_key_value_heads\", None) is None\n+            else config.num_key_value_heads\n         )\n \n         layer_switch = config.sliding_window_pattern if hasattr(config, \"sliding_window_pattern\") else 2  # 2 is for BC\n-        self.is_sliding = torch.tensor(\n-            [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)], dtype=torch.bool\n-        )\n+        self.is_sliding_list = [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)]\n         self.key_cache: List[torch.Tensor] = []\n         self.value_cache: List[torch.Tensor] = []\n-        global_cache_shape = (self.max_batch_size, self.num_key_value_heads, max_cache_len, self.head_dim)\n-        sliding_cache_shape = (\n-            self.max_batch_size,\n-            self.num_key_value_heads,\n-            self._sliding_window_max_len,\n-            self.head_dim,\n-        )\n+        global_cache_shape = (self.max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n+        sliding_cache_shape = (self.max_batch_size, self.num_key_value_heads, self.sliding_window_len, self.head_dim)\n         device = torch.device(device) if device is not None else None\n         for i in range(config.num_hidden_layers):\n             if layer_device_map is not None:\n@@ -1718,50 +1775,14 @@ def __init__(\n                 layer_device = device\n             # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             # breaks when updating the cache.\n-            cache_shape = global_cache_shape if not self.is_sliding[i] else sliding_cache_shape\n+            cache_shape = sliding_cache_shape if self.is_sliding_list[i] else global_cache_shape\n             new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n             new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n             torch._dynamo.mark_static_address(new_layer_key_cache)\n             torch._dynamo.mark_static_address(new_layer_value_cache)\n             self.key_cache.append(new_layer_key_cache)\n             self.value_cache.append(new_layer_value_cache)\n \n-    def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-        if cache_position.shape[0] >= max_cache_len:\n-            k_out = key_states[:, :, -max_cache_len:, :]\n-            v_out = value_states[:, :, -max_cache_len:, :]\n-            # Assumption: caches are all zeros at this point, `+=` is equivalent to `=` but compile-friendly\n-            self.key_cache[layer_idx] += k_out\n-            self.value_cache[layer_idx] += v_out\n-            # we should return the whole states instead of k_out, v_out to take the whole prompt\n-            # into consideration when building kv cache instead of just throwing away tokens outside of the window\n-            return key_states, value_states\n-\n-        slicing = torch.ones(max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n-        to_shift = cache_position > max_cache_len - 1\n-        cache_position = cache_position.clamp(0, max_cache_len - 1)\n-        indices = (slicing + to_shift[-1].int() - 1) % max_cache_len\n-        k_out = k_out[:, :, indices]\n-        v_out = v_out[:, :, indices]\n-\n-        k_out[:, :, cache_position] = key_states\n-        v_out[:, :, cache_position] = value_states\n-        # `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\n-        self.key_cache[layer_idx].zero_()\n-        self.value_cache[layer_idx].zero_()\n-\n-        self.key_cache[layer_idx] += k_out\n-        self.value_cache[layer_idx] += v_out\n-        return k_out, v_out\n-\n-    def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-        k_out[:, :, cache_position] = key_states\n-        v_out[:, :, cache_position] = value_states\n-\n-        self.key_cache[layer_idx] = k_out\n-        self.value_cache[layer_idx] = v_out\n-        return k_out, v_out\n-\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -1772,7 +1793,10 @@ def update(\n         if cache_kwargs is None:\n             cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n-        sliding_window = cache_kwargs.get(\"sliding_window\")\n+        if cache_position is None:\n+            raise ValueError(\"`cache_position` must be provided for HybridCache.\")\n+\n+        is_sliding_layer = self.is_sliding_list[layer_idx]\n \n         # These two `if` blocks are only reached in multigpu and if `layer_device_map` is not passed. They are used\n         # when the cache is initialized in the forward pass (e.g. Gemma2)\n@@ -1781,25 +1805,22 @@ def update(\n         if self.value_cache[layer_idx].device != value_states.device:\n             self.value_cache[layer_idx] = self.value_cache[layer_idx].to(value_states.device)\n \n-        k_out = self.key_cache[layer_idx]\n-        v_out = self.value_cache[layer_idx]\n-        key_states = key_states.to(k_out.dtype)\n-        value_states = value_states.to(v_out.dtype)\n-\n-        if sliding_window:\n-            update_fn = self._sliding_update\n+        k_cache = self.key_cache[layer_idx]\n+        v_cache = self.value_cache[layer_idx]\n+        key_states = key_states.to(k_cache.dtype)\n+        value_states = value_states.to(v_cache.dtype)\n+\n+        if is_sliding_layer:\n+            return _sliding_cache_update(\n+                k_cache,\n+                v_cache,\n+                key_states,\n+                value_states,\n+                cache_position,\n+                k_cache.shape[2],  # Use actual cache dim as max cache len\n+            )\n         else:\n-            update_fn = self._static_update\n-\n-        return update_fn(\n-            cache_position,\n-            layer_idx,\n-            key_states,\n-            value_states,\n-            k_out,\n-            v_out,\n-            k_out.shape[2],\n-        )\n+            return _static_cache_update(k_cache, v_cache, key_states, value_states, cache_position)\n \n     def get_max_cache_shape(self) -> Optional[int]:\n         return self.max_cache_len\n@@ -2033,7 +2054,7 @@ def __init__(\n \n         # TODO (joao): to enable this cache on multiple devicesuse the pattern from `OffloadedCache`, which keeps\n         # track of the original device of each layer\n-        unique_devices = set(layer_device_map.values())\n+        unique_devices = set(layer_device_map.values()) if layer_device_map else set()\n         if len(unique_devices) > 1:\n             raise ValueError(f\"OffloadedHybridCache does not support multiple devices. Got devices: {unique_devices}\")\n \n@@ -2292,7 +2313,7 @@ def __init__(\n \n         # TODO (joao): to enable this cache on multiple devicesuse the pattern from `OffloadedCache`, which keeps\n         # track of the original device of each layer\n-        unique_devices = set(layer_device_map.values())\n+        unique_devices = set(layer_device_map.values()) if layer_device_map else set()\n         if len(unique_devices) > 1:\n             raise ValueError(f\"OffloadedStaticCache does not support multiple devices. Got devices: {unique_devices}\")\n \n@@ -2369,6 +2390,9 @@ def update(\n             A tuple containing the updated key and value states.\n         \"\"\"\n \n+        key_states = key_states.to(self.key_cache[layer_idx].dtype)\n+        value_states = value_states.to(self.value_cache[layer_idx].dtype)\n+\n         if layer_idx == 0:\n             # Update seen tokens.\n             # TODO(gante): Remove this."
        },
        {
            "sha": "ea56763a65fd024c8a0d2ce1f32a724be845ee1f",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 317,
            "deletions": 17,
            "changes": 334,
            "blob_url": "https://github.com/huggingface/transformers/blob/d34e21e7dd392c76e7852d836b6f30ba1a2c5d62/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d34e21e7dd392c76e7852d836b6f30ba1a2c5d62/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=d34e21e7dd392c76e7852d836b6f30ba1a2c5d62",
            "patch": "@@ -46,10 +46,14 @@\n         Cache,\n         ClvpForCausalLM,\n         DynamicCache,\n+        Gemma2Config,\n         GenerationConfig,\n+        HybridCache,\n         LlamaConfig,\n+        SlidingWindowCache,\n         StaticCache,\n         convert_and_export_with_cache,\n+        pipeline,\n     )\n \n \n@@ -188,6 +192,21 @@ def _random_kvs(config):\n         self.assertTrue(cached_values.shape == (1, 1, 10, 128))\n \n \n+def _skip_on_failed_cache_prerequisites(test, cache_implementation):\n+    \"\"\"Function to skip tests on failed cache prerequisites, given a cache implementation\"\"\"\n+    # Installed dependencies\n+    if cache_implementation == \"quantized\" and not is_optimum_quanto_available():\n+        test.skipTest(\"Quanto is not available\")\n+    # Devices\n+    if \"offloaded\" in cache_implementation:\n+        has_accelerator = torch_device is not None and torch_device != \"cpu\"\n+        if not has_accelerator:\n+            test.skipTest(\"Offloaded caches require an accelerator\")\n+        if cache_implementation in [\"offloaded_static\", \"offloaded_hybrid_chunked\"]:\n+            if backend_device_count(torch_device) != 1:\n+                test.skipTest(\"Offloaded static caches require exactly 1 accelerator\")\n+\n+\n class CacheIntegrationTest(unittest.TestCase):\n     \"\"\"Fast cache integration tests that share the same small model\"\"\"\n \n@@ -200,24 +219,10 @@ def setUpClass(cls):\n         )\n         cls.model.config.sliding_window = 256  # hack to enable the use of caches with sliding windows\n \n-    def _skip_on_failed_cache_prerequisites(self, cache_implementation):\n-        \"\"\"Function to skip tests on failed cache prerequisites, given a cache implementation\"\"\"\n-        # Installed dependencies\n-        if cache_implementation == \"quantized\" and not is_optimum_quanto_available():\n-            self.skipTest(\"Quanto is not available\")\n-        # Devices\n-        if \"offloaded\" in cache_implementation:\n-            has_accelerator = torch_device is not None and torch_device != \"cpu\"\n-            if not has_accelerator:\n-                self.skipTest(\"Offloaded caches require an accelerator\")\n-            if cache_implementation in [\"offloaded_static\", \"offloaded_hybrid_chunked\"]:\n-                if backend_device_count(torch_device) != 1:\n-                    self.skipTest(\"Offloaded static caches require exactly 1 accelerator\")\n-\n     @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n     def test_cache_batched(self, cache_implementation):\n         \"\"\"Sanity check: caches' `.update` function expects batched inputs\"\"\"\n-        self._skip_on_failed_cache_prerequisites(cache_implementation)\n+        _skip_on_failed_cache_prerequisites(self, cache_implementation)\n \n         EXPECTED_GENERATION = [\"A sequence: 1, 2, 3, 4, 5, 6, 7, 8,\", \"A sequence: A, B, C, D, E, F, G, H\"]\n \n@@ -246,7 +251,7 @@ def test_cache_beam_search(self, cache_implementation):\n         Sanity check: caches' `reorder_cache` is operational. We can confirm this by looking at the beam indices\n         (an output sequence contains multiple beam indices).\n         \"\"\"\n-        self._skip_on_failed_cache_prerequisites(cache_implementation)\n+        _skip_on_failed_cache_prerequisites(self, cache_implementation)\n         if cache_implementation == \"offloaded_hybrid_chunked\":\n             # TODO (joao, cyril): something is off with `offloaded_hybrid_chunked` aka `OffloadedHybridCache`: the\n             # output sequence (and the corresponding beam scores, if we add `output_scores=True`) are significantly\n@@ -280,7 +285,7 @@ def test_cache_beam_search(self, cache_implementation):\n     @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n     def test_cache_extra_left_padding(self, cache_implementation):\n         \"\"\"Tests that adding extra left-padding does not affect the generation with the cache\"\"\"\n-        self._skip_on_failed_cache_prerequisites(cache_implementation)\n+        _skip_on_failed_cache_prerequisites(self, cache_implementation)\n \n         EXPECTED_GENERATION = [\"The cat's whiskers are also a sign of anxiety.\"]\n \n@@ -552,6 +557,28 @@ def test_static_cache_multi_accelerator(self):\n         _ = model(**inputs)\n         _ = model.generate(**inputs, max_new_tokens=2, cache_implementation=\"hybrid\")\n \n+    @require_torch_gpu\n+    @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n+    def test_cache_gptj_model(self, cache_implementation):\n+        \"\"\"Tests caches with GPT-J model. Regression test for https://github.com/huggingface/transformers/pull/34799\"\"\"\n+        _skip_on_failed_cache_prerequisites(self, cache_implementation)\n+\n+        model_id = \"hf-internal-testing/tiny-random-GPTJForCausalLM\"\n+        pipe = pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16)\n+        pipe.model.config.sliding_window = (\n+            256 if cache_implementation in [\"sliding_window\", \"hybrid\", \"hybrid_chunked\"] else None\n+        )\n+        out = pipe(\n+            \"hello world\",\n+            cache_implementation=cache_implementation,\n+            max_new_tokens=10,\n+            do_sample=False,\n+            disable_compile=True,\n+            return_tensors=True,\n+        )[0][\"generated_token_ids\"][-10:]\n+        EXPECTED_OUTPUT = [879, 175, 39, 141, 1000, 975, 951, 991, 683, 441]\n+        self.assertListEqual(out, EXPECTED_OUTPUT)\n+\n \n @require_torch\n class CacheExportIntegrationTest(unittest.TestCase):\n@@ -721,3 +748,276 @@ def test_hybrid_cache_exportability(self):\n             dynamic_shapes=dynamic_shapes,\n             strict=False,\n         )\n+\n+\n+class SyntheticCacheTest(unittest.TestCase):\n+    \"\"\"Tests cache behavior with simple dummy data.\"\"\"\n+\n+    def setUp(self):\n+        \"\"\"Set up common configuration and cache instances for all tests.\"\"\"\n+        self.window_size = 4\n+        self.max_cache_len = 4\n+        self.config = Gemma2Config(\n+            num_hidden_layers=1,\n+            num_key_value_heads=1,\n+            num_attention_heads=1,\n+            head_dim=1,\n+            hidden_size=1,\n+            sliding_window=self.window_size,\n+            sliding_window_pattern=2,  # Default pattern for hybrid sliding\n+        )\n+\n+    def test_static_cache_out_of_bounds(self):\n+        \"\"\"Test StaticCache raises IndexError for out-of-bounds positions.\"\"\"\n+        static_cache = StaticCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        pos_out_of_bounds = torch.tensor([self.max_cache_len])  # Position >= max_cache_len\n+\n+        with self.assertRaises(IndexError):\n+            static_cache.update(\n+                key_states=torch.tensor([[[[1.0]]]]),\n+                value_states=torch.tensor([[[[1.0]]]]),\n+                layer_idx=0,\n+                cache_kwargs={\"cache_position\": pos_out_of_bounds},\n+            )\n+\n+    def test_static_cache(self):\n+        \"\"\"Test StaticCache with manually prefilled states and hardcoded assertions.\n+\n+        Scenario 1: Fill up to near capacity\n+        prefill:       [1.0, 2.0, 0.0, 0.0]\n+        update pos 2:  [1.0, 2.0, 3.0, 0.0]\n+\n+        Scenario 2: Fill to capacity\n+        update pos 3:  [1.0, 2.0, 3.0, 4.0]\n+        \"\"\"\n+        # Scenario 1: Fill up to near capacity\n+        static_cache = StaticCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        static_cache.update(key_states=prefill, value_states=prefill, layer_idx=0, cache_kwargs=None)\n+        static_cache.update(\n+            key_states=torch.tensor(3.0)[None, None, None, None],\n+            value_states=torch.tensor(3.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([2])},\n+        )\n+        self.assertEqual(\n+            static_cache.key_cache[0][0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 0.0], \"StaticCache Scenario 1 failed\"\n+        )\n+\n+        # Scenario 2: Fill to capacity\n+        static_cache.update(\n+            key_states=torch.tensor(4.0)[None, None, None, None],\n+            value_states=torch.tensor(4.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([3])},\n+        )\n+        self.assertEqual(\n+            static_cache.key_cache[0][0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 4.0], \"StaticCache Scenario 2 failed\"\n+        )\n+\n+    def test_sliding_window_cache(self):\n+        \"\"\"Test SlidingWindowCache with manually prefilled states and hardcoded assertions.\n+\n+        Scenario 1: Update within window, no slide yet\n+        prefill:       [1.0, 2.0, 0.0, 0.0]\n+        update pos 2:  [1.0, 2.0, 3.0, 0.0]\n+\n+        Scenario 2: Update causing slide\n+        prefill:       [1.0, 2.0, 3.0, 4.0]\n+        update pos 4:  [2.0, 3.0, 4.0, 5.0] (shift happens as pos > window_size-1)\n+\n+        Scenario 3: Long prompt handling (prompt_len > window_size)\n+        input:         [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n+        result:        [3.0, 4.0, 5.0, 6.0] (keeps last window_size tokens)\n+        \"\"\"\n+        # Scenario 1: Update within window, no slide yet\n+        sliding_cache = SlidingWindowCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        sliding_cache.update(\n+            key_states=prefill,\n+            value_states=prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+        )\n+        sliding_cache.update(\n+            key_states=torch.tensor(3.0)[None, None, None, None],\n+            value_states=torch.tensor(3.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([2]), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            sliding_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 0.0],\n+            \"SlidingWindowCache Scenario 1 failed\",\n+        )\n+\n+        # Scenario 2: Update causing slide\n+        sliding_cache = SlidingWindowCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n+        sliding_cache.update(\n+            key_states=prefill,\n+            value_states=prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+        )\n+        sliding_cache.update(\n+            key_states=torch.tensor(5.0)[None, None, None, None],\n+            value_states=torch.tensor(5.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([4]), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            sliding_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [2.0, 3.0, 4.0, 5.0],\n+            \"SlidingWindowCache Scenario 2 failed\",\n+        )\n+\n+        # Scenario 3: Long prompt handling\n+        sliding_cache = SlidingWindowCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n+        sliding_cache.update(\n+            key_states=long_prefill,\n+            value_states=long_prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(6), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            sliding_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [3.0, 4.0, 5.0, 6.0],\n+            \"SlidingWindowCache Scenario 3 failed\",\n+        )\n+\n+    def test_hybrid_cache_static_mode(self):\n+        \"\"\"Test HybridCache in static mode with hardcoded assertions.\n+\n+        Scenario 1: Static layer behavior\n+        prefill:       [1.0, 2.0, 0.0, 0.0]\n+        update pos 2:  [1.0, 2.0, 3.0, 0.0]\n+\n+        Scenario 2: Fill to capacity\n+        update pos 3:  [1.0, 2.0, 3.0, 4.0]\n+        \"\"\"\n+        config = copy.deepcopy(self.config)\n+        config.sliding_window_pattern = 1  # Layer 0 is static (1 % 1 == 0)\n+\n+        # Scenario 1\n+        hybrid_cache_static_mode = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        hybrid_cache_static_mode.update(\n+            key_states=prefill,\n+            value_states=prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(4)},\n+        )\n+        hybrid_cache_static_mode.update(\n+            key_states=torch.tensor(3.0)[None, None, None, None],\n+            value_states=torch.tensor(3.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([2])},\n+        )\n+        self.assertEqual(\n+            hybrid_cache_static_mode.key_cache[0][0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 0.0],\n+            \"HybridCache Static Scenario 1 failed\",\n+        )\n+\n+        # Scenario 2\n+        hybrid_cache_static_mode.update(\n+            key_states=torch.tensor(4.0)[None, None, None, None],\n+            value_states=torch.tensor(4.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([3])},\n+        )\n+        self.assertEqual(\n+            hybrid_cache_static_mode.key_cache[0][0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 4.0],\n+            \"HybridCache Static Scenario 2 failed\",\n+        )\n+\n+    def test_hybrid_cache_sliding_mode(self):\n+        \"\"\"Test HybridCache in sliding mode with hardcoded assertions.\n+\n+        Scenario 1: Update within window, no slide yet\n+        prefill:       [1.0, 2.0, 0.0, 0.0]\n+        update pos 2:  [1.0, 2.0, 3.0, 0.0]\n+\n+        Scenario 2: Update causing first slide\n+        prefill:       [1.0, 2.0, 3.0, 4.0]\n+        update pos 4:  [2.0, 3.0, 4.0, 5.0] (shift happens as pos > window_size-1)\n+\n+        Scenario 3: Update causing subsequent slide\n+        update pos 5:  [3.0, 4.0, 5.0, 6.0] (shift continues)\n+\n+        Scenario 4: Long prompt handling (prompt_len > window_size)\n+        input:         [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n+        result:        [3.0, 4.0, 5.0, 6.0] (keeps last window_size tokens)\n+        \"\"\"\n+        # Scenario 1: Update within window, no slide yet\n+        hybrid_cache = HybridCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n+        hybrid_cache.update(\n+            key_states=prefill,\n+            value_states=prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+        )\n+        hybrid_cache.update(\n+            key_states=torch.tensor(3.0)[None, None, None, None],\n+            value_states=torch.tensor(3.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([2]), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 0.0],\n+            \"HybridCache Sliding Scenario 1 failed\",\n+        )\n+\n+        # Scenario 2: Update causing first slide\n+        hybrid_cache = HybridCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n+        hybrid_cache.update(\n+            key_states=prefill,\n+            value_states=prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(4), \"sliding_window\": self.window_size},\n+        )\n+        hybrid_cache.update(\n+            key_states=torch.tensor(5.0)[None, None, None, None],\n+            value_states=torch.tensor(5.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([4]), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [2.0, 3.0, 4.0, 5.0],\n+            \"HybridCache Sliding Scenario 2 failed\",\n+        )\n+\n+        # Scenario 3: Update causing subsequent slide\n+        hybrid_cache.update(\n+            key_states=torch.tensor(6.0)[None, None, None, None],\n+            value_states=torch.tensor(6.0)[None, None, None, None],\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([5]), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [3.0, 4.0, 5.0, 6.0],\n+            \"HybridCache Sliding Scenario 3 failed\",\n+        )\n+\n+        # Scenario 4: Long prompt handling\n+        hybrid_cache = HybridCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n+        hybrid_cache.update(\n+            key_states=long_prefill,\n+            value_states=long_prefill,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(6), \"sliding_window\": self.window_size},\n+        )\n+        self.assertEqual(\n+            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            [3.0, 4.0, 5.0, 6.0],\n+            \"HybridCache Sliding Scenario 4 failed\",\n+        )"
        }
    ],
    "stats": {
        "total": 620,
        "additions": 472,
        "deletions": 148
    }
}