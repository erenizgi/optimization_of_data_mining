{
    "author": "ydshieh",
    "message": "Fix `PhimoeIntegrationTest` (#41007)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "966b3dbcbefaa358ce1fe51b687b756fd5b877ed",
    "files": [
        {
            "sha": "ac6fa3c2672ae662647afa3202f5b4710565042b",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 57,
            "deletions": 24,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/966b3dbcbefaa358ce1fe51b687b756fd5b877ed/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/966b3dbcbefaa358ce1fe51b687b756fd5b877ed/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=966b3dbcbefaa358ce1fe51b687b756fd5b877ed",
            "patch": "@@ -14,12 +14,14 @@\n \n \"\"\"Testing suite for the PyTorch PhiMoE model.\"\"\"\n \n+import copy\n import unittest\n \n from parameterized import parameterized\n \n from transformers import PhimoeConfig, StaticCache, is_torch_available\n from transformers.testing_utils import (\n+    cleanup,\n     require_torch,\n     slow,\n     torch_device,\n@@ -130,31 +132,47 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n @slow\n @require_torch\n class PhimoeIntegrationTest(unittest.TestCase):\n-    def test_model_phimoe_instruct_logits(self):\n-        input_ids = {\n-            \"input_ids\": torch.tensor(\n-                [[1212, 318, 281, 1672, 2643, 290, 428, 318, 257, 1332]], dtype=torch.long, device=torch_device\n+    model = None\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = PhimoeForCausalLM.from_pretrained(\n+                \"microsoft/Phi-3.5-MoE-instruct\", dtype=\"auto\", device_map=\"auto\"\n             )\n-        }\n+        return cls.model\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n \n-        model = PhimoeForCausalLM.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\").to(torch_device)\n+    def test_model_phimoe_instruct_logits(self):\n+        input_ids = {\"input_ids\": torch.tensor([[1212, 318, 281, 1672]], dtype=torch.long, device=torch_device)}\n+\n+        model = self.get_model()\n         model.eval()\n \n-        output = model(**input_ids).logits\n+        with torch.no_grad():\n+            output = model(**input_ids).logits\n \n-        EXPECTED_OUTPUT = torch.tensor([[-3.5312, -2.5000, -1.2734,  0.3555, -0.7578, -0.4727,  0.5977, -0.4316,\n-          0.2256, -1.2188, -1.6797,  0.9961,  3.7656, 11.3125, -1.3828, -4.8438,\n-         -5.7500, -1.9375,  0.7227, -0.3438, -0.2100, -0.4277, -0.0444, -0.5352,\n-         -0.6406, -0.1016, -0.4258, -1.0234,  0.4297, -0.6250],\n-        [-0.9883,  0.1455, -0.4902,  2.3594,  0.7031,  3.1406,  0.4375,  0.2559,\n-          0.6172, -2.1094, -1.3359,  2.5938,  4.9062, 10.8125, -0.1094,  1.5781,\n-         -4.9375,  0.7148, -0.0972,  1.7656, -0.0801,  0.2217,  0.1875, -0.4629,\n-          1.5781,  0.3535,  0.0874,  0.6836, -0.0518, -1.2969]]).to(torch_device)  # fmt: skip\n+        EXPECTED_OUTPUT = torch.tensor(\n+            [\n+                    [-3.4844, -2.4531, -1.1719, 0.6055, -0.4922, -0.1001, 0.8086, -0.2422, 0.3477, -1.0078],\n+                    [-0.9766, 0.1631, -0.5508, 2.3594, 0.7031, 3.1719, 0.4141, 0.2305, 0.6055, -2.1250],\n+            ]\n+        ).to(device=torch_device, dtype=output.dtype)  # fmt: skip\n \n-        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output[0, :2, :10], EXPECTED_OUTPUT, rtol=1e-4, atol=1e-4)\n \n     def test_phimoe_instruct_generation(self):\n-        model = PhimoeForCausalLM.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n+        model = self.get_model()\n         tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n \n         messages = [\n@@ -166,17 +184,29 @@ def test_phimoe_instruct_generation(self):\n         ]\n         inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n \n-        outputs = model.generate(inputs, max_new_tokens=32)\n+        outputs = model.generate(inputs, max_new_tokens=30)\n         output_text = tokenizer.batch_decode(outputs)\n \n         EXPECTED_OUTPUT = [\n-            \"<|system|> You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|><|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> Certainly! Bananas and dragonfruits are both delicious and nutritious fruits that can be combined in various ways to create tast\"\n+            \"<|system|> You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|><|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> Certainly! Bananas and dragonfruits are both delicious and nutritious fruits that can be combined in various ways to create\",\n         ]\n-\n         self.assertListEqual(output_text, EXPECTED_OUTPUT)\n \n     def test_phimoe_instruct_with_static_cache(self):\n-        model = PhimoeForCausalLM.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n+        model = self.get_model()\n+        # Can't run with the real checkpoint, even if offloaded. Let's just use a tiny dummy one\n+        config = copy.deepcopy(model.config)\n+        config.num_hidden_layers = 2\n+        # make `head_dim = 128`\n+        config.hidden_size = 512\n+        config.num_attention_heads = 4\n+        config.num_key_value_heads = 1\n+        config.intermediate_size = 512\n+        config.max_position_embeddinqgs = 64\n+        config.num_local_experts = 4\n+        torch.manual_seed(42)\n+        model = PhimoeForCausalLM(config).to(torch_device)\n+        model.eval()\n         tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n \n         messages = [\n@@ -186,14 +216,17 @@ def test_phimoe_instruct_with_static_cache(self):\n             },\n             {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n         ]\n-        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n+        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n \n-        response_tokens = PhimoeMiniWithStaticCache.generate(model, inputs, 64)\n+        response_tokens = PhimoeMiniWithStaticCache.generate(model, inputs, max_seq_len=30)\n \n         output_text = tokenizer.batch_decode(torch.tensor([response_tokens], dtype=torch.long, device=torch_device))\n \n+        # This is dummy outputs. We actually check if it could run with static cache, not the output quality.\n         EXPECTED_OUTPUT = [\n-            \"<|system|> You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|><|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> Certainly! Bananas and dragonfruits are both delicious and nutritious fruits that can\"\n+            \"<|system|> You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|><|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> awards\"\n         ]\n \n         self.assertListEqual(output_text, EXPECTED_OUTPUT)"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 57,
        "deletions": 24
    }
}