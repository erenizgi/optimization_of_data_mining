{
    "author": "NouamaneTazi",
    "message": "parallelism goes brrr (#37877)\n\n* accept custom device_mesh\n\n* fix device_map\n\n* assert that num_heads % tp_size == 0\n\n* todo.\n\n* ReplicateParallel\n\n* handle tied weights\n\n* handle dtensor in save_pretrained with safe_serialization\n\n* tp test works\n\n* doesnt work\n\n* fix shard_and_distribute_module's rank should be local_rank\n\n* tp=4 is correct\n\n* dp+tp is broken\n\n* todo allreduce with dtensors on another dim is annoying\n\n* workaround to sync dp grads when using dtensors\n\n* loading a checkpoint works\n\n* wandb and compare losses with different tp/dp\n\n* cleaning\n\n* cleaning\n\n* .\n\n* .\n\n* logs\n\n* CP2 DP2 no mask works after commenting attn_mask and is_causal from scaled_dot_product_attention\n\n* DP=2 TP=2 now works even with tied embeddings\n\n* model.parameters() and model.module.parameters() are empty..\n\n* reformat sanity_check_tensor_sync\n\n* set atol=1e-4 for CP to pass\n\n* try populate _parameters from named_modules\n\n* refactors\nTP2 DP2 works\nCP2 DP2 works\n\n* is_causal=True and pack sequences, no attn mask, and preshuffle dataset\n\n* fix packing\n\n* CP=4 doesn't work\n\n* fix labels and position_ids for CP\n\n* DP CP works with transformers ðŸ¥³ðŸ¥³ðŸ¥³\n\n* refactor\n\n* add example cp\n\n* fixup\n\n* revert sdpa changes\n\n* example cleared\n\n* add CP, DP to the mesh init\n\n* nit\n\n* clean\n\n* use `ALL_PARALLEL_STYLES`\n\n* style\n\n* FSDP works\n\n* log on 1 rank\n\n* .\n\n* fix?\n\n* FSDP1 also has .parameters() bug\n\n* reported gradnorm when using FSDP1 is wrong, but loss is correct so it's okay\n\n* .\n\n* style and fixup\n\n* move stuff around\n\n* fix tests\n\n* style\n\n* let's make it a check\n\n* warning should be an info\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "1c2f36b480e02c9027d2523746d34e27b39e01a4",
    "files": [
        {
            "sha": "d56e63bc68fe2a2bf6ecd19dba3f4608fcb2b472",
            "filename": "examples/3D_parallel.py",
            "status": "added",
            "additions": 422,
            "deletions": 0,
            "changes": 422,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c2f36b480e02c9027d2523746d34e27b39e01a4/examples%2F3D_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c2f36b480e02c9027d2523746d34e27b39e01a4/examples%2F3D_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2F3D_parallel.py?ref=1c2f36b480e02c9027d2523746d34e27b39e01a4",
            "patch": "@@ -0,0 +1,422 @@\n+\"\"\":\n+This script is used to test training a model using Tensor Parallelism and Data Parallelism.\n+\n+Usage:\n+export CUDA_VISIBLE_DEVICES=0,1,2,3\n+export CUDA_VISIBLE_DEVICES=4,5,6,7\n+export CUDA_VISIBLE_DEVICES=5,6,7\n+TP_SIZE=2 DP_SIZE=2 torchrun --nproc_per_node=4 --rdzv_endpoint=localhost:29503 examples/3D_parallel.py\n+CP_SIZE=2 DP_SIZE=2 torchrun --nproc_per_node=4 examples/3D_parallel.py\n+CP_SIZE=2 TP_SIZE=2 torchrun --nproc_per_node=4 examples/3D_parallel.py\n+DP_SIZE=2 CP_SIZE=2 TP_SIZE=2 torchrun --nproc_per_node=8 examples/3D_parallel.py\n+\n+TP_SIZE=1 CP_SIZE=4 torchrun --nproc_per_node=4 examples/3D_parallel.py\n+TP_SIZE=1 DP_SIZE=4 torchrun --nproc_per_node=4 examples/3D_parallel.py\n+TP_SIZE=4 DP_SIZE=1 torchrun --nproc_per_node=4 --rdzv_endpoint=localhost:29503 examples/3D_parallel.py\n+IGNORE_SANITY=1 CP_SIZE=1 TP_SIZE=1 DP_SIZE=1 torchrun --nproc_per_node=1 --rdzv_endpoint=localhost:29504 examples/3D_parallel.py\n+ocalhost:29504 test_train.py\n+\"\"\"\n+\n+import logging\n+import os\n+from contextlib import nullcontext\n+from typing import Iterable\n+\n+import torch\n+import torch.distributed as dist\n+import torch.distributed.checkpoint as dcp\n+import torch.optim as optim\n+import wandb\n+from datasets import load_dataset\n+from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\n+from torch.distributed.checkpoint.stateful import Stateful\n+from torch.distributed.device_mesh import DeviceMesh\n+from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n+from torch.distributed.fsdp import ShardingStrategy\n+from torch.distributed.tensor import DTensor\n+from torch.distributed.tensor.experimental import context_parallel\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n+from torch.utils.data import DataLoader\n+from torch.utils.data.distributed import DistributedSampler\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+\n+# torch.use_deterministic_algorithms(True)\n+torch.backends.cudnn.deterministic = True\n+\n+# Set up logging\n+logging.basicConfig(\n+    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n+    datefmt=\"%m/%d/%Y %H:%M:%S\",\n+    level=logging.INFO,\n+)\n+logger = logging.getLogger(__name__)\n+\n+# from torch.distributed.tensor.experimental._attention import set_rotate_method\n+\n+# set_rotate_method(\"alltoall\")  # CP rotate shards using all-to-all\n+\n+\n+def main():\n+    tp_size = int(os.environ.get(\"TP_SIZE\", 1))\n+    dp_size = int(os.environ.get(\"DP_SIZE\", 1))\n+    cp_size = int(os.environ.get(\"CP_SIZE\", 1))  # Add CP size configuration\n+    sdpa_backend = SDPBackend.FLASH_ATTENTION  # For CP\n+    # sdpa_backend = SDPBackend.MATH # For CP\n+    global_batch_size = 8  # Desired global batch size\n+    seq_len = 1024  # Sequence length\n+    num_train_steps = 10000  # Number of training steps\n+    LR = 1e-5\n+    model_name = \"HuggingFaceTB/SmolLM2-1.7B\"\n+    # model_name = \"unsloth/Llama-3.2-1B\"\n+\n+    CHECKPOINT_DIR = f\"checkpoint_tp{tp_size}_dp{dp_size}_cp{cp_size}\"\n+\n+    # Initialize distributed environment\n+    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n+        dist.init_process_group(\"nccl\")\n+        rank = dist.get_rank()\n+        world_size = dist.get_world_size()\n+        local_rank = int(os.environ[\"LOCAL_RANK\"])\n+        torch.cuda.set_device(local_rank)\n+\n+        assert world_size == tp_size * dp_size * cp_size, (\n+            f\"World size ({world_size}) must equal TP size ({tp_size}) * DP size ({dp_size}) * CP size ({cp_size})\"\n+        )\n+\n+        mesh = torch.arange(world_size).reshape(dp_size, tp_size, cp_size)\n+        world_mesh = DeviceMesh(device_type=\"cuda\", mesh=mesh, mesh_dim_names=(\"dp\", \"tp\", \"cp\"))\n+        tp_mesh = world_mesh[\"tp\"]\n+        dp_mesh = world_mesh[\"dp\"]\n+        cp_mesh = world_mesh[\"cp\"]\n+        world_mesh[\"dp\", \"cp\"]._flatten(mesh_dim_name=\"dp_cp\")\n+        logger.info(f\"Created DeviceMesh: {world_mesh}\")\n+        logger.info(\n+            f\"Distributed setup - Rank: {rank}, World size: {world_size}, Local rank: {local_rank}, DP: {dp_mesh.get_local_rank()}, TP: {tp_mesh.get_local_rank()}, CP: {cp_mesh.get_local_rank()}\"\n+        )\n+\n+        if dist.get_rank() == 0:\n+            wandb.init(\n+                project=\"tp_dp_test\",\n+                config={\n+                    \"tp_size\": tp_size,\n+                    \"dp_size\": dp_size,\n+                    \"cp_size\": cp_size,\n+                    \"global_batch_size\": global_batch_size,\n+                    \"model_name\": model_name,\n+                    \"dataset\": \"roneneldan/TinyStories-1M\",\n+                    \"seq_len\": seq_len,\n+                    \"lr\": LR,\n+                    \"weight_decay\": 0.1,\n+                },\n+                name=f\"llama_tp{tp_size}_dp{dp_size}_cp{cp_size}\"\n+                if model_name == \"unsloth/Llama-3.2-1B\"\n+                else f\"tp{tp_size}_dp{dp_size}_cp{cp_size}\",\n+            )\n+            logger.info(\"Wandb initialized.\")\n+            # Log the current file to wandb\n+            wandb.save(\"test_train.py\")\n+\n+    # Load model and tokenizer\n+    logger.info(f\"Loading model and tokenizer from {model_name}\")\n+    tokenizer = AutoTokenizer.from_pretrained(model_name)\n+    if tokenizer.pad_token is None:\n+        tokenizer.pad_token = tokenizer.eos_token\n+        logger.info(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n+\n+    model = AutoModelForCausalLM.from_pretrained(\n+        model_name,\n+        device_mesh=tp_mesh if dist.is_initialized() else None,\n+        tp_plan=\"auto\",\n+        torch_dtype=torch.bfloat16,\n+    )\n+    logger.info(f\"Model loaded onto device mesh: {tp_mesh}\")\n+    device = torch.device(f\"cuda:{local_rank}\")\n+    logger.info(f\"Using device: {device} for non-model tensors\")\n+    use_ddp = False\n+    if dist.is_initialized() and dp_mesh.size() > 1:\n+        model = FSDP(model, device_mesh=dp_mesh, sharding_strategy=ShardingStrategy.NO_SHARD)\n+        use_ddp = True\n+        pass\n+\n+    model.train()\n+\n+    logger.info(\"Loading TinyStories dataset...\")\n+    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:1%]\")  # Use 1% for faster testing\n+\n+    def tokenize_function(examples):\n+        # Tokenize the text without padding\n+        tokenized_batch = tokenizer(\n+            examples[\"text\"], padding=False, truncation=True, max_length=seq_len, return_tensors=None\n+        )\n+        # Set labels to be the same as input_ids for Causal LM\n+        tokenized_batch[\"labels\"] = tokenized_batch[\"input_ids\"].copy()\n+        return tokenized_batch\n+\n+    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n+    logger.info(f\"Dataset loaded and tokenized. Size: {len(tokenized_dataset)}\")\n+\n+    # Create packed sequences\n+    def create_packed_sequences(examples):\n+        # Flatten all sequences\n+        all_tokens = []\n+        for input_ids in examples[\"input_ids\"]:\n+            all_tokens.extend(input_ids)\n+\n+        # Split into sequences of seq_len + 1 (for input + label)\n+        num_sequences = len(all_tokens) // (seq_len + 1)\n+        packed_input_ids = []\n+        packed_labels = []\n+\n+        for i in range(num_sequences):\n+            start_idx = i * (seq_len + 1)\n+            end_idx = start_idx + (seq_len + 1)\n+            # Get the full sequence\n+            full_sequence = all_tokens[start_idx:end_idx]\n+            # For input_ids, remove the last token\n+            packed_input_ids.append(full_sequence[:-1])\n+            # For labels, remove the first token\n+            packed_labels.append(full_sequence[1:])\n+\n+        return {\"input_ids\": packed_input_ids, \"labels\": packed_labels}\n+\n+    # Apply packing to the dataset\n+    packed_dataset = tokenized_dataset.map(\n+        create_packed_sequences,\n+        batched=True,\n+        remove_columns=tokenized_dataset.column_names,\n+        batch_size=1000,  # Process in batches for efficiency\n+        num_proc=60,\n+    )\n+    logger.info(f\"Dataset packed. New size: {len(packed_dataset)}\")\n+\n+    # Shuffle the packed dataset\n+    packed_dataset = packed_dataset.shuffle(seed=42)\n+    logger.info(\"Packed dataset shuffled\")\n+\n+    # Calculate local batch size\n+    if dist.is_initialized():\n+        assert global_batch_size % dp_mesh.size() == 0, (\n+            f\"Global batch size ({global_batch_size}) must be divisible by DP size ({dp_mesh.size()})\"\n+        )\n+        local_batch_size = global_batch_size // dp_mesh.size()\n+    else:\n+        local_batch_size = global_batch_size\n+\n+    logger.info(\n+        f\"Global batch size: {global_batch_size}, DP size: {dp_size if dist.is_initialized() else 1}, Local batch size: {local_batch_size}\"\n+    )\n+\n+    # Simple collate function since sequences are already packed\n+    def collate_fn(batch):\n+        input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n+        labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n+        return {\"input_ids\": input_ids, \"labels\": labels}\n+\n+    if dist.is_initialized():\n+        sampler = DistributedSampler(\n+            packed_dataset, num_replicas=dp_mesh.size(), rank=dp_mesh.get_local_rank(), shuffle=False\n+        )\n+    else:\n+        sampler = None\n+\n+    dataloader = DataLoader(\n+        packed_dataset,\n+        batch_size=local_batch_size,\n+        sampler=sampler,\n+        shuffle=False,\n+        collate_fn=collate_fn,\n+        pin_memory=True,\n+    )\n+    logger.info(f\"DataLoader created. Distributed: {dist.is_initialized()}\")\n+\n+    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.1)\n+\n+    # Training loop\n+    logger.info(f\"Starting training for {num_train_steps} steps...\")\n+    model.train()\n+    step = 0\n+    while step < num_train_steps:\n+        for batch in dataloader:\n+            if step >= num_train_steps:\n+                break  # Exit loop if max steps reached\n+\n+            # Move batch to appropriate device\n+            batch = {k: v.to(device) for k, v in batch.items()}\n+            optimizer.zero_grad()\n+\n+            # Add position_ids to batch before CP sharding\n+            batch_size = batch[\"input_ids\"].shape[0]\n+            position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device)\n+            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n+            batch[\"position_ids\"] = position_ids\n+            from torch.distributed.tensor.experimental._attention import _cp_options\n+\n+            _cp_options.enable_load_balance = False\n+\n+            with sdpa_kernel(sdpa_backend):  # TODO: ideally move this to attention implementation\n+                cp_context = (\n+                    nullcontext()\n+                    if cp_mesh.size() == 1\n+                    else context_parallel(\n+                        cp_mesh,\n+                        buffers=[\n+                            batch[\"input_ids\"],\n+                            batch[\"labels\"],\n+                            batch[\"position_ids\"],\n+                        ],\n+                        buffer_seq_dims=[1, 1, 1],\n+                    )\n+                )\n+                with cp_context:\n+                    # Pop labels from batch before model forward pass\n+                    labels = batch.pop(\"labels\")\n+                    outputs = model(**batch)  # [mbs, seq_len/cp]\n+                    loss = outputs.loss\n+                    logits = outputs.logits\n+\n+                    # Compute loss with shifted labels\n+                    loss = model.loss_function(\n+                        logits=logits, labels=None, shift_labels=labels, vocab_size=model.config.vocab_size\n+                    )\n+                    loss.backward()\n+\n+                # all reduce grads across dp_cp if applicable\n+                all_reduce_grads(model, world_mesh, use_ddp=use_ddp)\n+\n+                if hasattr(model, \"clip_grad_norm_\"):\n+                    gradnorm = model.clip_grad_norm_(max_norm=1.0, norm_type=2.0)  # TODO: fix reported gradnorm\n+                else:\n+                    # only works with FSDP's NO_SHARD otherwise we should use FSDP's clip_grad_norm_\n+                    assert len(list(model.parameters())) > 5, \"No parameters found in model. Probably DDP bug..\"\n+                    gradnorm = clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2.0, foreach=True)\n+\n+                optimizer.step()\n+                # allreduce loss across cp_dp before logging\n+                if dist.is_initialized() and (cp_mesh.size() > 1 or dp_mesh.size() > 1):\n+                    dist.all_reduce(loss, group=world_mesh[\"dp_cp\"].get_group(), op=dist.ReduceOp.AVG)\n+                current_loss = loss.item()\n+\n+                # Log loss and gradnorm to wandb (only on rank 0 of dp group)\n+                if not dist.is_initialized() or dist.get_rank() == 0:\n+                    logger.info(\n+                        f\"Step: {step} | GBS: {global_batch_size} | DP: {dp_mesh.size()} | TP: {tp_mesh.size()} | CP: {cp_mesh.size()} | Loss: {current_loss} | Gradnorm: {gradnorm} | lr: {LR}\"\n+                    )\n+                    wandb.log(\n+                        {\n+                            \"train/loss\": current_loss,\n+                            \"train/gradnorm\": gradnorm,\n+                            \"step\": step,\n+                            \"lr\": LR,\n+                            \"GBS\": global_batch_size,\n+                        }\n+                    )\n+\n+            step += 1  # Increment step count\n+\n+    logger.info(\"Training loop finished.\")\n+\n+    # Save model using DCP (only if distributed)\n+    if dist.is_initialized():\n+        state_dict = {\"app\": AppState(model, optimizer)}\n+        dcp.save(\n+            state_dict=state_dict,\n+            checkpoint_id=CHECKPOINT_DIR,\n+        )\n+        logger.info(f\"Saved checkpoint to {CHECKPOINT_DIR}\")\n+    else:\n+        # Fallback to regular save for non-distributed case\n+        save_dir = \"test_model_nondist\"\n+        model.save_pretrained(save_dir, safe_serialization=False)\n+        tokenizer.save_pretrained(save_dir)  # Save tokenizer too\n+        logger.info(f\"Saved model to {save_dir}\")\n+\n+    dist.destroy_process_group()\n+    logger.info(\"Cleaned up distributed process group\")\n+    # Finish wandb run on rank 0\n+    if dist.get_rank() == 0:\n+        wandb.finish()\n+        logger.info(\"Wandb run finished.\")\n+\n+\n+def all_reduce_grads(model, world_mesh, use_ddp):\n+    \"\"\"All reduce gradients across dp_cp if applicable.\"\"\"\n+    cp_mesh = world_mesh[\"cp\"]\n+    if use_ddp:\n+        # DDP/FSDP takes care of syncing grads\n+        mesh = cp_mesh\n+    else:\n+        mesh = world_mesh[\"dp\", \"cp\"]._flatten(mesh_dim_name=\"dp_cp\")\n+    if dist.is_initialized() and mesh.size() > 1:\n+        for name, param in model.named_parameters():\n+            if param.grad is not None:\n+                # Workaround for cross-mesh communication limitation with DTensor gradients\n+                if isinstance(param.grad, DTensor):\n+                    local_grad = param.grad.to_local()\n+                    # Ensure grad requires grad for inplace modification checks (might not be needed)\n+                    # local_grad = local_grad.detach().requires_grad_(True)\n+                    torch.distributed.all_reduce(local_grad, op=torch.distributed.ReduceOp.SUM, group=mesh.get_group())\n+                    local_grad = local_grad / mesh.size()\n+                    # Assign averaged grad back - need careful handling if DTensor structure is complex\n+                    # This simple assignment might work if the grad structure matches param structure\n+                    param.grad = DTensor.from_local(\n+                        local_grad, device_mesh=param.grad.device_mesh, placements=param.grad.placements\n+                    )\n+                else:\n+                    # Handle regular tensors if any exist (e.g. buffers not converted to DTensor)\n+                    torch.distributed.all_reduce(param.grad, op=torch.distributed.ReduceOp.AVG, group=mesh.get_group())\n+\n+\n+class AppState(Stateful):\n+    \"\"\"Wrapper for checkpointing the Application State including model and optimizer.\"\"\"\n+\n+    def __init__(self, model, optimizer=None):\n+        self.model = model\n+        self.optimizer = optimizer\n+\n+    def state_dict(self):\n+        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n+        return {\"model\": model_state_dict, \"optim\": optimizer_state_dict}\n+\n+    def load_state_dict(self, state_dict):\n+        set_state_dict(\n+            self.model, self.optimizer, model_state_dict=state_dict[\"model\"], optim_state_dict=state_dict[\"optim\"]\n+        )\n+\n+\n+def clip_grad_norm_(\n+    parameters: Iterable[torch.Tensor],\n+    max_norm: float,\n+    norm_type: float = 2.0,\n+    error_if_nonfinite: bool = False,\n+    foreach: bool | None = None,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Clip the gradient norm of an iterable of parameters.\n+    \"\"\"\n+    # Filter out parameters with no gradients\n+    parameters = [p for p in parameters if p.grad is not None]\n+    assert len(parameters) > 0, \"No parameters with gradients found\"\n+\n+    # Calculate total norm\n+    if norm_type == float(\"inf\"):\n+        total_norm = max(p.grad.detach().abs().max() for p in parameters)\n+    else:\n+        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type)\n+\n+    # Convert DTensor to local tensor if needed\n+    if isinstance(total_norm, DTensor):\n+        total_norm = total_norm.full_tensor()\n+\n+    # Clip gradients\n+    clip_coef = max_norm / (total_norm + 1e-6)\n+    if clip_coef < 1:\n+        for p in parameters:\n+            p.grad.detach().mul_(clip_coef)\n+\n+    return total_norm\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "1c7e88e5e4e535e3dd92ddba0a98dac139c40cf4",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "added",
            "additions": 780,
            "deletions": 0,
            "changes": 780,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c2f36b480e02c9027d2523746d34e27b39e01a4/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c2f36b480e02c9027d2523746d34e27b39e01a4/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=1c2f36b480e02c9027d2523746d34e27b39e01a4",
            "patch": "@@ -0,0 +1,780 @@\n+\"\"\":\n+This script is used to test training a model using Tensor Parallelism and Data Parallelism.\n+\n+Usage:\n+export CUDA_VISIBLE_DEVICES=0,1,2,3\n+export CUDA_VISIBLE_DEVICES=4,5,6,7\n+export CUDA_VISIBLE_DEVICES=5,6,7\n+TP_SIZE=2 DP_SIZE=2 torchrun --nproc_per_node=4 --rdzv_endpoint=localhost:29503 test_train.py\n+CP_SIZE=2 DP_SIZE=2 torchrun --nproc_per_node=4 test_train.py\n+CP_SIZE=2 TP_SIZE=2 torchrun --nproc_per_node=4 test_train.py\n+\n+TP_SIZE=1 CP_SIZE=4 torchrun --nproc_per_node=4 test_train.py\n+TP_SIZE=1 DP_SIZE=4 torchrun --nproc_per_node=4 test_train.py\n+TP_SIZE=4 DP_SIZE=1 torchrun --nproc_per_node=4 --rdzv_endpoint=localhost:29503 test_train.py\n+IGNORE_SANITY=1 CP_SIZE=1 TP_SIZE=1 DP_SIZE=1 torchrun --nproc_per_node=1 --rdzv_endpoint=l\n+ocalhost:29504 test_train.py\n+\"\"\"\n+\n+import logging\n+import os\n+from contextlib import nullcontext\n+from typing import Dict, Iterable, Optional\n+\n+import torch\n+import torch.distributed as dist\n+import torch.distributed.checkpoint as dcp\n+import torch.nn as nn\n+import torch.optim as optim\n+import wandb\n+from datasets import load_dataset\n+from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\n+from torch.distributed.checkpoint.stateful import Stateful\n+from torch.distributed.device_mesh import DeviceMesh\n+from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n+from torch.distributed.fsdp import ShardingStrategy\n+from torch.distributed.tensor import DTensor\n+from torch.distributed.tensor.experimental import context_parallel\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n+from torch.utils.data import DataLoader, default_collate\n+from torch.utils.data.distributed import DistributedSampler\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+\n+ignore_sanity_checks = int(os.environ.get(\"IGNORE_SANITY\", 0)) == 1\n+# torch.use_deterministic_algorithms(True)\n+torch.backends.cudnn.deterministic = True\n+\n+# Set up logging\n+logging.basicConfig(\n+    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n+    datefmt=\"%m/%d/%Y %H:%M:%S\",\n+    level=logging.INFO,\n+)\n+logger = logging.getLogger(__name__)\n+\n+# from torch.distributed.tensor.experimental._attention import set_rotate_method\n+\n+# set_rotate_method(\"alltoall\")  # rotate shards using all-to-all\n+\n+\n+def main():\n+    tp_size = int(os.environ.get(\"TP_SIZE\", 1))\n+    dp_size = int(os.environ.get(\"DP_SIZE\", 4))\n+    cp_size = int(os.environ.get(\"CP_SIZE\", 1))  # Add CP size configuration\n+    sdpa_backend = SDPBackend.FLASH_ATTENTION  # For CP\n+    # sdpa_backend = SDPBackend.MATH # For CP\n+    global_batch_size = 8  # Desired global batch size\n+    seq_len = 1024  # Sequence length\n+    num_train_steps = 10000  # Number of training steps\n+    LR = 1e-5\n+    model_name = \"HuggingFaceTB/SmolLM2-1.7B\"\n+    # model_name = \"unsloth/Llama-3.2-1B\"\n+\n+    CHECKPOINT_DIR = f\"checkpoint_tp{tp_size}_dp{dp_size}_cp{cp_size}\"\n+\n+    # Initialize distributed environment\n+    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n+        dist.init_process_group(\"nccl\")\n+        rank = dist.get_rank()\n+        world_size = dist.get_world_size()\n+        local_rank = int(os.environ[\"LOCAL_RANK\"])\n+        torch.cuda.set_device(local_rank)\n+\n+        assert world_size == tp_size * dp_size * cp_size, (\n+            f\"World size ({world_size}) must equal TP size ({tp_size}) * DP size ({dp_size}) * CP size ({cp_size})\"\n+        )\n+\n+        mesh = torch.arange(world_size).reshape(dp_size, tp_size, cp_size)\n+        world_mesh = DeviceMesh(device_type=\"cuda\", mesh=mesh, mesh_dim_names=(\"dp\", \"tp\", \"cp\"))\n+        tp_mesh = world_mesh[\"tp\"]\n+        dp_mesh = world_mesh[\"dp\"]\n+        cp_mesh = world_mesh[\"cp\"]\n+        world_mesh[\"dp\", \"cp\"]._flatten(mesh_dim_name=\"dp_cp\")\n+        logger.info(f\"Created DeviceMesh: {world_mesh}\")\n+        logger.info(\n+            f\"Distributed setup - Rank: {rank}, World size: {world_size}, Local rank: {local_rank}, DP: {dp_mesh.get_local_rank()}, TP: {tp_mesh.get_local_rank()}, CP: {cp_mesh.get_local_rank()}\"\n+        )\n+\n+        if dist.get_rank() == 0:\n+            wandb.init(\n+                project=\"tp_dp_test\",\n+                config={\n+                    \"tp_size\": tp_size,\n+                    \"dp_size\": dp_size,\n+                    \"cp_size\": cp_size,\n+                    \"global_batch_size\": global_batch_size,\n+                    \"model_name\": model_name,\n+                    \"dataset\": \"roneneldan/TinyStories-1M\",\n+                    \"seq_len\": seq_len,\n+                    \"lr\": LR,\n+                    \"weight_decay\": 0.1,\n+                },\n+                name=f\"llama_tp{tp_size}_dp{dp_size}_cp{cp_size}\"\n+                if model_name == \"unsloth/Llama-3.2-1B\"\n+                else f\"tp{tp_size}_dp{dp_size}_cp{cp_size}\",\n+            )\n+            logger.info(f\"ignore_sanity_checks is set to: {ignore_sanity_checks}\")\n+            logger.info(\"Wandb initialized.\")\n+            # Log the current file to wandb\n+            wandb.save(\"test_train.py\")\n+\n+    else:\n+        logger.info(\"Running in non-distributed mode. DeviceMesh not applicable.\")\n+        rank = 0\n+        world_size = 1\n+        local_rank = 0\n+        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+        wandb.init(\n+            project=\"tp_dp_test\",\n+            config={\n+                \"tp_size\": 1,\n+                \"dp_size\": 1,\n+                \"global_batch_size\": global_batch_size,\n+                \"model_name\": model_name,\n+                \"dataset\": \"roneneldan/TinyStories-1M\",\n+                \"seq_len\": seq_len,\n+            },\n+            name=\"llama_tp1_dp1_nondist\" if model_name == \"unsloth/Llama-3.2-1B\" else \"tp1_dp1_nondist\",\n+        )\n+        logger.info(\"Wandb initialized for non-distributed run.\")\n+\n+    # Load model and tokenizer\n+    logger.info(f\"Loading model and tokenizer from {model_name}\")\n+    tokenizer = AutoTokenizer.from_pretrained(model_name)\n+    if tokenizer.pad_token is None:\n+        tokenizer.pad_token = tokenizer.eos_token\n+        logger.info(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n+\n+    model = AutoModelForCausalLM.from_pretrained(\n+        model_name,\n+        device_mesh=tp_mesh if dist.is_initialized() else None,\n+        tp_plan=\"auto\",\n+        torch_dtype=torch.bfloat16,\n+    )\n+    logger.info(f\"Model loaded onto device mesh: {tp_mesh}\")\n+\n+    if dist.is_initialized():\n+        assert model.config.num_key_value_heads % tp_mesh.size() == 0, (\n+            f\"num_key_value_heads={model.config.num_key_value_heads} must be divisible by tp_size={tp_mesh.size()}\"\n+        )\n+        device = torch.device(f\"cuda:{local_rank}\")\n+    else:\n+        model = model.to(device)\n+\n+    logger.info(f\"Using device: {device} for non-model tensors\")\n+    use_ddp = False\n+    if dist.is_initialized() and dp_mesh.size() > 1:\n+        # FSDP1\n+        model = FSDP(model, device_mesh=dp_mesh, sharding_strategy=ShardingStrategy.NO_SHARD)\n+        # FSDP2\n+        # for transformer_block in model.model.layers:\n+        #     fully_shard(transformer_block, mesh=dp_mesh, reshard_after_forward=False)\n+        # fully_shard(model.model, mesh=dp_mesh, reshard_after_forward=False)\n+        # DDP\n+        # replicate(model, device_mesh=dp_mesh, bucket_cap_mb=100)\n+        # assert len(list(model.parameters()))>5, \"No parameters found in model. Probably DDP/FSDP bug..\" # TODO: we should be cautious abt using model.parameters()\n+        use_ddp = True\n+\n+    model.train()\n+    assert len(list(model.parameters())) > 0, \"No parameters found in model. Probably DDP bug..\"\n+    assert len([p for p in model.parameters() if p.requires_grad]) > 0, (\n+        \"No gradients found in model. Probably DDP bug..\"\n+    )\n+\n+    if dist.is_initialized() and not ignore_sanity_checks:\n+        # assert model is replicated across all dp\n+        for name, param in model.named_parameters():\n+            sanity_check_tensor_sync(param, dp_mesh)\n+\n+        # assert model is different across tp (only for sharded params)\n+        for name, param in model.named_parameters():\n+            if isinstance(param, DTensor) and param.placements[0].is_shard():\n+                # Only check sharded parameters for non-sync across TP\n+                sanity_check_tensor_sync(param, tp_mesh, not_sync=True)\n+            elif isinstance(param, DTensor) and param.placements[0].is_replicate():\n+                # Replicated parameters should be the same across TP\n+                sanity_check_tensor_sync(param, tp_mesh)\n+\n+        # assert model is replicated across cp\n+        for name, param in model.named_parameters():\n+            sanity_check_tensor_sync(param, cp_mesh)\n+\n+    # Load and preprocess TinyStories dataset\n+    logger.info(\"Loading TinyStories dataset...\")\n+    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:1%]\")  # Use 1% for faster testing\n+\n+    def tokenize_function(examples):\n+        # Tokenize the text without padding\n+        tokenized_batch = tokenizer(\n+            examples[\"text\"], padding=False, truncation=True, max_length=seq_len, return_tensors=None\n+        )\n+        # Set labels to be the same as input_ids for Causal LM\n+        tokenized_batch[\"labels\"] = tokenized_batch[\"input_ids\"].copy()\n+        return tokenized_batch\n+\n+    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n+    logger.info(f\"Dataset loaded and tokenized. Size: {len(tokenized_dataset)}\")\n+\n+    # Create packed sequences\n+    def create_packed_sequences(examples):\n+        # Flatten all sequences\n+        all_tokens = []\n+        for input_ids in examples[\"input_ids\"]:\n+            all_tokens.extend(input_ids)\n+\n+        # Split into sequences of seq_len + 1 (for input + label)\n+        num_sequences = len(all_tokens) // (seq_len + 1)\n+        packed_input_ids = []\n+        packed_labels = []\n+\n+        for i in range(num_sequences):\n+            start_idx = i * (seq_len + 1)\n+            end_idx = start_idx + (seq_len + 1)\n+            # Get the full sequence\n+            full_sequence = all_tokens[start_idx:end_idx]\n+            # For input_ids, remove the last token\n+            packed_input_ids.append(full_sequence[:-1])\n+            # For labels, remove the first token\n+            packed_labels.append(full_sequence[1:])\n+\n+        return {\"input_ids\": packed_input_ids, \"labels\": packed_labels}\n+\n+    # Apply packing to the dataset\n+    packed_dataset = tokenized_dataset.map(\n+        create_packed_sequences,\n+        batched=True,\n+        remove_columns=tokenized_dataset.column_names,\n+        batch_size=1000,  # Process in batches for efficiency\n+        num_proc=60,\n+    )\n+    logger.info(f\"Dataset packed. New size: {len(packed_dataset)}\")\n+\n+    # Shuffle the packed dataset\n+    packed_dataset = packed_dataset.shuffle(seed=42)\n+    logger.info(\"Packed dataset shuffled\")\n+\n+    # Calculate local batch size\n+    if dist.is_initialized():\n+        assert global_batch_size % dp_mesh.size() == 0, (\n+            f\"Global batch size ({global_batch_size}) must be divisible by DP size ({dp_mesh.size()})\"\n+        )\n+        local_batch_size = global_batch_size // dp_mesh.size()\n+    else:\n+        local_batch_size = global_batch_size\n+\n+    logger.info(\n+        f\"Global batch size: {global_batch_size}, DP size: {dp_size if dist.is_initialized() else 1}, Local batch size: {local_batch_size}\"\n+    )\n+\n+    # Simple collate function since sequences are already packed\n+    def collate_fn(batch):\n+        input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n+        labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n+        return {\"input_ids\": input_ids, \"labels\": labels}\n+\n+    if dist.is_initialized():\n+        sampler = DistributedSampler(\n+            packed_dataset, num_replicas=dp_mesh.size(), rank=dp_mesh.get_local_rank(), shuffle=False\n+        )\n+    else:\n+        sampler = None\n+\n+    dataloader = DataLoader(\n+        packed_dataset,\n+        batch_size=local_batch_size,\n+        sampler=sampler,\n+        shuffle=False,\n+        collate_fn=collate_fn,\n+    )\n+    logger.info(f\"DataLoader created. Distributed: {dist.is_initialized()}\")\n+\n+    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.1)\n+\n+    # Training loop\n+    logger.info(f\"Starting training for {num_train_steps} steps...\")\n+    model.train()\n+    step = 0\n+    while step < num_train_steps:\n+        for batch in dataloader:\n+            if step >= num_train_steps:\n+                break  # Exit loop if max steps reached\n+\n+            # Move batch to appropriate device\n+            batch = {k: v.to(device) for k, v in batch.items()}\n+\n+            # Sanity checks for batch distribution (only if distributed)\n+            if dist.is_initialized() and not ignore_sanity_checks:\n+                # check batch is same across all tp\n+                sanity_check_tensor_sync(batch[\"input_ids\"], tp_mesh)\n+                # check batch is different across dp\n+                sanity_check_tensor_sync(batch[\"input_ids\"], dp_mesh, not_sync=True)\n+\n+            optimizer.zero_grad()\n+\n+            # Add position_ids to batch before CP sharding\n+            batch_size = batch[\"input_ids\"].shape[0]\n+            position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device)\n+            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n+            batch[\"position_ids\"] = position_ids\n+            from torch.distributed.tensor.experimental._attention import _cp_options\n+\n+            _cp_options.enable_load_balance = False\n+\n+            with sdpa_kernel(sdpa_backend):  # TODO: ideally move this to attention implementation\n+                cp_context = (\n+                    nullcontext()\n+                    if cp_mesh.size() == 1\n+                    else context_parallel(\n+                        cp_mesh,\n+                        buffers=[\n+                            batch[\"input_ids\"],\n+                            batch[\"labels\"],\n+                            batch[\"position_ids\"],\n+                        ],  # TODO: need to add attention mask\n+                        buffer_seq_dims=[1, 1, 1],\n+                    )\n+                )\n+                with cp_context:\n+                    # Pop labels from batch before model forward pass\n+                    labels = batch.pop(\"labels\")\n+                    outputs = model(**batch)  # [mbs, seq_len/cp]\n+                    loss = outputs.loss\n+                    logits = outputs.logits\n+\n+                    # Compute loss with shifted labels\n+                    loss = model.loss_function(\n+                        logits=logits, labels=None, shift_labels=labels, vocab_size=model.config.vocab_size\n+                    )\n+\n+                    # Sanity checks for logits\n+                    if dist.is_initialized() and not ignore_sanity_checks:\n+                        # sanity_check_tensor_sync(logits, tp_mesh) # TODO: only true without sequence parallel\n+                        sanity_check_tensor_sync(logits, dp_mesh, not_sync=True)\n+                        sanity_check_tensor_sync(logits, cp_mesh, not_sync=True)\n+\n+                    loss.backward()\n+\n+                # all reduce grads across dp_cp if applicable\n+                all_reduce_grads(model, world_mesh, use_ddp=use_ddp)\n+\n+                # Sanity checks for gradients (only if distributed)\n+                if dist.is_initialized() and not ignore_sanity_checks:\n+                    # check grads are not same across all tp (for sharded grads)\n+                    for name, param in model.named_parameters():\n+                        if param.grad is not None and isinstance(param.grad, DTensor):\n+                            if param.grad.placements[0].is_shard():\n+                                sanity_check_tensor_sync(param.grad, tp_mesh, not_sync=True)\n+                            elif param.grad.placements[0].is_replicate():\n+                                sanity_check_tensor_sync(param.grad, tp_mesh)\n+                    # check grads are same across dp\n+                    for name, param in model.named_parameters():\n+                        if param.grad is not None and dp_mesh.size() > 1:\n+                            sanity_check_tensor_sync(param.grad, dp_mesh)\n+                    # check grads are same across cp\n+                    for name, param in model.named_parameters():\n+                        if param.grad is not None and cp_mesh.size() > 1:\n+                            sanity_check_tensor_sync(param.grad, cp_mesh)\n+\n+                # Calculate gradient norm and clip gradients\n+                if hasattr(model, \"clip_grad_norm_\"):\n+                    # when using FSDP or DDP, model.parameters() doesn't work\n+                    gradnorm = model.clip_grad_norm_(max_norm=1.0, norm_type=2.0)\n+                else:\n+                    assert len(list(model.parameters())) > 2, \"No parameters found in model. Probably DDP bug..\"\n+                    assert len([p for p in model.parameters() if p.requires_grad]) > 2, (\n+                        \"No gradients found in model. Probably DDP bug..\"\n+                    )\n+                    assert len([p for p in model.parameters() if p.grad is not None]) > 2, (\n+                        \"No gradients found in model. Probably DDP bug..\"\n+                    )\n+                    # only works with FSDP's NO_SHARD otherwise we should use FSDP's clip_grad_norm_\n+                    gradnorm = clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2.0, foreach=True)\n+\n+                optimizer.step()\n+                # Sanity checks for updated model parameters (only if distributed)\n+                if dist.is_initialized() and not ignore_sanity_checks:\n+                    # check updated model is different across all tp (for sharded params)\n+                    for name, param in model.named_parameters():\n+                        if isinstance(param, DTensor):\n+                            if param.placements[0].is_shard():\n+                                sanity_check_tensor_sync(param, tp_mesh, not_sync=True)\n+                            elif param.placements[0].is_replicate():\n+                                sanity_check_tensor_sync(param, tp_mesh)\n+                    # check updated model is same across dp\n+                    for name, param in model.named_parameters():\n+                        sanity_check_tensor_sync(param, dp_mesh)\n+                    # check updated model is same across cp\n+                    for name, param in model.named_parameters():\n+                        sanity_check_tensor_sync(param, cp_mesh)\n+\n+                # allreduce loss across cp_dp before logging\n+                if dist.is_initialized() and (cp_mesh.size() > 1 or dp_mesh.size() > 1):\n+                    dist.all_reduce(loss, group=world_mesh[\"dp_cp\"].get_group(), op=dist.ReduceOp.AVG)\n+                current_loss = loss.item()\n+\n+                # Log loss and gradnorm to wandb (only on rank 0 of dp group)\n+                if not dist.is_initialized() or dist.get_rank() == 0:\n+                    logger.info(\n+                        f\"Step: {step} | GBS: {global_batch_size} | DP: {dp_mesh.size()} | TP: {tp_mesh.size()} | CP: {cp_mesh.size()} | Loss: {current_loss} | Gradnorm: {gradnorm} | lr: {LR}\"\n+                    )\n+                    wandb.log(\n+                        {\n+                            \"train/loss\": current_loss,\n+                            \"train/gradnorm\": gradnorm,\n+                            \"step\": step,\n+                            \"lr\": LR,\n+                            \"GBS\": global_batch_size,\n+                        }\n+                    )\n+\n+            step += 1  # Increment step count\n+\n+    logger.info(\"Training loop finished.\")\n+\n+    # Save model using DCP (only if distributed)\n+    if dist.is_initialized():\n+        state_dict = {\"app\": AppState(model, optimizer)}\n+        dcp.save(\n+            state_dict=state_dict,\n+            checkpoint_id=CHECKPOINT_DIR,\n+        )\n+        logger.info(f\"Saved checkpoint to {CHECKPOINT_DIR}\")\n+    else:\n+        # Fallback to regular save for non-distributed case\n+        save_dir = \"test_model_nondist\"\n+        model.save_pretrained(save_dir, safe_serialization=False)\n+        tokenizer.save_pretrained(save_dir)  # Save tokenizer too\n+        logger.info(f\"Saved model to {save_dir}\")\n+\n+    # Example of loading the checkpoint (only if distributed)\n+    if dist.is_initialized():\n+        # Create a new model instance\n+        logger.info(\"Creating new model instance for verification\")\n+        new_model = AutoModelForCausalLM.from_pretrained(\n+            model_name,\n+            device_mesh=tp_mesh,\n+            torch_dtype=torch.bfloat16,  # Use same dtype\n+        )\n+        new_optimizer = optim.AdamW(new_model.parameters(), lr=LR)\n+\n+        # Load checkpoint into new model\n+        state_dict = {\"app\": AppState(new_model, new_optimizer)}\n+        dcp.load(\n+            state_dict=state_dict,\n+            checkpoint_id=CHECKPOINT_DIR,\n+        )\n+        logger.info(\"Loaded checkpoint into new model\")\n+\n+        # Verify model weights match\n+        logger.info(\"Verifying model weights match...\")\n+        for (name1, param1), (name2, param2) in zip(model.named_parameters(), new_model.named_parameters()):\n+            torch.testing.assert_close(\n+                param1.to_local(),\n+                param2.to_local(),\n+                rtol=1e-3,\n+                atol=1e-3,\n+                msg=f\"Weights mismatch in {name1} vs {name2}\",\n+            )\n+\n+        # Verify optimizer states match\n+        logger.info(\"Verifying optimizer states match...\")\n+        for name1, state1 in optimizer.state_dict().items():\n+            state2 = new_optimizer.state_dict()[name1]\n+            if name1 == \"state\":\n+                # Compare state dictionaries for each parameter\n+                for param_id, param_state1 in state1.items():\n+                    param_state2 = state2[param_id]\n+                    # Compare each state component (step, exp_avg, exp_avg_sq)\n+                    for key, value1 in param_state1.items():\n+                        value2 = param_state2[key]\n+                        if isinstance(value1, DTensor):\n+                            # Convert DTensors to local tensors for comparison\n+                            torch.testing.assert_close(\n+                                value1.to_local(),\n+                                value2.to_local(),\n+                                rtol=1e-5,\n+                                atol=1e-5,\n+                                msg=f\"Optimizer state mismatch in state[{param_id}][{key}]\",\n+                            )\n+                        else:\n+                            torch.testing.assert_close(\n+                                value1,\n+                                value2,\n+                                rtol=1e-5,\n+                                atol=1e-5,\n+                                msg=f\"Optimizer state mismatch in state[{param_id}][{key}]\",\n+                            )\n+            elif name1 == \"param_groups\":\n+                # Compare param_groups (excluding the actual params list)\n+                for i, (group1, group2) in enumerate(zip(state1, state2)):\n+                    for key in group1:\n+                        if key != \"params\":  # Skip comparing the params list\n+                            assert group1[key] == group2[key], f\"Param group mismatch in param_groups[{i}][{key}]\"\n+\n+        # Run a forward pass with both models to verify outputs match\n+        logger.info(\"Running forward pass verification...\")\n+        with torch.no_grad():\n+            # Use the last batch for verification\n+            batch = {k: v.to(device) for k, v in batch.items()}  # Ensure batch is on correct device\n+            original_outputs = model(**batch)\n+            new_outputs = new_model(**batch)\n+            torch.testing.assert_close(\n+                original_outputs.logits.to_local(),\n+                new_outputs.logits.to_local(),\n+                rtol=1e-3,\n+                atol=1e-3,\n+                msg=\"Model outputs do not match!\",\n+            )  # Increased tolerance slightly for bf16\n+\n+    # Clean up distributed environment and finish wandb run\n+    if dist.is_initialized():\n+        dist.destroy_process_group()\n+        logger.info(\"Cleaned up distributed process group\")\n+        # Finish wandb run on rank 0\n+        if dist.get_rank() == 0:\n+            wandb.finish()\n+            logger.info(\"Wandb run finished.\")\n+    else:\n+        wandb.finish()\n+        logger.info(\"Wandb run finished.\")\n+\n+\n+def all_reduce_grads(model, world_mesh, use_ddp):\n+    \"\"\"All reduce gradients across dp_cp if applicable.\"\"\"\n+    cp_mesh = world_mesh[\"cp\"]\n+    if use_ddp:\n+        # DDP takes care of syncing grads\n+        mesh = cp_mesh\n+    else:\n+        mesh = world_mesh[\"dp\", \"cp\"]._flatten(mesh_dim_name=\"dp_cp\")\n+    if dist.is_initialized() and mesh.size() > 1:\n+        for name, param in model.named_parameters():\n+            if param.grad is not None:\n+                # Workaround for cross-mesh communication limitation with DTensor gradients\n+                if isinstance(param.grad, DTensor):\n+                    local_grad = param.grad.to_local()\n+                    # Ensure grad requires grad for inplace modification checks (might not be needed)\n+                    # local_grad = local_grad.detach().requires_grad_(True)\n+                    torch.distributed.all_reduce(local_grad, op=torch.distributed.ReduceOp.SUM, group=mesh.get_group())\n+                    local_grad = local_grad / mesh.size()\n+                    # Assign averaged grad back - need careful handling if DTensor structure is complex\n+                    # This simple assignment might work if the grad structure matches param structure\n+                    param.grad = DTensor.from_local(\n+                        local_grad, device_mesh=param.grad.device_mesh, placements=param.grad.placements\n+                    )\n+                else:\n+                    # Handle regular tensors if any exist (e.g. buffers not converted to DTensor)\n+                    torch.distributed.all_reduce(param.grad, op=torch.distributed.ReduceOp.AVG, group=mesh.get_group())\n+\n+\n+class ContextParallelCollator:\n+    \"\"\"Collator for context parallel training that splits sequences into chunks.\"\"\"\n+\n+    def __init__(self, cp_mesh: Optional[DeviceMesh] = None):\n+        self.cp_mesh = cp_mesh\n+\n+    def __call__(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n+        batch = default_collate(batch)\n+        if self.cp_mesh is not None and self.cp_mesh.size() > 1:\n+            # Get sequence length from the input batch\n+            seq_len = batch[\"input_ids\"].shape[1]\n+            assert seq_len % self.cp_mesh.size() == 0, (\n+                f\"Sequence length {seq_len} must be divisible by CP size {self.cp_mesh.size()}\"\n+            )\n+            chunk_size = seq_len // self.cp_mesh.size()\n+            cp_rank = self.cp_mesh.get_local_rank()\n+            start_idx = cp_rank * chunk_size\n+            end_idx = start_idx + chunk_size\n+\n+            # Keep only the local chunk of the sequence\n+            batch[\"input_ids\"] = batch[\"input_ids\"][:, start_idx:end_idx]\n+            batch[\"attention_mask\"] = batch[\"attention_mask\"][:, start_idx:end_idx]\n+            batch[\"labels\"] = batch[\"labels\"][:, start_idx:end_idx]\n+\n+        return batch\n+\n+\n+class AppState(Stateful):\n+    \"\"\"Wrapper for checkpointing the Application State including model and optimizer.\"\"\"\n+\n+    def __init__(self, model, optimizer=None):\n+        self.model = model\n+        self.optimizer = optimizer\n+\n+    def state_dict(self):\n+        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n+        return {\"model\": model_state_dict, \"optim\": optimizer_state_dict}\n+\n+    def load_state_dict(self, state_dict):\n+        set_state_dict(\n+            self.model, self.optimizer, model_state_dict=state_dict[\"model\"], optim_state_dict=state_dict[\"optim\"]\n+        )\n+\n+\n+def sanity_check_tensor_sync(\n+    tensor: torch.Tensor, mesh: DeviceMesh, rtol: float = 1e-4, atol: float = 1e-4, not_sync: bool = False\n+) -> None:\n+    \"\"\"\n+    Verify that a tensor is synchronized (or not synchronized) across all processes in the mesh's process group.\n+    Handles both regular tensors and DTensors.\n+\n+    Args:\n+        tensor (torch.Tensor): The tensor to check for synchronization (can be DTensor)\n+        mesh (DeviceMesh): The device mesh containing the process group\n+        rtol (float): Relative tolerance for comparison\n+        atol (float): Absolute tolerance for comparison\n+        not_sync (bool): If True, asserts that tensors are NOT synchronized. If False, asserts they are synchronized.\n+    \"\"\"\n+    if not dist.is_initialized() or mesh.size() == 1:\n+        return  # No need to check in non-distributed mode\n+\n+    # Get the process group from the mesh\n+    pg = mesh.get_group()\n+\n+    # Convert DTensor to local tensor if needed\n+    if hasattr(tensor, \"to_local\"):\n+        local_tensor = tensor.to_local()\n+    else:\n+        local_tensor = tensor\n+\n+    # Gather tensors from all processes\n+    world_size = dist.get_world_size(pg)\n+    gathered_tensors = [torch.empty_like(local_tensor) for _ in range(world_size)]\n+    dist.all_gather(gathered_tensors, local_tensor, group=pg)\n+\n+    # Compare each tensor with the first one\n+    for i in range(1, world_size):\n+        try:\n+            torch.testing.assert_close(gathered_tensors[0], gathered_tensors[i], rtol=rtol, atol=atol)\n+        except AssertionError as e:\n+            if not_sync:\n+                continue\n+            # # Add detailed debugging for logit synchronization issues\n+            # print(f\"\\nLogit synchronization error between rank 0 and rank {i}:\")\n+            # print(f\"Tensor shape: {gathered_tensors[0].shape}\")\n+            # print(f\"Number of mismatched elements: {(gathered_tensors[0] != gathered_tensors[i]).sum()}\")\n+            # print(f\"Percentage of mismatched elements: {((gathered_tensors[0] != gathered_tensors[i]).sum() / gathered_tensors[0].numel() * 100):.2f}%\")\n+\n+            # # Find the first few mismatches\n+            # mismatches = torch.nonzero(gathered_tensors[0] != gathered_tensors[i])\n+            # print(\"\\nFirst few mismatches:\")\n+            # for idx in mismatches[:5]:\n+            #     idx = tuple(idx.tolist())\n+            #     print(f\"Index {idx}:\")\n+            #     print(f\"Rank 0 value: {gathered_tensors[0][idx]}\")\n+            #     print(f\"Rank {i} value: {gathered_tensors[i][idx]}\")\n+            #     print(f\"Absolute difference: {abs(gathered_tensors[0][idx] - gathered_tensors[i][idx])}\")\n+            #     print(f\"Relative difference: {abs(gathered_tensors[0][idx] - gathered_tensors[i][idx]) / max(abs(gathered_tensors[0][idx]), abs(gathered_tensors[i][idx]))}\")\n+\n+            # # Check if differences are systematic (e.g., all positive or negative)\n+            # diff = gathered_tensors[0] - gathered_tensors[i]\n+            # print(f\"\\nDifference statistics:\")\n+            # print(f\"Mean difference: {diff.mean()}\")\n+            # print(f\"Std difference: {diff.std()}\")\n+            # print(f\"Max positive difference: {diff.max()}\")\n+            # print(f\"Max negative difference: {diff.min()}\")\n+            raise e\n+\n+\n+def clip_grad_norm_(\n+    parameters: Iterable[torch.Tensor],\n+    max_norm: float,\n+    norm_type: float = 2.0,\n+    error_if_nonfinite: bool = False,\n+    foreach: bool | None = None,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Clip the gradient norm of an iterable of parameters.\n+    \"\"\"\n+    # Filter out parameters with no gradients\n+    parameters = [p for p in parameters if p.grad is not None]\n+    assert len(parameters) > 0, \"No parameters with gradients found\"\n+\n+    # Calculate total norm\n+    if norm_type == float(\"inf\"):\n+        total_norm = max(p.grad.detach().abs().max() for p in parameters)\n+    else:\n+        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type)\n+\n+    # Convert DTensor to local tensor if needed\n+    if isinstance(total_norm, DTensor):\n+        total_norm = total_norm.full_tensor()\n+\n+    # Clip gradients\n+    clip_coef = max_norm / (total_norm + 1e-6)\n+    if clip_coef < 1:\n+        for p in parameters:\n+            p.grad.detach().mul_(clip_coef)\n+\n+    return total_norm\n+\n+\n+def check_params_sync(model_params, original_params):\n+    \"\"\"\n+    Check if original_params are being updated in sync with model parameters.\n+\n+    Args:\n+        model_params: Iterator of model parameters after update\n+        original_params: List of original parameters before DDP wrapping\n+    \"\"\"\n+    for mp, op in zip(model_params, original_params):\n+        if isinstance(mp, DTensor):\n+            mp = mp.to_local()\n+        if isinstance(op, DTensor):\n+            op = op.to_local()\n+        if not torch.allclose(mp.data, op.data, rtol=0, atol=0):\n+            raise RuntimeError(f\"Parameters out of sync: model param {mp.data} != original param {op.data}\")\n+    return True\n+\n+\n+def get_parameters(model: nn.Module) -> Iterable[torch.Tensor]:\n+    \"\"\"\n+    Get all parameters from a model by iterating over its modules.\n+    This is an alternative to model.parameters() that works with DTensor models.\n+\n+    Args:\n+        model (nn.Module): The model to get parameters from\n+\n+    Returns:\n+        Iterable[torch.Tensor]: An iterator over all parameters in the model\n+    \"\"\"\n+    for name, module in model._modules.items():\n+        # Look for parameters in module attributes\n+        for attr_name, attr in module.__dict__.items():\n+            if isinstance(attr, torch.Tensor) and attr.requires_grad:\n+                yield attr\n+        # Recursively get parameters from submodules\n+        for param in get_parameters(module):\n+            yield param\n+\n+\n+def update_model_parameters(model: nn.Module) -> None:\n+    \"\"\"\n+    Update model._parameters using named_modules() to ensure all parameters are properly tracked.\n+\n+    Args:\n+        model (nn.Module): The model to update parameters for\n+    \"\"\"\n+    # Clear existing parameters\n+    model._parameters = {}\n+\n+    # Add parameters from named_modules\n+    for name, module in model.named_modules():\n+        # Skip the root module itself\n+        if name == \"\":\n+            continue\n+\n+        # Get the parameter name by removing 'module.' prefix if it exists\n+        param_name = name.replace(\"module.\", \"\")\n+\n+        # Add weight and bias parameters if they exist\n+        if hasattr(module, \"weight\") and module.weight is not None:\n+            model._parameters[f\"{param_name}.weight\"] = module.weight\n+        if hasattr(module, \"bias\") and module.bias is not None:\n+            model._parameters[f\"{param_name}.bias\"] = module.bias\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "22cc75b20f048f12d20998c325b8bf34046cf542",
            "filename": "examples/pytorch/context_parallel.py",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c2f36b480e02c9027d2523746d34e27b39e01a4/examples%2Fpytorch%2Fcontext_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c2f36b480e02c9027d2523746d34e27b39e01a4/examples%2Fpytorch%2Fcontext_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontext_parallel.py?ref=1c2f36b480e02c9027d2523746d34e27b39e01a4",
            "patch": "@@ -0,0 +1,94 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import os\n+\n+import torch\n+import torch.distributed as dist\n+from torch.distributed.device_mesh import init_device_mesh\n+from torch.distributed.tensor.experimental import context_parallel\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n+from torch.nn.parallel import DistributedDataParallel as DDP\n+\n+from transformers import AutoModelForCausalLM\n+from transformers.loss.loss_utils import ForCausalLMLoss\n+\n+\n+world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n+cp_mesh = init_device_mesh(\"cuda\", (world_size,))\n+rank = torch.distributed.get_node_local_rank()\n+\n+device = \"cuda\"\n+dtype = torch.bfloat16\n+sdpa_backend = SDPBackend.FLASH_ATTENTION\n+\n+# prepare inputs\n+batch_size = 1\n+seq_len = 128\n+\n+input_ids = torch.randint(low=8, high=64, size=(batch_size, seq_len), device=device)\n+\n+ignore_index = -100\n+# When using CP, we need to use `shift_labels`\n+shift_labels = torch.nn.functional.pad(input_ids, (0, 1), value=ignore_index)\n+shift_labels = shift_labels[..., 1:].contiguous()\n+\n+position_ids = (\n+    torch.cumsum(torch.ones(size=input_ids.size(), dtype=input_ids.dtype, device=input_ids.device), dim=1) - 1\n+)\n+\n+# sync input as they are created randomly\n+dist.broadcast(input_ids, src=0)\n+dist.broadcast(shift_labels, src=0)\n+dist.broadcast(position_ids, src=0)\n+\n+# model and optimizer\n+repo_id = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n+model = AutoModelForCausalLM.from_pretrained(repo_id, torch_dtype=dtype, device_map=device)\n+optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n+\n+model.train()\n+model.zero_grad()\n+optimizer.zero_grad()\n+\n+# For loss\n+vocab_size = model.config.vocab_size\n+\n+# so training could be synced\n+model = DDP(model, device_ids=[rank])\n+\n+# prepare for CP\n+buffers = (input_ids, shift_labels, position_ids)\n+buffer_seq_dims = (1, 1, 1)\n+# `no_restore_buffers=set(buffers)` is required if `loss.backward` is outside `context_parallel`.\n+# no_restore_buffers = set(buffers)\n+no_restore_buffers = None\n+\n+# run with CP\n+with sdpa_kernel(sdpa_backend):\n+    with context_parallel(\n+        cp_mesh,\n+        buffers=buffers,\n+        buffer_seq_dims=buffer_seq_dims,\n+        no_restore_buffers=no_restore_buffers,\n+    ):\n+        outputs = model(input_ids, shift_labels=shift_labels, position_ids=position_ids)\n+        print(outputs.logits.shape)\n+\n+        # So far we need to compute `loss` outside `model.forward` when using `shift_labels`\n+        # loss = outputs.loss\n+        loss = ForCausalLMLoss(logits=outputs.logits, labels=None, shift_labels=shift_labels, vocab_size=vocab_size)\n+\n+        # This could be outside `context_parallel` context if `no_restore_buffers` is specified\n+        loss.backward()\n+        optimizer.step()"
        },
        {
            "sha": "1b87a554d3cb6c9a0c80acc66a4ef7be07372a07",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c2f36b480e02c9027d2523746d34e27b39e01a4/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c2f36b480e02c9027d2523746d34e27b39e01a4/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=1c2f36b480e02c9027d2523746d34e27b39e01a4",
            "patch": "@@ -142,7 +142,7 @@\n else:\n     _import_structure[\"tensor_parallel\"] = [\n         \"shard_and_distribute_module\",\n-        \"SUPPORTED_TP_STYLES\",\n+        \"ALL_PARALLEL_STYLES\",\n         \"translate_to_torch_parallel_style\",\n     ]\n try:\n@@ -271,7 +271,7 @@\n         pass\n     else:\n         from .tensor_parallel import (\n-            SUPPORTED_TP_STYLES,\n+            ALL_PARALLEL_STYLES,\n             shard_and_distribute_module,\n             translate_to_torch_parallel_style,\n         )"
        },
        {
            "sha": "ef868148b87d8d24eedb26efab3fa2737687c8e1",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 200,
            "deletions": 76,
            "changes": 276,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c2f36b480e02c9027d2523746d34e27b39e01a4/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c2f36b480e02c9027d2523746d34e27b39e01a4/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=1c2f36b480e02c9027d2523746d34e27b39e01a4",
            "patch": "@@ -13,11 +13,15 @@\n # limitations under the License.\n from __future__ import annotations\n \n+import operator\n+import os\n import re\n-from functools import lru_cache, partial\n-from typing import List, Optional, Tuple, Union\n+from collections.abc import MutableMapping\n+from functools import partial, reduce\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n+import torch.distributed as dist\n from torch import nn\n \n from ..utils import is_torch_greater_or_equal, logging\n@@ -35,6 +39,56 @@\n     from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n \n \n+def initialize_tensor_parallelism(tp_plan, tp_size=None):\n+    r\"\"\"\n+    Sets up the device mesh and initilized the backend for tensor parallelism.\n+    This function is called when the model is loaded and the TP plan is set to 'auto'.\n+    \"\"\"\n+    if tp_plan is None:\n+        return None, None, None\n+\n+    if not is_torch_greater_or_equal(\"2.5\"):\n+        raise EnvironmentError(\"Tensor parallel is only supported for `torch>=2.5`.\")\n+\n+    # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n+    device_type = torch._C._get_accelerator().type\n+    if not torch.distributed.is_initialized():\n+        try:\n+            rank = int(os.environ[\"RANK\"])\n+            local_rank = int(os.environ[\"LOCAL_RANK\"])\n+            world_size = int(os.environ[\"WORLD_SIZE\"])\n+\n+            backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"ccl\", \"hpu\": \"hccl\"}\n+            backend = backend_map.get(device_type)\n+            if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", 0)):\n+                backend = \"ccl\"\n+\n+            torch.distributed.init_process_group(backend=backend, rank=rank, world_size=world_size)\n+            current_device = getattr(torch, device_type)\n+            if device_type != \"cpu\":\n+                current_device.set_device(local_rank)\n+\n+        except Exception as e:\n+            raise EnvironmentError(\n+                \"We tried to initialize torch.distributed for you, but it failed. Make \"\n+                \"sure you init torch distributed in your script to use `tp_plan='auto'`.\"\n+            ) from e\n+    index = current_device.current_device() if device_type != \"cpu\" else None\n+    tp_device = torch.device(device_type, index)\n+\n+    # Silence output for non-primary ranks\n+    if index is not None and index > 0:\n+        import sys\n+\n+        sys.stdout = open(os.devnull, \"w\")\n+        sys.stderr = open(os.devnull, \"w\")\n+\n+    device_map = tp_device\n+    tp_size = tp_size if tp_size is not None else torch.distributed.get_world_size()\n+    device_mesh = torch.distributed.init_device_mesh(tp_device.type, (tp_size,))\n+    return tp_device, device_map, device_mesh\n+\n+\n def _blocks_to_block_sizes(total_size: int, blocks: Union[int, List[int]]) -> List[int]:\n     \"\"\"\n     Convert block count or proportions to block sizes.\n@@ -220,18 +274,38 @@ def repack_weights(\n \n \n def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n-    if dim == 0:\n-        size_ = empty_param.shape[0]\n-        param = param[rank * (size_ // device_mesh.size()) : (rank + 1) * (size_ // device_mesh.size()), ...]\n-    elif dim == 1 or dim == -2:\n-        size_ = empty_param.shape[-2]\n-        param = param[..., rank * (size_ // device_mesh.size()) : (rank + 1) * (size_ // device_mesh.size()), :]\n-    elif dim == 2 or dim == -1:\n-        size_ = empty_param.shape[-1]\n-        param = param[..., rank * (size_ // device_mesh.size()) : (rank + 1) * (size_ // device_mesh.size())]\n-    else:\n-        raise ValueError(f\"Unsupported dim {dim}, only dim 0, 1 or 2 are supported\")\n-    return param\n+    \"\"\"\n+    Generalized tensor sharding across a multi-dimensional device mesh.\n+\n+    Args:\n+        param (torch.Tensor): The tensor to shard.\n+        empty_param (torch.Tensor): A tensor used for shape reference.\n+        device_mesh (torch.Tensor): Shape [d_0, ..., d_n] representing the mesh.\n+        rank (int): Global rank of the current process/device.\n+        dim (int): Dimension along which to shard the tensor.\n+    \"\"\"\n+    param_dim = empty_param.dim()\n+    if dim < 0:\n+        dim = param_dim + dim\n+    if dim >= param_dim:\n+        raise ValueError(f\"dim {dim} is out of bounds for tensor of dimension {param_dim}\")\n+\n+    # Flatten the mesh to get the total number of devices\n+    mesh_shape = device_mesh.shape\n+    world_size = reduce(operator.mul, mesh_shape)\n+\n+    if rank >= world_size:\n+        raise ValueError(f\"Rank {rank} is out of bounds for mesh size {world_size}\")\n+\n+    shard_size = empty_param.shape[dim] // world_size\n+    start = rank * shard_size\n+    end = start + shard_size\n+\n+    # Construct slicing index dynamically\n+    slice_indices = [slice(None)] * param_dim\n+    slice_indices[dim] = slice(start, end)\n+\n+    return param[tuple(slice_indices)]\n \n \n def distribute_module(\n@@ -339,6 +413,41 @@ def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         )\n \n \n+class ReplicateParallel(TensorParallelLayer):\n+    \"\"\"\n+    This class is used to replicate computation in a TP layer (used in SP regions when we don't use sequence parallelism for example)\n+    \"\"\"\n+\n+    def __init__(self, *, use_dtensor=True, use_local_output=True):\n+        super().__init__()\n+        self.input_layouts = (Replicate(),)\n+        self.output_layouts = (Replicate(),)\n+        self.desired_input_layouts = (Replicate(),)\n+        self.use_local_output = use_local_output\n+        self.use_dtensor = use_dtensor\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        # TODO: figure out dynamo support for instance method and switch this to instance method\n+        # annotate module input placements/sharding with input_layouts\n+        input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+\n+        return input_tensor\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        return outputs.to_local() if use_local_output else outputs\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        param = param[...].to(param_casting_dtype)\n+        if to_contiguous:\n+            param = param.contiguous()\n+        param = DTensor.from_local(param, device_mesh, [Replicate()], run_check=False)\n+        return param\n+\n+\n class ColwiseParallel(TensorParallelLayer):\n     \"\"\"\n     General tensor parallel layer for transformers.\n@@ -611,52 +720,62 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n-SUPPORTED_TP_STYLES = {\n-    \"colwise\",\n-    \"rowwise\",\n-    \"colwise_rep\",\n-    \"rowwise_rep\",\n-    \"local_colwise\",\n-    \"local_rowwise\",\n-    \"local\",\n-    \"gather\",\n-    \"local_packed_rowwise\",\n-    \"sequence_parallel\",\n-}\n+class ParallelInterface(MutableMapping):\n+    \"\"\"\n+    Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n+    with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n+    it needs to declare a new instance of this class inside the `modeling_<model>.py`, and declare it on that instance.\n+    \"\"\"\n \n+    # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n+    # a new instance is created (in order to locally override a given function)\n+    _global_mapping = {\n+        \"colwise\": ColwiseParallel(),\n+        \"rowwise\": RowwiseParallel(),\n+        \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n+        \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n+        \"local_colwise\": ColwiseParallel(use_dtensor=False),\n+        \"local_rowwise\": RowwiseParallel(use_dtensor=False),\n+        \"local\": IsolatedParallel(),\n+        \"gather\": GatherParallel(),\n+        \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n+        \"sequence_parallel\": SequenceParallel(),\n+        \"replicate\": ReplicateParallel(),\n+    }\n \n-@lru_cache\n-def translate_to_torch_parallel_style(style: str):\n-    \"\"\"\n-    In model configurations, we use a neutral type (string) to specify parallel\n-    styles, here we translate them into torch.distributed tensor-parallel\n-    types.\n-    \"\"\"\n-    if not isinstance(style, str):\n-        raise ValueError(f\"Unsupported parallel style type {type(style)}, expected str\")\n-\n-    if style == \"colwise\":\n-        return ColwiseParallel()\n-    elif style == \"rowwise\":\n-        return RowwiseParallel()\n-    elif style == \"colwise_rep\":\n-        return ColwiseParallel(output_layouts=Replicate())\n-    elif style == \"rowwise_rep\":\n-        return RowwiseParallel(input_layouts=Replicate())\n-    elif style == \"local_colwise\":\n-        return ColwiseParallel(use_dtensor=False)\n-    elif style == \"local_rowwise\":\n-        return RowwiseParallel(use_dtensor=False)\n-    elif style == \"local\":\n-        return IsolatedParallel()\n-    elif style == \"gather\":\n-        return GatherParallel()\n-    elif style == \"local_packed_rowwise\":\n-        return PackedRowwiseParallel(use_dtensor=False)\n-    elif style == \"sequence_parallel\":\n-        return SequenceParallel()\n-    else:\n-        raise ValueError(f\"Unsupported parallel style value: {style}\")\n+    def __init__(self):\n+        self._local_mapping = {}\n+\n+    def __getitem__(self, key):\n+        # First check if instance has a local override\n+        if key in self._local_mapping:\n+            return self._local_mapping[key]\n+        return self._global_mapping[key]\n+\n+    def __setitem__(self, key, value):\n+        # Allow local update of the default functions without impacting other instances\n+        self._local_mapping.update({key: value})\n+\n+    def __delitem__(self, key):\n+        del self._local_mapping[key]\n+\n+    def __iter__(self):\n+        # Ensure we use all keys, with the overwritten ones on top\n+        return iter({**self._global_mapping, **self._local_mapping})\n+\n+    def __len__(self):\n+        return len(self._global_mapping.keys() | self._local_mapping.keys())\n+\n+    @classmethod\n+    def register(cls, key: str, value: Callable):\n+        cls._global_mapping.update({key: value})\n+\n+    def valid_keys(self) -> List[str]:\n+        return list(self.keys())\n+\n+\n+# Global AttentionInterface shared by all models which do not need to overwrite any of the existing ones\n+ALL_PARALLEL_STYLES: ParallelInterface = ParallelInterface()\n \n \n def convert_local_tensor_to_dtensor(\n@@ -722,23 +841,27 @@ def __init__(self):\n \n     # 1. We add hooks to the layer being loaded:\n     if current_module_plan is not None:\n-        tp_layer = translate_to_torch_parallel_style(current_module_plan)\n+        tp_layer = ALL_PARALLEL_STYLES[current_module_plan]\n         try:\n             tp_layer.prepare_module_tp(module, device_mesh)\n         except NotImplementedError as e:\n             print(\n                 f\"Trying to prepare {layer_name}, but it's not supported. Corresponding module: {module} Fix it's TP plan: {e}\"\n             )\n+        module._hf_tp_plan = current_module_plan\n+        module.__repr__ = lambda: f\"{module.__repr__()}\\nTP Plan: {current_module_plan}\"\n \n     # 2. We add hooks to the parent module if needed\n     if \".\" in layer_name:\n         parent_layer_name = layer_name.rsplit(\".\", 1)[0]\n         generic_name = re.sub(r\"\\d+\", \"*\", parent_layer_name)\n         # The module itself needs hooks\n         if module_plan := tp_plan.get(generic_name, False):\n-            tp_layer = translate_to_torch_parallel_style(module_plan)\n+            tp_layer = ALL_PARALLEL_STYLES[module_plan]\n             module_to_tp_ = model.get_submodule(parent_layer_name)\n             tp_layer.prepare_module_tp(module_to_tp_, device_mesh)\n+            module_to_tp_._hf_tp_plan = current_module_plan\n+            module_to_tp_.__repr__ = lambda: f\"{module_to_tp_.__repr__()}\\nTP Plan: {current_module_plan}\"\n \n \n def shard_and_distribute_module(\n@@ -760,28 +883,29 @@ def shard_and_distribute_module(\n \n     current_module_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n \n+    if current_module_plan is None:\n+        current_module_plan = \"replicate\"\n+        if dist.get_rank() == 0:\n+            logger.info(f\"Tensor parallel plan for {param_name} not found, using default 'replicate' plan.\")\n+    else:\n+        if dist.get_rank() == 0:\n+            logger.info(f\"Tensor parallel plan for {param_name}: {current_module_plan}\")\n+\n     # Add hooks to the module if not done yet\n     # add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)\n     if not getattr(module_to_tp, \"_is_hooked\", False):\n         add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)\n         module_to_tp._is_hooked = True\n \n-    if current_module_plan is not None:\n-        try:\n-            tp_layer = translate_to_torch_parallel_style(current_module_plan)\n-            param = tp_layer.partition_tensor(\n-                param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n-            )\n-        except NotImplementedError as e:\n-            print(\n-                f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\"\n-            )\n-    else:\n-        # TODO log no plan modules in set\n-        # print(\"No plan for\", parameter_name,end =\"\\n\")\n-        param = param[...].to(param_casting_dtype)\n-        if is_contiguous:\n-            param = param.contiguous()\n+    try:\n+        tp_layer = ALL_PARALLEL_STYLES[current_module_plan]\n+        param = tp_layer.partition_tensor(\n+            param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n+        )\n+    except NotImplementedError as e:\n+        print(\n+            f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\"\n+        )\n \n     # SUPER IMPORTANT we have to use setattr\n     # otherwise loading is crazy slow"
        },
        {
            "sha": "ece787c7b6751648cf50d405bcf319f6ced06182",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 58,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c2f36b480e02c9027d2523746d34e27b39e01a4/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c2f36b480e02c9027d2523746d34e27b39e01a4/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1c2f36b480e02c9027d2523746d34e27b39e01a4",
            "patch": "@@ -62,8 +62,9 @@\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.tensor_parallel import (\n-    SUPPORTED_TP_STYLES,\n+    ALL_PARALLEL_STYLES,\n     _get_parameter_tp_plan,\n+    initialize_tensor_parallelism,\n     repack_weights,\n     replace_state_dict_local_with_dtensor,\n     shard_and_distribute_module,\n@@ -797,7 +798,7 @@ def _load_state_dict_into_meta_model(\n                 param_name,\n                 casting_dtype,\n                 to_contiguous,\n-                int(os.environ[\"RANK\"]),  # the rank\n+                device_mesh.get_local_rank(),\n                 device_mesh,\n             )\n         else:\n@@ -1964,9 +1965,9 @@ def post_init(self):\n \n         if self._tp_plan is not None and is_torch_greater_or_equal(\"2.3\"):\n             for _, v in self._tp_plan.items():\n-                if v not in SUPPORTED_TP_STYLES:\n+                if v not in ALL_PARALLEL_STYLES:\n                     raise ValueError(\n-                        f\"Unsupported tensor parallel style {v}. Supported styles are {SUPPORTED_TP_STYLES}\"\n+                        f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\"\n                     )\n \n     def dequantize(self):\n@@ -3559,6 +3560,7 @@ def save_pretrained(\n             state_dict = replace_state_dict_local_with_dtensor(state_dict, self._tp_plan, self._device_mesh)\n \n         if safe_serialization:\n+            # TODO: fix safe_serialization for tied weights\n             # Safetensors does not allow tensor aliasing.\n             # We're going to remove aliases before saving\n             ptrs = collections.defaultdict(list)\n@@ -4040,6 +4042,8 @@ def from_pretrained(\n                 `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.\n             tp_size (`str`, *optional*):\n                 A torch tensor parallel degree. If not provided would default to world size.\n+            device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n+                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.\n             offload_folder (`str` or `os.PathLike`, *optional*):\n                 If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n             offload_state_dict (`bool`, *optional*):\n@@ -4137,6 +4141,7 @@ def from_pretrained(\n         gguf_file = kwargs.pop(\"gguf_file\", None)\n         tp_plan = kwargs.pop(\"tp_plan\", None)\n         tp_size = kwargs.pop(\"tp_size\", None)\n+        device_mesh = kwargs.pop(\"device_mesh\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n \n         # Load models with hardcoded key mapping on class for VLMs only,  to keep BC and standardize model\n@@ -4172,59 +4177,13 @@ def from_pretrained(\n \n         # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n         # `device_map` pointing to the correct device\n-        device_mesh = None\n-        if tp_plan is not None:\n-            if not is_torch_greater_or_equal(\"2.5\"):\n-                raise EnvironmentError(\"tensor parallel is only supported for `torch>=2.5`.\")\n-\n-            # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n-            device_type = torch._C._get_accelerator().type\n-\n-            if not torch.distributed.is_initialized():\n-                try:\n-                    rank = int(os.environ[\"RANK\"])\n-                    world_size = int(os.environ[\"WORLD_SIZE\"])\n-                    if device_type == \"cuda\":\n-                        torch.distributed.init_process_group(\n-                            \"nccl\", rank=rank, world_size=world_size, init_method=\"env://\"\n-                        )\n-                        torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n-                    elif device_type == \"cpu\":\n-                        cpu_backend = \"ccl\" if int(os.environ.get(\"CCL_WORKER_COUNT\", 0)) else \"gloo\"\n-                        torch.distributed.init_process_group(cpu_backend, rank=rank, world_size=world_size)\n-                    elif device_type == \"xpu\":\n-                        torch.distributed.init_process_group(\"ccl\", rank=rank, world_size=world_size)\n-                        torch.xpu.set_device(int(os.environ[\"LOCAL_RANK\"]))\n-                    elif device_type == \"hpu\":\n-                        torch.distributed.init_process_group(\"hccl\", rank=rank, world_size=world_size)\n-                        torch.hpu.set_device(int(os.environ[\"LOCAL_RANK\"]))\n-\n-                except Exception as e:\n-                    raise EnvironmentError(\n-                        \"We tried to initialize torch.distributed for you, but it failed, make\"\n-                        \"sure you init torch distributed in your script to use `tp_plan='auto'`\"\n-                    ) from e\n-\n-            # Get device with index assuming equal number of devices per host\n-            if device_type == \"xpu\":\n-                index = torch.xpu.current_device()\n-            elif device_type == \"hpu\":\n-                index = torch.hpu.current_device()\n-            else:\n-                index = None if device_type == \"cpu\" else torch.cuda.current_device()\n-            tp_device = torch.device(device_type, index)\n-\n-            if index is not None and index > 0:\n-                import sys\n-\n-                sys.stdout = open(os.devnull, \"w\")\n-                sys.stderr = open(os.devnull, \"w\")\n-            # This is the easiest way to dispatch to the current process device\n-            device_map = tp_device\n-\n-            # Assuming sharding the model onto the world when tp_size not provided\n-            tp_size = tp_size if tp_size is not None else torch.distributed.get_world_size()\n-            device_mesh = torch.distributed.init_device_mesh(tp_device.type, (tp_size,))\n+        if device_mesh is None:\n+            tp_plan, device_map, device_mesh = initialize_tensor_parallelism(tp_plan, tp_size=None)\n+        else:\n+            # TODO: make device_mesh support multiple dimensions\n+            if device_mesh.ndim == 1:\n+                raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n+            device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n \n         if use_auth_token is not None:\n             warnings.warn(\n@@ -5142,7 +5101,7 @@ def _load_pretrained_model(\n                         name,\n                         casting_dtype,\n                         to_contiguous,\n-                        os.environ[\"RANK\"],\n+                        device_mesh.get_local_rank(),\n                         device_mesh,\n                     )\n "
        }
    ],
    "stats": {
        "total": 1651,
        "additions": 1515,
        "deletions": 136
    }
}