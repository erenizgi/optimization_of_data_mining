{
    "author": "SunMarc",
    "message": "update torch req for 4-bit optimizer (#33144)\n\nupdate req",
    "sha": "7ee4363d19f6503318601478933d59a9333c99d6",
    "files": [
        {
            "sha": "f62184c1904c54483b86fd4cf64ba43964c07c60",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ee4363d19f6503318601478933d59a9333c99d6/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ee4363d19f6503318601478933d59a9333c99d6/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=7ee4363d19f6503318601478933d59a9333c99d6",
            "patch": "@@ -1479,10 +1479,10 @@ def optimizer_hook(param):\n                     \"You need to have `torchao>=0.4.0` in order to use torch 4-bit optimizers.\"\n                     \"Install it with `pip install torchao` or follow the instructions here: https://github.com/pytorch/ao\"\n                 )\n-            if version.parse(importlib.metadata.version(\"torch\")) < version.parse(\"2.3\"):\n+            if version.parse(importlib.metadata.version(\"torch\")) <= version.parse(\"2.4\"):\n                 raise ImportError(\n-                    \"You need to have `torch>=2.3` in order to use torch 4-bit optimizers. \"\n-                    \"Install it with `pip install --upgrade torch`\"\n+                    \"You need to have `torch>2.4` in order to use torch 4-bit optimizers. \"\n+                    \"Install it with `pip install --upgrade torch` it is available on pipy. Otherwise, you need to install torch nightly.\"\n                 )\n             from torchao.prototype.low_bit_optim import AdamW4bit\n "
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}