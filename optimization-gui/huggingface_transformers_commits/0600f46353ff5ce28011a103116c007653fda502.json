{
    "author": "maximizemaxwell",
    "message": "ğŸŒ [i18n-KO] Translated encoder-decoder.md to Korean (#34880)\n\n* Initial version of translation, english still remaining\r\n\r\n* Revised Translation, removed english. _toctree not updated\r\n\r\n* updated _toctree.yml && 3rd ver translation\r\n\r\n* updated _toctree.yml && 3rd ver translation\r\n\r\n* Update encoder-decoder.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update encoder-decoder.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update encoder-decoder.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update encoder-decoder.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update encoder-decoder.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update encoder-decoder.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>",
    "sha": "0600f46353ff5ce28011a103116c007653fda502",
    "files": [
        {
            "sha": "7e9567769cca1a372d6e47e0f5d33d2eaf521a7f",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0600f46353ff5ce28011a103116c007653fda502/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0600f46353ff5ce28011a103116c007653fda502/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=0600f46353ff5ce28011a103116c007653fda502",
            "patch": "@@ -380,8 +380,8 @@\n         title: (ë²ˆì—­ì¤‘) DPR\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) ELECTRA\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) Encoder Decoder Models\n+      - local: model_doc/encoder-decoder\n+        title: ì¸ì½”ë” ë””ì½”ë” ëª¨ë¸\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) ERNIE\n       - local: in_translation"
        },
        {
            "sha": "c5c553561395361bf2ea588f50a5bb3756d213a8",
            "filename": "docs/source/ko/model_doc/encoder-decoder.md",
            "status": "added",
            "additions": 167,
            "deletions": 0,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/0600f46353ff5ce28011a103116c007653fda502/docs%2Fsource%2Fko%2Fmodel_doc%2Fencoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0600f46353ff5ce28011a103116c007653fda502/docs%2Fsource%2Fko%2Fmodel_doc%2Fencoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fencoder-decoder.md?ref=0600f46353ff5ce28011a103116c007653fda502",
            "patch": "@@ -0,0 +1,167 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸[[Encoder Decoder Models]]\n+\n+## ê°œìš”[[Overview]]\n+\n+[`EncoderDecoderModel`]ì€ ì‚¬ì „ í•™ìŠµëœ ìë™ ì¸ì½”ë”©(autoencoding) ëª¨ë¸ì„ ì¸ì½”ë”ë¡œ, ì‚¬ì „ í•™ìŠµëœ ìê°€ íšŒê·€(autoregressive) ëª¨ë¸ì„ ë””ì½”ë”ë¡œ í™œìš©í•˜ì—¬ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤(sequence-to-sequence) ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë° ì´ìš©ë©ë‹ˆë‹¤.\n+\n+ì‚¬ì „ í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™œìš©í•´ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì‹œí€€ìŠ¤ ìƒì„±(sequence generation) ì‘ì—…ì— íš¨ê³¼ì ì´ë¼ëŠ” ì ì´ Sascha Rothe, Shashi Narayan, Aliaksei Severynì˜ ë…¼ë¬¸ [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461)ì—ì„œ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+[`EncoderDecoderModel`]ì´ í•™ìŠµ/ë¯¸ì„¸ ì¡°ì •ëœ í›„ì—ëŠ” ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìì„¸í•œ ì‚¬ìš©ë²•ì€ ì˜ˆì œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+ì´ ì•„í‚¤í…ì²˜ì˜ í•œ ê°€ì§€ ì‘ìš© ì‚¬ë¡€ëŠ” ë‘ ê°œì˜ ì‚¬ì „ í•™ìŠµëœ [`BertModel`]ì„ ê°ê° ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ í™œìš©í•˜ì—¬ ìš”ì•½ ëª¨ë¸(summarization model)ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” Yang Liuì™€ Mirella Lapataì˜ ë…¼ë¬¸ [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345)ì—ì„œ ì œì‹œëœ ë°” ìˆìŠµë‹ˆë‹¤.\n+\n+## ëª¨ë¸ ì„¤ì •ì—ì„œ `EncoderDecoderModel`ì„ ë¬´ì‘ìœ„ ì´ˆê¸°í™”í•˜ê¸°[[Randomly initializing `EncoderDecoderModel` from model configurations.]]\n+\n+[`EncoderDecoderModel`]ì€ ì¸ì½”ë”ì™€ ë””ì½”ë” ì„¤ì •(config)ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬´ì‘ìœ„ ì´ˆê¸°í™”ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œëŠ” [`BertModel`] ì„¤ì •ì„ ì¸ì½”ë”ë¡œ, ê¸°ë³¸ [`BertForCausalLM`] ì„¤ì •ì„ ë””ì½”ë”ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+```python\n+>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n+\n+>>> config_encoder = BertConfig()\n+>>> config_decoder = BertConfig()\n+\n+>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n+>>> model = EncoderDecoderModel(config=config)\n+```\n+\n+## ì‚¬ì „ í•™ìŠµëœ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ `EncoderDecoderModel` ì´ˆê¸°í™”í•˜ê¸°[[Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.]]\n+\n+[`EncoderDecoderModel`]ì€ ì‚¬ì „ í•™ìŠµëœ ì¸ì½”ë” ì²´í¬í¬ì¸íŠ¸ì™€ ì‚¬ì „ í•™ìŠµëœ ë””ì½”ë” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•´ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. BERTì™€ ê°™ì€ ëª¨ë“  ì‚¬ì „ í•™ìŠµëœ ìë™ ì¸ì½”ë”©(auto-encoding) ëª¨ë¸ì€ ì¸ì½”ë”ë¡œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, GPT2ì™€ ê°™ì€ ìê°€ íšŒê·€(autoregressive) ëª¨ë¸ì´ë‚˜ BARTì˜ ë””ì½”ë”ì™€ ê°™ì´ ì‚¬ì „ í•™ìŠµëœ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ë””ì½”ë” ëª¨ë¸ì„ ë””ì½”ë”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë””ì½”ë”ë¡œ ì„ íƒí•œ ì•„í‚¤í…ì²˜ì— ë”°ë¼ êµì°¨ ì–´í…ì…˜(cross-attention) ë ˆì´ì–´ê°€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ ì¸ì½”ë”ì™€ ë””ì½”ë” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì´ìš©í•´ [`EncoderDecoderModel`]ì„ ì´ˆê¸°í™”í•˜ë ¤ë©´, ëª¨ë¸ì„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •(fine-tuning)í•´ì•¼ í•©ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [the *Warm-starting-encoder-decoder blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder)ì— ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì„ ìœ„í•´ `EncoderDecoderModel` í´ë˜ìŠ¤ëŠ” [`EncoderDecoderModel.from_encoder_decoder_pretrained`] ë©”ì„œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n+\n+\n+```python\n+>>> from transformers import EncoderDecoderModel, BertTokenizer\n+\n+>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\")\n+```\n+\n+## ê¸°ì¡´ `EncoderDecoderModel` ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì¶”ë¡ í•˜ê¸°[[Loading an existing `EncoderDecoderModel` checkpoint and perform inference.]]\n+\n+`EncoderDecoderModel` í´ë˜ìŠ¤ì˜ ë¯¸ì„¸ ì¡°ì •(fine-tuned)ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ë ¤ë©´, Transformersì˜ ë‹¤ë¥¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ë§ˆì°¬ê°€ì§€ë¡œ [`EncoderDecoderModel`]ì—ì„œ ì œê³µí•˜ëŠ” `from_pretrained(...)`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë ¤ë©´ [`generate`] ë©”ì„œë“œë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìë™ íšŒê·€(autoregressive) ë°©ì‹ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” íƒìš• ë””ì½”ë”©(greedy decoding), ë¹” ì„œì¹˜(beam search), ë‹¤í•­ ìƒ˜í”Œë§(multinomial sampling) ë“± ë‹¤ì–‘í•œ ë””ì½”ë”© ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.\n+\n+```python\n+>>> from transformers import AutoTokenizer, EncoderDecoderModel\n+\n+>>> # ë¯¸ì„¸ ì¡°ì •ëœ seq2seq ëª¨ë¸ê³¼ ëŒ€ì‘í•˜ëŠ” í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°\n+>>> model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n+\n+>>> # let's perform inference on a long piece of text\n+>>> ARTICLE_TO_SUMMARIZE = (\n+...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n+...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n+...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n+... )\n+>>> input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids\n+\n+>>> # ìê¸°íšŒê·€ì ìœ¼ë¡œ ìš”ì•½ ìƒì„± (ê¸°ë³¸ì ìœ¼ë¡œ ê·¸ë¦¬ë”” ë””ì½”ë”© ì‚¬ìš©)\n+>>> generated_ids = model.generate(input_ids)\n+>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+>>> print(generated_text)\n+nearly 800 thousand customers were affected by the shutoffs. the aim is to reduce the risk of wildfires. nearly 800, 000 customers were expected to be affected by high winds amid dry conditions. pg & e said it scheduled the blackouts to last through at least midday tomorrow.\n+```\n+\n+## `TFEncoderDecoderModel`ì— Pytorch ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°[[Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.]]\n+\n+[`TFEncoderDecoderModel.from_pretrained`] ë©”ì„œë“œëŠ” í˜„ì¬ Pytorch ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë©”ì„œë“œì— `from_pt=True`ë¥¼ ì „ë‹¬í•˜ë©´ ì˜ˆì™¸(exception)ê°€ ë°œìƒí•©ë‹ˆë‹¤. íŠ¹ì • ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì— ëŒ€í•œ Pytorch ì²´í¬í¬ì¸íŠ¸ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš°, ë‹¤ìŒê³¼ ê°™ì€ í•´ê²° ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```python\n+>>> # íŒŒì´í† ì¹˜ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œí•˜ëŠ” í•´ê²° ë°©ë²•\n+>>> from transformers import EncoderDecoderModel, TFEncoderDecoderModel\n+\n+>>> _model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n+\n+>>> _model.encoder.save_pretrained(\"./encoder\")\n+>>> _model.decoder.save_pretrained(\"./decoder\")\n+\n+>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n+...     \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n+... )\n+>>> # ì´ ë¶€ë¶„ì€ íŠ¹ì • ëª¨ë¸ì˜ êµ¬ì²´ì ì¸ ì„¸ë¶€ì‚¬í•­ì„ ë³µì‚¬í•  ë•Œì—ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+>>> model.config = _model.config\n+```\n+\n+## í•™ìŠµ[[Training]]\n+\n+ëª¨ë¸ì´ ìƒì„±ëœ í›„ì—ëŠ” BART, T5 ë˜ëŠ” ê¸°íƒ€ ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •(fine-tuning)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+ë³´ì‹œë‹¤ì‹œí”¼, ì†ì‹¤(loss)ì„ ê³„ì‚°í•˜ë ¤ë©´ ë‹¨ 2ê°œì˜ ì…ë ¥ë§Œ í•„ìš”í•©ë‹ˆë‹¤: `input_ids`(ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•œ `input_ids`)ì™€ `labels`(ëª©í‘œ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•œ `input_ids`).\n+\n+```python\n+>>> from transformers import BertTokenizer, EncoderDecoderModel\n+\n+>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\")\n+\n+>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n+>>> model.config.pad_token_id = tokenizer.pad_token_id\n+\n+>>> input_ids = tokenizer(\n+...     \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was  finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n+...     return_tensors=\"pt\",\n+... ).input_ids\n+\n+>>> labels = tokenizer(\n+...     \"the eiffel tower surpassed the washington monument to become the tallest structure in the world. it was the first structure to reach a height of 300 metres in paris in 1930. it is now taller than the chrysler building by 5. 2 metres ( 17 ft ) and is the second tallest free - standing structure in paris.\",\n+...     return_tensors=\"pt\",\n+... ).input_ids\n+\n+>>> # forward í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì í•©í•œ decoder_input_idsë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n+>>> loss = model(input_ids=input_ids, labels=labels).loss\n+```\n+í›ˆë ¨ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [colab](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=ZwQIEhKOrJpl) ë…¸íŠ¸ë¶ì„ ì°¸ì¡°í•˜ì„¸ìš”. \n+\n+ì´ ëª¨ë¸ì€ [thomwolf](https://github.com/thomwolf)ê°€ ê¸°ì—¬í–ˆìœ¼ë©°, ì´ ëª¨ë¸ì— ëŒ€í•œ TensorFlow ë° Flax ë²„ì „ì€ [ydshieh](https://github.com/ydshieh)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\n+\n+\n+## EncoderDecoderConfig\n+\n+[[autodoc]] EncoderDecoderConfig\n+\n+<frameworkcontent>\n+<pt>\n+\n+## EncoderDecoderModel\n+\n+[[autodoc]] EncoderDecoderModel\n+    - forward\n+    - from_encoder_decoder_pretrained\n+\n+</pt>\n+<tf>\n+\n+## TFEncoderDecoderModel\n+\n+[[autodoc]] TFEncoderDecoderModel\n+    - call\n+    - from_encoder_decoder_pretrained\n+\n+</tf>\n+<jax>\n+\n+## FlaxEncoderDecoderModel\n+\n+[[autodoc]] FlaxEncoderDecoderModel\n+    - __call__\n+    - from_encoder_decoder_pretrained\n+\n+</jax>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 171,
        "additions": 169,
        "deletions": 2
    }
}