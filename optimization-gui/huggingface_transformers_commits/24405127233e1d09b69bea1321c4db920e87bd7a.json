{
    "author": "dvrogozh",
    "message": "multi-gpu: fix tensor device placements for various models (#35763)\n\n* milti-gpu: fix inputs_embeds + position_embeds\n\nFixing the following errors in few models:\n```\n>       hidden_states = inputs_embeds + pos_embeds\nE       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, xpu:2 and xpu:3!\n```\n\nFixes: #35762\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\n\n* multi-gpu: fix tensor device placements for various models\n\nFixes: #35762\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\n\n* Apply make fix-copies\n\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\n\n---------\n\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>",
    "sha": "24405127233e1d09b69bea1321c4db920e87bd7a",
    "files": [
        {
            "sha": "90755045b635d7e47960705e9338fa2fa203ef6b",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1115,7 +1115,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "df65f0aeb949898cae82231f6399ff3c34b832cd",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -845,7 +845,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "0a9421409e254e7eb33b43d4633919f41acd0519",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1491,7 +1491,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "dcb24817e303e08158bbfa95b8997056b06dcb7f",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -688,7 +688,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "69e7c579f9ce78a592a5e78cf86dce476401f03e",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -765,7 +765,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "11353a0a990c9b1e95a4990ad5f8e0dca0ca8198",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -766,7 +766,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "fceefbe2c7529b33c16e20d1bd1f0ebc6e7c4228",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1220,7 +1220,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "16aeefcb1c881270d07682306fa368fb9ad21d84",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1004,7 +1004,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "7d31b8d3d323bf850923f9cfbf7e8d990ec7384c",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1583,7 +1583,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "e36ea9cef222215354014fa7dd8db4e04cf8ac8a",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1147,7 +1147,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "d7a40ed5c5ffb28db22a56eb61775dcfda6ee524",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -309,6 +309,7 @@ def slow_forward(\n                 )  # [batch, intermediate_size, seq_len]\n             else:\n                 conv_state = cache_params.update_conv_state(self.layer_idx, hidden_states, cache_position)\n+                conv_state = conv_state.to(self.conv1d.weight.device)\n                 hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n                 if self.use_conv_bias:\n                     hidden_states += self.conv1d.bias"
        },
        {
            "sha": "aeb742e16dd0eedf8663cf91b7e6c64b76be5aba",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -737,7 +737,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "4b3bbb8126a75709ac8bdc7ee66be0840dfd8d81",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -778,7 +778,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "f1fddcda107a2d81e443b45a824101293f0193ef",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -746,7 +746,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "931bba9ba965e4ec1eeb8f4473bbffd1e5aaa1e1",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -818,7 +818,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n         position_embeds = self.wpe(position_ids)\n-        hidden_states = inputs_embeds + position_embeds\n+        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n \n         # Attention mask.\n         _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None"
        },
        {
            "sha": "4729ee098da3e3cb6977c744f6f85ed85207cfc5",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -959,7 +959,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n         position_embeds = self.wpe(position_ids)\n-        hidden_states = inputs_embeds + position_embeds\n+        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n \n         if token_type_ids is not None:\n             token_type_embeds = self.wte(token_type_ids)"
        },
        {
            "sha": "8598d51e687130b817f61a4b1cb26a720b0032b8",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -897,7 +897,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "f420a8ceb206bc53277ebd6dac9526acad3202c3",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -740,7 +740,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "d5153fb3f828106acc3a1b87cb3ceac2fa70b20c",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -767,7 +767,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "8c9de2dbced15dc1ae8c535fed4506859f53abb6",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -996,7 +996,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "d0579bb8a7a41f021aad95a0195884bd04dec695",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -749,7 +749,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "d877b8323b3b749ef64c841b3b775487808586f7",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1235,7 +1235,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "fc6f862be2584621bac37dde0ffcaf578c9859da",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -733,7 +733,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "8c6f1f059bfc113dc48b40923600bccff317ee9e",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1468,7 +1468,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "eb676c295a4f307c04857269a2d1099a78a4fc91",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1305,7 +1305,7 @@ def inputs_merger(\n         special_image_token_mask = input_ids == self.image_token_id\n         new_inputs_embeds = inputs_embeds.clone()\n         reshaped_image_hidden_states = image_hidden_states.view(-1, vision_hidden_size)\n-        new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states\n+        new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states.to(new_inputs_embeds.device)\n         return new_inputs_embeds\n \n     @add_start_docstrings_to_model_forward("
        },
        {
            "sha": "be8b0d6567ec95f39f53298827ac0cc977ca7dfe",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -773,7 +773,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n         position_embeds = self.wpe(position_ids)\n-        hidden_states = inputs_embeds + position_embeds\n+        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n \n         if token_type_ids is not None:\n             token_type_embeds = self.wte(token_type_ids)"
        },
        {
            "sha": "b705da44eba4c733c0f004bfd0b9735c0abf6cc8",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1612,7 +1612,7 @@ def generate(\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"image_token_index\", None) is not None:\n             special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \""
        },
        {
            "sha": "dcf77863a149e099555f818f646d9c7bb0f3752b",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1497,7 +1497,7 @@ def forward(\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_index\", None) is not None:\n             special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n@@ -1647,7 +1647,7 @@ def generate(\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_index\", None) is not None:\n             special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \""
        },
        {
            "sha": "4fd3a00708d1e1f868b668f8a1b4d510d5337a4b",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -304,7 +304,7 @@ def forward(\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_index\", None) is not None:\n             special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n@@ -454,7 +454,7 @@ def generate(\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_index\", None) is not None:\n             special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \""
        },
        {
            "sha": "283174ba3cfdb783a40e02aa5e415561f581a7e1",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1229,7 +1229,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "0d65e1417f52494dfdb445de34f08d9273d4f071",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -735,7 +735,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "84ea0443d2f1859cb0279a8e855b7bec0e8b853b",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1705,7 +1705,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "960da998909f6c1dd996b93c29950d8309abef90",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -261,6 +261,7 @@ def slow_forward(self, input_states, cache_params: Optional[MambaCache]=None, ca\n                 hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])     # [batch, intermediate_size, seq_len]\n             else:\n                 conv_state = cache_params.update_conv_state(self.layer_idx, hidden_states, cache_position)\n+                conv_state = conv_state.to(self.conv1d.weight.device)\n                 hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n                 if self.use_conv_bias:\n                     hidden_states += self.conv1d.bias"
        },
        {
            "sha": "8412ecef1cf9b13c60e5c8285202279c38ccf6b9",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -743,7 +743,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     config_class = MBartConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"MBartDecoderLayer\", \"MBartAttention\"]\n+    _no_split_modules = [\"MBartDecoderLayer\", \"MBartEncoderLayer\", \"MBartAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "af36b233357799761d5c87cab890a6e6d02a88d6",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1204,7 +1204,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "b300c7c646f2e8c6ddacec687b26cc247fcb842b",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -737,7 +737,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "d1531c58a8a6dbb953c40363b59ec8b4490e7265",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -255,7 +255,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "170d54eca1b2789533d618640b806e0b124855ca",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -871,7 +871,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "bf097e86f3b2d08a10c913567a8888d13ee2ca3e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1183,7 +1183,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "e8b8194516e3b8524f07b2054ca86bc68d781518",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1099,7 +1099,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "ae9a3fd804dcbb9779d19d2bf3d4107bc097fbf1",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1434,7 +1434,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n@@ -1746,7 +1748,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "129255a90b5b5ff22969790a86b0d37c4179b8ae",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1297,7 +1297,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "829a3283d0a30d59a6dd4bf601321e165d88e94e",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -984,7 +984,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "6b7abaa96af24f231c347abd54218b6faf1874e4",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -711,7 +711,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "89ef5e1050bb45908db7631f909a1390282fa2eb",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -712,7 +712,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "1969acf2f5b1a6dbb1adb415414559acf95abcce",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -805,7 +805,7 @@ def forward(\n         if self.project_in is not None:\n             inputs_embeds = self.project_in(inputs_embeds)\n \n-        hidden_states = inputs_embeds + pos_embeds\n+        hidden_states = inputs_embeds + pos_embeds.to(inputs_embeds.device)\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "9c589036815be96ccc23a9162311d792611094af",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -784,7 +784,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "33d86999fdf8fc04b529fa11cd0302b95eed9f68",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -709,7 +709,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "fc4787b7883b8ca1a71815b34de95ca9435a0b4b",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -807,7 +807,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "1cea9a2ea28bdc3d0c6a8a27e091f7f4be134ab9",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1318,7 +1318,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "71cf2f2555116b55983d0dfc7e62ab6a000efd14",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1692,7 +1692,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "351482a75e581b81dee062de3ec304e4359264cd",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1105,7 +1105,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "bec0cb46ef2993a6ad94b1cdb6afcb4f483778c7",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -720,7 +720,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "f7c3c7f9c097a141d830a4058663deed632a502f",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1331,7 +1331,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "3e4aa05a22bfe0abd367baeb3e1dd709663811a9",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1206,7 +1206,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "3ecb495b2a1d3fb3ed0cb6a15338ce9e9181be4a",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1273,7 +1273,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n@@ -1454,7 +1456,7 @@ def get_rope_index(\n             )\n             image_index, video_index = 0, 0\n             for i, input_ids in enumerate(total_input_ids):\n-                input_ids = input_ids[attention_mask[i] == 1]\n+                input_ids = input_ids[attention_mask[i].to(input_ids.device) == 1]\n                 image_nums, video_nums = 0, 0\n                 vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n                 vision_tokens = input_ids[vision_start_indices + 1]\n@@ -1666,6 +1668,7 @@ def forward(\n                 position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n                 if cache_position is not None:  # otherwise `deltas` is an int `0`\n                     delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                    delta = delta.to(position_ids.device)\n                 position_ids = position_ids.add(delta)\n                 position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n "
        },
        {
            "sha": "c401b772db746f142961ae7b77aa2c433091f273",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1039,7 +1039,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "d64953d72b69b12029dd6c2d18bd6b29a7bdedd3",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -733,7 +733,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 if attention_mask.shape[-1] > target_length:\n                     attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "a09392c8567187edd9c449cfc10aa0b527c087fd",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1241,7 +1241,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "a91c81ba79b71af5ff9906e9744566e70b0a641b",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1310,7 +1310,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "80c6d37ba9ffc48ccb5eb56f38fc948e1a11c753",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1643,7 +1643,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "25d7a74eabfa52286f7892819d189294e0808df8",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -954,7 +954,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "f6ffab0629937672fa39f715c4f807904f2a7d04",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -1480,7 +1480,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                 mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype"
        },
        {
            "sha": "190f50ca401d4208f961c60a9e4a7761ef0d02a1",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24405127233e1d09b69bea1321c4db920e87bd7a/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=24405127233e1d09b69bea1321c4db920e87bd7a",
            "patch": "@@ -593,7 +593,9 @@ def forward(\n                 encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n             )\n \n-        hidden_states = inputs_embeds + self.embed_positions(position_ids, past_key_values_length)\n+        hidden_states = inputs_embeds + self.embed_positions(position_ids, past_key_values_length).to(\n+            inputs_embeds.device\n+        )\n         hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)\n \n         if self.gradient_checkpointing and self.training:"
        }
    ],
    "stats": {
        "total": 243,
        "additions": 177,
        "deletions": 66
    }
}