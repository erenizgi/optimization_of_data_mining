{
    "author": "xadupre",
    "message": "Make _compute_dynamic_ntk_parameters exportable (#39171)\n\n* Make _compute_dynamic_ntk_parameters exportable\n\n* add unit test",
    "sha": "f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32",
    "files": [
        {
            "sha": "4786cce27356e6b88e039c3e745cea6c180ddaa2",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32",
            "patch": "@@ -215,7 +215,15 @@ def _compute_dynamic_ntk_parameters(\n     attention_factor = 1.0  # Unused in this type of RoPE\n \n     # seq_len: default to max_position_embeddings, e.g. at init time\n-    seq_len = seq_len if seq_len is not None and seq_len > max_position_embeddings else max_position_embeddings\n+    if seq_len is None:\n+        seq_len = max_position_embeddings\n+    elif isinstance(seq_len, torch.Tensor):\n+        seq_len = torch.maximum(\n+            seq_len,\n+            torch.tensor(max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n+        )\n+    else:\n+        seq_len = max(seq_len, max_position_embeddings)\n \n     # Compute the inverse frequencies\n     base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))"
        },
        {
            "sha": "761a785f369ff4478c2602b7f952812f2088e52d",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32",
            "patch": "@@ -220,6 +220,9 @@ def test_dynamic_rope_numerically(self):\n             inv_freq, _ = rope_fn(config=config, device=torch_device, seq_len=1)\n             torch.testing.assert_close(inv_freq, default_inv_freq)\n \n+            inv_freq, _ = rope_fn(config=config, device=torch_device, seq_len=torch.tensor(1, dtype=torch.int64))\n+            torch.testing.assert_close(inv_freq, default_inv_freq)\n+\n         # Check 2: if we provide `seq_len` larger than the model's original training sequence length, the frequencies\n         # will scale up (i.e., the inverse frequencies will scale down).\n         factor = 10.0"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 12,
        "deletions": 1
    }
}