{
    "author": "molbap",
    "message": "Model debugger upgrades (#37391)\n\n* debugging improvements\n\n* add debugging details\n\n* add more debugging details\n\n* debug more\n\n* clean up layers + output\n\n* add summary json file\n\n* cleanup\n\n* copies :eyes:\n\n* remove hooks + add documentation\n\n* draft a small test, why not\n\n* respect the format (respect it)\n\n* fixup imports\n\n* nit\n\n* add tests and configurable pruning of layers",
    "sha": "4afd3f4820b0504963e460ecd725a3349eea8be3",
    "files": [
        {
            "sha": "6d30668c634badf713760a95947635f7ae8a1366",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 145,
            "deletions": 3,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/4afd3f4820b0504963e460ecd725a3349eea8be3/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4afd3f4820b0504963e460ecd725a3349eea8be3/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=4afd3f4820b0504963e460ecd725a3349eea8be3",
            "patch": "@@ -28,7 +28,7 @@ Most of those are only useful if you are adding new models in the library.\n \n This context manager is a power user tool intended for model adders.\n It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\n-To note, this context manager enforces `torch.inference_mode()`.\n+To note, this context manager enforces `torch.no_grad()`.\n \n ### Rationale\n \n@@ -43,6 +43,7 @@ import torch\n from PIL import Image\n import requests\n from transformers import LlavaProcessor, LlavaForConditionalGeneration\n+from transformers.model_debugging_utils import model_addition_debugger_context\n torch.random.manual_seed(673)\n \n # load pretrained model and processor\n@@ -60,12 +61,153 @@ prompt = \"<image>Describe this image.\"\n inputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n \n # call forward method (not .generate!)\n-with model_addition_debugger_context(model, \"optional_path_to_your_output_file.json\"):\n+with model_addition_debugger_context(\n+  model,\n+  debug_path=\"optional_path_to_your_directory\",\n+  do_prune_layers=False # This will output ALL the layers of a model.\n+  ):\n     output = model.forward(**inputs)\n \n ```\n \n \n-[[autodoc]] model_addition_debugger\n+### Reading results\n+\n+The debugger generates two files from the forward call, both with the same base name, \n+but ending either with `_SUMMARY.json` or with `_FULL_TENSORS.json`. \n+\n+The first one will contain a summary of each module's _input_ and _output_ tensor values and shapes.\n+\n+```json\n+{\n+  \"module_path\": \"MolmoForConditionalGeneration\",\n+  \"inputs\": {\n+    \"args\": [],\n+    \"kwargs\": {\n+      \"input_ids\": {\n+        \"shape\": \"torch.Size([1, 589])\",\n+        \"dtype\": \"torch.int64\"\n+      },\n+      \"attention_mask\": {\n+        \"shape\": \"torch.Size([1, 589])\",\n+        \"dtype\": \"torch.int64\"\n+      },\n+      \"pixel_values\": {\n+        \"shape\": \"torch.Size([1, 5, 576, 588])\",\n+        \"dtype\": \"torch.float32\",\n+        \"mean\": \"tensor(-8.9514e-01, device='cuda:0')\",\n+        \"std\": \"tensor(9.2586e-01, device='cuda:0')\",\n+        \"min\": \"tensor(-1.7923e+00, device='cuda:0')\",\n+        \"max\": \"tensor(1.8899e+00, device='cuda:0')\"\n+    }\n+  },\n+  \"children\": [\n+    {\n+      \"module_path\": \"MolmoForConditionalGeneration.language_model.model.embed_tokens\",\n+      \"inputs\": {\n+        \"args\": [\n+          {\n+            \"shape\": \"torch.Size([1, 589])\",\n+            \"dtype\": \"torch.int64\"\n+          }\n+        ]\n+      },\n+      \"outputs\": {\n+        \"shape\": \"torch.Size([1, 589, 3584])\",\n+        \"dtype\": \"torch.float32\",\n+        \"mean\": \"tensor(6.5460e-06, device='cuda:0')\",\n+        \"std\": \"tensor(2.3807e-02, device='cuda:0')\",\n+        \"min\": \"tensor(-3.3398e-01, device='cuda:0')\",\n+        \"max\": \"tensor(3.9453e-01, device='cuda:0')\"\n+      }\n+    },\n+    {\n+      \"module_path\": \"MolmoForConditionalGeneration.vision_tower\",\n+      \"inputs\": {\n+        \"args\": [\n+          {\n+            \"shape\": \"torch.Size([5, 1, 576, 588])\",\n+            \"dtype\": \"torch.float32\",\n+            \"mean\": \"tensor(-8.9514e-01, device='cuda:0')\",\n+            \"std\": \"tensor(9.2586e-01, device='cuda:0')\",\n+            \"min\": \"tensor(-1.7923e+00, device='cuda:0')\",\n+            \"max\": \"tensor(1.8899e+00, device='cuda:0')\"\n+          }\n+        ],\n+        \"kwargs\": {\n+          \"output_hidden_states\": \"True\"\n+        }\n+      },\n+      \"children\": [\n+        { ... and so on\n+```\n+\n+The `_FULL_TENSORS.json` file will display a full view of all tensors, which is useful\n+for comparing two files. \n+```json\n+      \"pixel_values\": {\n+        \"shape\": \"torch.Size([1, 5, 576, 588])\",\n+        \"dtype\": \"torch.float32\",\n+        \"value\": [\n+          \"tensor([[[[-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          ...,\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00]],\",\n+          \"\",\n+          \"         [[-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          ...,\",\n+          \"          [-1.4857e+00, -1.4820e+00, -1.2100e+00,  ..., -6.0979e-01, -5.9650e-01, -3.8527e-01],\",\n+          \"          [-1.6755e+00, -1.7221e+00, -1.4518e+00,  ..., -7.5577e-01, -7.4658e-01, -5.5592e-01],\",\n+          \"          [-7.9957e-01, -8.2162e-01, -5.7014e-01,  ..., -1.3689e+00, -1.3169e+00, -1.0678e+00]],\",\n+          \"\",\n+          \"         [[-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          ...,\",\n+          \"          [-3.0322e-01, -5.0645e-01, -5.8436e-01,  ..., -6.2439e-01, -7.9160e-01, -8.1188e-01],\",\n+          \"          [-4.4921e-01, -6.5653e-01, -7.2656e-01,  ..., -3.4702e-01, -5.2146e-01, -5.1326e-01],\",\n+          \"          [-3.4702e-01, -5.3647e-01, -5.4170e-01,  ..., -1.0915e+00, -1.1968e+00, -1.0252e+00]],\",\n+          \"\",\n+          \"         [[-1.1207e+00, -1.2718e+00, -1.0678e+00,  ..., 1.2013e-01, -1.3126e-01, -1.7197e-01],\",\n+          \"          [-6.9738e-01, -9.1166e-01, -8.5454e-01,  ..., -5.5050e-02, -2.8134e-01, -4.2793e-01],\",\n+          \"          [-3.4702e-01, -5.5148e-01, -5.8436e-01,  ..., 1.9312e-01, -8.6235e-02, -2.1463e-01],\",\n+          \"          ...,\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00]],\",\n+          \"\",\n+          \"         [[-1.0039e+00, -9.5669e-01, -6.5546e-01,  ..., -1.4711e+00, -1.4219e+00, -1.1389e+00],\",\n+          \"          [-1.0039e+00, -9.5669e-01, -6.5546e-01,  ..., -1.7193e+00, -1.6771e+00, -1.4091e+00],\",\n+          \"          [-1.6317e+00, -1.6020e+00, -1.2669e+00,  ..., -1.2667e+00, -1.2268e+00, -8.9720e-01],\",\n+          \"          ...,\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00],\",\n+          \"          [-1.7923e+00, -1.7521e+00, -1.4802e+00,  ..., -1.7923e+00, -1.7521e+00, -1.4802e+00]]]], device='cuda:0')\"\n+        ],\n+        \"mean\": \"tensor(-8.9514e-01, device='cuda:0')\",\n+        \"std\": \"tensor(9.2586e-01, device='cuda:0')\",\n+        \"min\": \"tensor(-1.7923e+00, device='cuda:0')\",\n+        \"max\": \"tensor(1.8899e+00, device='cuda:0')\"\n+      },\n+```\n+\n+### Comparing between implementations\n+\n+Once the forward passes of two models have been traced by the debugger, one can compare the `json` output files. See below: we can see slight differences between these two implementations' key projection layer. Inputs are mostly identical, but not quite. Looking through the file differences makes it easier to pinpoint which layer is wrong. \n+\n+\n+![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/files_difference_debugging.png)\n+\n+\n+### Limitations and scope\n+\n+This feature will only work for torch-based models, and would require more work and case-by-case approach for say `jax`-based models that are usually compiled. Models relying heavily on external kernel calls may work, but trace will probably miss some things. Regardless, any python implementation that aims at mimicking another implementation can be traced once instead of reran N times with breakpoints.\n+\n+If you pass `do_prune_layers=False` to your model debugger, ALL the layers will be outputted to `json`. Else, only the first and last layer will be shown. This is useful when some layers (typically cross-attention) appear only after N layers. \n \n [[autodoc]] model_addition_debugger_context"
        },
        {
            "sha": "b7ba86b64ffcc3571a07c1f332fda0b6a20b6df1",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4afd3f4820b0504963e460ecd725a3349eea8be3/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4afd3f4820b0504963e460ecd725a3349eea8be3/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=4afd3f4820b0504963e460ecd725a3349eea8be3",
            "patch": "@@ -344,7 +344,6 @@\n     _import_structure[\"utils.dummy_pt_objects\"] = [name for name in dir(dummy_pt_objects) if not name.startswith(\"_\")]\n else:\n     _import_structure[\"model_debugging_utils\"] = [\n-        \"model_addition_debugger\",\n         \"model_addition_debugger_context\",\n     ]\n     _import_structure[\"activations\"] = []\n@@ -910,7 +909,6 @@\n             convert_and_export_with_cache,\n         )\n         from .model_debugging_utils import (\n-            model_addition_debugger,\n             model_addition_debugger_context,\n         )\n         from .modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update"
        },
        {
            "sha": "009ac0c6b2daf5a2e3aae000cbca57f4af8bc372",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 158,
            "deletions": 103,
            "changes": 261,
            "blob_url": "https://github.com/huggingface/transformers/blob/4afd3f4820b0504963e460ecd725a3349eea8be3/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4afd3f4820b0504963e460ecd725a3349eea8be3/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=4afd3f4820b0504963e460ecd725a3349eea8be3",
            "patch": "@@ -17,7 +17,8 @@\n import json\n import os\n import re\n-from contextlib import contextmanager\n+from contextlib import contextmanager, redirect_stdout\n+from io import StringIO\n from typing import Optional\n \n from transformers.utils.import_utils import requires\n@@ -28,9 +29,7 @@\n if is_torch_available():\n     import torch\n     import torch.distributed.tensor\n-    from torch import nn\n \n-    from .modeling_utils import PreTrainedModel\n \n from .utils import logging\n \n@@ -87,21 +86,64 @@ def _serialize_io(value):\n \n     if hasattr(value, \"_local_tensor\"):\n         # DTensor-like handling, just use local tensor attribute\n-        return {\n+        torch.set_printoptions(sci_mode=True)\n+        val_repr = _repr_to_list(value)\n+        out = {\n             \"shape\": repr(value._local_tensor.shape),\n             \"dtype\": repr(value._local_tensor.dtype),\n-            \"value\": _sanitize_repr_for_diff(repr(value)),\n+            \"value\": val_repr,\n         }\n+        if value._local_tensor.dtype in {torch.float16, torch.float32, torch.bfloat16}:\n+            value = value._local_tensor.clone()\n+            out.update(\n+                {\n+                    \"mean\": _sanitize_repr_for_diff(repr(value.mean())),\n+                    \"std\": _sanitize_repr_for_diff(repr(value.std())),\n+                    \"min\": _sanitize_repr_for_diff(repr(value.min())),\n+                    \"max\": _sanitize_repr_for_diff(repr(value.max())),\n+                }\n+            )\n+        return out\n \n     if isinstance(value, torch.Tensor):\n-        # standard PyTorch Tensor\n-        # return also the shape of such\n-        return {\"shape\": repr(value.shape), \"dtype\": repr(value.dtype), \"value\": _sanitize_repr_for_diff(repr(value))}\n+        torch.set_printoptions(sci_mode=True)\n+        val_repr = _repr_to_list(value)\n+        out = {\n+            \"shape\": repr(value.shape),\n+            \"dtype\": repr(value.dtype),\n+            \"value\": val_repr,\n+        }\n+        if value.dtype in {torch.float16, torch.float32, torch.bfloat16}:\n+            out.update(\n+                {\n+                    \"mean\": _sanitize_repr_for_diff(repr(value.mean())),\n+                    \"std\": _sanitize_repr_for_diff(repr(value.std())),\n+                    \"min\": _sanitize_repr_for_diff(repr(value.min())),\n+                    \"max\": _sanitize_repr_for_diff(repr(value.max())),\n+                }\n+            )\n+        return out\n \n-    # fallback for everything else (bool, int, float, None, or custom class)\n     return _sanitize_repr_for_diff(repr(value))\n \n \n+def _repr_to_list(value: torch.Tensor):\n+    \"\"\"\n+    Converts a tensor into a sanitized multi-line string representation.\n+\n+    Args:\n+        value (`torch.Tensor`): The tensor to represent.\n+\n+    Returns:\n+        `List[str]`: List of string lines representing the tensor.\n+    \"\"\"\n+    torch.set_printoptions(sci_mode=True, linewidth=120)\n+    with StringIO() as buf, redirect_stdout(buf):\n+        print(value)  # to redirected stdout to avoid line splits\n+        raw = buf.getvalue()\n+    return _sanitize_repr_for_diff(raw).splitlines()\n+\n+\n def prune_outputs_if_children(node):\n     # if there are children, remove this node's \"outputs\"\n     # so we only see outputs at the leaf level\n@@ -111,22 +153,106 @@ def prune_outputs_if_children(node):\n             prune_outputs_if_children(child)\n \n \n+LAYER_SUFFIX_RE = re.compile(r\"(.*)\\.(\\d+)$\")  # should be generic enough, ends with a number\n+\n+\n+def is_layer_block(node):\n+    \"\"\"\n+    Checks whether a node represents a layer block with submodules.\n+\n+    Args:\n+        node (`dict`): A node from the call tree.\n+\n+    Returns:\n+        `bool`: Whether the node is a layer block.\n+    \"\"\"\n+    match = LAYER_SUFFIX_RE.match(node.get(\"module_path\", \"\"))\n+    if not match or not node.get(\"children\"):\n+        return False\n+    number = match.group(2)\n+    return any(f\".{number}.\" in child.get(\"module_path\", \"\") for child in node[\"children\"])\n+\n+\n+def prune_intermediate_layers(node):\n+    \"\"\"\n+    Recursively removes intermediate layers from the tree to improve readability.\n+    Keeps at least the first and last layers if many consecutive layers are present.\n+\n+    Args:\n+        node (`dict`): The root or subnode to prune recursively.\n+    \"\"\"\n+    if not node.get(\"children\"):\n+        return\n+    layer_blocks = [(i, child) for i, child in enumerate(node[\"children\"]) if is_layer_block(child)]\n+\n+    if len(layer_blocks) > 2:\n+        to_remove = [i for i, _ in layer_blocks[1:-1]]\n+        node[\"children\"] = [child for i, child in enumerate(node[\"children\"]) if i not in to_remove]\n+\n+    for child in node[\"children\"]:\n+        prune_intermediate_layers(child)\n+\n+\n def log_model_debug_trace(debug_path, model):\n     if debug_path:\n         try:\n-            os.makedirs(debug_path, exist_ok=False)\n-            output_path = os.path.join(debug_path, model._debugger_module_dump_name + \"_debug_tree.json\")\n+            os.makedirs(debug_path, exist_ok=True)\n+            base = os.path.join(debug_path, model._debugger_module_dump_name + \"_debug_tree\")\n         except Exception as e:\n             raise ValueError(f\"Unexpected or existing debug_path={debug_path}. {e}\")\n     else:\n-        output_path = model._debugger_module_dump_name + \"_debug_tree.json\"\n-    logger.info(f\"Writing model trace at {output_path}\")\n-    with open(output_path, \"w\") as outfile:\n-        prune_outputs_if_children(model._call_tree)\n-        json.dump(model._call_tree, outfile, indent=2)\n+        base = model._debugger_module_dump_name + \"_debug_tree\"\n+\n+    logger.info(f\"Writing model trace at {base}.json\")\n+    full_path = base + \"_FULL_TENSORS.json\"\n+    summary_path = base + \"_SUMMARY.json\"\n+\n+    prune_outputs_if_children(model._call_tree)\n+\n+    with open(full_path, \"w\") as f:\n+        json.dump(model._call_tree, f, indent=2)\n+\n+    # summary-only version for readability - traversing the tree again #TODO optimize?\n+    def strip_values(node):\n+        def clean(val):\n+            if isinstance(val, dict):\n+                val.pop(\"value\", None)\n+                for v in val.values():\n+                    clean(v)\n+            elif isinstance(val, list):\n+                for item in val:\n+                    clean(item)\n+\n+        clean(node.get(\"inputs\", {}))\n+        clean(node.get(\"outputs\", {}))\n \n+        for child in node.get(\"children\", []):\n+            strip_values(child)\n+\n+    tree_copy = json.loads(json.dumps(model._call_tree))  # deep copy\n+    strip_values(tree_copy)\n+\n+    with open(summary_path, \"w\") as f:\n+        json.dump(tree_copy, f, indent=2)\n+\n+\n+def _attach_debugger_logic(\n+    model,\n+    debug_path: Optional[str] = \".\",\n+    do_prune_layers: Optional[bool] = True,\n+):\n+    \"\"\"\n+    Attaches a debugging wrapper to every module in the model.\n+\n+    This records structured inputs and outputs during the forward pass into a call tree.\n+\n+    Args:\n+        model (`PreTrainedModel`, `nn.Module`): Model to wrap.\n+        debug_path (`str`): Optional directory to dump debug JSON files.\n+        do_prune_layers (`bool`, *optional*, defaults to `True`): Whether to prune intermediate layers.\n+    \"\"\"\n+    class_name = model.__class__.__name__\n \n-def _attach_debugger_logic(model, class_name, debug_path: str):\n     # Prepare data structures on the model object\n     model._call_tree = {\"module_path\": class_name, \"inputs\": None, \"outputs\": None, \"children\": []}\n     model._debugger_model_call_stack = []\n@@ -147,7 +273,7 @@ def wrapped_forward(*inps, **kws):\n                     \"children\": [],\n                 }\n                 model._debugger_model_call_stack.append(node)\n-            with torch.inference_mode():\n+            with torch.no_grad():\n                 out = orig_forward(*inps, **kws)\n \n             if _is_rank_zero():\n@@ -188,7 +314,6 @@ def top_wrapped_forward(*inps, **kws):\n             model._debugger_model_call_stack.append(top_node)\n \n         out = real_top_forward(*inps, **kws)\n-\n         if _is_rank_zero() and model._debugger_model_call_stack:\n             top_node[\"outputs\"] = _serialize_io(out)\n             finished = model._debugger_model_call_stack.pop()\n@@ -198,98 +323,24 @@ def top_wrapped_forward(*inps, **kws):\n             # prune empty stuff for visibility\n             [model._call_tree.pop(k, None) for k in list(model._call_tree.keys()) if not model._call_tree[k]]\n \n+            # prune layers that are not 0 or last\n+            if do_prune_layers:\n+                prune_intermediate_layers(model._call_tree)\n+            # Write final JSON trace here\n+            log_model_debug_trace(debug_path=debug_path, model=model)\n         return out\n \n     model.forward = top_wrapped_forward\n \n-    # Final hook for writing JSON on forward-end\n-    def final_hook(_, inputs, outputs):\n-        if _is_rank_zero() and model._debugger_model_call_stack:\n-            finished = model._debugger_model_call_stack.pop()\n-            model._call_tree[\"inputs\"] = finished[\"inputs\"]\n-            model._call_tree[\"outputs\"] = finished[\"outputs\"]\n-            model._call_tree[\"children\"] = finished[\"children\"]\n-\n-        if _is_rank_zero():\n-            log_model_debug_trace(debug_path=debug_path, model=model)\n-\n-    model.register_forward_hook(final_hook)\n-    # Optionally also for a couple possible hooks that have specific names. It should be just one.\n-    # This means modules that are not typically called \"forward\" within the model. But we should not need to recurse\n-    # through them.\n-    possible_model_calls = [\"language_model\", \"model\"]\n-    for model_call in possible_model_calls:\n-        this_model_call = getattr(model, model_call, None)\n-        if this_model_call and isinstance(this_model_call, (nn.Module, PreTrainedModel)):\n-            this_model_call.register_forward_hook(final_hook)\n-            break  # exit the loop after finding one (unsure, but should be just one call.)\n-\n-\n-@requires(backends=(\"torch\",))\n-def model_addition_debugger(cls):\n-    \"\"\"\n-    # Model addition debugger - a model adder tracer\n-    This decorator is a power user tool intended for model adders.\n-    It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\n-    To note, this decorator enforces `torch.inference_mode()`.\n-    ## Usage\n-\n-    add decorator to your model class\n-    ```python\n-    from ...modeling_utils import model_addition_debugger\n-\n-    @model_addition_debugger\n-    class MyModel(nn.Module) # Can inherit from PreTrainedModel too\n-        # ... nothing else changes\n-    ```\n-    Then, in a separate script (example is for Llava)\n-\n-    ```python\n-    import torch\n-    from PIL import Image\n-    import requests\n-    from transformers import LlavaProcessor, LlavaForConditionalGeneration\n-    torch.random.manual_seed(673)\n-\n-    # load pretrained model and processor\n-    model_id = \"llava-hf/llava-1.5-7b-hf\"\n-    processor = LlavaProcessor.from_pretrained(model_id)\n-    model = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n-\n-    # create random image input\n-    random_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())\n-\n-    # prompt\n-    prompt = \"<image>Describe this image.\"\n-\n-    # process inputs\n-    inputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n-\n-    # call forward method (not .generate!)\n-    with torch.no_grad():\n-        output = model.forward(**inputs)\n-    ```\n-\n-    \"\"\"\n-    orig_init = cls.__init__\n-\n-    @functools.wraps(cls.__init__)\n-    def wrapped_init(self, *args, **kwargs):\n-        orig_init(self, *args, **kwargs)\n-        _attach_debugger_logic(self, cls.__name__)\n-\n-    cls.__init__ = wrapped_init\n-    return cls\n-\n \n @requires(backends=(\"torch\",))\n @contextmanager\n-def model_addition_debugger_context(model, debug_path: Optional[str] = None):\n+def model_addition_debugger_context(model, debug_path: Optional[str] = None, do_prune_layers: Optional[bool] = True):\n     \"\"\"\n     # Model addition debugger - context manager for model adders\n     This context manager is a power user tool intended for model adders.\n     It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\n-    To note, this context manager enforces `torch.inference_mode()`.\n+    To note, this context manager enforces `torch.no_grad()`.\n \n     ## Usage\n \n@@ -300,6 +351,7 @@ def model_addition_debugger_context(model, debug_path: Optional[str] = None):\n     from PIL import Image\n     import requests\n     from transformers import LlavaProcessor, LlavaForConditionalGeneration\n+    from transformers.model_debugging_utils import model_addition_debugger_context\n     torch.random.manual_seed(673)\n \n     # load pretrained model and processor\n@@ -317,13 +369,16 @@ def model_addition_debugger_context(model, debug_path: Optional[str] = None):\n     inputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n \n     # call forward method (not .generate!)\n-    with model_addition_debugger_context(model):\n+    with model_addition_debugger_context(model, debug_path=\"Your_debug_path\", do_prune_layers=False):\n         output = model.forward(**inputs)\n     ```\n \n     \"\"\"\n-    _attach_debugger_logic(model, model.__class__.__name__, debug_path)\n+    orig_forwards = {m: m.forward for _, m in model.named_modules()}\n+    orig_forwards[model] = model.forward\n+    _attach_debugger_logic(model, debug_path, do_prune_layers)\n     try:\n         yield model\n     finally:\n-        pass\n+        for module_instance, forward_method in orig_forwards.items():\n+            module_instance.forward = forward_method"
        },
        {
            "sha": "be4e47b9fc6649a8d1452709735c9c542c3490a6",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4afd3f4820b0504963e460ecd725a3349eea8be3/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4afd3f4820b0504963e460ecd725a3349eea8be3/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=4afd3f4820b0504963e460ecd725a3349eea8be3",
            "patch": "@@ -538,10 +538,6 @@ def convert_and_export_with_cache(*args, **kwargs):\n     requires_backends(convert_and_export_with_cache, [\"torch\"])\n \n \n-def model_addition_debugger(*args, **kwargs):\n-    requires_backends(model_addition_debugger, [\"torch\"])\n-\n-\n def model_addition_debugger_context(*args, **kwargs):\n     requires_backends(model_addition_debugger_context, [\"torch\"])\n "
        },
        {
            "sha": "30419f8b7efa318f44c3d48c13edb221795b5e3a",
            "filename": "tests/utils/test_model_debugging_utils.py",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/4afd3f4820b0504963e460ecd725a3349eea8be3/tests%2Futils%2Ftest_model_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4afd3f4820b0504963e460ecd725a3349eea8be3/tests%2Futils%2Ftest_model_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_model_debugging_utils.py?ref=4afd3f4820b0504963e460ecd725a3349eea8be3",
            "patch": "@@ -0,0 +1,122 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import json\n+import os\n+import tempfile\n+import unittest\n+from pathlib import Path\n+\n+from transformers import is_torch_available\n+from transformers.model_debugging_utils import model_addition_debugger_context\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    class ToyModel(nn.Module):\n+        def __init__(self):\n+            super().__init__()\n+            self.embed = nn.Embedding(10, 4)\n+            self.linear_1 = nn.Linear(4, 8)\n+            self.linear_2 = nn.Linear(8, 2)\n+            self.act = nn.ReLU()\n+\n+        def forward(self, input_ids: str):\n+            hidden_states = self.embed(input_ids).mean(dim=1)\n+            hidden_states = self.act(self.linear_1(hidden_states))\n+            return self.linear_2(hidden_states)\n+\n+    class TestModelAdditionDebugger(unittest.TestCase):\n+        def setUp(self):\n+            self.model = ToyModel()\n+            self.inputs = {\"input_ids\": torch.randint(0, 10, (1, 3))}\n+\n+        def tearDown(self):\n+            gc.collect()\n+\n+        def test_debugger_outputs(self):\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                with model_addition_debugger_context(self.model, debug_path=str(tmpdir)):\n+                    _ = self.model.forward(**self.inputs)\n+\n+                base = f\"{self.model.__class__.__name__}_debug_tree\"\n+                summary = Path(os.path.join(tmpdir, f\"{base}_SUMMARY.json\"))\n+                full = Path(os.path.join(tmpdir, f\"{base}_FULL_TENSORS.json\"))\n+                self.assertTrue(os.path.isfile(summary) and os.path.isfile(full))\n+                data = json.loads(summary.read_text())\n+                self.assertTrue({\"module_path\", \"inputs\", \"children\"} <= data.keys())\n+                self.assertTrue(data[\"children\"])\n+\n+    class ToyLayer(nn.Module):\n+        def __init__(self, layer_index):\n+            super().__init__()\n+            self.layer_index = layer_index\n+            self.layer_operation = nn.Linear(4, 4)\n+\n+        def forward(self, hidden_states):\n+            return self.layer_operation(hidden_states)\n+\n+    class ToyModelWithLayers(nn.Module):\n+        def __init__(self):\n+            super().__init__()\n+            self.input_proj = nn.Linear(4, 4)\n+            self.layers = nn.ModuleList([ToyLayer(layer_index) for layer_index in range(6)])\n+            self.output_proj = nn.Linear(4, 2)\n+\n+        def forward(self, x):\n+            x = self.input_proj(x)\n+            for layer in self.layers:\n+                x = layer(x)\n+            return self.output_proj(x)\n+\n+    class TestModelWithLayers(unittest.TestCase):\n+        def setUp(self):\n+            self.inputs = {\"input_ids\": torch.randint(0, 10, (1, 3))}\n+            self.model_with_layers = ToyModelWithLayers()\n+            self.dense_input = {\"x\": torch.randn(1, 4)}\n+\n+        def tearDown(self):\n+            gc.collect()\n+\n+        def test_layer_pruning_behavior(self):\n+            # No pruning: expect all 6 layers\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                with model_addition_debugger_context(self.model_with_layers, debug_path=tmpdir, do_prune_layers=False):\n+                    _ = self.model_with_layers(**self.dense_input)\n+\n+                summary_path = os.path.join(tmpdir, \"ToyModelWithLayers_debug_tree_SUMMARY.json\")\n+                with open(summary_path) as f:\n+                    data = json.load(f)\n+                self.assertEqual(set(data.keys()), {\"module_path\", \"inputs\", \"children\"})\n+                for layer_index in range(6):\n+                    self.assertEqual(\n+                        data[\"children\"][layer_index + 1][\"module_path\"],\n+                        f\"ToyModelWithLayers.layers.{int(layer_index)}\",\n+                    )\n+\n+            # Pruning: expect only 2 layers (0 and 5)\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                with model_addition_debugger_context(self.model_with_layers, debug_path=tmpdir, do_prune_layers=True):\n+                    _ = self.model_with_layers(**self.dense_input)\n+\n+                summary_path = os.path.join(tmpdir, \"ToyModelWithLayers_debug_tree_SUMMARY.json\")\n+                with open(summary_path) as f:\n+                    data = json.load(f)\n+                self.assertEqual(set(data.keys()), {\"module_path\", \"inputs\", \"children\"})\n+                self.assertEqual(data[\"children\"][1][\"module_path\"], \"ToyModelWithLayers.layers.0\")\n+                self.assertEqual(data[\"children\"][2][\"module_path\"], \"ToyModelWithLayers.layers.5\")"
        }
    ],
    "stats": {
        "total": 537,
        "additions": 425,
        "deletions": 112
    }
}