{
    "author": "jla524",
    "message": "Fix image preview in multi-GPU inference docs (#35303)\n\nfix: link for img",
    "sha": "927c3e39ec1fb78e571c0ec2521ae59ed05720f2",
    "files": [
        {
            "sha": "ea9421747c13dffd44793f66e19050321d2c3b52",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/927c3e39ec1fb78e571c0ec2521ae59ed05720f2/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/927c3e39ec1fb78e571c0ec2521ae59ed05720f2/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=927c3e39ec1fb78e571c0ec2521ae59ed05720f2",
            "patch": "@@ -64,5 +64,5 @@ You can benefit from considerable speedups for inference, especially for inputs\n For a single forward pass on [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel) with a sequence length of 512 and various batch sizes, the expected speedup is as follows:\n \n <div style=\"text-align: center\">\n-<img src=\"huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct, seqlen = 512, python, w_ compile.png\">\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n </div>"
        },
        {
            "sha": "35e5bac465a33f29c1317fbff4002547d4ac2ae8",
            "filename": "docs/source/zh/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/927c3e39ec1fb78e571c0ec2521ae59ed05720f2/docs%2Fsource%2Fzh%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/927c3e39ec1fb78e571c0ec2521ae59ed05720f2/docs%2Fsource%2Fzh%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fperf_infer_gpu_multi.md?ref=927c3e39ec1fb78e571c0ec2521ae59ed05720f2",
            "patch": "@@ -64,5 +64,5 @@ torchrun --nproc-per-node 4 demo.py\n 以下是 [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel) 模型在序列长度为 512 且不同批量大小情况下的单次前向推理的预期加速效果：\n \n <div style=\"text-align: center\">\n-<img src=\"huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct, seqlen = 512, python, w_ compile.png\">\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n </div>"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}