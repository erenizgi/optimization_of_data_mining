{
    "author": "cjfghk5697",
    "message": "ğŸŒ [i18n-KO] Translated `altclip.md` to Korean (#34594)\n\n* docs: ko: model_doc/timesformer.md\n\n* feat: nmt draft\n\n* Apply suggestions from code review\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\nCo-authored-by: timdalxx <48753785+jeongiin@users.noreply.github.com>\n\n* Update docs/source/ko/model_doc/altclip.md\n\n* add snippet\n\n---------\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\nCo-authored-by: timdalxx <48753785+jeongiin@users.noreply.github.com>",
    "sha": "18e896bd8f90254c8ec4d9c4d97b699310c67db5",
    "files": [
        {
            "sha": "671be81f952c3e04d5d6b0d07eef86991e39d9ef",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e896bd8f90254c8ec4d9c4d97b699310c67db5/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e896bd8f90254c8ec4d9c4d97b699310c67db5/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=18e896bd8f90254c8ec4d9c4d97b699310c67db5",
            "patch": "@@ -692,8 +692,8 @@\n       sections:\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) ALIGN\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) AltCLIP\n+      - local: model_doc/altclip\n+        title: AltCLIP\n       - local: model_doc/blip-2\n         title: BLIP-2\n       - local: model_doc/blip"
        },
        {
            "sha": "1236bcc9aaa7da8da5fb4820bb87b3f9413cb805",
            "filename": "docs/source/ko/model_doc/altclip.md",
            "status": "added",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e896bd8f90254c8ec4d9c4d97b699310c67db5/docs%2Fsource%2Fko%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e896bd8f90254c8ec4d9c4d97b699310c67db5/docs%2Fsource%2Fko%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Faltclip.md?ref=18e896bd8f90254c8ec4d9c4d97b699310c67db5",
            "patch": "@@ -0,0 +1,78 @@\n+# AltCLIP\n+\n+## ê°œìš”[[overview]]\n+\n+AltCLIP ëª¨ë¸ì€ Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wuì˜ [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679v2) ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. AltCLIP(CLIPì˜ ì–¸ì–´ ì¸ì½”ë”ë¥¼ ë³€ê²½í•˜ì—¬ ì–¸ì–´ ê¸°ëŠ¥ í™•ì¥)ì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë° í…ìŠ¤íŠ¸-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ í›ˆë ¨ëœ ì‹ ê²½ë§ì…ë‹ˆë‹¤. CLIPì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë” XLM-Rë¡œ êµì²´í•˜ì—¬, ê±°ì˜ ëª¨ë“  ì‘ì—…ì—ì„œ CLIPê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìœ¼ë©°, ì›ë˜ CLIPì˜ ë‹¤êµ­ì–´ ì´í•´ì™€ ê°™ì€ ê¸°ëŠ¥ë„ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ë³¸ ì—°êµ¬ì—ì„œëŠ” ê°•ë ¥í•œ ì´ì¤‘ ì–¸ì–´ ë©€í‹°ëª¨ë‹¬ í‘œí˜„ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê°œë…ì ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. OpenAIì—ì„œ ì¶œì‹œí•œ ì‚¬ì „ í›ˆë ¨ëœ ë©€í‹°ëª¨ë‹¬ í‘œí˜„ ëª¨ë¸ CLIPì—ì„œ ì‹œì‘í•˜ì—¬, ê·¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë” XLM-Rë¡œ êµì²´í•˜ê³ , êµì‚¬ í•™ìŠµê³¼ ëŒ€ì¡° í•™ìŠµìœ¼ë¡œ êµ¬ì„±ëœ 2ë‹¨ê³„ í›ˆë ¨ ìŠ¤í‚¤ë§ˆë¥¼ í†µí•´ ì–¸ì–´ì™€ ì´ë¯¸ì§€ í‘œí˜„ì„ ì •ë ¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê´‘ë²”ìœ„í•œ ì‘ì—… í‰ê°€ë¥¼ í†µí•´ ìš°ë¦¬ì˜ ë°©ë²•ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤. ImageNet-CN, Flicker30k-CN, COCO-CNì„ í¬í•¨í•œ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, ê±°ì˜ ëª¨ë“  ì‘ì—…ì—ì„œ CLIPê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” CLIPì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ë‹¨ìˆœíˆ ë³€ê²½í•˜ì—¬ ë‹¤êµ­ì–´ ì´í•´ì™€ ê°™ì€ í™•ì¥ ê¸°ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.*\n+\n+ì´ ëª¨ë¸ì€ [jongjyh](https://huggingface.co/jongjyh)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒê³¼ ì˜ˆì œ[[usage-tips-and-example]]\n+\n+AltCLIPì˜ ì‚¬ìš©ë²•ì€ CLIPê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ì°¨ì´ì ì€ í…ìŠ¤íŠ¸ ì¸ì½”ë”ì— ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì–´í…ì…˜ ëŒ€ì‹  ì–‘ë°©í–¥ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ë©°, XLM-Rì˜ [CLS] í† í°ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n+\n+AltCLIPì€ ë©€í‹°ëª¨ë‹¬ ë¹„ì „ ë° ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ì„± ê³„ì‚° ë° ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AltCLIPì€ ViTì™€ ê°™ì€ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  íŠ¹ì§•ì„ ì–»ê³ , ì–‘ë°©í–¥ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ ì–»ìŠµë‹ˆë‹¤. ì´í›„ í…ìŠ¤íŠ¸ì™€ ì‹œê°ì  íŠ¹ì§• ëª¨ë‘ ë™ì¼í•œ ì°¨ì›ì˜ ì ì¬ ê³µê°„ìœ¼ë¡œ íˆ¬ì‚¬ë©ë‹ˆë‹¤. íˆ¬ì‚¬ëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ íŠ¹ì§• ê°„ì˜ ë‚´ì ì„ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+\n+ì´ë¯¸ì§€ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ì…ë ¥í•˜ê¸° ìœ„í•´, ê° ì´ë¯¸ì§€ë¥¼ ì¼ì •í•œ í¬ê¸°ì˜ ê²¹ì¹˜ì§€ ì•ŠëŠ” íŒ¨ì¹˜ ì‹œí€€ìŠ¤ë¡œ ë¶„í• í•œ ë’¤, ì´ë¥¼ ì„ í˜• ì„ë² ë”©í•©ë‹ˆë‹¤. ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ [CLS] í† í°ì´ ì¶”ê°€ë©ë‹ˆë‹¤. ì €ìë“¤ì€ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ë„ ì¶”ê°€í•˜ì—¬ ê²°ê³¼ ë²¡í„° ì‹œí€€ìŠ¤ë¥¼ í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ì…ë ¥í•©ë‹ˆë‹¤. [`CLIPImageProcessor`]ëŠ” ëª¨ë¸ì„ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ í¬ê¸° ì¡°ì •í•˜ê³  ì •ê·œí™”í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+[`AltCLIPProcessor`]ëŠ” [`CLIPImageProcessor`]ì™€ [`XLMRobertaTokenizer`]ë¥¼ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¬¶ì–´ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆì œëŠ” [`AltCLIPProcessor`]ì™€ [`AltCLIPModel`]ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ì„± ì ìˆ˜ë¥¼ ì–»ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+```python\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> from transformers import AltCLIPModel, AltCLIPProcessor\n+\n+>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n+>>> processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n+\n+>>> outputs = model(**inputs)\n+>>> logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜\n+>>> probs = logits_per_image.softmax(dim=1)  # ë¼ë²¨ ë§ˆë‹¤ í™•ë¥ ì„ ì–»ê¸° ìœ„í•´ softmax ì ìš©\n+```\n+<Tip>\n+\n+ì´ ëª¨ë¸ì€ `CLIPModel`ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë¯€ë¡œ, ì›ë˜ CLIPì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+</Tip>\n+\n+## AltCLIPConfig\n+\n+[[autodoc]] AltCLIPConfig\n+    - from_text_vision_configs\n+\n+## AltCLIPTextConfig\n+\n+[[autodoc]] AltCLIPTextConfig\n+\n+## AltCLIPVisionConfig\n+\n+[[autodoc]] AltCLIPVisionConfig\n+\n+## AltCLIPProcessor\n+\n+[[autodoc]] AltCLIPProcessor\n+\n+## AltCLIPModel\n+\n+[[autodoc]] AltCLIPModel\n+    - forward\n+    - get_text_features\n+    - get_image_features\n+\n+## AltCLIPTextModel\n+\n+[[autodoc]] AltCLIPTextModel\n+    - forward\n+\n+## AltCLIPVisionModel\n+\n+[[autodoc]] AltCLIPVisionModel\n+    - forward"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 80,
        "deletions": 2
    }
}