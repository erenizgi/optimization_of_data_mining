{
    "author": "Cyrilvallez",
    "message": "[cache] Remove all deprecated classes (#43168)\n\nremove deprecated classes",
    "sha": "cb39692dc902a1791d9963e8610267786a07812e",
    "files": [
        {
            "sha": "8d5f259d69631eeeff8893a54a71c4352d80e7ec",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -258,20 +258,6 @@ A [`StoppingCriteria`] can be used to change when to stop generation (other than\n \n [[autodoc]] EncoderDecoderCache\n \n-[[autodoc]] QuantoQuantizedCache\n-\n-[[autodoc]] HQQQuantizedCache\n-\n-[[autodoc]] OffloadedCache\n-\n-[[autodoc]] OffloadedStaticCache\n-\n-[[autodoc]] HybridCache\n-\n-[[autodoc]] HybridChunkedCache\n-\n-[[autodoc]] SlidingWindowCache\n-\n ## Watermark Utils\n \n [[autodoc]] WatermarkingConfig"
        },
        {
            "sha": "2655772df96221d842fb571b5e13b6b6a496f249",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -235,22 +235,8 @@ generation_output[:2]\n \n [[autodoc]] QuantizedCache\n \n-[[autodoc]] QuantoQuantizedCache\n-\n-[[autodoc]] HQQQuantizedCache\n-\n-[[autodoc]] OffloadedCache\n-\n [[autodoc]] StaticCache\n \n-[[autodoc]] OffloadedStaticCache\n-\n-[[autodoc]] HybridCache\n-\n-[[autodoc]] HybridChunkedCache\n-\n-[[autodoc]] SlidingWindowCache\n-\n [[autodoc]] EncoderDecoderCache\n \n ## 워터마크 유틸리티 (Watermark Utils) [[transformers.WatermarkDetector]]"
        },
        {
            "sha": "fcdc28371667eea5dae481a57b66b692ea4614aa",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -363,15 +363,6 @@\n         \"StaticSlidingWindowLayer\",\n         \"QuantoQuantizedLayer\",\n         \"HQQQuantizedLayer\",\n-        \"SlidingWindowLayer\",\n-        \"ChunkedSlidingLayer\",\n-        \"HQQQuantizedCache\",\n-        \"HybridCache\",\n-        \"HybridChunkedCache\",\n-        \"OffloadedCache\",\n-        \"OffloadedStaticCache\",\n-        \"QuantoQuantizedCache\",\n-        \"SlidingWindowCache\",\n         \"Cache\",\n         \"DynamicCache\",\n         \"EncoderDecoderCache\",\n@@ -482,20 +473,12 @@\n if TYPE_CHECKING:\n     # All modeling imports\n     from .cache_utils import Cache as Cache\n-    from .cache_utils import ChunkedSlidingLayer as ChunkedSlidingLayer\n     from .cache_utils import DynamicCache as DynamicCache\n     from .cache_utils import DynamicLayer as DynamicLayer\n     from .cache_utils import EncoderDecoderCache as EncoderDecoderCache\n-    from .cache_utils import HQQQuantizedCache as HQQQuantizedCache\n     from .cache_utils import HQQQuantizedLayer as HQQQuantizedLayer\n-    from .cache_utils import HybridCache as HybridCache\n-    from .cache_utils import OffloadedCache as OffloadedCache\n-    from .cache_utils import OffloadedStaticCache as OffloadedStaticCache\n     from .cache_utils import QuantizedCache as QuantizedCache\n-    from .cache_utils import QuantoQuantizedCache as QuantoQuantizedCache\n     from .cache_utils import QuantoQuantizedLayer as QuantoQuantizedLayer\n-    from .cache_utils import SlidingWindowCache as SlidingWindowCache\n-    from .cache_utils import SlidingWindowLayer as SlidingWindowLayer\n     from .cache_utils import StaticCache as StaticCache\n     from .cache_utils import StaticLayer as StaticLayer\n     from .cache_utils import StaticSlidingWindowLayer as StaticSlidingWindowLayer"
        },
        {
            "sha": "a79672d76e0583606a192d2ed89c2ff476f51a1b",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 110,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -588,7 +588,7 @@ def __init__(\n             from optimum.quanto import MaxOptimizer, qint2, qint4\n         else:\n             raise ImportError(\n-                \"You need optimum-quanto package version to be greater or equal than 0.2.5 to use `QuantoQuantizedCache`. \"\n+                \"You need optimum-quanto package version to be greater or equal than 0.2.5 to use `QuantoQuantizedLayer`. \"\n             )\n \n         if self.nbits not in [2, 4]:\n@@ -1293,112 +1293,3 @@ def is_sliding(self):\n     @property\n     def is_compileable(self) -> bool:\n         return self.self_attention_cache.is_compileable\n-\n-\n-### Deprecated classes\n-\n-\n-class SlidingWindowLayer(StaticSlidingWindowLayer):\n-    def __init__(self, max_cache_len: int, sliding_window: int):\n-        logger.warning_once(\n-            \"`SlidingWindowLayer` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticSlidingWindowLayer` instead, which is a better name for it.\"\n-        )\n-        super().__init__(max_cache_len, sliding_window)\n-\n-\n-class ChunkedSlidingLayer(StaticSlidingWindowLayer):\n-    def __init__(self, max_cache_len: int, sliding_window: int):\n-        logger.warning_once(\n-            \"`ChunkedSlidingLayer` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticSlidingWindowLayer` instead, which has the exact same functionalities.\"\n-        )\n-        super().__init__(max_cache_len, sliding_window)\n-\n-\n-class OffloadedCache(DynamicCache):\n-    def __init__(self) -> None:\n-        logger.warning_once(\n-            \"`OffloadedCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `DynamicCache(offloading=True)` instead\"\n-        )\n-        super().__init__(offloading=True)\n-\n-\n-class OffloadedStaticCache(StaticCache):\n-    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n-        logger.warning_once(\n-            \"`OffloadedStaticCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticCache(..., offloading=True)` instead\"\n-        )\n-        super().__init__(config=config, max_cache_len=max_cache_len, offloading=True)\n-\n-\n-class SlidingWindowCache(StaticCache):\n-    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n-        logger.warning_once(\n-            \"`SlidingWindowCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n-        )\n-        super().__init__(config=config, max_cache_len=max_cache_len)\n-\n-\n-class HybridCache(StaticCache):\n-    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n-        logger.warning_once(\n-            \"`HybridCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n-        )\n-        super().__init__(config=config, max_cache_len=max_cache_len)\n-\n-\n-class HybridChunkedCache(StaticCache):\n-    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n-        logger.warning_once(\n-            \"`HybridChunkedCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n-        )\n-        super().__init__(config=config, max_cache_len=max_cache_len)\n-\n-\n-class OffloadedHybridCache(StaticCache):\n-    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n-        logger.warning_once(\n-            \"`OffloadedHybridCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `StaticCache(..., offload=True)` instead which will correctly infer the type of each layer.\"\n-        )\n-        super().__init__(config=config, max_cache_len=max_cache_len, offloading=True)\n-\n-\n-class QuantoQuantizedCache(QuantizedCache):\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        nbits: int = 4,\n-        axis_key: int = 0,\n-        axis_value: int = 0,\n-        q_group_size: int = 64,\n-        residual_length: int = 128,\n-    ):\n-        logger.warning_once(\n-            \"`QuantoQuantizedCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `QuantizedCache(backend='quanto', ...)` instead.\"\n-        )\n-        super().__init__(\"quanto\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n-\n-\n-class HQQQuantizedCache(QuantizedCache):\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        nbits: int = 4,\n-        axis_key: int = 0,\n-        axis_value: int = 0,\n-        q_group_size: int = 64,\n-        residual_length: int = 128,\n-    ):\n-        logger.warning_once(\n-            \"`HQQQuantizedCache` is deprecated and will be removed in version v4.59 \"\n-            \"Use `QuantizedCache(backend='hqq', ...)` instead.\"\n-        )\n-        super().__init__(\"hqq\", config, nbits, axis_key, axis_value, q_group_size, residual_length)"
        },
        {
            "sha": "a5cb9dbae7dcd379150081b7996c5522c48eb5cb",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -23,55 +23,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class HQQQuantizedCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class HybridCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class OffloadedCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class OffloadedStaticCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class QuantizedCache(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class QuantoQuantizedCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class SlidingWindowCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class StaticCache(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "3eac2db68c14f2dcc3a010b122ca9d0c7c77fbd0",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -174,7 +174,7 @@ def test_export_text_with_hybrid_cache(self):\n             ),\n         )\n \n-        # Export + HybridCache\n+        # Export\n         model.eval()\n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export("
        },
        {
            "sha": "9cd6d63c1e84a6ef934366a113ff655417690476",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -585,18 +585,6 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n     def test_generate_continue_from_past_key_values(self):\n         pass\n \n-    @unittest.skip(\n-        \"Moshi either needs default generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n-    )\n-    def test_greedy_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"Moshi either needs default generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n-    )\n-    def test_beam_search_generate_dict_outputs_use_cache(self):\n-        pass\n-\n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @unittest.skip(reason=\"Unimplemented. Relies on `test_eager_matches_sdpa_generate` to check correctness.\")\n     def test_eager_matches_sdpa_inference("
        },
        {
            "sha": "c56f17dd8222d30d973457e69267f751435eab07",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb39692dc902a1791d9963e8610267786a07812e/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb39692dc902a1791d9963e8610267786a07812e/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=cb39692dc902a1791d9963e8610267786a07812e",
            "patch": "@@ -177,7 +177,7 @@ def test_cache_beam_search(self, cache_implementation):\n         \"\"\"\n         _skip_on_failed_cache_prerequisites(self, cache_implementation)\n         if cache_implementation == \"offloaded_hybrid_chunked\":\n-            # TODO (joao, cyril): something is off with `offloaded_hybrid_chunked` aka `OffloadedHybridCache`: the\n+            # TODO (joao, cyril): something is off with `offloaded_hybrid_chunked`: the\n             # output sequence (and the corresponding beam scores, if we add `output_scores=True`) are significantly\n             # different from the other caches.\n             self.skipTest(\"`offloaded_hybrid_chunked` fails this test\")"
        }
    ],
    "stats": {
        "total": 214,
        "additions": 3,
        "deletions": 211
    }
}