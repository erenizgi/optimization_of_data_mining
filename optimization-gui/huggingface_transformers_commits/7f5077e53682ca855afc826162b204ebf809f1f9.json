{
    "author": "threewebcode",
    "message": "fix typos in the tests directory (#36717)",
    "sha": "7f5077e53682ca855afc826162b204ebf809f1f9",
    "files": [
        {
            "sha": "b24beedcd4fa15bc2536e1dc1be409d7a4d5a355",
            "filename": "scripts/benchmark/trainer-benchmark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/scripts%2Fbenchmark%2Ftrainer-benchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/scripts%2Fbenchmark%2Ftrainer-benchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fbenchmark%2Ftrainer-benchmark.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -18,7 +18,7 @@\n #\n # --variations allows you to compare variations in multiple dimensions.\n #\n-# as the first dimention has 2 options and the second 3 in our example, this will run the trainer 6\n+# as the first dimension has 2 options and the second 3 in our example, this will run the trainer 6\n # times adding one of:\n #\n #    1. --tf32 0 --fp16 0"
        },
        {
            "sha": "85736a9422abd3befe7a33b11d8ba9f2150cf45a",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -405,7 +405,7 @@ def bad_deepspeed_create_sinusoidal_positions(num_pos: int, dim: int) -> torch.T\n         self.assertFalse(torch.allclose(good_deepspeed_sin_cos, bad_deepspeed_sin_cos))\n         torch.testing.assert_close(good_torch_sin_cos, good_deepspeed_sin_cos.cpu())\n \n-        # Finally, we can see that the incorrect pattern is okay on vanilla torch, demostrating that this issue is\n+        # Finally, we can see that the incorrect pattern is okay on vanilla torch, demonstrating that this issue is\n         # exclusive to DeepSpeed\n         bad_torch_sin_cos = bad_deepspeed_create_sinusoidal_positions(\n             model.config.max_position_embeddings, model.config.rotary_dim"
        },
        {
            "sha": "ef8010074b45afdc94f2c0ba52adfce0f949d39f",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -193,7 +193,7 @@ def test_validate(self):\n             generation_config_bad_temperature.update(temperature=None)\n         self.assertEqual(len(captured_warnings), 0)\n \n-        # Impossible sets of contraints/parameters will raise an exception\n+        # Impossible sets of constraints/parameters will raise an exception\n         with self.assertRaises(ValueError):\n             GenerationConfig(do_sample=False, num_beams=1, num_return_sequences=2)\n         with self.assertRaises(ValueError):"
        },
        {
            "sha": "7ba1502d4297e52333b0487d4772651c63cc23b5",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -751,7 +751,7 @@ def test_forced_bos_token_logits_processor(self):\n         scores = self._get_uniform_logits(batch_size, vocab_size)\n         processed_scores = logits_processor(input_ids, scores)\n         self.assertTrue(torch.isneginf(processed_scores[:, bos_token_id + 1 :]).all())\n-        # score for bos_token_id shold be zero\n+        # score for bos_token_id should be zero\n         self.assertListEqual(processed_scores[:, bos_token_id].tolist(), 4 * [0])\n \n         # processor should not change logits in-place\n@@ -972,7 +972,7 @@ def test_watermarking_processor(self):\n \n         watermark = WatermarkLogitsProcessor(vocab_size=vocab_size, device=input_ids.device)\n \n-        # use fixed id for last token, needed for reprodicibility and tests\n+        # use fixed id for last token, needed for reproducibility and tests\n         input_ids[:, -1] = 10\n         scores_wo_bias = scores[:, -1].clone()\n         out = watermark(input_ids=input_ids, scores=scores)"
        },
        {
            "sha": "375c10c67243ba0f9f00c485f242e8be35dc2b09",
            "filename": "tests/generation/test_stopping_criteria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_stopping_criteria.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -256,7 +256,7 @@ def test_criterias_per_row(self):\n             ]\n         )\n \n-        # trigger stopping when at leat one criteria is satisfied, one value per batch\n+        # trigger stopping when at least one criteria is satisfied, one value per batch\n         self.assertTrue(criteria(inputs[\"input_ids\"], scores))\n \n         # return False when neither is satisfied\n@@ -283,7 +283,7 @@ def test_criterias_per_row_batched(self):\n             ]\n         )\n \n-        # trigger stopping when at leat one criteria is satisfied\n+        # trigger stopping when at least one criteria is satisfied\n         self.assertListEqual(criteria(inputs[\"input_ids\"], scores).tolist(), [True, False, False])\n \n         # False when neither is satisfied"
        },
        {
            "sha": "daa9e2f70ca43b49a3f561bab3ebfbc0dfab60a0",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -173,7 +173,7 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n     def _check_similar_generate_outputs(self, output_1, output_2, atol=1e-5, rtol=1e-5):\n         \"\"\"\n         Checks whether a pair of generate outputs are similar. Two `generate` call outputs are considered similar in\n-        the following siturations:\n+        the following situations:\n         1. The sequences are the same\n         2. The sequences are different, but the scores up to (and including) the first mismatch are nearly identical\n         \"\"\"\n@@ -1617,7 +1617,7 @@ def test_past_key_values_format(self):\n             embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n             per_head_embed_dim = embed_dim // num_attention_heads\n \n-            # some models have diffent num-head for query vs key/value so we need to assign correct value\n+            # some models have different num-head for query vs key/value so we need to assign correct value\n             # BUT only after `per_head_embed_dim` is set\n             num_attention_heads = (\n                 text_config.num_key_value_heads\n@@ -2316,7 +2316,7 @@ def test_inherits_generation_mixin(self):\n     def _test_attention_implementation(self, attn_implementation):\n         \"\"\"\n         Compares the output of generate with the eager attention implementation against other implementations.\n-        NOTE: despite the test logic being the same, different implementations actually need diferent decorators, hence\n+        NOTE: despite the test logic being the same, different implementations actually need different decorators, hence\n         this separate function.\n         \"\"\"\n         max_new_tokens = 30\n@@ -4619,7 +4619,7 @@ def test_encoder_decoder_generate_attention_mask(self):\n         self.assertTrue(diff < 1e-4)\n \n     def test_generate_input_ids_as_kwarg(self):\n-        \"\"\"Test that `input_ids` work equaly as a positional and keyword argument in decoder-only models\"\"\"\n+        \"\"\"Test that `input_ids` work equally as a positional and keyword argument in decoder-only models\"\"\"\n         article = \"I need input_ids to generate\"\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\", max_length=15)\n@@ -4636,7 +4636,7 @@ def test_generate_input_ids_as_kwarg(self):\n         self.assertEqual(output_sequences.shape, (1, 15))\n \n     def test_generate_input_ids_as_encoder_kwarg(self):\n-        \"\"\"Test that `input_ids` work equaly as a positional and keyword argument in encoder-decoder models\"\"\"\n+        \"\"\"Test that `input_ids` work equally as a positional and keyword argument in encoder-decoder models\"\"\"\n         article = \"Justin Timberlake and Jessica Biel, welcome to parenthood.\"\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n         model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-bart\")"
        },
        {
            "sha": "b4e58fd7a0bc26590493a8cdc4e1fdfbe6fdb37a",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -35,7 +35,7 @@\n \n class TestTensorParallel(TestCasePlus):\n     def torchrun(self, script: str):\n-        \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necesary.\"\"\"\n+        \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n         with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n             tmp.write(script)\n             tmp.flush()"
        },
        {
            "sha": "248b43c2f8f8d24270fe3d8ba14871639055146e",
            "filename": "tests/test_modeling_tf_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftest_modeling_tf_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftest_modeling_tf_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_tf_common.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -599,7 +599,7 @@ def prepare_layer_head_mask(i, attention_heads, num_hidden_layers):\n             if model.config.is_encoder_decoder:\n                 signature = inspect.signature(model.call)\n                 arg_names = [*signature.parameters.keys()]\n-                if \"decoder_head_mask\" in arg_names:  # necessary diferentiation because of T5 model\n+                if \"decoder_head_mask\" in arg_names:  # necessary differentiation because of T5 model\n                     inputs[\"decoder_head_mask\"] = head_mask\n                 if \"cross_attn_head_mask\" in arg_names:\n                     inputs[\"cross_attn_head_mask\"] = head_mask"
        },
        {
            "sha": "4b8d774e66957a4b0ad966eb00cbed978b8cf5b8",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -241,7 +241,7 @@ def bytes2megabytes(x):\n     return int(x / 2**20)\n \n \n-# Copied from acclerate: https://github.com/huggingface/accelerate/blob/ee163b66fb7848892519e804688cb4ae981aacbe/src/accelerate/test_utils/scripts/external_deps/test_peak_memory_usage.py#L40C1-L73C68\n+# Copied from accelerate: https://github.com/huggingface/accelerate/blob/ee163b66fb7848892519e804688cb4ae981aacbe/src/accelerate/test_utils/scripts/external_deps/test_peak_memory_usage.py#L40C1-L73C68\n class TorchTracemalloc:\n     def __enter__(self):\n         gc.collect()\n@@ -4086,7 +4086,7 @@ def forward(self, x):\n             # Functional check\n             self.assertAlmostEqual(loss, orig_loss)\n \n-            # AOT Autograd recomputaion and nvfuser recomputation optimization\n+            # AOT Autograd recomputation and nvfuser recomputation optimization\n             # aggressively fuses the operations and reduce the memory footprint.\n             self.assertGreater(orig_peak_mem, peak_mem * 2)\n "
        },
        {
            "sha": "793225f5ae80a76d61bfe802d5825f03acbe55a3",
            "filename": "tests/trainer/test_trainer_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_seq2seq.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -186,7 +186,7 @@ def prepare_data(examples):\n \n     @require_torch\n     def test_bad_generation_config_fail_early(self):\n-        # Tests that a bad geneartion config causes the trainer to fail early\n+        # Tests that a bad generation config causes the trainer to fail early\n         model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n         tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n         data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\", padding=\"longest\")"
        },
        {
            "sha": "4e755f1d4a52938c8501611a6bef36e205019437",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -436,7 +436,7 @@ class TFNewBertPreTrainedModel(PreTrainedModel):\n \n             self.init_file(file_name, bert_test)\n             duplicate_module(file_name, bert_model_patterns, new_bert_model_patterns)\n-            # There should not be a new Copied from statement, the old one should be adapated.\n+            # There should not be a new Copied from statement, the old one should be adapted.\n             self.check_result(dest_file_name, bert_expected)\n \n             self.init_file(file_name, bert_test)"
        },
        {
            "sha": "b245f279a8e7a74804d2ae2142a702f8a1a19228",
            "filename": "tests/utils/test_image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_image_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_image_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_utils.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -996,7 +996,7 @@ def test_get_image_size(self):\n         image = np.random.randint(0, 256, (3, 32, 64))\n         self.assertEqual(get_image_size(image), (32, 64))\n \n-        # Test the channel dimension can be overriden\n+        # Test the channel dimension can be overridden\n         image = np.random.randint(0, 256, (3, 32, 64))\n         self.assertEqual(get_image_size(image, channel_dim=ChannelDimension.LAST), (3, 32))\n "
        },
        {
            "sha": "233fbcde2ea27baf6fdc8f42185e6744652c522e",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -411,7 +411,7 @@ def test_llama3_rope_numerically(self):\n             self.assertEqual(attention_scale, 1.0)\n \n         # Check 2: based on `low_freq_factor` and `high_freq_factor`, the frequencies will be scaled between 1 and\n-        # `factor` (similar to yarn). Low frequencies get scaled by `factor`, high frequences see no change, medium\n+        # `factor` (similar to yarn). Low frequencies get scaled by `factor`, high frequencies see no change, medium\n         # frequencies are scaled by a value in between. Changing `low_freq_factor` and `high_freq_factor` changes what\n         # is considered low, medium, and high frequencies.\n         factor = 10.0"
        },
        {
            "sha": "71a400579f63640c2a868dc12af82ce1d510ccae",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5077e53682ca855afc826162b204ebf809f1f9/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=7f5077e53682ca855afc826162b204ebf809f1f9",
            "patch": "@@ -1686,7 +1686,7 @@ def forward(self):\n     def test_isin_mps_friendly(self):\n         \"\"\"tests that our custom `isin_mps_friendly` matches `torch.isin`\"\"\"\n         random_ids = torch.randint(0, 100, (100,))\n-        # We can match against an interger\n+        # We can match against an integer\n         random_test_integer = torch.randint(0, 100, (1,)).item()\n         self.assertTrue(\n             torch.equal(\n@@ -1911,7 +1911,7 @@ def test_unknown_quantization_config(self):\n     @require_torch_gpu\n     def test_loading_is_fast_on_gpu(self, model_id: str, max_loading_time: float):\n         \"\"\"\n-        This test is used to avoid regresion on https://github.com/huggingface/transformers/pull/36380.\n+        This test is used to avoid regression on https://github.com/huggingface/transformers/pull/36380.\n         10s should be more than enough for both models, and allows for some margin as loading time are quite\n         unstable. Before #36380, it used to take more than 40s, so 10s is still reasonable.\n         Note that we run this test in a subprocess, to ensure that cuda is not already initialized/warmed-up."
        }
    ],
    "stats": {
        "total": 44,
        "additions": 22,
        "deletions": 22
    }
}