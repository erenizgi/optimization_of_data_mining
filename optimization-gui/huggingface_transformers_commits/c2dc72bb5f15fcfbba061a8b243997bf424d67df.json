{
    "author": "BenjaminBossan",
    "message": "TST Fix PEFT integration test bitsandbytes config (#39082)\n\nTST Fix PEFT integration test bitsandbytes config\n\nThe PEFT integration tests still used load_in_{4,8}_bit, which is\ndeprecated, moving to properly setting BitsAndBytesConfig. For 4bit,\nalso ensure that nf4 is being used to prevent\n\n> RuntimeError: quant_type must be nf4 on CPU, got fp4",
    "sha": "c2dc72bb5f15fcfbba061a8b243997bf424d67df",
    "files": [
        {
            "sha": "7efa5252e849aca35cb51b000c0957758aa2b12b",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 27,
            "deletions": 5,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2dc72bb5f15fcfbba061a8b243997bf424d67df/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2dc72bb5f15fcfbba061a8b243997bf424d67df/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=c2dc72bb5f15fcfbba061a8b243997bf424d67df",
            "patch": "@@ -25,6 +25,7 @@\n     AutoModelForCausalLM,\n     AutoModelForSequenceClassification,\n     AutoTokenizer,\n+    BitsAndBytesConfig,\n     OPTForCausalLM,\n     Trainer,\n     TrainingArguments,\n@@ -76,6 +77,12 @@ def _check_lora_correctly_converted(self, model):\n \n         return is_peft_loaded\n \n+    def _get_bnb_4bit_config(self):\n+        return BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n+\n+    def _get_bnb_8bit_config(self):\n+        return BitsAndBytesConfig(load_in_8bit=True)\n+\n     def test_peft_from_pretrained(self):\n         \"\"\"\n         Simple test that tests the basic usage of PEFT model through `from_pretrained`.\n@@ -431,7 +438,10 @@ def test_peft_from_pretrained_kwargs(self):\n         \"\"\"\n         for model_id in self.peft_test_model_ids:\n             for transformers_class in self.transformers_test_model_classes:\n-                peft_model = transformers_class.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n+                bnb_config = self._get_bnb_8bit_config()\n+                peft_model = transformers_class.from_pretrained(\n+                    model_id, device_map=\"auto\", quantization_config=bnb_config\n+                )\n \n                 module = peft_model.model.decoder.layers[0].self_attn.v_proj\n                 self.assertTrue(module.__class__.__name__ == \"Linear8bitLt\")\n@@ -449,7 +459,10 @@ def test_peft_save_quantized(self):\n         # 4bit\n         for model_id in self.peft_test_model_ids:\n             for transformers_class in self.transformers_test_model_classes:\n-                peft_model = transformers_class.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n+                bnb_config = self._get_bnb_4bit_config()\n+                peft_model = transformers_class.from_pretrained(\n+                    model_id, device_map=\"auto\", quantization_config=bnb_config\n+                )\n \n                 module = peft_model.model.decoder.layers[0].self_attn.v_proj\n                 self.assertTrue(module.__class__.__name__ == \"Linear4bit\")\n@@ -465,7 +478,10 @@ def test_peft_save_quantized(self):\n         # 8-bit\n         for model_id in self.peft_test_model_ids:\n             for transformers_class in self.transformers_test_model_classes:\n-                peft_model = transformers_class.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n+                bnb_config = self._get_bnb_8bit_config()\n+                peft_model = transformers_class.from_pretrained(\n+                    model_id, device_map=\"auto\", quantization_config=bnb_config\n+                )\n \n                 module = peft_model.model.decoder.layers[0].self_attn.v_proj\n                 self.assertTrue(module.__class__.__name__ == \"Linear8bitLt\")\n@@ -489,7 +505,10 @@ def test_peft_save_quantized_regression(self):\n         # 4bit\n         for model_id in self.peft_test_model_ids:\n             for transformers_class in self.transformers_test_model_classes:\n-                peft_model = transformers_class.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n+                bnb_config = self._get_bnb_4bit_config()\n+                peft_model = transformers_class.from_pretrained(\n+                    model_id, device_map=\"auto\", quantization_config=bnb_config\n+                )\n \n                 module = peft_model.model.decoder.layers[0].self_attn.v_proj\n                 self.assertTrue(module.__class__.__name__ == \"Linear4bit\")\n@@ -505,7 +524,10 @@ def test_peft_save_quantized_regression(self):\n         # 8-bit\n         for model_id in self.peft_test_model_ids:\n             for transformers_class in self.transformers_test_model_classes:\n-                peft_model = transformers_class.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n+                bnb_config = self._get_bnb_8bit_config()\n+                peft_model = transformers_class.from_pretrained(\n+                    model_id, device_map=\"auto\", quantization_config=bnb_config\n+                )\n \n                 module = peft_model.model.decoder.layers[0].self_attn.v_proj\n                 self.assertTrue(module.__class__.__name__ == \"Linear8bitLt\")"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 27,
        "deletions": 5
    }
}