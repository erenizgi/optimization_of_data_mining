{
    "author": "yao-matrix",
    "message": "enable internvl UTs on XPU (#37779)\n\n* enable internvl UTs on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style per comments\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>",
    "sha": "34f26e2c3efcccd80da64015c6434a9bbe408070",
    "files": [
        {
            "sha": "e51126f14eade4d387be0737a7d3f425e39fe8b8",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 92,
            "deletions": 15,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/34f26e2c3efcccd80da64015c6434a9bbe408070/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34f26e2c3efcccd80da64015c6434a9bbe408070/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=34f26e2c3efcccd80da64015c6434a9bbe408070",
            "patch": "@@ -27,11 +27,13 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_av,\n     require_bitsandbytes,\n+    require_deterministic_for_xpu,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -177,7 +179,7 @@ def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_\n         model = InternVLForConditionalGeneration(config=config)\n         model.to(torch_device)\n         model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+        with torch.autocast(device_type=torch_device, dtype=torch.float16):\n             logits = model(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n@@ -279,7 +281,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class InternVLQwen2IntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.small_model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n@@ -326,14 +328,22 @@ def test_qwen2_small_model_integration_forward(self):\n             output = model(**inputs)\n \n         actual_logits = output.logits[0, -1, :5].cpu()\n-        expected_logits = torch.tensor([11.9375, 14.8750, 14.0625, 10.7500, 6.9062], dtype=torch.bfloat16)\n+        expected_logits_all = Expectations(\n+            {\n+                (\"xpu\", 3): torch.tensor([11.7500, 14.7500, 14.1250, 10.5625, 6.7812], dtype=torch.bfloat16),\n+                (\"cuda\", 7): torch.tensor([11.9375, 14.8750, 14.0625, 10.7500, 6.9062], dtype=torch.bfloat16),\n+            }\n+        )  # fmt: skip\n+        expected_logits = expected_logits_all.get_expectation()\n+\n         self.assertTrue(\n             torch.allclose(actual_logits, expected_logits, atol=0.1),\n             f\"Actual logits: {actual_logits}\"\n             f\"\\nExpected logits: {expected_logits}\"\n             f\"\\nDifference: {torch.abs(actual_logits - expected_logits)}\",\n         )\n \n+    @require_deterministic_for_xpu\n     def test_qwen2_small_model_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n@@ -346,7 +356,15 @@ def test_qwen2_small_model_integration_generate_text_only(self):\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\"\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light.\",\n+                (\"cuda\", 7): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     def test_qwen2_small_model_integration_generate_chat_template(self):\n@@ -375,6 +393,7 @@ def test_qwen2_small_model_integration_generate_chat_template(self):\n         expected_output = \"The image shows two cats lying on a pink blanket. The cat on the left is a tabby\"\n         self.assertEqual(decoded_output, expected_output)\n \n+    @require_deterministic_for_xpu\n     def test_qwen2_small_model_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n@@ -404,7 +423,15 @@ def test_qwen2_small_model_integration_batched_generate(self):\n         )\n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of'  # fmt: skip\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate\"',\n+                (\"cuda\", 7): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -455,7 +482,14 @@ def test_qwen2_small_model_integration_batched_generate_multi_image(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = 'user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Angle'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Foreground\",\n+                (\"cuda\", 7): \"user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Angle\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -495,14 +529,21 @@ def test_qwen2_medium_model_integration_video(self):\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n-        expected_output = 'The man is performing a forehand shot.'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"The man is performing a volley.\",\n+                (\"cuda\", 7): \"The man is performing a forehand shot.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n \n     @require_av\n+    @require_deterministic_for_xpu\n     def test_qwen2_small_model_integration_interleaved_images_videos(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n@@ -564,15 +605,27 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n \n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n-        expected_output = 'user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image**: This shows the Statue of Liberty on Liberty Island, with the'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image:**\\n   - The Statue of Liberty is prominently featured on an\",\n+                (\"cuda\", 7): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image**: This shows the Statue of Liberty on Liberty Island, with the\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot.\",\n+                (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -590,7 +643,7 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class InternVLLlamaIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.small_model_checkpoint = \"OpenGVLab/InternVL2_5-2B-MPO-hf\"\n@@ -711,7 +764,13 @@ def test_llama_small_model_integration_batched_generate(self):\n \n         # Check first output\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n-        expected_output = 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\",\n+                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -880,7 +939,13 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n \n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n-        expected_output = 'user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. Upon closer inspection, the differences between the two images are:\\n\\n1. **'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After re-examining the images, I can see that they are actually\",\n+                (\"cuda\", 7): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. Upon closer inspection, the differences between the two images are:\\n\\n1. **\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -889,7 +954,13 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n+                (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -898,7 +969,13 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n \n         # Check third output\n         decoded_output = processor.decode(output[2], skip_special_tokens=True)\n-        expected_output = 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, untouched dreams.'  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n+                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, untouched dreams.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,"
        },
        {
            "sha": "62508002febc6a797785106337df295beda6ff8c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34f26e2c3efcccd80da64015c6434a9bbe408070/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34f26e2c3efcccd80da64015c6434a9bbe408070/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=34f26e2c3efcccd80da64015c6434a9bbe408070",
            "patch": "@@ -80,7 +80,6 @@\n     require_bitsandbytes,\n     require_deepspeed,\n     require_flash_attn,\n-    require_non_xpu,\n     require_safetensors,\n     require_torch,\n     require_torch_accelerator,\n@@ -2604,7 +2603,7 @@ def test_inputs_embeds_matches_input_ids(self):\n                     )[0]\n             torch.testing.assert_close(out_embeds, out_ids)\n \n-    @require_non_xpu\n+    @require_torch_gpu\n     @require_torch_multi_gpu\n     def test_multi_gpu_data_parallel_forward(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -3874,7 +3873,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 with sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n-    @require_non_xpu\n     @require_torch_sdpa\n     @require_torch_accelerator\n     @slow\n@@ -3887,8 +3885,8 @@ def test_sdpa_can_compile_dynamic(self):\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:\n             self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n-        else:\n-            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+        elif device_type not in [\"cuda\", \"rocm\", \"xpu\"]:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU, or an Intel XPU\")\n \n         torch.compiler.reset()\n "
        }
    ],
    "stats": {
        "total": 115,
        "additions": 95,
        "deletions": 20
    }
}