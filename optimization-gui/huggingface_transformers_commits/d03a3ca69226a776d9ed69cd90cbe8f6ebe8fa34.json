{
    "author": "vasqu",
    "message": "[`OPT`] Fix attention scaling (#38290)\n\n* fix opt attention scaling\n\n* add comment to why we do this",
    "sha": "d03a3ca69226a776d9ed69cd90cbe8f6ebe8fa34",
    "files": [
        {
            "sha": "eef54b02ec03a97d9f963ad98903415368252b88",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d03a3ca69226a776d9ed69cd90cbe8f6ebe8fa34/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d03a3ca69226a776d9ed69cd90cbe8f6ebe8fa34/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=d03a3ca69226a776d9ed69cd90cbe8f6ebe8fa34",
            "patch": "@@ -154,7 +154,11 @@ def forward(\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         bsz, tgt_len, _ = hidden_states.size()\n \n-        # get query proj\n+        # Scaling is susceptible to floating point arithmetics' inprecisions\n+        # which can lead to different results (this is dependent from model\n+        # to model, e.g. whisper is one such case). We therefore keep the\n+        # original order of scaling to follow the original implementation\n+        # and enforce no scaling (1.0) in the attention call below.\n         query_states = self.q_proj(hidden_states) * self.scaling\n         query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n@@ -187,7 +191,7 @@ def forward(\n             value_states,\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout,\n-            scaling=self.scaling,\n+            scaling=1.0,\n             **kwargs,\n         )\n "
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}