{
    "author": "Sai-Suraj-27",
    "message": "Removed some duplicated code (#35637)\n\n* Removed duplicate class field definition.\r\n\r\n* Removed duplicate code in try-except block.\r\n\r\n---------\r\n\r\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "91f14f1fc41a2d2bb10bfc76535bdffa32850efc",
    "files": [
        {
            "sha": "0d7eca8aa1dc068394d9f401041ee44ca578d69c",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/91f14f1fc41a2d2bb10bfc76535bdffa32850efc/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91f14f1fc41a2d2bb10bfc76535bdffa32850efc/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=91f14f1fc41a2d2bb10bfc76535bdffa32850efc",
            "patch": "@@ -196,7 +196,6 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n-    _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "b9b24d681b2417f8fa574de589ba92fb5ca7ad04",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/91f14f1fc41a2d2bb10bfc76535bdffa32850efc/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91f14f1fc41a2d2bb10bfc76535bdffa32850efc/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=91f14f1fc41a2d2bb10bfc76535bdffa32850efc",
            "patch": "@@ -2292,13 +2292,6 @@ def _from_pretrained(\n                 \"Unable to load vocabulary from file. \"\n                 \"Please check that the provided vocabulary is accessible and not corrupted.\"\n             )\n-        except RuntimeError as e:\n-            if \"sentencepiece_processor.cc\" in str(e):\n-                logger.info(\n-                    \"Unable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\"\n-                    \"(SentencePiece RuntimeError: Tried to load SPM model with non-SPM vocab file).\",\n-                )\n-                return False\n \n         if added_tokens_decoder != {} and max(list(added_tokens_decoder.keys())[-1], 0) > tokenizer.vocab_size:\n             logger.info("
        }
    ],
    "stats": {
        "total": 8,
        "additions": 0,
        "deletions": 8
    }
}