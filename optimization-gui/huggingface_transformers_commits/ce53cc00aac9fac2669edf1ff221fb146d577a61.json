{
    "author": "Rocketknight1",
    "message": "[V5] Return a BatchEncoding dict from apply_chat_template by default again (#42567)\n\n* Flip the default return type for `apply_chat_template` to match the underlying tokenizer\n\n* Remove test_tokenization_for_chat tests, which no longer do anything useful\n\n* Remove test_tokenization_for_chat tests, which no longer do anything useful\n\n* Fix test_encode_message tests\n\n* Fix test_encode_message tests\n\n* nit fix\n\n* Trigger tests\n\n* Remove test_tokenization_for_chat\n\n* make fixup\n\n* Add a little test to make sure that doesn't happen again\n\n* make fixup",
    "sha": "ce53cc00aac9fac2669edf1ff221fb146d577a61",
    "files": [
        {
            "sha": "a0f072de767823cd39eb3358a930ab8373b9c9a6",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -3195,7 +3195,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         return_assistant_tokens_mask: bool = False,\n         tokenizer_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n@@ -3268,14 +3268,11 @@ def apply_chat_template(\n             set, will return a dict of tokenizer outputs instead.\n         \"\"\"\n \n-        if return_dict and not tokenize:\n-            raise ValueError(\n-                \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n-                \"of tokenizer outputs to return.\"\n-            )\n+        if not tokenize:\n+            return_dict = False  # dicts are only returned by the tokenizer anyway\n \n-        if return_assistant_tokens_mask and not return_dict:\n-            raise ValueError(\"`return_assistant_tokens_mask=True` is incompatible with `return_dict=False`\")\n+        if return_assistant_tokens_mask and not (return_dict and tokenize):\n+            raise ValueError(\"`return_assistant_tokens_mask=True` requires `return_dict=True` and `tokenize=True`\")\n \n         if tokenizer_kwargs is None:\n             tokenizer_kwargs = {}\n@@ -3390,13 +3387,17 @@ def encode_message_with_chat_template(\n             )\n \n         if conversation_history is None or len(conversation_history) == 0:\n-            return self.apply_chat_template([message], add_generation_prompt=False, tokenize=True, **kwargs)\n+            return self.apply_chat_template(\n+                [message], add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+            )\n \n         conversation = conversation_history + [message]\n-        tokens = self.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, **kwargs)\n+        tokens = self.apply_chat_template(\n+            conversation, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+        )\n \n         prefix_tokens = self.apply_chat_template(\n-            conversation_history, add_generation_prompt=False, tokenize=True, **kwargs\n+            conversation_history, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n         )\n         # It's possible that the prefix tokens are not a prefix of the full list of tokens.\n         # For example, if the prefix is `<s>User: Hi` and the full conversation is `<s>User: Hi</s><s>Assistant: Hello`."
        },
        {
            "sha": "37fece0709494ff23cd9eb4ce895c4083305778f",
            "filename": "tests/models/blenderbot/test_tokenization_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fblenderbot%2Ftest_tokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fblenderbot%2Ftest_tokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_tokenization_blenderbot.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -21,23 +21,3 @@ def test_pretokenized_inputs(self, *args, **kwargs):\n         # The issue is that when you have a sequence with leading spaces, splitting it\n         # with .split() loses the leading spaces, so the tokenization results differ\n         pass\n-\n-    def test_tokenization_for_chat(self):\n-        tok = self.get_tokenizer()\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tok.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 2],\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 228, 3490, 287, 2273, 304, 21, 2],\n-            [3490, 287, 2273, 304, 21, 228, 228, 6950, 8, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
        },
        {
            "sha": "6d0ea31f3f8af7870181795908dfa6c561a7728c",
            "filename": "tests/models/bloom/test_tokenization_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -17,7 +17,7 @@\n from datasets import load_dataset\n \n from transformers import TokenizersBackend\n-from transformers.testing_utils import require_jinja, require_tokenizers, slow\n+from transformers.testing_utils import require_tokenizers, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -129,28 +129,6 @@ def test_encodings_from_xnli_dataset(self):\n         predicted_text = [tokenizer.decode(x, clean_up_tokenization_spaces=False) for x in output_tokens]\n         self.assertListEqual(predicted_text, input_text)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_tokenizer()\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2],\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2, 229126, 427, 11890, 1152, 17, 2],\n-            [229126, 427, 11890, 1152, 17, 2, 59414, 4, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_add_prefix_space_fast(self):\n         tokenizer_w_prefix = self.get_tokenizer(add_prefix_space=True)\n         tokenizer_wo_prefix = self.get_tokenizer(add_prefix_space=False)"
        },
        {
            "sha": "75de9835fa019b676b8cccb64e6d5a4f7163a053",
            "filename": "tests/models/cohere/test_tokenization_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -73,32 +73,6 @@ def test_pretrained_model_lists(self):\n         self.assertGreaterEqual(len(self.tokenizer_class.pretrained_vocab_files_map), 1)\n         self.assertGreaterEqual(len(list(self.tokenizer_class.pretrained_vocab_files_map.values())[0]), 1)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_tokenizer()\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65, 59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59, 45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8],\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65,\n-            59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8,\n-            36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59,\n-            45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61,\n-            58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 43, 48, 41, 60, 42, 55, 60, 71, 60, 55, 51, 45, 54, 99, 38,\n-            54, 567, 235, 693, 276, 411, 243, 22, 8]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_jinja\n     def test_tokenization_for_tool_use(self):\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "8e409064320c55101cd097fae20479e811bb1a90",
            "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -16,7 +16,7 @@\n import unittest\n \n from transformers import AutoTokenizer, GPT2Tokenizer\n-from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n+from transformers.testing_utils import require_tiktoken, require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -67,26 +67,6 @@ def test_special_tokens_mask_input_pairs_and_bos_token(self):\n                 filtered_sequence = [x for x in filtered_sequence if x is not None]\n                 self.assertEqual(encoded_sequence, filtered_sequence)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [[1639, 389, 257, 7613, 8537, 13645, 13, 50256, 15496, 0, 50256], [1639, 389, 257, 7613, 8537, 13645, 13, 50256, 15496, 0, 50256, 35284, 284, 1826, 345, 13, 50256], [35284, 284, 1826, 345, 13, 50256, 15496, 0, 50256]]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_tiktoken\n     def test_tokenization_tiktoken(self):\n         from tiktoken import encoding_name_for_model"
        },
        {
            "sha": "7dbcd524e810ffff80e8ce66217d9b56618872c8",
            "filename": "tests/models/gpt_sw3/test_tokenization_gpt_sw3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -15,7 +15,7 @@\n import unittest\n \n from transformers import GPTSw3Tokenizer\n-from transformers.testing_utils import get_tests_dir, require_jinja, require_sentencepiece, require_tokenizers, slow\n+from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -129,36 +129,3 @@ def test_tokenizer_integration(self):\n             model_name=\"AI-Sweden-Models/gpt-sw3-126m\",\n             sequences=sequences,\n         )\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPTSw3Tokenizer(SAMPLE_VOCAB, name_or_path=\"test\")\n-        tokenizer.chat_template = (\n-            \"{{ eos_token }}{{ bos_token }}\"\n-            \"{% for message in messages %}\"\n-            \"{% if message['role'] == 'user' %}{{ 'User: ' + message['content']}}\"\n-            \"{% else %}{{ 'Bot: ' + message['content']}}{% endif %}\"\n-            \"{{ message['text'] }}{{ bos_token }}\"\n-            \"{% endfor %}\"\n-            \"Bot:\"\n-        )\n-        # This is in English, but it's just here to make sure the chat control tokens are being added properly\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419]\n-            ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
        },
        {
            "sha": "673f8def31596be29b250db0f1a4e778a394a387",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce53cc00aac9fac2669edf1ff221fb146d577a61/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=ce53cc00aac9fac2669edf1ff221fb146d577a61",
            "patch": "@@ -950,7 +950,9 @@ def test_chat_template(self):\n             dummy_conversation, chat_template=dummy_template, tokenize=True, return_dict=False\n         )\n         dict_output = tokenizer.apply_chat_template(\n-            dummy_conversation, chat_template=dummy_template, tokenize=True, return_dict=True\n+            dummy_conversation,\n+            chat_template=dummy_template,\n+            tokenize=True,  # This also checks return_dict=True is the default\n         )\n         self.assertEqual(dict_output[\"input_ids\"], output)  # Test return_dict behaviour matches\n "
        }
    ],
    "stats": {
        "total": 154,
        "additions": 18,
        "deletions": 136
    }
}