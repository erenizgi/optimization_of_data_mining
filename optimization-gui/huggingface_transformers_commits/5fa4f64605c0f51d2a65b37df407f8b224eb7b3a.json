{
    "author": "philkuz",
    "message": "ðŸš¨ðŸš¨ðŸš¨ fix(Mask2Former): torch export ðŸš¨ðŸš¨ðŸš¨ (#34393)\n\n* fix(Mask2Former): torch export\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* revert level_start_index and create a level_start_index_list\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* Add a comment to explain the level_start_index_list\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* Address comment\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* add torch.export.export test\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* rename arg\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* remove spatial_shapes\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* Use the version check from pytorch_utils\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n* [run_slow] mask2former\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>\r\n\r\n---------\r\n\r\nSigned-off-by: Phillip Kuznetsov <philkuz@gimletlabs.ai>",
    "sha": "5fa4f64605c0f51d2a65b37df407f8b224eb7b3a",
    "files": [
        {
            "sha": "4cc96b1652dbf9f2c92251a4d88ab7ff4bf7e910",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 37,
            "deletions": 25,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fa4f64605c0f51d2a65b37df407f8b224eb7b3a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fa4f64605c0f51d2a65b37df407f8b224eb7b3a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=5fa4f64605c0f51d2a65b37df407f8b224eb7b3a",
            "patch": "@@ -926,7 +926,7 @@ def forward(\n         encoder_attention_mask=None,\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n-        spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -936,7 +936,8 @@ def forward(\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n-        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+        total_elements = sum(height * width for height, width in spatial_shapes_list)\n+        if total_elements != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n             )\n@@ -957,7 +958,11 @@ def forward(\n         )\n         # batch_size, num_queries, n_heads, n_levels, n_points, 2\n         if reference_points.shape[-1] == 2:\n-            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)\n+            offset_normalizer = torch.tensor(\n+                [[shape[1], shape[0]] for shape in spatial_shapes_list],\n+                dtype=torch.long,\n+                device=reference_points.device,\n+            )\n             sampling_locations = (\n                 reference_points[:, :, None, :, None, :]\n                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n@@ -970,7 +975,7 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+        output = multi_scale_deformable_attention(value, spatial_shapes_list, sampling_locations, attention_weights)\n         output = self.output_proj(output)\n \n         return output, attention_weights\n@@ -1001,7 +1006,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_embeddings: torch.Tensor = None,\n         reference_points=None,\n-        spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -1015,8 +1020,8 @@ def forward(\n                 Position embeddings, to be added to `hidden_states`.\n             reference_points (`torch.FloatTensor`, *optional*):\n                 Reference points.\n-            spatial_shapes (`torch.LongTensor`, *optional*):\n-                Spatial shapes of the backbone feature maps.\n+            spatial_shapes_list (`list` of `tuple`):\n+                Spatial shapes of the backbone feature maps as a list of tuples.\n             level_start_index (`torch.LongTensor`, *optional*):\n                 Level start index.\n             output_attentions (`bool`, *optional*):\n@@ -1033,7 +1038,7 @@ def forward(\n             encoder_attention_mask=attention_mask,\n             position_embeddings=position_embeddings,\n             reference_points=reference_points,\n-            spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n         )\n@@ -1086,13 +1091,13 @@ def __init__(self, config: Mask2FormerConfig):\n         )\n \n     @staticmethod\n-    def get_reference_points(spatial_shapes, valid_ratios, device):\n+    def get_reference_points(spatial_shapes_list, valid_ratios, device):\n         \"\"\"\n         Get reference points for each feature map. Used in decoder.\n \n         Args:\n-            spatial_shapes (`torch.LongTensor`):\n-                Spatial shapes of each feature map, has shape of `(num_feature_levels, 2)`.\n+            spatial_shapes_list (`list` of `tuple`):\n+                Spatial shapes of the backbone feature maps as a list of tuples.\n             valid_ratios (`torch.FloatTensor`):\n                 Valid ratios of each feature map, has shape of `(batch_size, num_feature_levels, 2)`.\n             device (`torch.device`):\n@@ -1101,7 +1106,7 @@ def get_reference_points(spatial_shapes, valid_ratios, device):\n             `torch.FloatTensor` of shape `(batch_size, num_queries, num_feature_levels, 2)`\n         \"\"\"\n         reference_points_list = []\n-        for lvl, (height, width) in enumerate(spatial_shapes):\n+        for lvl, (height, width) in enumerate(spatial_shapes_list):\n             ref_y, ref_x = torch.meshgrid(\n                 torch.linspace(0.5, height - 0.5, height, dtype=valid_ratios.dtype, device=device),\n                 torch.linspace(0.5, width - 0.5, width, dtype=valid_ratios.dtype, device=device),\n@@ -1122,7 +1127,7 @@ def forward(\n         inputs_embeds=None,\n         attention_mask=None,\n         position_embeddings=None,\n-        spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         valid_ratios=None,\n         output_attentions=None,\n@@ -1140,8 +1145,8 @@ def forward(\n                 [What are attention masks?](../glossary#attention-mask)\n             position_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                 Position embeddings that are added to the queries and keys in each self-attention layer.\n-            spatial_shapes (`torch.LongTensor` of shape `(num_feature_levels, 2)`):\n-                Spatial shapes of each feature map.\n+            spatial_shapes_list (`list` of `tuple`):\n+                Spatial shapes of each feature map as a list of tuples.\n             level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`):\n                 Starting index of each feature map.\n             valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`):\n@@ -1162,7 +1167,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         hidden_states = inputs_embeds\n-        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=inputs_embeds.device)\n+        reference_points = self.get_reference_points(spatial_shapes_list, valid_ratios, device=inputs_embeds.device)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -1176,7 +1181,7 @@ def forward(\n                 attention_mask,\n                 position_embeddings=position_embeddings,\n                 reference_points=reference_points,\n-                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n                 level_start_index=level_start_index,\n                 output_attentions=output_attentions,\n             )\n@@ -1302,9 +1307,9 @@ def forward(\n         ]\n \n         # Prepare encoder inputs (by flattening)\n-        spatial_shapes = [(embed.shape[2], embed.shape[3]) for embed in input_embeds]\n+        spatial_shapes_list = [(embed.shape[2], embed.shape[3]) for embed in input_embeds]\n         input_embeds_flat = torch.cat([embed.flatten(2).transpose(1, 2) for embed in input_embeds], 1)\n-        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=input_embeds_flat.device)\n+        spatial_shapes = torch.as_tensor(spatial_shapes_list, dtype=torch.long, device=input_embeds_flat.device)\n         masks_flat = torch.cat([mask.flatten(1) for mask in masks], 1)\n \n         position_embeddings = [embed.flatten(2).transpose(1, 2) for embed in position_embeddings]\n@@ -1320,7 +1325,7 @@ def forward(\n                 inputs_embeds=input_embeds_flat,\n                 attention_mask=masks_flat,\n                 position_embeddings=level_pos_embed_flat,\n-                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n                 level_start_index=level_start_index,\n                 valid_ratios=valid_ratios,\n                 output_attentions=output_attentions,\n@@ -1331,18 +1336,23 @@ def forward(\n         last_hidden_state = encoder_outputs.last_hidden_state\n         batch_size = last_hidden_state.shape[0]\n \n+        # We compute level_start_index_list separately from the tensor version level_start_index\n+        # to avoid iterating over a tensor which breaks torch.compile/export.\n+        level_start_index_list = [0]\n+        for height, width in spatial_shapes_list[:-1]:\n+            level_start_index_list.append(level_start_index_list[-1] + height * width)\n         split_sizes = [None] * self.num_feature_levels\n         for i in range(self.num_feature_levels):\n             if i < self.num_feature_levels - 1:\n-                split_sizes[i] = level_start_index[i + 1] - level_start_index[i]\n+                split_sizes[i] = level_start_index_list[i + 1] - level_start_index_list[i]\n             else:\n-                split_sizes[i] = last_hidden_state.shape[1] - level_start_index[i]\n+                split_sizes[i] = last_hidden_state.shape[1] - level_start_index_list[i]\n \n-        encoder_output = torch.split(last_hidden_state, [size.item() for size in split_sizes], dim=1)\n+        encoder_output = torch.split(last_hidden_state, split_sizes, dim=1)\n \n         # Compute final features\n         outputs = [\n-            x.transpose(1, 2).view(batch_size, -1, spatial_shapes[i][0], spatial_shapes[i][1])\n+            x.transpose(1, 2).view(batch_size, -1, spatial_shapes_list[i][0], spatial_shapes_list[i][1])\n             for i, x in enumerate(encoder_output)\n         ]\n \n@@ -1876,7 +1886,9 @@ def forward(\n             else:\n                 level_index = idx % self.num_feature_levels\n \n-                attention_mask[torch.where(attention_mask.sum(-1) == attention_mask.shape[-1])] = False\n+                where = (attention_mask.sum(-1) != attention_mask.shape[-1]).to(attention_mask.dtype)\n+                # Multiply the attention mask instead of indexing to avoid issue in torch.export.\n+                attention_mask = attention_mask * where.unsqueeze(-1)\n \n                 layer_outputs = decoder_layer(\n                     hidden_states,"
        },
        {
            "sha": "a3caefe14ab50123b75fab2ed1e4db87fd9bb9f7",
            "filename": "tests/models/mask2former/test_modeling_mask2former.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fa4f64605c0f51d2a65b37df407f8b224eb7b3a/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fa4f64605c0f51d2a65b37df407f8b224eb7b3a/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py?ref=5fa4f64605c0f51d2a65b37df407f8b224eb7b3a",
            "patch": "@@ -20,6 +20,7 @@\n \n from tests.test_modeling_common import floats_tensor\n from transformers import Mask2FormerConfig, is_torch_available, is_vision_available\n+from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n from transformers.testing_utils import (\n     require_timm,\n     require_torch,\n@@ -481,3 +482,28 @@ def test_with_segmentation_maps_and_loss(self):\n             outputs = model(**inputs)\n \n         self.assertTrue(outputs.loss is not None)\n+\n+    def test_export(self):\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+        model = Mask2FormerForUniversalSegmentation.from_pretrained(self.model_checkpoints).to(torch_device).eval()\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(image, return_tensors=\"pt\").to(torch_device)\n+\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"pixel_values\"], inputs[\"pixel_mask\"]),\n+            strict=True,\n+        )\n+        with torch.no_grad():\n+            eager_outputs = model(**inputs)\n+            exported_outputs = exported_program.module().forward(inputs[\"pixel_values\"], inputs[\"pixel_mask\"])\n+        self.assertEqual(eager_outputs.masks_queries_logits.shape, exported_outputs.masks_queries_logits.shape)\n+        self.assertTrue(\n+            torch.allclose(eager_outputs.masks_queries_logits, exported_outputs.masks_queries_logits, atol=TOLERANCE)\n+        )\n+        self.assertEqual(eager_outputs.class_queries_logits.shape, exported_outputs.class_queries_logits.shape)\n+        self.assertTrue(\n+            torch.allclose(eager_outputs.class_queries_logits, exported_outputs.class_queries_logits, atol=TOLERANCE)\n+        )"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 63,
        "deletions": 25
    }
}