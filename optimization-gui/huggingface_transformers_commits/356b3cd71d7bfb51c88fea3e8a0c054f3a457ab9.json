{
    "author": "qubvel",
    "message": "Fix missing return type for MLCD docs (#37527)\n\n* Fix missing return type for docs\n\n* trigger",
    "sha": "356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9",
    "files": [
        {
            "sha": "574537a7ad78c0ccc48f845fa0cfe16a8509a304",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9",
            "patch": "@@ -33,6 +33,7 @@\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n     logging,\n+    replace_return_docstrings,\n     torch_int,\n )\n from .configuration_mlcd import MLCDVisionConfig\n@@ -631,6 +632,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @add_start_docstrings_to_model_forward(MLCD_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=MLCDVisionConfig)\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "60ff12d827eb2fbc53bcc0903c125b2704d002c9",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=356b3cd71d7bfb51c88fea3e8a0c054f3a457ab9",
            "patch": "@@ -32,6 +32,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     logging,\n+    replace_return_docstrings,\n )\n from ..clip.modeling_clip import (\n     CLIPMLP,\n@@ -548,6 +549,7 @@ def _init_weights(self, module):\n )\n class MLCDVisionModel(CLIPVisionModel):\n     @add_start_docstrings_to_model_forward(MLCD_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=MLCDVisionConfig)\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}