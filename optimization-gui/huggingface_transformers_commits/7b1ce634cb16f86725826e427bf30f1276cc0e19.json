{
    "author": "yonigozlan",
    "message": "Improve compiled RT-DETR inference speed  (#33412)\n\n* modify rt detr to improve inference times when compiled\r\n\r\n* Remove redundant \"to\"\r\n\r\n* Fix conditional lru_cache and missing shapes_list\r\n\r\n* nit unnecessary list creation\r\n\r\n* Fix compile error when ninja not available and custon kernel activated",
    "sha": "7b1ce634cb16f86725826e427bf30f1276cc0e19",
    "files": [
        {
            "sha": "4e32434901cdc7e4e32a1fb5cac94125e643c264",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 64,
            "deletions": 22,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b1ce634cb16f86725826e427bf30f1276cc0e19/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b1ce634cb16f86725826e427bf30f1276cc0e19/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=7b1ce634cb16f86725826e427bf30f1276cc0e19",
            "patch": "@@ -18,7 +18,7 @@\n import os\n import warnings\n from dataclasses import dataclass\n-from functools import lru_cache, partial\n+from functools import lru_cache, partial, wraps\n from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n@@ -737,7 +737,9 @@ def multi_scale_deformable_attention(\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)\n+    # Ignore copy\n+    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+\n     sampling_grids = 2 * sampling_locations - 1\n     sampling_value_list = []\n     for level_id, (height, width) in enumerate(value_spatial_shapes):\n@@ -849,6 +851,7 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -858,7 +861,10 @@ def forward(\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n-        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+\n+        # Ignore copy\n+        total_elements = sum(shape[0] * shape[1] for shape in spatial_shapes_list)\n+        if total_elements != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n             )\n@@ -893,9 +899,12 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n+        # Ignore copy\n+        if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n             # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+            output = multi_scale_deformable_attention(\n+                value, spatial_shapes_list, sampling_locations, attention_weights\n+            )\n         else:\n             try:\n                 # custom kernel\n@@ -909,7 +918,9 @@ def forward(\n                 )\n             except Exception:\n                 # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+                output = multi_scale_deformable_attention(\n+                    value, spatial_shapes_list, sampling_locations, attention_weights\n+                )\n         output = self.output_proj(output)\n \n         return output, attention_weights\n@@ -1064,6 +1075,7 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1114,6 +1126,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n         )\n@@ -1299,14 +1312,16 @@ def __init__(self, config: RTDetrConfig):\n             self.pan_blocks.append(RTDetrCSPRepLayer(config))\n \n     @staticmethod\n-    def build_2d_sincos_position_embedding(width, height, embed_dim=256, temperature=10000.0):\n-        grid_w = torch.arange(int(width), dtype=torch.float32)\n-        grid_h = torch.arange(int(height), dtype=torch.float32)\n+    def build_2d_sincos_position_embedding(\n+        width, height, embed_dim=256, temperature=10000.0, device=\"cpu\", dtype=torch.float32\n+    ):\n+        grid_w = torch.arange(int(width), dtype=dtype, device=device)\n+        grid_h = torch.arange(int(height), dtype=dtype, device=device)\n         grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing=\"ij\")\n         if embed_dim % 4 != 0:\n             raise ValueError(\"Embed dimension must be divisible by 4 for 2D sin-cos position embedding\")\n         pos_dim = embed_dim // 4\n-        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n+        omega = torch.arange(pos_dim, dtype=dtype, device=device) / pos_dim\n         omega = 1.0 / (temperature**omega)\n \n         out_w = grid_w.flatten()[..., None] @ omega[None]\n@@ -1372,8 +1387,13 @@ def forward(\n                 src_flatten = hidden_states[enc_ind].flatten(2).permute(0, 2, 1)\n                 if self.training or self.eval_size is None:\n                     pos_embed = self.build_2d_sincos_position_embedding(\n-                        width, height, self.encoder_hidden_dim, self.positional_encoding_temperature\n-                    ).to(src_flatten.device, src_flatten.dtype)\n+                        width,\n+                        height,\n+                        self.encoder_hidden_dim,\n+                        self.positional_encoding_temperature,\n+                        device=src_flatten.device,\n+                        dtype=src_flatten.dtype,\n+                    )\n                 else:\n                     pos_embed = None\n \n@@ -1441,6 +1461,7 @@ def forward(\n         position_embeddings=None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         valid_ratios=None,\n         output_attentions=None,\n@@ -1512,6 +1533,7 @@ def forward(\n                 encoder_hidden_states=encoder_hidden_states,\n                 reference_points=reference_points_input,\n                 spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n                 level_start_index=level_start_index,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n@@ -1575,6 +1597,27 @@ def forward(\n         )\n \n \n+def compile_compatible_lru_cache(*lru_args, **lru_kwargs):\n+    def decorator(func):\n+        @wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if not torch.compiler.is_compiling():\n+                # Cache the function only if the model is not being compiled\n+                # check if the function is already cached, otherwise create it\n+                if not hasattr(self, f\"_cached_{func.__name__}\"):\n+                    self.__setattr__(\n+                        f\"_cached_{func.__name__}\", lru_cache(*lru_args, **lru_kwargs)(func.__get__(self))\n+                    )\n+                return self.__getattribute__(f\"_cached_{func.__name__}\")(*args, **kwargs)\n+            else:\n+                # Otherwise, just call the original function\n+                return func(self, *args, **kwargs)\n+\n+        return wrapper\n+\n+    return decorator\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting raw hidden states without any head on top.\n@@ -1626,7 +1669,7 @@ def __init__(self, config: RTDetrConfig):\n \n         # init encoder output anchors and valid_mask\n         if config.anchor_image_size:\n-            self.anchors, self.valid_mask = self.generate_anchors()\n+            self.anchors, self.valid_mask = self.generate_anchors(dtype=self.dtype)\n \n         # Create decoder input projection layers\n         # https://github.com/lyuwenyu/RT-DETR/blob/94f5e16708329d2f2716426868ec89aa774af016/rtdetr_pytorch/src/zoo/rtdetr/rtdetr_decoder.py#L412\n@@ -1669,12 +1712,8 @@ def unfreeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(True)\n \n-    @lru_cache(maxsize=32)\n-    def generate_anchors(self, spatial_shapes=None, grid_size=0.05):\n-        # We always generate anchors in float32 to preserve equivalence between\n-        # dynamic and static anchor inference\n-        dtype = torch.float32\n-\n+    @compile_compatible_lru_cache(maxsize=32)\n+    def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dtype=torch.float32):\n         if spatial_shapes is None:\n             spatial_shapes = [\n                 [int(self.config.anchor_image_size[0] / s), int(self.config.anchor_image_size[1] / s)]\n@@ -1683,10 +1722,12 @@ def generate_anchors(self, spatial_shapes=None, grid_size=0.05):\n         anchors = []\n         for level, (height, width) in enumerate(spatial_shapes):\n             grid_y, grid_x = torch.meshgrid(\n-                torch.arange(end=height, dtype=dtype), torch.arange(end=width, dtype=dtype), indexing=\"ij\"\n+                torch.arange(end=height, dtype=dtype, device=device),\n+                torch.arange(end=width, dtype=dtype, device=device),\n+                indexing=\"ij\",\n             )\n             grid_xy = torch.stack([grid_x, grid_y], -1)\n-            valid_wh = torch.tensor([width, height]).to(dtype)\n+            valid_wh = torch.tensor([width, height], device=device).to(dtype)\n             grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_wh\n             wh = torch.ones_like(grid_xy) * grid_size * (2.0**level)\n             anchors.append(torch.concat([grid_xy, wh], -1).reshape(-1, height * width, 4))\n@@ -1826,7 +1867,7 @@ def forward(\n             # Pass spatial_shapes as tuple to make it hashable and make sure\n             # lru_cache is working for generate_anchors()\n             spatial_shapes_tuple = tuple(spatial_shapes_list)\n-            anchors, valid_mask = self.generate_anchors(spatial_shapes_tuple)\n+            anchors, valid_mask = self.generate_anchors(spatial_shapes_tuple, device=device, dtype=dtype)\n         else:\n             anchors, valid_mask = self.anchors, self.valid_mask\n \n@@ -1873,6 +1914,7 @@ def forward(\n             encoder_attention_mask=attention_mask,\n             reference_points=init_reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 64,
        "deletions": 22
    }
}