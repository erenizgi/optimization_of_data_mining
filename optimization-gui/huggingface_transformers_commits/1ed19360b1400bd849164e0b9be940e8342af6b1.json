{
    "author": "vasqu",
    "message": "[`FlexAttention`] Reenable flex for encoder-decoder and make the test more robust (#38321)\n\n* reenable most flex attention test cases\n\n* style\n\n* trigger\n\n* trigger",
    "sha": "1ed19360b1400bd849164e0b9be940e8342af6b1",
    "files": [
        {
            "sha": "2442baa2436339487a263fae23b81b6a46647bb1",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -494,8 +494,7 @@ class BartPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "f12eeac69730295343306f2e1a6d2325838bb664",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -348,8 +348,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "78d6da134b80addf63d268f55587ef5137ef9d13",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -175,8 +175,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "4c001a354463f6848bb116b57a4ff80dd1ab3002",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -464,8 +464,7 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "49cff8f620ef457dcac760f707d554ac9a615c31",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -452,8 +452,7 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "eafcbff89ae5fa3fc864bcdce1a110ed2966937e",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -551,8 +551,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "0b4695c1e28ca8ce510f79df1a748a6889d38f07",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -140,8 +140,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "115345407e699cafd5c9f953f092ce7a615a8c41",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -738,8 +738,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "c0454452f029ee2b5a4be774946f2b0dd627ea13",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -131,8 +131,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "f3488672721a2276de52dec7d83127f48fd4cad8",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -530,8 +530,7 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"M2M100EncoderLayer\", \"M2M100DecoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n     _supports_static_cache = False"
        },
        {
            "sha": "a604820f2cce2782f93cb6b53c4068b074f3e12f",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -468,8 +468,7 @@ class MarianPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "4f3253eeb442a9c0832c8bf41b95f9dfaea1ced5",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -498,8 +498,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MBartDecoderLayer\", \"MBartEncoderLayer\", \"MBartAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "42b05671333c75bd42e6d6daae842ee8265e5bee",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -436,8 +436,7 @@ class MusicgenPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MusicgenDecoderLayer\", \"MusicgenAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # compilation errors occurr atm\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n@@ -1361,8 +1360,7 @@ class MusicgenForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # compilation errors occurr atm\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4e1ea39e754e68251943208afcbf80d084a70869",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -410,8 +410,7 @@ class MusicgenMelodyPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MusicgenMelodyDecoderLayer\", \"MusicgenMelodyAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # compilation errors occurr atm\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n@@ -1292,8 +1291,7 @@ class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # compilation errors occurr atm\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "303ae89fd02b84d96b3552f8a64baf0fcdc84827",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -463,8 +463,7 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "bf94379ccae79ed1765bc2e310250303942ec0fa",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -764,8 +764,7 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     # Flaky logits\n     _supports_sdpa = False\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n "
        },
        {
            "sha": "695a0ed458f70fb34054aaf4d7615973ac9081a8",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -78,8 +78,7 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "1394e87f56078bf102ba036f61d9980fab36c09b",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -64,8 +64,7 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "aa4ea810711092164a4c4a0973186ca182007d7d",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -529,7 +529,6 @@ class Speech2TextPreTrainedModel(PreTrainedModel):\n     # Current tests always assume certain inputs to be passed\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n-    # Compile issues\n     _supports_flex_attn = False\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "dc960efbbcf34047390640c464ce8b55b8526a9c",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -636,7 +636,6 @@ class TimeSeriesTransformerPreTrainedModel(PreTrainedModel):\n     # Current tests always assume certain inputs to be passed\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n-    # Compile issues\n     _supports_flex_attn = False\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "4fdce328e9e64081383c2ea1688272b7789864f7",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -847,8 +847,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "5a9133089aeb1155743d1f6b6d7bf4d932dd2346",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -151,8 +151,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "50ee4c198d25b1d8d966c80b5cc42bbbf6b15528",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -850,8 +850,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "f86c397a047cb64c95b3eff1f06deed0d6023406",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -161,8 +161,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "ae3510f175e08c979998257960b904c946934409",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -1096,8 +1096,7 @@ class Wav2Vec2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    # Compile issues\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "621ab67da03b7a72bd4ca523b01597b43e3ef0ae",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ed19360b1400bd849164e0b9be940e8342af6b1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ed19360b1400bd849164e0b9be940e8342af6b1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=1ed19360b1400bd849164e0b9be940e8342af6b1",
            "patch": "@@ -4570,20 +4570,29 @@ def test_flex_attention_with_grads(self):\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             config._attn_implementation = \"flex_attention\"\n-            # Flex Attention can not use dropout\n+            # Flex Attention cannot use dropout\n             if hasattr(config, \"attention_dropout\"):\n                 config.attention_dropout = 0\n             if hasattr(config, \"attention_probs_dropout_prob\"):\n                 config.attention_probs_dropout_prob = 0\n \n+            # Flex attention relies on triton on compilation\n+            # However, triton cannot handle hidden dimensions of less than 16\n+            # --> forcing at least a hidden dim of 16\n+            config.hidden_size *= max(\n+                16 // getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads), 1\n+            )\n+            if hasattr(config, \"head_dim\"):\n+                config.head_dim = max(16, config.head_dim)\n+\n             model = model_class(config).to(device=torch_device)\n             self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n \n             # Elaborate workaround for encoder-decoder models as some do not specify their main input\n             dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n             if config.is_encoder_decoder:\n-                dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"]\n-                dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"]\n+                dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"].to(torch_device)\n+                dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"].to(torch_device)\n \n             # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n             _ = model(**dummy_inputs)"
        }
    ],
    "stats": {
        "total": 92,
        "additions": 37,
        "deletions": 55
    }
}