{
    "author": "DerekLiu35",
    "message": "Fix some failing AWQ tests (#37383)\n\n* update AwqQuantizer\n\n* fix style\n\n* add an arg to get_modules_to_not_convert to add get_keys_to_not_convert(model)",
    "sha": "c5c648dd74b711992a6af114c3b1524aece99b97",
    "files": [
        {
            "sha": "317bce0759294d8515ebdadac34c6148bc374f72",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c5c648dd74b711992a6af114c3b1524aece99b97/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c5c648dd74b711992a6af114c3b1524aece99b97/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=c5c648dd74b711992a6af114c3b1524aece99b97",
            "patch": "@@ -263,14 +263,17 @@ def get_modules_to_not_convert(\n         model: \"PreTrainedModel\",\n         skip_modules: Optional[List[str]] = None,\n         keep_in_fp32_modules: Optional[List[str]] = None,\n+        add_default_skips: bool = False,\n     ):\n         from ..integrations import get_keys_to_not_convert\n \n-        modules_to_not_convert = []\n-        if skip_modules is None:\n+        if skip_modules is None or add_default_skips:\n             modules_to_not_convert = get_keys_to_not_convert(model)\n         else:\n-            modules_to_not_convert = skip_modules\n+            modules_to_not_convert = []\n+\n+        if skip_modules is not None:\n+            modules_to_not_convert.extend(skip_modules)\n \n         if keep_in_fp32_modules is not None:\n             modules_to_not_convert.extend(keep_in_fp32_modules)"
        },
        {
            "sha": "8e63e2f5bf68d697022fecba3dfc875c190c3a17",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c5c648dd74b711992a6af114c3b1524aece99b97/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c5c648dd74b711992a6af114c3b1524aece99b97/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=c5c648dd74b711992a6af114c3b1524aece99b97",
            "patch": "@@ -92,6 +92,9 @@ def update_torch_dtype(self, torch_dtype):\n         if torch_dtype is None:\n             torch_dtype = torch.float16\n             logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `torch_dtype` manually.\")\n+        elif torch_dtype == torch.bfloat16:\n+            logger.warning(\"`torch.bfloat16` is not supported for AWQ kernels yet. Casting to `torch.float16`.\")\n+            torch_dtype = torch.float16\n         elif torch_dtype != torch.float16:\n             logger.warning(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\")\n         return torch_dtype\n@@ -102,7 +105,7 @@ def _process_model_before_weight_loading(\n         from ..integrations import replace_quantization_scales, replace_with_awq_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules, add_default_skips=True\n         )\n \n         model, has_been_replaced = replace_with_awq_linear("
        }
    ],
    "stats": {
        "total": 14,
        "additions": 10,
        "deletions": 4
    }
}