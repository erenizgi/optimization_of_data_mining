{
    "author": "Rocketknight1",
    "message": "Cleanup reference to TFBertTokenizer and TFGPT2Tokenizer (#42182)\n\n* Cleanup reference to TFBertTokenizer\n\n* Remove the GPT2 TF tokenizer too",
    "sha": "d372b827547e0414809daf120cea4ba525bf601d",
    "files": [
        {
            "sha": "04aeefe38a815ab4afe2b7ec6cfb087b0375d543",
            "filename": "src/transformers/models/bert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -22,7 +22,6 @@\n     from .modeling_bert import *\n     from .tokenization_bert import *\n     from .tokenization_bert_fast import *\n-    from .tokenization_bert_tf import *\n else:\n     import sys\n "
        },
        {
            "sha": "4724af1e281eb758ba1e345450006d26f7220679",
            "filename": "src/transformers/models/gpt2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -22,7 +22,6 @@\n     from .modeling_gpt2 import *\n     from .tokenization_gpt2 import *\n     from .tokenization_gpt2_fast import *\n-    from .tokenization_gpt2_tf import *\n else:\n     import sys\n "
        },
        {
            "sha": "a3bf0fa74208fd08d3edbf091a95a2bf46940ad0",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -109,10 +109,6 @@ class DecoratedItem:\n     # Deprecated\n     \"InputExample\",\n     \"InputFeatures\",\n-    # Signature is *args/**kwargs\n-    \"TFSequenceSummary\",\n-    \"TFBertTokenizer\",\n-    \"TFGPT2Tokenizer\",\n     # Missing arguments in the docstring\n     \"ASTFeatureExtractor\",\n     \"AlbertModel\","
        }
    ],
    "stats": {
        "total": 6,
        "additions": 0,
        "deletions": 6
    }
}