{
    "author": "keetrap",
    "message": "Add Fast Chinese-CLIP Processor (#37012)\n\n* Add Fast Chinese-CLIP Processor\n\n* Update dummy_torchvision_objects.py\n\n* Fix tests",
    "sha": "4f1dbe815292e685f90708b6bca5bd85b2978fdf",
    "files": [
        {
            "sha": "f44cdbd145646d3136b711b805de40e0fa6224e1",
            "filename": "docs/source/en/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -90,6 +90,11 @@ Currently, following scales of pretrained Chinese-CLIP models are available on \n [[autodoc]] ChineseCLIPImageProcessor\n     - preprocess\n \n+## ChineseCLIPImageProcessorFast\n+\n+[[autodoc]] ChineseCLIPImageProcessorFast\n+    - preprocess\n+\n ## ChineseCLIPFeatureExtractor\n \n [[autodoc]] ChineseCLIPFeatureExtractor"
        },
        {
            "sha": "68eff8e4131594bdaf488dcb69b4c5fe1aacd0bd",
            "filename": "docs/source/ja/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -86,6 +86,11 @@ Chinese-CLIP モデルは、[OFA-Sys](https://huggingface.co/OFA-Sys) によっ\n [[autodoc]] ChineseCLIPImageProcessor\n     - preprocess\n \n+## ChineseCLIPImageProcessorFast\n+\n+[[autodoc]] ChineseCLIPImageProcessorFast\n+    - preprocess\n+\n ## ChineseCLIPFeatureExtractor\n \n [[autodoc]] ChineseCLIPFeatureExtractor"
        },
        {
            "sha": "a5cdeb314fd150d6d7539b99b6d765338f0674b4",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -64,7 +64,7 @@\n             (\"blip-2\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"bridgetower\", (\"BridgeTowerImageProcessor\",)),\n             (\"chameleon\", (\"ChameleonImageProcessor\",)),\n-            (\"chinese_clip\", (\"ChineseCLIPImageProcessor\",)),\n+            (\"chinese_clip\", (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")),\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"conditional_detr\", (\"ConditionalDetrImageProcessor\",)),"
        },
        {
            "sha": "9c4476b9080ff514fe273fde33d96b5c0edaba2b",
            "filename": "src/transformers/models/chinese_clip/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2F__init__.py?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_chinese_clip import *\n     from .feature_extraction_chinese_clip import *\n     from .image_processing_chinese_clip import *\n+    from .image_processing_chinese_clip_fast import *\n     from .modeling_chinese_clip import *\n     from .processing_chinese_clip import *\n else:"
        },
        {
            "sha": "a1cb38b8a25f726256e2c47a8b65890efd72361d",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip_fast.py",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -0,0 +1,40 @@\n+# coding=utf-8\n+# Copyright 2025 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Chinese-CLIP.\"\"\"\n+\n+from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n+from ...utils import add_start_docstrings\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast ChineseCLIP image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class ChineseCLIPImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+\n+__all__ = [\"ChineseCLIPImageProcessorFast\"]"
        },
        {
            "sha": "3523c782f3ac38cc2e9a327a1f5fd8759f6f0141",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -44,7 +44,7 @@ class ChineseCLIPProcessor(ProcessorMixin):\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"ChineseCLIPImageProcessor\"\n+    image_processor_class = (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):"
        },
        {
            "sha": "7acae860b08aa64d4ffae69989ea20780efcdfe1",
            "filename": "tests/models/chinese_clip/test_image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 33,
            "deletions": 25,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f1dbe815292e685f90708b6bca5bd85b2978fdf/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f1dbe815292e685f90708b6bca5bd85b2978fdf/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py?ref=4f1dbe815292e685f90708b6bca5bd85b2978fdf",
            "patch": "@@ -16,14 +16,17 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import ChineseCLIPImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import ChineseCLIPImageProcessorFast\n+\n \n class ChineseCLIPImageProcessingTester:\n     def __init__(\n@@ -91,6 +94,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class ChineseCLIPImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ChineseCLIPImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ChineseCLIPImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -101,24 +105,26 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 224, \"width\": 224})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 224, \"width\": 224})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n \n     @unittest.skip(\n         reason=\"ChineseCLIPImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\"\n@@ -131,6 +137,7 @@ def test_call_numpy_4_channels(self):\n @require_vision\n class ChineseCLIPImageProcessingTestFourChannels(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ChineseCLIPImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ChineseCLIPImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -142,15 +149,16 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     @unittest.skip(reason=\"ChineseCLIPImageProcessor does not support 4 channels yet\")  # FIXME Amy\n     def test_call_numpy(self):"
        }
    ],
    "stats": {
        "total": 113,
        "additions": 86,
        "deletions": 27
    }
}